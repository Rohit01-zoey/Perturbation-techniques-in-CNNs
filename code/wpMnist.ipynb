{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the functions(GENERAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_init(seed=2):\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  W1 = np.random.rand(20,784) - 0.5\n",
    "  b1 = np.random.rand(20,1) - 0.5\n",
    "  W2 = np.random.rand(10,20) - 0.5 \n",
    "  b2 = np.random.rand(10,1) - 0.5 \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other generic functions egs-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "def crossEntropy(y,y_pre):\n",
    "  loss=-np.sum(np.multiply(y, np.log(y_pre)), axis = 0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2\n",
    "  A2 = softmax(Z2)\n",
    "  \n",
    "\n",
    "  return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "\n",
    "\n",
    "  return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(Z1, A1, Z2, A2, W1, W2, X, y):\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ2 = (A2 - Y)\n",
    "  \n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T)\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1)\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis=1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "\n",
    "      dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 74.06666666666666\n",
      "Val accuracy: 74.04285714285714\n",
      "Iteration: 2\n",
      "Train accuracy: 81.43333333333334\n",
      "Val accuracy: 81.6\n",
      "Iteration: 3\n",
      "Train accuracy: 84.46825396825398\n",
      "Val accuracy: 84.62857142857143\n",
      "Iteration: 4\n",
      "Train accuracy: 86.27460317460317\n",
      "Val accuracy: 86.24285714285715\n",
      "Iteration: 5\n",
      "Train accuracy: 87.45396825396826\n",
      "Val accuracy: 87.2\n",
      "Iteration: 6\n",
      "Train accuracy: 88.32063492063492\n",
      "Val accuracy: 88.1\n",
      "Iteration: 7\n",
      "Train accuracy: 88.95079365079364\n",
      "Val accuracy: 88.7\n",
      "Iteration: 8\n",
      "Train accuracy: 89.44603174603175\n",
      "Val accuracy: 89.18571428571428\n",
      "Iteration: 9\n",
      "Train accuracy: 89.90476190476191\n",
      "Val accuracy: 89.72857142857143\n",
      "Iteration: 10\n",
      "Train accuracy: 90.27142857142857\n",
      "Val accuracy: 90.14285714285715\n",
      "Iteration: 11\n",
      "Train accuracy: 90.61111111111111\n",
      "Val accuracy: 90.5\n",
      "Iteration: 12\n",
      "Train accuracy: 90.90158730158731\n",
      "Val accuracy: 90.84285714285714\n",
      "Iteration: 13\n",
      "Train accuracy: 91.13968253968254\n",
      "Val accuracy: 91.18571428571428\n",
      "Iteration: 14\n",
      "Train accuracy: 91.35714285714286\n",
      "Val accuracy: 91.44285714285715\n",
      "Iteration: 15\n",
      "Train accuracy: 91.53968253968254\n",
      "Val accuracy: 91.55714285714286\n",
      "Iteration: 16\n",
      "Train accuracy: 91.73333333333333\n",
      "Val accuracy: 91.75714285714285\n",
      "Iteration: 17\n",
      "Train accuracy: 91.89206349206349\n",
      "Val accuracy: 91.91428571428571\n",
      "Iteration: 18\n",
      "Train accuracy: 92.08253968253969\n",
      "Val accuracy: 92.0\n",
      "Iteration: 19\n",
      "Train accuracy: 92.23015873015873\n",
      "Val accuracy: 92.01428571428572\n",
      "Iteration: 20\n",
      "Train accuracy: 92.37301587301587\n",
      "Val accuracy: 92.10000000000001\n"
     ]
    }
   ],
   "source": [
    "batch_grad_descent(x_train,y_train,20, 0.1, print_op=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WP(W1, b1, W2, b2, pert, lossBeforePert, X, y):\n",
    "    m = y.shape[0] #m is the number of training examples\n",
    "    Y = one_hot_encoding(y)\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    lossAfterPertW1 = np.zeros_like(W1)\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1pert = W1.copy()\n",
    "            W1pert[i, j] += pert\n",
    "            _, _, _, A2pert = forward(X, W1pert, b1, W2, b2)\n",
    "            lossAfterPertW1[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    dW1 = 1/m * (lossAfterPertW1-lossBeforePert)/pert\n",
    "\n",
    "\n",
    "    db1 = np.zeros_like(b1)\n",
    "    lossAfterPertb1 = np.zeros_like(b1)\n",
    "    for i in range(b1.shape[0]):\n",
    "        b1pert = b1.copy()\n",
    "        b1pert[i]+=pert\n",
    "        _, _, _, A2pert = forward(X, W1, b1pert, W2, b2)\n",
    "        lossAfterPertb1[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    db1 = 1/m * (lossAfterPertb1-lossBeforePert)/pert\n",
    "\n",
    "    \n",
    "    dW2 = np.zeros_like(W2)\n",
    "    lossAfterPertW2 = np.zeros_like(W2)\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2pert = W2.copy()\n",
    "            W2pert[i, j] += pert\n",
    "            _, _, _, A2pert = forward(X, W1, b1, W2pert, b2)\n",
    "            lossAfterPertW2[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    dW2 = 1/m * (lossAfterPertW2-lossBeforePert)/pert\n",
    "\n",
    "\n",
    "    db2 = np.zeros_like(b2)\n",
    "    lossAfterPertb2 = np.zeros_like(b2)\n",
    "    for i in range(b2.shape[0]):\n",
    "        b2pert = b2.copy()\n",
    "        b2pert[i]+=pert\n",
    "        _, _, _, A2pert = forward(X, W1, b1, W2, b2pert)\n",
    "        lossAfterPertb2[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    db2 = 1/m * (lossAfterPertb2-lossBeforePert)/pert\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentWP(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "      print(f\"BP Iter {i+1} -> sub iter {j} : {accuracy(predictions(A2), Y1)}\", end = \"\\r\", flush = True)\n",
    "\n",
    "      #dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "      loss = np.sum(crossEntropy(one_hot_encoding(Y1), A2))\n",
    "      dW1, db1, dW2, db2 = WP(W1, b1, W2, b2, pert=0.1, lossBeforePert=loss, X=X1, y=Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr = lr)\n",
    "\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1 sub iter 99 : 74.285714285714295\n",
      "Train accuracy: 73.65396825396824\n",
      "Val accuracy: 73.55714285714285\n",
      "Iteration: 2 sub iter 99 : 81.74603174603175\n",
      "Train accuracy: 80.5063492063492\n",
      "Val accuracy: 80.54285714285714\n",
      "Iteration: 3 sub iter 99 : 84.76190476190476\n",
      "Train accuracy: 83.32063492063492\n",
      "Val accuracy: 83.35714285714285\n",
      "Iteration: 4 sub iter 99 : 86.03174603174604\n",
      "Train accuracy: 84.85079365079365\n",
      "Val accuracy: 84.55714285714285\n",
      "Iteration: 5 sub iter 99 : 86.66666666666667\n",
      "Train accuracy: 85.74126984126984\n",
      "Val accuracy: 85.5\n",
      "Iteration: 6 sub iter 99 : 87.30158730158737\n",
      "Train accuracy: 86.39206349206349\n",
      "Val accuracy: 86.15714285714286\n",
      "Iteration: 7 sub iter 99 : 88.09523809523809\n",
      "Train accuracy: 86.91428571428571\n",
      "Val accuracy: 86.72857142857143\n",
      "Iteration: 8 sub iter 99 : 88.09523809523809\n",
      "Train accuracy: 87.40476190476191\n",
      "Val accuracy: 87.04285714285714\n",
      "Iteration: 9 sub iter 99 : 88.09523809523809\n",
      "Train accuracy: 87.8015873015873\n",
      "Val accuracy: 87.57142857142857\n",
      "Iteration: 10 sub iter 99 : 88.41269841269843\n",
      "Train accuracy: 88.15238095238095\n",
      "Val accuracy: 87.82857142857144\n",
      "Iteration: 11 sub iter 99 : 88.41269841269847\n",
      "Train accuracy: 88.48730158730159\n",
      "Val accuracy: 88.2\n",
      "Iteration: 12 sub iter 99 : 88.57142857142857\n",
      "Train accuracy: 88.75873015873016\n",
      "Val accuracy: 88.64285714285714\n",
      "Iteration: 13 sub iter 99 : 89.20634920634922\n",
      "Train accuracy: 89.05396825396825\n",
      "Val accuracy: 88.85714285714286\n",
      "Iteration: 14 sub iter 99 : 89.84126984126985\n",
      "Train accuracy: 89.23174603174603\n",
      "Val accuracy: 89.05714285714285\n",
      "Iteration: 15 sub iter 99 : 90.15873015873017\n",
      "Train accuracy: 89.36666666666667\n",
      "Val accuracy: 89.32857142857142\n",
      "BP Iter 16 -> sub iter 13 : 86.98412698412699\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\wpMnist.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=0'>1</a>\u001b[0m _, _, _, _, train_acc, val_acc, train_loss, val_loss, sum_weights \u001b[39m=\u001b[39m batch_grad_descentWP(x_train,y_train,\u001b[39m20\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\wpMnist.ipynb Cell 24\u001b[0m in \u001b[0;36mbatch_grad_descentWP\u001b[1;34m(X, Y, iter, lr, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=34'>35</a>\u001b[0m   \u001b[39m#dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=35'>36</a>\u001b[0m   loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(crossEntropy(one_hot_encoding(Y1), A2))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=36'>37</a>\u001b[0m   dW1, db1, dW2, db2 \u001b[39m=\u001b[39m WP(W1, b1, W2, b2, pert\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, lossBeforePert\u001b[39m=\u001b[39;49mloss, X\u001b[39m=\u001b[39;49mX1, y\u001b[39m=\u001b[39;49mY1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=39'>40</a>\u001b[0m   W1, b1, W2, b2 \u001b[39m=\u001b[39m param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr \u001b[39m=\u001b[39m lr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m(print_op) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\wpMnist.ipynb Cell 24\u001b[0m in \u001b[0;36mWP\u001b[1;34m(W1, b1, W2, b2, pert, lossBeforePert, X, y)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=7'>8</a>\u001b[0m         W1pert \u001b[39m=\u001b[39m W1\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=8'>9</a>\u001b[0m         W1pert[i, j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pert\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=9'>10</a>\u001b[0m         _, _, _, A2pert \u001b[39m=\u001b[39m forward(X, W1pert, b1, W2, b2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=10'>11</a>\u001b[0m         lossAfterPertW1[i, j] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(crossEntropy(one_hot_encoding(y), A2pert))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=11'>12</a>\u001b[0m dW1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mm \u001b[39m*\u001b[39m (lossAfterPertW1\u001b[39m-\u001b[39mlossBeforePert)\u001b[39m/\u001b[39mpert\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\wpMnist.ipynb Cell 24\u001b[0m in \u001b[0;36mforward\u001b[1;34m(x_train, W1, b1, W2, b2)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=3'>4</a>\u001b[0m A1 \u001b[39m=\u001b[39m relu(Z1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=5'>6</a>\u001b[0m Z2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2,A1) \u001b[39m+\u001b[39m b2\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=6'>7</a>\u001b[0m A2 \u001b[39m=\u001b[39m softmax(Z2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m Z1, A1, Z2, A2\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\wpMnist.ipynb Cell 24\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(Z)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(Z):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=5'>6</a>\u001b[0m   \u001b[39m#return np.exp(Z) / np.sum(np.exp(Z),0)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=6'>7</a>\u001b[0m   Z \u001b[39m=\u001b[39m Z\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39;49mmax(Z, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/wpMnist.ipynb#ch0000020?line=7'>8</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mexp(Z) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(Z),\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2679\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2680\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2793\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[0;32m   2794\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, _, _, _, train_acc, val_acc, train_loss, val_loss, sum_weights = batch_grad_descentWP(x_train,y_train,20, 0.1, print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
