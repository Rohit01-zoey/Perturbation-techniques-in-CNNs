{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveText(fPath, arr):\n",
    "    #dim1, dim2 = arr.shape\n",
    "    f = open(fPath, 'a')\n",
    "    np.savetxt(f, arr.flatten(), newline = ', ')\n",
    "    f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(500,784) - 0.5\n",
    "  b1 = np.random.rand(500,1) - 0.5\n",
    "  W2 = np.random.rand(500,500) - 0.5\n",
    "  b2 = np.random.rand(500,1) - 0.5\n",
    "  W3 = np.random.rand(10,500) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr, factor=0):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    #storing the weights:\n",
    "    start = time.time()\n",
    "    basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "    saveText(basePath+\"W1.txt\", W1)\n",
    "    saveText(basePath+\"W2.txt\", W2)\n",
    "    saveText(basePath+\"W3.txt\", W3)\n",
    "    saveText(basePath+\"b1.txt\", b1)\n",
    "    saveText(basePath+\"b2.txt\", b2)\n",
    "    saveText(basePath+\"b3.txt\", b3)\n",
    "    end = time.time()\n",
    "    print(\"###Saving weights : {time}\".format(time = end - start))\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/Y.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsToTrain = 100 #please update the rest of the functions with this since I have hardcoded them for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerToTrain = input(\"Do you want to train the model again? : \")\n",
    "if answerToTrain.strip().lower()=='yes':\n",
    "    W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batch_grad_descent(x_train,y_train,epochsToTrain, 0.01,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromLine(line, shape):\n",
    "    #line = \"xx, xxx, x,....., \"\n",
    "    lineElements = np.array([float(x) for x in line.split(\", \")[:-1]]).reshape(shape)\n",
    "    return lineElements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVth(mu, sigma, shape):\n",
    "  #last dimension represents the binary rep for each weight\n",
    "  return np.random.normal(loc=mu, scale=sigma, size=shape) #each bit is represented by an sram so we need those many vth values for each mosfet in this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, mu, sigma, vDD, precision, step, discreteSteps):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "\n",
    "  iOn = ((vDD - Vth)**2)*1e-06#scaling the current according to Ioff values arbitraryfor now!!\n",
    "\n",
    "  \n",
    "  iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "\n",
    "  \n",
    "  iOff = np.random.uniform(low=0, high=1e-10, size = sizeI)#no negative value\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray += np.sign(weightArray) * np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel)\n",
    "\n",
    "\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the printing function for progress bar etc\n",
    "import sys\n",
    "def show(j,  count, size=60, prefix = \"\", out=sys.stdout):\n",
    "        x = int(size*j/count)\n",
    "        print(\"{}[{}{}] {}/{}\".format(prefix, u\"█\"*x, \".\"*(size-x), j, count), end='\\r', file=out, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readWriteTransform(readPath, writePath, shape, precision, step, discreteSteps, epochsToTrain, mu, sigma, vDD):\n",
    "    #basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "    fileOpen = open(readPath, 'r')\n",
    "    iter = 1\n",
    "    for line in fileOpen:\n",
    "        start = time.time()\n",
    "        saveText(writePath, weightTransformWithVariability(loadFromLine(line, shape), mu, sigma, vDD, precision, step, discreteSteps))\n",
    "        end = time.time()\n",
    "        #print(\"#Finished reading and transforming Line {lineNo} with Time taken = {tTime}\".format(lineNo = iter, tTime = end-start))\n",
    "        show(iter, epochsToTrain, 100, \"Processed Line {lineNo} & Time taken = {tTime}\".format(lineNo = iter, tTime = round(end-start,2)))\n",
    "        iter += 1\n",
    "        \n",
    "    fileOpen.close()\n",
    "    print(\"##Fin.##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the variability conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 10#setting the precision value of the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wRange = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfLevels = 2**precision - 1 #no of levels of quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearing out the variability transformed weights text files for storing the new transformed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variability transformed weights HAVE been cleared!!\n"
     ]
    }
   ],
   "source": [
    "answer = input(\"Your response to clear variability transformed weight text files : \")\n",
    "if answer.lower().strip()=='yes':\n",
    "    basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "    fileToClear = open(basePath+\"W1var.txt\", \"w\")\n",
    "    fileToClear.write(\"\")\n",
    "    fileToClear.close()\n",
    "    fileToClear = open(basePath+\"W2var.txt\", \"w\")\n",
    "    fileToClear.write(\"\")\n",
    "    fileToClear.close()\n",
    "    fileToClear = open(basePath+\"W3var.txt\", \"w\")\n",
    "    fileToClear.write(\"\")\n",
    "    fileToClear.close()\n",
    "    fileToClear = open(basePath+\"b1var.txt\", \"w\")\n",
    "    fileToClear.write(\"\")\n",
    "    fileToClear.close()\n",
    "    fileToClear = open(basePath+\"b2var.txt\", \"w\")\n",
    "    fileToClear.write(\"\")\n",
    "    fileToClear.close()\n",
    "    fileToClear = open(basePath+\"b3var.txt\", \"w\")\n",
    "    fileToClear.write(\"\")\n",
    "    fileToClear.close()\n",
    "    print(\"Variability transformed weights HAVE been cleared!!\")\n",
    "else:\n",
    "    print(\"Variability transformed weights have NOT been cleared. You can use those weights!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the variability transformed weights in the appropriate text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, vDD = 0.7, 0.1, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fin.##d Line 100 & Time taken = 1.44[████████████████████████████████████████████████████████████████████████████████████████████████████] 100/100\n"
     ]
    }
   ],
   "source": [
    "basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "readWriteTransform(basePath+\"W1.txt\", basePath+\"W1var.txt\", (500, 784), precision, step, discreteSteps, epochsToTrain,mu, sigma, vDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fin.##d Line 100 & Time taken = 0.0[████████████████████████████████████████████████████████████████████████████████████████████████████] 100/100\n"
     ]
    }
   ],
   "source": [
    "basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "readWriteTransform(basePath+\"b1.txt\", basePath+\"b1var.txt\", (500, 1), precision, step, discreteSteps, epochsToTrain,mu, sigma, vDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fin.##d Line 100 & Time taken = 0.94[████████████████████████████████████████████████████████████████████████████████████████████████████] 100/100\n"
     ]
    }
   ],
   "source": [
    "basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "readWriteTransform(basePath+\"W2.txt\", basePath+\"W2var.txt\", (500, 500), precision, step, discreteSteps, epochsToTrain,mu, sigma, vDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fin.##d Line 100 & Time taken = 0.0[████████████████████████████████████████████████████████████████████████████████████████████████████] 100/100\n"
     ]
    }
   ],
   "source": [
    "basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "readWriteTransform(basePath+\"b2.txt\", basePath+\"b2var.txt\", (500, 1), precision, step, discreteSteps, epochsToTrain,mu, sigma, vDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fin.##d Line 100 & Time taken = 0.02[████████████████████████████████████████████████████████████████████████████████████████████████████] 100/100\n"
     ]
    }
   ],
   "source": [
    "basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "readWriteTransform(basePath+\"W3.txt\", basePath+\"W3var.txt\", (10, 500), precision, step, discreteSteps, epochsToTrain,mu, sigma, vDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fin.##d Line 100 & Time taken = 0.0[████████████████████████████████████████████████████████████████████████████████████████████████████] 100/100\n"
     ]
    }
   ],
   "source": [
    "basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "readWriteTransform(basePath+\"b3.txt\", basePath+\"b3var.txt\", (10, 1), precision, step, discreteSteps, epochsToTrain,mu, sigma, vDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-chip Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100  Train accuracy: 98.18095238095238  Val accuracy: 93.61428571428571\r"
     ]
    }
   ],
   "source": [
    "basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "fileW1var = open(basePath+\"W1var.txt\", 'r')\n",
    "fileb1var = open(basePath+\"b1var.txt\", 'r')\n",
    "fileW2var = open(basePath+\"W2var.txt\", 'r')\n",
    "fileb2var = open(basePath+\"b2var.txt\", 'r')\n",
    "fileW3var = open(basePath+\"W3var.txt\", 'r')\n",
    "fileb3var = open(basePath+\"b3var.txt\", 'r')\n",
    "trainAccOnChip = []\n",
    "valAccOnChip = []\n",
    "iter = 1\n",
    "X, Y = x_train,y_train\n",
    "for line in zip(fileW1var, fileb1var, fileW2var, fileb2var, fileW3var, fileb3var):\n",
    "    W1, b1, W2, b2, W3, b3 = line\n",
    "    W1 = loadFromLine(W1, (500, 784))\n",
    "    b1 = loadFromLine(b1, (500,1))\n",
    "    W2 = loadFromLine(W2, (500, 500))\n",
    "    b2 = loadFromLine(b2, (500,1))\n",
    "    W3 = loadFromLine(W3, (10, 500))\n",
    "    b3 = loadFromLine(b3, (10, 1))\n",
    "    #obtain training loss\n",
    "    #print(\"Iteration {no}\".format(no = iter))\n",
    "    _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "    #for i in range(0, Y.shape[0]):\n",
    "    # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "    #train_loss.append(train_loss_score)\n",
    "    #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "    #obtain training accuracy\n",
    "    trainScore = accuracy(predictions(A3_train), Y)\n",
    "    trainAccOnChip.append(trainScore)\n",
    "    #print(f'Train accuracy: {trainScore}')\n",
    "\n",
    "    ##obtain validation loss\n",
    "    _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "    #for i in range(0, y_val.shape[0]):\n",
    "    # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "    #val_loss.append(val_loss_score)\n",
    "    #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "    ##obtain validation accuracy\n",
    "    val_score = accuracy(predictions(A3_val), y_val)\n",
    "    valAccOnChip.append(val_score)\n",
    "    #print(f'Val accuracy: {val_score}')\n",
    "    print(f\"Iteration {iter}  Train accuracy: {trainScore}  Val accuracy: {val_score}\",  end='\\r', flush=True)\n",
    "    iter +=1\n",
    "fileW1var.close()\n",
    "fileb1var.close()\n",
    "fileW2var.close()\n",
    "fileb2var.close()\n",
    "fileW3var.close()\n",
    "fileb3var.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cf42253340>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, 'rx')\n",
    "plt.plot(trainAccOnChip, 'bo')\n",
    "plt.legend([\"Training accuracy (software)\", \"Training accuracy (on chip)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
