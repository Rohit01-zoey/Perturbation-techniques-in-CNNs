{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveText(fPath, arr):\n",
    "    dim1, dim2 = arr.shape\n",
    "    f = open(fPath, 'a')\n",
    "    np.savetxt(f, arr.flatten(), newline = ', ')\n",
    "    f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(500,784) - 0.5\n",
    "  b1 = np.random.rand(500,1) - 0.5\n",
    "  W2 = np.random.rand(500,500) - 0.5\n",
    "  b2 = np.random.rand(500,1) - 0.5\n",
    "  W3 = np.random.rand(10,500) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr, factor=0):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    #storing the weights:\n",
    "    start = time.time()\n",
    "    basePath = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\\"\n",
    "    saveText(basePath+\"W1.txt\", W1)\n",
    "    saveText(basePath+\"W2.txt\", W2)\n",
    "    saveText(basePath+\"W3.txt\", W3)\n",
    "    saveText(basePath+\"b1.txt\", b1)\n",
    "    saveText(basePath+\"b2.txt\", b2)\n",
    "    saveText(basePath+\"b3.txt\", b3)\n",
    "    end = time.time()\n",
    "    print(\"###Saving weights : {time}\".format(time = end - start))\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/Y.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "###Saving weights : 2.1587517261505127\n",
      "Iteration: 1\n",
      "Train accuracy: 82.6\n",
      "Val accuracy: 81.98571428571428\n",
      "###Saving weights : 2.204078197479248\n",
      "Iteration: 2\n",
      "Train accuracy: 86.66507936507936\n",
      "Val accuracy: 85.67142857142858\n",
      "###Saving weights : 2.311230182647705\n",
      "Iteration: 3\n",
      "Train accuracy: 88.46190476190476\n",
      "Val accuracy: 87.18571428571428\n",
      "###Saving weights : 2.3825206756591797\n",
      "Iteration: 4\n",
      "Train accuracy: 89.57301587301588\n",
      "Val accuracy: 88.1\n",
      "###Saving weights : 4.922435760498047\n",
      "Iteration: 5\n",
      "Train accuracy: 90.45238095238095\n",
      "Val accuracy: 88.57142857142857\n",
      "###Saving weights : 4.6862287521362305\n",
      "Iteration: 6\n",
      "Train accuracy: 91.08730158730158\n",
      "Val accuracy: 89.05714285714285\n",
      "###Saving weights : 4.066746950149536\n",
      "Iteration: 7\n",
      "Train accuracy: 91.6920634920635\n",
      "Val accuracy: 89.34285714285714\n",
      "###Saving weights : 2.1148321628570557\n",
      "Iteration: 8\n",
      "Train accuracy: 92.12380952380951\n",
      "Val accuracy: 90.04285714285714\n",
      "###Saving weights : 2.07637095451355\n",
      "Iteration: 9\n",
      "Train accuracy: 92.56349206349206\n",
      "Val accuracy: 90.41428571428571\n",
      "###Saving weights : 2.0337796211242676\n",
      "Iteration: 10\n",
      "Train accuracy: 92.92222222222222\n",
      "Val accuracy: 90.64285714285715\n",
      "###Saving weights : 2.0152056217193604\n",
      "Iteration: 11\n",
      "Train accuracy: 93.23015873015873\n",
      "Val accuracy: 90.88571428571429\n",
      "###Saving weights : 2.011759042739868\n",
      "Iteration: 12\n",
      "Train accuracy: 93.47936507936508\n",
      "Val accuracy: 91.05714285714286\n",
      "###Saving weights : 2.3477935791015625\n",
      "Iteration: 13\n",
      "Train accuracy: 93.67301587301587\n",
      "Val accuracy: 91.24285714285715\n",
      "###Saving weights : 2.070615768432617\n",
      "Iteration: 14\n",
      "Train accuracy: 93.8920634920635\n",
      "Val accuracy: 91.4\n",
      "###Saving weights : 2.049060344696045\n",
      "Iteration: 15\n",
      "Train accuracy: 94.06825396825397\n",
      "Val accuracy: 91.64285714285715\n",
      "###Saving weights : 2.047499179840088\n",
      "Iteration: 16\n",
      "Train accuracy: 94.26031746031745\n",
      "Val accuracy: 91.74285714285715\n",
      "###Saving weights : 2.122152805328369\n",
      "Iteration: 17\n",
      "Train accuracy: 94.43174603174603\n",
      "Val accuracy: 91.88571428571429\n",
      "###Saving weights : 1.9729108810424805\n",
      "Iteration: 18\n",
      "Train accuracy: 94.56984126984128\n",
      "Val accuracy: 91.97142857142858\n",
      "###Saving weights : 2.091641664505005\n",
      "Iteration: 19\n",
      "Train accuracy: 94.74285714285713\n",
      "Val accuracy: 92.04285714285714\n",
      "###Saving weights : 2.098381280899048\n",
      "Iteration: 20\n",
      "Train accuracy: 94.83492063492064\n",
      "Val accuracy: 92.10000000000001\n",
      "###Saving weights : 2.0356290340423584\n",
      "Iteration: 21\n",
      "Train accuracy: 94.93968253968254\n",
      "Val accuracy: 92.08571428571429\n",
      "###Saving weights : 2.030675172805786\n",
      "Iteration: 22\n",
      "Train accuracy: 95.07142857142857\n",
      "Val accuracy: 92.15714285714286\n",
      "###Saving weights : 2.022559404373169\n",
      "Iteration: 23\n",
      "Train accuracy: 95.1920634920635\n",
      "Val accuracy: 92.18571428571428\n",
      "###Saving weights : 2.224231481552124\n",
      "Iteration: 24\n",
      "Train accuracy: 95.32857142857142\n",
      "Val accuracy: 92.25714285714287\n",
      "###Saving weights : 2.0824379920959473\n",
      "Iteration: 25\n",
      "Train accuracy: 95.43809523809523\n",
      "Val accuracy: 92.27142857142857\n",
      "###Saving weights : 2.0894532203674316\n",
      "Iteration: 26\n",
      "Train accuracy: 95.55396825396826\n",
      "Val accuracy: 92.25714285714287\n",
      "###Saving weights : 2.0483951568603516\n",
      "Iteration: 27\n",
      "Train accuracy: 95.65555555555557\n",
      "Val accuracy: 92.34285714285714\n",
      "###Saving weights : 2.050520658493042\n",
      "Iteration: 28\n",
      "Train accuracy: 95.74603174603175\n",
      "Val accuracy: 92.37142857142857\n",
      "###Saving weights : 2.10483980178833\n",
      "Iteration: 29\n",
      "Train accuracy: 95.84603174603174\n",
      "Val accuracy: 92.45714285714286\n",
      "###Saving weights : 2.077495574951172\n",
      "Iteration: 30\n",
      "Train accuracy: 95.92857142857143\n",
      "Val accuracy: 92.5\n",
      "###Saving weights : 2.0818073749542236\n",
      "Iteration: 31\n",
      "Train accuracy: 96.02857142857142\n",
      "Val accuracy: 92.60000000000001\n",
      "###Saving weights : 2.013751268386841\n",
      "Iteration: 32\n",
      "Train accuracy: 96.10634920634921\n",
      "Val accuracy: 92.64285714285714\n",
      "###Saving weights : 2.096442699432373\n",
      "Iteration: 33\n",
      "Train accuracy: 96.2\n",
      "Val accuracy: 92.65714285714286\n",
      "###Saving weights : 2.1607277393341064\n",
      "Iteration: 34\n",
      "Train accuracy: 96.26825396825397\n",
      "Val accuracy: 92.67142857142858\n",
      "###Saving weights : 1.9840338230133057\n",
      "Iteration: 35\n",
      "Train accuracy: 96.34285714285714\n",
      "Val accuracy: 92.75714285714287\n",
      "###Saving weights : 2.0828135013580322\n",
      "Iteration: 36\n",
      "Train accuracy: 96.42063492063492\n",
      "Val accuracy: 92.81428571428572\n",
      "###Saving weights : 2.1002273559570312\n",
      "Iteration: 37\n",
      "Train accuracy: 96.4888888888889\n",
      "Val accuracy: 92.85714285714286\n",
      "###Saving weights : 2.017768144607544\n",
      "Iteration: 38\n",
      "Train accuracy: 96.53968253968253\n",
      "Val accuracy: 92.85714285714286\n",
      "###Saving weights : 2.213214159011841\n",
      "Iteration: 39\n",
      "Train accuracy: 96.615873015873\n",
      "Val accuracy: 92.9\n",
      "###Saving weights : 2.031179428100586\n",
      "Iteration: 40\n",
      "Train accuracy: 96.67777777777778\n",
      "Val accuracy: 92.92857142857143\n",
      "###Saving weights : 2.099881410598755\n",
      "Iteration: 41\n",
      "Train accuracy: 96.74285714285715\n",
      "Val accuracy: 92.92857142857143\n",
      "###Saving weights : 2.134657859802246\n",
      "Iteration: 42\n",
      "Train accuracy: 96.8047619047619\n",
      "Val accuracy: 92.98571428571428\n",
      "###Saving weights : 2.01672625541687\n",
      "Iteration: 43\n",
      "Train accuracy: 96.89682539682539\n",
      "Val accuracy: 92.97142857142858\n",
      "###Saving weights : 2.0190258026123047\n",
      "Iteration: 44\n",
      "Train accuracy: 96.95555555555555\n",
      "Val accuracy: 93.02857142857142\n",
      "###Saving weights : 2.040158987045288\n",
      "Iteration: 45\n",
      "Train accuracy: 97.01269841269841\n",
      "Val accuracy: 93.07142857142857\n",
      "###Saving weights : 2.2390072345733643\n",
      "Iteration: 46\n",
      "Train accuracy: 97.0936507936508\n",
      "Val accuracy: 93.04285714285714\n",
      "###Saving weights : 2.170311450958252\n",
      "Iteration: 47\n",
      "Train accuracy: 97.16984126984127\n",
      "Val accuracy: 93.07142857142857\n",
      "###Saving weights : 2.0336577892303467\n",
      "Iteration: 48\n",
      "Train accuracy: 97.24603174603175\n",
      "Val accuracy: 93.10000000000001\n",
      "###Saving weights : 2.0612380504608154\n",
      "Iteration: 49\n",
      "Train accuracy: 97.29841269841269\n",
      "Val accuracy: 93.15714285714286\n",
      "###Saving weights : 2.0966928005218506\n",
      "Iteration: 50\n",
      "Train accuracy: 97.39682539682539\n",
      "Val accuracy: 93.17142857142858\n",
      "###Saving weights : 2.2006657123565674\n",
      "Iteration: 51\n",
      "Train accuracy: 97.45555555555555\n",
      "Val accuracy: 93.2\n",
      "###Saving weights : 2.042853593826294\n",
      "Iteration: 52\n",
      "Train accuracy: 97.52063492063492\n",
      "Val accuracy: 93.21428571428572\n",
      "###Saving weights : 2.239824056625366\n",
      "Iteration: 53\n",
      "Train accuracy: 97.57460317460317\n",
      "Val accuracy: 93.27142857142857\n",
      "###Saving weights : 2.086026191711426\n",
      "Iteration: 54\n",
      "Train accuracy: 97.63492063492063\n",
      "Val accuracy: 93.27142857142857\n",
      "###Saving weights : 1.985114574432373\n",
      "Iteration: 55\n",
      "Train accuracy: 97.68412698412699\n",
      "Val accuracy: 93.27142857142857\n",
      "###Saving weights : 2.041220188140869\n",
      "Iteration: 56\n",
      "Train accuracy: 97.74920634920635\n",
      "Val accuracy: 93.31428571428572\n",
      "###Saving weights : 1.8009579181671143\n",
      "Iteration: 57\n",
      "Train accuracy: 97.82063492063492\n",
      "Val accuracy: 93.28571428571428\n",
      "###Saving weights : 3.041273355484009\n",
      "Iteration: 58\n",
      "Train accuracy: 97.86666666666667\n",
      "Val accuracy: 93.28571428571428\n",
      "###Saving weights : 3.2909350395202637\n",
      "Iteration: 59\n",
      "Train accuracy: 97.89682539682539\n",
      "Val accuracy: 93.28571428571428\n",
      "###Saving weights : 2.3776464462280273\n",
      "Iteration: 60\n",
      "Train accuracy: 97.96507936507936\n",
      "Val accuracy: 93.30000000000001\n",
      "###Saving weights : 3.3322670459747314\n",
      "Iteration: 61\n",
      "Train accuracy: 98.00952380952381\n",
      "Val accuracy: 93.30000000000001\n",
      "###Saving weights : 2.999220609664917\n",
      "Iteration: 62\n",
      "Train accuracy: 98.05714285714285\n",
      "Val accuracy: 93.28571428571428\n",
      "###Saving weights : 3.354231119155884\n",
      "Iteration: 63\n",
      "Train accuracy: 98.11269841269842\n",
      "Val accuracy: 93.30000000000001\n",
      "###Saving weights : 2.2070820331573486\n",
      "Iteration: 64\n",
      "Train accuracy: 98.14761904761905\n",
      "Val accuracy: 93.31428571428572\n",
      "###Saving weights : 1.5629708766937256\n",
      "Iteration: 65\n",
      "Train accuracy: 98.18571428571428\n",
      "Val accuracy: 93.34285714285714\n",
      "###Saving weights : 3.3140673637390137\n",
      "Iteration: 66\n",
      "Train accuracy: 98.22222222222223\n",
      "Val accuracy: 93.32857142857142\n",
      "###Saving weights : 3.878390312194824\n",
      "Iteration: 67\n",
      "Train accuracy: 98.26190476190476\n",
      "Val accuracy: 93.31428571428572\n",
      "###Saving weights : 3.2538163661956787\n",
      "Iteration: 68\n",
      "Train accuracy: 98.31428571428572\n",
      "Val accuracy: 93.38571428571429\n",
      "###Saving weights : 3.2157135009765625\n",
      "Iteration: 69\n",
      "Train accuracy: 98.34603174603174\n",
      "Val accuracy: 93.4\n",
      "###Saving weights : 3.1107301712036133\n",
      "Iteration: 70\n",
      "Train accuracy: 98.38571428571429\n",
      "Val accuracy: 93.4\n",
      "###Saving weights : 3.389169216156006\n",
      "Iteration: 71\n",
      "Train accuracy: 98.41111111111111\n",
      "Val accuracy: 93.4\n",
      "###Saving weights : 3.0657992362976074\n",
      "Iteration: 72\n",
      "Train accuracy: 98.43492063492063\n",
      "Val accuracy: 93.42857142857143\n",
      "###Saving weights : 3.3055973052978516\n",
      "Iteration: 73\n",
      "Train accuracy: 98.47301587301587\n",
      "Val accuracy: 93.47142857142858\n",
      "###Saving weights : 3.13157320022583\n",
      "Iteration: 74\n",
      "Train accuracy: 98.50634920634921\n",
      "Val accuracy: 93.47142857142858\n",
      "###Saving weights : 2.411707878112793\n",
      "Iteration: 75\n",
      "Train accuracy: 98.53809523809524\n",
      "Val accuracy: 93.5\n",
      "###Saving weights : 1.972269058227539\n",
      "Iteration: 76\n",
      "Train accuracy: 98.54920634920636\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 3.2885091304779053\n",
      "Iteration: 77\n",
      "Train accuracy: 98.57460317460317\n",
      "Val accuracy: 93.51428571428572\n",
      "###Saving weights : 2.1060614585876465\n",
      "Iteration: 78\n",
      "Train accuracy: 98.6015873015873\n",
      "Val accuracy: 93.51428571428572\n",
      "###Saving weights : 3.081071138381958\n",
      "Iteration: 79\n",
      "Train accuracy: 98.62857142857143\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.1079962253570557\n",
      "Iteration: 80\n",
      "Train accuracy: 98.65714285714286\n",
      "Val accuracy: 93.51428571428572\n",
      "###Saving weights : 1.849691390991211\n",
      "Iteration: 81\n",
      "Train accuracy: 98.67777777777778\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.4215211868286133\n",
      "Iteration: 82\n",
      "Train accuracy: 98.70952380952382\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 3.334773302078247\n",
      "Iteration: 83\n",
      "Train accuracy: 98.73015873015873\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 1.9988160133361816\n",
      "Iteration: 84\n",
      "Train accuracy: 98.76031746031745\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.1722562313079834\n",
      "Iteration: 85\n",
      "Train accuracy: 98.78730158730158\n",
      "Val accuracy: 93.51428571428572\n",
      "###Saving weights : 3.2082860469818115\n",
      "Iteration: 86\n",
      "Train accuracy: 98.81269841269841\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.417692184448242\n",
      "Iteration: 87\n",
      "Train accuracy: 98.82698412698413\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 3.1871793270111084\n",
      "Iteration: 88\n",
      "Train accuracy: 98.84126984126983\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.148385524749756\n",
      "Iteration: 89\n",
      "Train accuracy: 98.86666666666667\n",
      "Val accuracy: 93.51428571428572\n",
      "###Saving weights : 3.131443977355957\n",
      "Iteration: 90\n",
      "Train accuracy: 98.88412698412698\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.36751389503479\n",
      "Iteration: 91\n",
      "Train accuracy: 98.9031746031746\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 3.370516300201416\n",
      "Iteration: 92\n",
      "Train accuracy: 98.92380952380952\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 3.2885706424713135\n",
      "Iteration: 93\n",
      "Train accuracy: 98.94920634920635\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 3.4002954959869385\n",
      "Iteration: 94\n",
      "Train accuracy: 98.98095238095237\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.068019151687622\n",
      "Iteration: 95\n",
      "Train accuracy: 99.00952380952381\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 1.5634100437164307\n",
      "Iteration: 96\n",
      "Train accuracy: 99.02857142857144\n",
      "Val accuracy: 93.5\n",
      "###Saving weights : 3.4540114402770996\n",
      "Iteration: 97\n",
      "Train accuracy: 99.05714285714285\n",
      "Val accuracy: 93.54285714285714\n",
      "###Saving weights : 1.526017427444458\n",
      "Iteration: 98\n",
      "Train accuracy: 99.08571428571429\n",
      "Val accuracy: 93.51428571428572\n",
      "###Saving weights : 3.254028797149658\n",
      "Iteration: 99\n",
      "Train accuracy: 99.0984126984127\n",
      "Val accuracy: 93.52857142857142\n",
      "###Saving weights : 3.3073978424072266\n",
      "Iteration: 100\n",
      "Train accuracy: 99.13015873015874\n",
      "Val accuracy: 93.5\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batch_grad_descent(x_train,y_train,100, 0.01,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20a421fb190>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc, 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFromLine(line, shape):\n",
    "    #line = \"xx, xxx, x,....., \"\n",
    "    lineElements = np.array([float(x) for x in line.split(\", \")[:-1]]).reshape(shape)\n",
    "    return lineElements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePathTest = \"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\weights\\\\test.txt\"\n",
    "arr = np.random.randint(1, 30000, (1000, 1000))+0.1234\n",
    "saveText(basePathTest, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14305.1234  5509.1234 29442.1234 ...  3399.1234 16239.1234 24502.1234]\n",
      " [13581.1234  4259.1234 15433.1234 ... 29485.1234 21327.1234  9006.1234]\n",
      " [15492.1234 28407.1234 15597.1234 ...  7927.1234  6704.1234 24827.1234]\n",
      " ...\n",
      " [10950.1234  5683.1234 26154.1234 ... 17928.1234  3324.1234  3376.1234]\n",
      " [ 9744.1234 22710.1234  2449.1234 ...  3370.1234  3109.1234   212.1234]\n",
      " [11699.1234  8407.1234  2663.1234 ...  7898.1234  4009.1234 17086.1234]]\n",
      "0.546917200088501\n"
     ]
    }
   ],
   "source": [
    "ff = open(basePathTest)\n",
    "for line in ff:\n",
    "    start = time.time()\n",
    "    print(loadFromLine(line, (1000,1000)))\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
