{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight perturbation implementation in python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the iris dataset for this! Let us look at how the dataset looks like...before that lets import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df stores the dataframe\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Rohit\\\\Desktop\\\\Perturbation Techniques in CNNs\\\\Perturbation-techniques-in-CNNs\\\\data\\\\Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how our dataset looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>146</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>147</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>148</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>149</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0      1            5.1           3.5            1.4           0.2   \n",
       "1      2            4.9           3.0            1.4           0.2   \n",
       "2      3            4.7           3.2            1.3           0.2   \n",
       "3      4            4.6           3.1            1.5           0.2   \n",
       "4      5            5.0           3.6            1.4           0.2   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "145  146            6.7           3.0            5.2           2.3   \n",
       "146  147            6.3           2.5            5.0           1.9   \n",
       "147  148            6.5           3.0            5.2           2.0   \n",
       "148  149            6.2           3.4            5.4           2.3   \n",
       "149  150            5.9           3.0            5.1           1.8   \n",
       "\n",
       "            Species  \n",
       "0       Iris-setosa  \n",
       "1       Iris-setosa  \n",
       "2       Iris-setosa  \n",
       "3       Iris-setosa  \n",
       "4       Iris-setosa  \n",
       "..              ...  \n",
       "145  Iris-virginica  \n",
       "146  Iris-virginica  \n",
       "147  Iris-virginica  \n",
       "148  Iris-virginica  \n",
       "149  Iris-virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it has 1 redundant column 'Id' which we will drop, 4 feature columns and finally the target variable viz 'Species'\n",
    "\n",
    "Further, we want to sample the rows at random for our train and test datasets hence, we reorder the rows randomly as done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>146</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "31    32            5.4           3.4            1.5           0.4   \n",
       "135  136            7.7           3.0            6.1           2.3   \n",
       "57    58            4.9           2.4            3.3           1.0   \n",
       "145  146            6.7           3.0            5.2           2.3   \n",
       "43    44            5.0           3.5            1.6           0.6   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "134  135            6.1           2.6            5.6           1.4   \n",
       "75    76            6.6           3.0            4.4           1.4   \n",
       "82    83            5.8           2.7            3.9           1.2   \n",
       "68    69            6.2           2.2            4.5           1.5   \n",
       "108  109            6.7           2.5            5.8           1.8   \n",
       "\n",
       "             Species  \n",
       "31       Iris-setosa  \n",
       "135   Iris-virginica  \n",
       "57   Iris-versicolor  \n",
       "145   Iris-virginica  \n",
       "43       Iris-setosa  \n",
       "..               ...  \n",
       "134   Iris-virginica  \n",
       "75   Iris-versicolor  \n",
       "82   Iris-versicolor  \n",
       "68   Iris-versicolor  \n",
       "108   Iris-virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reordering so that we sample our train and test dataset randomly\n",
    "df = df.sample(frac = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the 'Id' column\n",
    "#df = df.drop([\"Id\"], axis = 1)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    1\n",
       "146    1\n",
       "147    1\n",
       "148    1\n",
       "149    1\n",
       "Name: Species, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#endocing the target variable\n",
    "y = df.Species.replace({\"Iris-setosa\" : 0, \"Iris-virginica\" : 1, \"Iris-versicolor\":2})\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we must normalise all the feature columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07056264, 0.09265065, 0.02754646, 0.01150299],\n",
       "       [0.06779548, 0.07941484, 0.02754646, 0.01150299],\n",
       "       [0.06502832, 0.08470916, 0.02557886, 0.01150299],\n",
       "       [0.06364474, 0.082062  , 0.02951407, 0.01150299],\n",
       "       [0.06917906, 0.09529781, 0.02754646, 0.01150299],\n",
       "       [0.07471338, 0.10323929, 0.03344928, 0.02300599],\n",
       "       [0.06364474, 0.09000348, 0.02754646, 0.01725449],\n",
       "       [0.06917906, 0.09000348, 0.02951407, 0.01150299],\n",
       "       [0.06087757, 0.07676768, 0.02754646, 0.01150299],\n",
       "       [0.06779548, 0.082062  , 0.02951407, 0.0057515 ],\n",
       "       [0.07471338, 0.09794497, 0.02951407, 0.01150299],\n",
       "       [0.0664119 , 0.09000348, 0.03148167, 0.01150299],\n",
       "       [0.0664119 , 0.07941484, 0.02754646, 0.0057515 ],\n",
       "       [0.05949399, 0.07941484, 0.02164365, 0.0057515 ],\n",
       "       [0.08024771, 0.10588645, 0.02361125, 0.01150299],\n",
       "       [0.07886413, 0.1164751 , 0.02951407, 0.02300599],\n",
       "       [0.07471338, 0.10323929, 0.02557886, 0.02300599],\n",
       "       [0.07056264, 0.09265065, 0.02754646, 0.01725449],\n",
       "       [0.07886413, 0.10059213, 0.03344928, 0.01725449],\n",
       "       [0.07056264, 0.10059213, 0.02951407, 0.01725449],\n",
       "       [0.07471338, 0.09000348, 0.03344928, 0.01150299],\n",
       "       [0.07056264, 0.09794497, 0.02951407, 0.02300599],\n",
       "       [0.06364474, 0.09529781, 0.01967604, 0.01150299],\n",
       "       [0.07056264, 0.08735632, 0.03344928, 0.02875749],\n",
       "       [0.0664119 , 0.09000348, 0.03738448, 0.01150299],\n",
       "       [0.06917906, 0.07941484, 0.03148167, 0.01150299],\n",
       "       [0.06917906, 0.09000348, 0.03148167, 0.02300599],\n",
       "       [0.07194622, 0.09265065, 0.02951407, 0.01150299],\n",
       "       [0.07194622, 0.09000348, 0.02754646, 0.01150299],\n",
       "       [0.06502832, 0.08470916, 0.03148167, 0.01150299],\n",
       "       [0.0664119 , 0.082062  , 0.03148167, 0.01150299],\n",
       "       [0.07471338, 0.09000348, 0.02951407, 0.02300599],\n",
       "       [0.07194622, 0.10853361, 0.02951407, 0.0057515 ],\n",
       "       [0.07609697, 0.11118077, 0.02754646, 0.01150299],\n",
       "       [0.06779548, 0.082062  , 0.02951407, 0.0057515 ],\n",
       "       [0.06917906, 0.08470916, 0.02361125, 0.01150299],\n",
       "       [0.07609697, 0.09265065, 0.02557886, 0.01150299],\n",
       "       [0.06779548, 0.082062  , 0.02951407, 0.0057515 ],\n",
       "       [0.06087757, 0.07941484, 0.02557886, 0.01150299],\n",
       "       [0.07056264, 0.09000348, 0.02951407, 0.01150299],\n",
       "       [0.06917906, 0.09265065, 0.02557886, 0.01725449],\n",
       "       [0.06226115, 0.06088471, 0.02557886, 0.01725449],\n",
       "       [0.06087757, 0.08470916, 0.02557886, 0.01150299],\n",
       "       [0.06917906, 0.09265065, 0.03148167, 0.03450898],\n",
       "       [0.07056264, 0.10059213, 0.03738448, 0.02300599],\n",
       "       [0.0664119 , 0.07941484, 0.02754646, 0.01725449],\n",
       "       [0.07056264, 0.10059213, 0.03148167, 0.01150299],\n",
       "       [0.06364474, 0.08470916, 0.02754646, 0.01150299],\n",
       "       [0.0733298 , 0.09794497, 0.02951407, 0.01150299],\n",
       "       [0.06917906, 0.08735632, 0.02754646, 0.01150299],\n",
       "       [0.09685068, 0.08470916, 0.09247741, 0.08052096],\n",
       "       [0.0885492 , 0.08470916, 0.0885422 , 0.08627246],\n",
       "       [0.0954671 , 0.082062  , 0.09641262, 0.08627246],\n",
       "       [0.07609697, 0.06088471, 0.07870418, 0.07476947],\n",
       "       [0.08993278, 0.07412052, 0.0905098 , 0.08627246],\n",
       "       [0.07886413, 0.07412052, 0.0885422 , 0.07476947],\n",
       "       [0.08716562, 0.08735632, 0.09247741, 0.09202396],\n",
       "       [0.06779548, 0.06353187, 0.06493095, 0.05751497],\n",
       "       [0.09131636, 0.07676768, 0.0905098 , 0.07476947],\n",
       "       [0.07194622, 0.07147336, 0.07673657, 0.08052096],\n",
       "       [0.06917906, 0.05294323, 0.06886616, 0.05751497],\n",
       "       [0.08163129, 0.07941484, 0.08263939, 0.08627246],\n",
       "       [0.08301487, 0.05823755, 0.07870418, 0.05751497],\n",
       "       [0.08439845, 0.07676768, 0.09247741, 0.08052096],\n",
       "       [0.07748055, 0.07676768, 0.07083376, 0.07476947],\n",
       "       [0.09269994, 0.082062  , 0.0865746 , 0.08052096],\n",
       "       [0.07748055, 0.07941484, 0.0885422 , 0.08627246],\n",
       "       [0.08024771, 0.07147336, 0.08067178, 0.05751497],\n",
       "       [0.08578203, 0.05823755, 0.0885422 , 0.08627246],\n",
       "       [0.07748055, 0.06617903, 0.07673657, 0.06326647],\n",
       "       [0.08163129, 0.08470916, 0.09444501, 0.10352695],\n",
       "       [0.08439845, 0.07412052, 0.07870418, 0.07476947],\n",
       "       [0.08716562, 0.06617903, 0.09641262, 0.08627246],\n",
       "       [0.08439845, 0.07412052, 0.09247741, 0.06901797],\n",
       "       [0.0885492 , 0.07676768, 0.08460699, 0.07476947],\n",
       "       [0.09131636, 0.07941484, 0.0865746 , 0.08052096],\n",
       "       [0.09408352, 0.07412052, 0.09444501, 0.08052096],\n",
       "       [0.09269994, 0.07941484, 0.09838022, 0.09777546],\n",
       "       [0.08301487, 0.07676768, 0.0885422 , 0.08627246],\n",
       "       [0.07886413, 0.06882619, 0.06886616, 0.05751497],\n",
       "       [0.07609697, 0.06353187, 0.07476897, 0.06326647],\n",
       "       [0.07609697, 0.06353187, 0.07280136, 0.05751497],\n",
       "       [0.08024771, 0.07147336, 0.07673657, 0.06901797],\n",
       "       [0.08301487, 0.07147336, 0.10034783, 0.09202396],\n",
       "       [0.07471338, 0.07941484, 0.0885422 , 0.08627246],\n",
       "       [0.08301487, 0.09000348, 0.0885422 , 0.09202396],\n",
       "       [0.09269994, 0.082062  , 0.09247741, 0.08627246],\n",
       "       [0.08716562, 0.06088471, 0.0865746 , 0.07476947],\n",
       "       [0.07748055, 0.07941484, 0.08067178, 0.07476947],\n",
       "       [0.07609697, 0.06617903, 0.07870418, 0.07476947],\n",
       "       [0.07609697, 0.06882619, 0.0865746 , 0.06901797],\n",
       "       [0.08439845, 0.07941484, 0.0905098 , 0.08052096],\n",
       "       [0.08024771, 0.06882619, 0.07870418, 0.06901797],\n",
       "       [0.06917906, 0.06088471, 0.06493095, 0.05751497],\n",
       "       [0.07748055, 0.07147336, 0.08263939, 0.07476947],\n",
       "       [0.07886413, 0.07941484, 0.08263939, 0.06901797],\n",
       "       [0.07886413, 0.07676768, 0.08263939, 0.07476947],\n",
       "       [0.08578203, 0.07676768, 0.08460699, 0.07476947],\n",
       "       [0.07056264, 0.06617903, 0.05902813, 0.06326647],\n",
       "       [0.07886413, 0.07412052, 0.08067178, 0.07476947],\n",
       "       [0.08716562, 0.08735632, 0.11805627, 0.14378743],\n",
       "       [0.08024771, 0.07147336, 0.10034783, 0.10927845],\n",
       "       [0.09823426, 0.07941484, 0.11608866, 0.12078145],\n",
       "       [0.08716562, 0.07676768, 0.11018585, 0.10352695],\n",
       "       [0.08993278, 0.07941484, 0.11412106, 0.12653294],\n",
       "       [0.10515217, 0.07941484, 0.12986189, 0.12078145],\n",
       "       [0.06779548, 0.06617903, 0.0885422 , 0.09777546],\n",
       "       [0.10100143, 0.07676768, 0.12395908, 0.10352695],\n",
       "       [0.09269994, 0.06617903, 0.11412106, 0.10352695],\n",
       "       [0.09961785, 0.09529781, 0.12002387, 0.14378743],\n",
       "       [0.08993278, 0.08470916, 0.10034783, 0.11502995],\n",
       "       [0.0885492 , 0.07147336, 0.10428304, 0.10927845],\n",
       "       [0.09408352, 0.07941484, 0.10821824, 0.12078145],\n",
       "       [0.07886413, 0.06617903, 0.09838022, 0.11502995],\n",
       "       [0.08024771, 0.07412052, 0.10034783, 0.13803594],\n",
       "       [0.0885492 , 0.08470916, 0.10428304, 0.13228444],\n",
       "       [0.08993278, 0.07941484, 0.10821824, 0.10352695],\n",
       "       [0.10653575, 0.10059213, 0.1318295 , 0.12653294],\n",
       "       [0.10653575, 0.06882619, 0.13576471, 0.13228444],\n",
       "       [0.08301487, 0.05823755, 0.09838022, 0.08627246],\n",
       "       [0.0954671 , 0.08470916, 0.11215345, 0.13228444],\n",
       "       [0.07748055, 0.07412052, 0.09641262, 0.11502995],\n",
       "       [0.10653575, 0.07412052, 0.1318295 , 0.11502995],\n",
       "       [0.08716562, 0.07147336, 0.09641262, 0.10352695],\n",
       "       [0.09269994, 0.08735632, 0.11215345, 0.12078145],\n",
       "       [0.09961785, 0.08470916, 0.11805627, 0.10352695],\n",
       "       [0.08578203, 0.07412052, 0.09444501, 0.10352695],\n",
       "       [0.08439845, 0.07941484, 0.09641262, 0.10352695],\n",
       "       [0.0885492 , 0.07412052, 0.11018585, 0.12078145],\n",
       "       [0.09961785, 0.07941484, 0.11412106, 0.09202396],\n",
       "       [0.10238501, 0.07412052, 0.12002387, 0.10927845],\n",
       "       [0.10930291, 0.10059213, 0.12592669, 0.11502995],\n",
       "       [0.0885492 , 0.07412052, 0.11018585, 0.12653294],\n",
       "       [0.08716562, 0.07412052, 0.10034783, 0.08627246],\n",
       "       [0.08439845, 0.06882619, 0.11018585, 0.08052096],\n",
       "       [0.10653575, 0.07941484, 0.12002387, 0.13228444],\n",
       "       [0.08716562, 0.09000348, 0.11018585, 0.13803594],\n",
       "       [0.0885492 , 0.082062  , 0.10821824, 0.10352695],\n",
       "       [0.08301487, 0.07941484, 0.09444501, 0.10352695],\n",
       "       [0.0954671 , 0.082062  , 0.10625064, 0.12078145],\n",
       "       [0.09269994, 0.082062  , 0.11018585, 0.13803594],\n",
       "       [0.0954671 , 0.082062  , 0.10034783, 0.13228444],\n",
       "       [0.08024771, 0.07147336, 0.10034783, 0.10927845],\n",
       "       [0.09408352, 0.08470916, 0.11608866, 0.13228444],\n",
       "       [0.09269994, 0.08735632, 0.11215345, 0.14378743],\n",
       "       [0.09269994, 0.07941484, 0.10231543, 0.13228444],\n",
       "       [0.08716562, 0.06617903, 0.09838022, 0.10927845],\n",
       "       [0.08993278, 0.07941484, 0.10231543, 0.11502995],\n",
       "       [0.08578203, 0.09000348, 0.10625064, 0.13228444],\n",
       "       [0.08163129, 0.07941484, 0.10034783, 0.10352695]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = normalize(X,axis=0)\n",
    "#X = np.c_[X, np.ones(X.shape[0])]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished prepocessing our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X[:130, :], y[:130], X[130:, :], y[130:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56     2\n",
       "104    1\n",
       "69     2\n",
       "55     2\n",
       "132    1\n",
       "      ..\n",
       "71     2\n",
       "106    1\n",
       "14     0\n",
       "92     2\n",
       "102    1\n",
       "Name: Species, Length: 135, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the architecture of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(neuron_no, H):\n",
    "  \"\"\"returns a list containing np arrays of weights at each synpase\n",
    "\n",
    "  Args:\n",
    "      neuron_no (list): List of integers which represent the number of neurons at each layer\n",
    "      H (int): The total depth of the neural network\n",
    "\n",
    "  Returns:\n",
    "      List: a list containing np arrays of weights at each synpase\n",
    "  \"\"\"\n",
    "  assert len(neuron_no)==H+1\n",
    "  W = []\n",
    "  b = []\n",
    "  for i in range(0, H):\n",
    "    W.append(np.random.rand(neuron_no[i]+1, neuron_no[i+1]) -0.5)\n",
    "  #for i in range(1, H+1):\n",
    "  #  b.append(np.random.rand(neuron_no[i],1))\n",
    "  return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the feedforward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Array_Function = Callable[[np.ndarray], np.ndarray]\n",
    "# A Chain is a list of functions\n",
    "Chain = List[Array_Function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPass(chain: Chain, W, input : np.ndarray) ->np.ndarray:\n",
    "  f1 = chain[0] #first activation function\n",
    "  f2 = chain[1]\n",
    "  f3 = chain[2]\n",
    "  z1 = np.dot(W[0].T, input)\n",
    "  a1 = f1(z1)\n",
    "  \n",
    "  a1 = np.r_[a1, [np.ones(a1.shape[1])]]\n",
    "  #print(a1.shape)\n",
    "  z2 = np.matmul(W[1].T, a1)\n",
    "  a2 = f2(z2)\n",
    "  #print(a2.shape)\n",
    "  a2 = np.r_[a2, [np.ones(a2.shape[1])]]\n",
    "  z3 = np.dot(W[2].T, a2)\n",
    "  #print(z3.shape)\n",
    "  o = f3(z3)\n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPassgen(chain: Chain, W, input : np.ndarray, H) -> np.ndarray:\n",
    "    z1 = input\n",
    "    for i in range(0, H):\n",
    "        activation = chain[i]\n",
    "        a1 = activation(np.dot(W[i].T, np.r_[z1, [np.ones(z1.shape[1])]]))\n",
    "        z1 = a1\n",
    "    return z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(target:np.ndarray, predicted) ->float:\n",
    "    \"\"\"Calculates and returns the cross entropy loss given the predicted values \n",
    "        from our model and the actual target values\n",
    "\n",
    "    Args:\n",
    "        target (np.ndarray): one hot encoded target values from the dataset\n",
    "        predicted ([type]): values predicted from our neural network\n",
    "\n",
    "    Returns:\n",
    "        float: The cross entropy loss \n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    #notice that target has 3 values in each row same as predicted!\n",
    "    for i in range(0, len(target)):\n",
    "        loss.append(np.matmul(target[i, :], np.log(1e-15 + predicted[:, i])))#to prevent 0 calculation!\n",
    "    return -np.sum(np.array(loss))/len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator(x : np.ndarray, y: np.ndarray) -> np.ndarray :\n",
    "    \"\"\"Basically calcuates the similarity between 2 arrays - \n",
    "    \n",
    "    spikes only when 2 corresponding values are equal. The sum of its elements would\n",
    "    \n",
    "    give us the number of elements which are equal in the 2 arrays\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): an array\n",
    "        y (np.ndarray): an array\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: similarity array of the same size as x or y with 0's and 1's\n",
    "    \"\"\"\n",
    "    assert len(x)==len(y)\n",
    "    return np.array([1 if x[i]==y[i] else 0 for i in range(0, len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target : np.ndarray, predicted : np.ndarray) -> float:\n",
    "    \"\"\"Returns the accuracy = correct prediction divided by total predictions made\n",
    "\n",
    "    Args:\n",
    "        target (np.ndarray): target values from the dataset\n",
    "        predicted (np.ndarray): the values which are prdicted having the highest probability\n",
    "\n",
    "    Returns:\n",
    "        float: accuracy of the prediction made\n",
    "    \"\"\"\n",
    "    #acc = np.sum(indicator(target, predicted))/len(target)\n",
    "    acc = 1.0/len(target)*np.sum(target == predicted)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y:np.ndarray, len_hot) -> np.ndarray:\n",
    "  shape = (y.size, len_hot)\n",
    "  one_hot_encode = np.zeros(shape)\n",
    "  row_index = np.arange(y.size)\n",
    "  one_hot_encode[row_index, y] = 1\n",
    "  return one_hot_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x : np.ndarray) -> np.ndarray:\n",
    "  return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x : np.ndarray) -> np.ndarray:\n",
    "  e_x = np.exp(x - np.max(x))\n",
    "  return e_x/e_x.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the perturbation and training functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyPert(W:List, pert, k, i,j):\n",
    "  W[k][i, j] +=pert\n",
    "  return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradients(W, pert, X_train,y_train):#UPDATE!!\n",
    "  gradients = initialise([4, 3, 5, 3], 3)\n",
    "  for k in range(0,len(W)):\n",
    "    for i in range(0, W[k].shape[0]):\n",
    "      for j in range(0, W[k].shape[1]):\n",
    "        Dw = ApplyPert(W, pert, k, i, j)\n",
    "        h_calc = ForwardPass(chain, Dw, np.array(X_train).T)\n",
    "        loss2 = cross_entropy(one_hot(np.array(y_train)), h_calc)\n",
    "        grad = (loss2 - loss1)/pert\n",
    "        gradients[k][i, j] = grad\n",
    "  return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_my_model(N, H, epochs:int, chain:Chain, X_train:np.ndarray, y_train, X_test:np.ndarray, y_test, pert, lr, reg_lam):\n",
    "  W = initialise(N, H)\n",
    "  loss_arr = []\n",
    "  loss_test_arr = []\n",
    "  acc_arr = []\n",
    "  acc_test_arr = []\n",
    "  for i in range(0, epochs):\n",
    "    print(\"Training EPoch ---- \", i, \" -----\")\n",
    "    output = ForwardPassgen(chain, W, np.array(X_train).T, H)\n",
    "    loss = cross_entropy(one_hot(np.array(y_train), output.shape[0]), output) + (reg_lam/(2*len(y_train)))*(np.sum(np.array([np.sum(W[i]**2) for i in range(0, H)])))\n",
    "    loss_arr.append(loss)\n",
    "    #acc_arr.append(accuracy(np.array(y_train) ,output.argsort(axis = 0)[-1, :]))\n",
    "    acc_arr.append(accuracy(np.array(y_train) ,np.argmax(output, axis = 0)))\n",
    "    output_test = ForwardPassgen(chain, W, np.array(X_test).T, H)\n",
    "    loss_test_arr.append(cross_entropy(one_hot(np.array(y_test), output.shape[0]), output_test))\n",
    "    acc_test_arr.append(accuracy(np.array(y_test) ,output_test.argsort(axis = 0)[-1, :]))\n",
    "    print(\"Loss epoch \", i, \" is = \", loss)\n",
    "    print(\"---------------------------------\")\n",
    "    gradients = initialise(N, H)\n",
    "    for k in range(0,len(W)):\n",
    "      print(\"Started \", k, \" -------------\")\n",
    "      for i in range(0, W[k].shape[0]):\n",
    "        for j in range(0, W[k].shape[1]):\n",
    "          Dw = ApplyPert(W, pert, k, i, j)\n",
    "          output_pert = ForwardPassgen(chain, Dw, np.array(X_train).T, H)\n",
    "          loss_pert = cross_entropy(one_hot(np.array(y_train), output.shape[0]), output_pert)\n",
    "          grad = (loss_pert - loss)/pert\n",
    "          gradients[k][i, j] = grad\n",
    "    for i in range(0, len(W)):\n",
    "      W[i] += (-lr*gradients[i])\n",
    "      #W[i] += (lr*gradients[i])\n",
    "  return loss_arr, acc_arr, acc_test_arr, W, output, loss_test_arr, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-aa64fad0ad5b>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-aa64fad0ad5b>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    results = [pool_obj.apply(calc_grad_par, args = () for g in list(permutation(range)))]\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from intertools import permutations\n",
    "def train_my_model_par(N, H, epochs:int, chain:Chain, X_train:np.ndarray, y_train, X_test:np.ndarray, y_test, pert, lr):\n",
    "  W = initialise(N, H)\n",
    "  loss_arr = []\n",
    "  loss_test_arr = []\n",
    "  acc_arr = []\n",
    "  acc_test_arr = []\n",
    "  for i in range(0, epochs):\n",
    "    print(\"Training EPoch ---- \", i, \" -----\")\n",
    "    output = ForwardPassgen(chain, W, np.array(X_train).T, H)\n",
    "    loss = cross_entropy(one_hot(np.array(y_train)), output)\n",
    "    loss_arr.append(loss)\n",
    "    #acc_arr.append(accuracy(np.array(y_train) ,output.argsort(axis = 0)[-1, :]))\n",
    "    acc_arr.append(accuracy(np.array(y_train) ,np.argmax(output, axis = 0)))\n",
    "    output_test = ForwardPassgen(chain, W, np.array(X_test).T, H)\n",
    "    loss_test_arr.append(cross_entropy(one_hot(np.array(y_test)), output_test))\n",
    "    acc_test_arr.append(accuracy(np.array(y_test) ,output_test.argsort(axis = 0)[-1, :]))\n",
    "    print(\"Loss epoch \", i, \" is = \", loss)\n",
    "    print(\"---------------------------------\")\n",
    "    gradients = initialise(N, H)\n",
    "    pool_obj = mp.Pool(mp.cpu_count)\n",
    "    results = [pool_obj.apply(calc_grad_par, args = () for g in list(permutation(range)))]\n",
    "    pool_obj.close\n",
    "    #for k in range(0,len(W)):\n",
    "    #  print(\"Started \", k, \" -------------\")\n",
    "    #  for i in range(0, W[k].shape[0]):\n",
    "    #    for j in range(0, W[k].shape[1]):\n",
    "    #      Dw = ApplyPert(W, pert, k, i, j)\n",
    "    #      output_pert = ForwardPassgen(chain, Dw, np.array(X_train).T, H)\n",
    "    #      loss_pert = cross_entropy(one_hot(np.array(y_train)), output_pert)\n",
    "    #      grad = (loss_pert - loss)/pert\n",
    "    #      gradients[k][i, j] = grad\n",
    "    for i in range(0, len(W)):\n",
    "      W[i] += (-lr*gradients[i])\n",
    "      #W[i] += (lr*gradients[i])\n",
    "  return loss_arr, acc_arr, acc_test_arr, W, output, loss_test_arr, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_par(W, chain:Chain, X_train:np.ndarray, y_train, X_test:np.ndarray, y_test, pert, lr, g, loss):\n",
    "#g = (k, i, j)\n",
    "    Dw = ApplyPert(W, pert, g[0], g[1], g[2])\n",
    "    output_pert = ForwardPassgen(chain, Dw, np.array(X_train).T, H)\n",
    "    loss_pert = cross_entropy(one_hot(np.array(y_train)), output_pert)\n",
    "    grad = (loss_pert - loss)/pert\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [4, 3]\n",
    "H = len(N)-1\n",
    "W = initialise(N, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = []\n",
    "for i in range(0, H):\n",
    "    W.append(np.random.rand(N[i]+1, N[i+1]) -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.37534339,  0.18345312,  0.20430936],\n",
       "        [ 0.00968697, -0.09716409,  0.13818898],\n",
       "        [-0.32083148,  0.31482756,  0.01250293],\n",
       "        [ 0.45968579, -0.39265603, -0.10482939],\n",
       "        [-0.41879098,  0.31141348, -0.29071511]])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9974938351592753"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([np.sum(W[i]**2) for i in range(0, H)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = [softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 0.005\n",
    "delta = 0.0001\n",
    "reg_lam = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EPoch ----  0  -----\n",
      "Loss epoch  0  is =  1.1757577129425258\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  1  -----\n",
      "Loss epoch  1  is =  1.175634581922145\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  2  -----\n",
      "Loss epoch  2  is =  1.1755116886041428\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  3  -----\n",
      "Loss epoch  3  is =  1.1753890325011556\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  4  -----\n",
      "Loss epoch  4  is =  1.1752666131266827\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  5  -----\n",
      "Loss epoch  5  is =  1.1751444299950955\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  6  -----\n",
      "Loss epoch  6  is =  1.1750224826216276\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  7  -----\n",
      "Loss epoch  7  is =  1.1749007705223729\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  8  -----\n",
      "Loss epoch  8  is =  1.1747792932143006\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  9  -----\n",
      "Loss epoch  9  is =  1.1746580502152337\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  10  -----\n",
      "Loss epoch  10  is =  1.1745370410438607\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  11  -----\n",
      "Loss epoch  11  is =  1.1744162652197359\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  12  -----\n",
      "Loss epoch  12  is =  1.1742957222632713\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  13  -----\n",
      "Loss epoch  13  is =  1.1741754116957435\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  14  -----\n",
      "Loss epoch  14  is =  1.1740553330392842\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  15  -----\n",
      "Loss epoch  15  is =  1.1739354858168944\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  16  -----\n",
      "Loss epoch  16  is =  1.1738158695524266\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  17  -----\n",
      "Loss epoch  17  is =  1.17369648377059\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  18  -----\n",
      "Loss epoch  18  is =  1.173577327996964\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  19  -----\n",
      "Loss epoch  19  is =  1.1734584017579726\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  20  -----\n",
      "Loss epoch  20  is =  1.173339704580901\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  21  -----\n",
      "Loss epoch  21  is =  1.1732212359938945\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  22  -----\n",
      "Loss epoch  22  is =  1.1731029955259489\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  23  -----\n",
      "Loss epoch  23  is =  1.172984982706917\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  24  -----\n",
      "Loss epoch  24  is =  1.1728671970675033\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  25  -----\n",
      "Loss epoch  25  is =  1.1727496381392708\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  26  -----\n",
      "Loss epoch  26  is =  1.1726323054546324\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  27  -----\n",
      "Loss epoch  27  is =  1.172515198546849\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  28  -----\n",
      "Loss epoch  28  is =  1.1723983169500425\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  29  -----\n",
      "Loss epoch  29  is =  1.1722816601991775\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  30  -----\n",
      "Loss epoch  30  is =  1.17216522783007\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  31  -----\n",
      "Loss epoch  31  is =  1.1720490193793867\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  32  -----\n",
      "Loss epoch  32  is =  1.1719330343846457\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  33  -----\n",
      "Loss epoch  33  is =  1.1718172723842069\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  34  -----\n",
      "Loss epoch  34  is =  1.1717017329172874\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  35  -----\n",
      "Loss epoch  35  is =  1.1715864155239366\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  36  -----\n",
      "Loss epoch  36  is =  1.171471319745062\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  37  -----\n",
      "Loss epoch  37  is =  1.1713564451224097\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  38  -----\n",
      "Loss epoch  38  is =  1.1712417911985777\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  39  -----\n",
      "Loss epoch  39  is =  1.1711273575169956\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  40  -----\n",
      "Loss epoch  40  is =  1.1710131436219493\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  41  -----\n",
      "Loss epoch  41  is =  1.1708991490585583\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  42  -----\n",
      "Loss epoch  42  is =  1.1707853733727855\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  43  -----\n",
      "Loss epoch  43  is =  1.1706718161114373\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  44  -----\n",
      "Loss epoch  44  is =  1.1705584768221609\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  45  -----\n",
      "Loss epoch  45  is =  1.1704453550534317\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  46  -----\n",
      "Loss epoch  46  is =  1.1703324503545727\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  47  -----\n",
      "Loss epoch  47  is =  1.1702197622757466\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  48  -----\n",
      "Loss epoch  48  is =  1.1701072903679577\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  49  -----\n",
      "Loss epoch  49  is =  1.1699950341830325\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  50  -----\n",
      "Loss epoch  50  is =  1.1698829932736368\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  51  -----\n",
      "Loss epoch  51  is =  1.1697711671932831\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  52  -----\n",
      "Loss epoch  52  is =  1.1696595554963063\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  53  -----\n",
      "Loss epoch  53  is =  1.1695481577378746\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  54  -----\n",
      "Loss epoch  54  is =  1.1694369734739944\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  55  -----\n",
      "Loss epoch  55  is =  1.1693260022615004\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  56  -----\n",
      "Loss epoch  56  is =  1.169215243658058\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  57  -----\n",
      "Loss epoch  57  is =  1.1691046972221648\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  58  -----\n",
      "Loss epoch  58  is =  1.168994362513148\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  59  -----\n",
      "Loss epoch  59  is =  1.168884239091158\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  60  -----\n",
      "Loss epoch  60  is =  1.1687743265171786\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  61  -----\n",
      "Loss epoch  61  is =  1.168664624353021\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  62  -----\n",
      "Loss epoch  62  is =  1.1685551321613163\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  63  -----\n",
      "Loss epoch  63  is =  1.1684458495055245\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  64  -----\n",
      "Loss epoch  64  is =  1.1683367759499357\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  65  -----\n",
      "Loss epoch  65  is =  1.1682279110596545\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  66  -----\n",
      "Loss epoch  66  is =  1.168119254400615\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  67  -----\n",
      "Loss epoch  67  is =  1.168010805539569\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  68  -----\n",
      "Loss epoch  68  is =  1.1679025640440903\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  69  -----\n",
      "Loss epoch  69  is =  1.1677945294825796\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  70  -----\n",
      "Loss epoch  70  is =  1.1676867014242451\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  71  -----\n",
      "Loss epoch  71  is =  1.167579079439125\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  72  -----\n",
      "Loss epoch  72  is =  1.1674716630980657\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  73  -----\n",
      "Loss epoch  73  is =  1.1673644519727442\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  74  -----\n",
      "Loss epoch  74  is =  1.1672574456356413\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  75  -----\n",
      "Loss epoch  75  is =  1.1671506436600574\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  76  -----\n",
      "Loss epoch  76  is =  1.1670440456201077\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  77  -----\n",
      "Loss epoch  77  is =  1.1669376510907215\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  78  -----\n",
      "Loss epoch  78  is =  1.1668314596476383\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  79  -----\n",
      "Loss epoch  79  is =  1.1667254708674157\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  80  -----\n",
      "Loss epoch  80  is =  1.1666196843274173\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  81  -----\n",
      "Loss epoch  81  is =  1.1665140996058128\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  82  -----\n",
      "Loss epoch  82  is =  1.1664087162815926\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  83  -----\n",
      "Loss epoch  83  is =  1.1663035339345493\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  84  -----\n",
      "Loss epoch  84  is =  1.1661985521452873\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  85  -----\n",
      "Loss epoch  85  is =  1.1660937704952055\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  86  -----\n",
      "Loss epoch  86  is =  1.1659891885665226\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  87  -----\n",
      "Loss epoch  87  is =  1.1658848059422582\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  88  -----\n",
      "Loss epoch  88  is =  1.1657806222062321\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  89  -----\n",
      "Loss epoch  89  is =  1.165676636943072\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  90  -----\n",
      "Loss epoch  90  is =  1.1655728497382103\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  91  -----\n",
      "Loss epoch  91  is =  1.165469260177873\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  92  -----\n",
      "Loss epoch  92  is =  1.1653658678490928\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  93  -----\n",
      "Loss epoch  93  is =  1.1652626723397002\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  94  -----\n",
      "Loss epoch  94  is =  1.165159673238326\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  95  -----\n",
      "Loss epoch  95  is =  1.1650568701343937\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  96  -----\n",
      "Loss epoch  96  is =  1.164954262618132\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  97  -----\n",
      "Loss epoch  97  is =  1.1648518502805607\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  98  -----\n",
      "Loss epoch  98  is =  1.1647496327134939\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  99  -----\n",
      "Loss epoch  99  is =  1.1646476095095424\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  100  -----\n",
      "Loss epoch  100  is =  1.1645457802621129\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  101  -----\n",
      "Loss epoch  101  is =  1.1644441445654\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  102  -----\n",
      "Loss epoch  102  is =  1.164342702014395\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  103  -----\n",
      "Loss epoch  103  is =  1.164241452204871\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  104  -----\n",
      "Loss epoch  104  is =  1.1641403947334008\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  105  -----\n",
      "Loss epoch  105  is =  1.1640395291973402\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  106  -----\n",
      "Loss epoch  106  is =  1.163938855194836\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  107  -----\n",
      "Loss epoch  107  is =  1.1638383723248196\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  108  -----\n",
      "Loss epoch  108  is =  1.1637380801870083\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  109  -----\n",
      "Loss epoch  109  is =  1.1636379783819069\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  110  -----\n",
      "Loss epoch  110  is =  1.1635380665108068\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  111  -----\n",
      "Loss epoch  111  is =  1.1634383441757712\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  112  -----\n",
      "Loss epoch  112  is =  1.1633388109796603\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  113  -----\n",
      "Loss epoch  113  is =  1.1632394665261108\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  114  -----\n",
      "Loss epoch  114  is =  1.163140310419536\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  115  -----\n",
      "Loss epoch  115  is =  1.1630413422651271\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  116  -----\n",
      "Loss epoch  116  is =  1.1629425616688627\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  117  -----\n",
      "Loss epoch  117  is =  1.1628439682374891\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  118  -----\n",
      "Loss epoch  118  is =  1.1627455615785374\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  119  -----\n",
      "Loss epoch  119  is =  1.1626473413003102\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  120  -----\n",
      "Loss epoch  120  is =  1.1625493070118895\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  121  -----\n",
      "Loss epoch  121  is =  1.1624514583231247\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  122  -----\n",
      "Loss epoch  122  is =  1.1623537948446399\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  123  -----\n",
      "Loss epoch  123  is =  1.1622563161878359\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  124  -----\n",
      "Loss epoch  124  is =  1.162159021964876\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  125  -----\n",
      "Loss epoch  125  is =  1.1620619117887026\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  126  -----\n",
      "Loss epoch  126  is =  1.161964985273023\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  127  -----\n",
      "Loss epoch  127  is =  1.1618682420323065\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  128  -----\n",
      "Loss epoch  128  is =  1.1617716816818038\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  129  -----\n",
      "Loss epoch  129  is =  1.1616753038375152\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  130  -----\n",
      "Loss epoch  130  is =  1.161579108116217\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  131  -----\n",
      "Loss epoch  131  is =  1.1614830941354477\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  132  -----\n",
      "Loss epoch  132  is =  1.16138726151351\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  133  -----\n",
      "Loss epoch  133  is =  1.1612916098694652\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  134  -----\n",
      "Loss epoch  134  is =  1.161196138823138\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  135  -----\n",
      "Loss epoch  135  is =  1.161100847995112\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  136  -----\n",
      "Loss epoch  136  is =  1.1610057370067335\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  137  -----\n",
      "Loss epoch  137  is =  1.160910805480103\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  138  -----\n",
      "Loss epoch  138  is =  1.1608160530380804\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  139  -----\n",
      "Loss epoch  139  is =  1.1607214793042775\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  140  -----\n",
      "Loss epoch  140  is =  1.1606270839030701\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  141  -----\n",
      "Loss epoch  141  is =  1.1605328664595818\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  142  -----\n",
      "Loss epoch  142  is =  1.1604388265996906\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  143  -----\n",
      "Loss epoch  143  is =  1.1603449639500298\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  144  -----\n",
      "Loss epoch  144  is =  1.160251278137983\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  145  -----\n",
      "Loss epoch  145  is =  1.1601577687916835\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  146  -----\n",
      "Loss epoch  146  is =  1.1600644355400112\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  147  -----\n",
      "Loss epoch  147  is =  1.1599712780125964\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  148  -----\n",
      "Loss epoch  148  is =  1.159878295839815\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  149  -----\n",
      "Loss epoch  149  is =  1.1597854886527945\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  150  -----\n",
      "Loss epoch  150  is =  1.1596928560834046\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  151  -----\n",
      "Loss epoch  151  is =  1.1596003977642537\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  152  -----\n",
      "Loss epoch  152  is =  1.1595081133287053\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  153  -----\n",
      "Loss epoch  153  is =  1.159416002410858\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  154  -----\n",
      "Loss epoch  154  is =  1.1593240646455514\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  155  -----\n",
      "Loss epoch  155  is =  1.1592322996683666\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  156  -----\n",
      "Loss epoch  156  is =  1.1591407071156241\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  157  -----\n",
      "Loss epoch  157  is =  1.1590492866243851\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  158  -----\n",
      "Loss epoch  158  is =  1.1589580378324444\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  159  -----\n",
      "Loss epoch  159  is =  1.1588669603783317\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  160  -----\n",
      "Loss epoch  160  is =  1.1587760539013157\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  161  -----\n",
      "Loss epoch  161  is =  1.1586853180413943\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  162  -----\n",
      "Loss epoch  162  is =  1.1585947524393085\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  163  -----\n",
      "Loss epoch  163  is =  1.1585043567365232\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  164  -----\n",
      "Loss epoch  164  is =  1.1584141305752367\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  165  -----\n",
      "Loss epoch  165  is =  1.1583240735983729\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  166  -----\n",
      "Loss epoch  166  is =  1.15823418544959\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  167  -----\n",
      "Loss epoch  167  is =  1.1581444657732771\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  168  -----\n",
      "Loss epoch  168  is =  1.158054914214547\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  169  -----\n",
      "Loss epoch  169  is =  1.157965530419234\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  170  -----\n",
      "Loss epoch  170  is =  1.1578763140339037\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  171  -----\n",
      "Loss epoch  171  is =  1.1577872647058434\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  172  -----\n",
      "Loss epoch  172  is =  1.1576983820830582\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  173  -----\n",
      "Loss epoch  173  is =  1.1576096658142851\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  174  -----\n",
      "Loss epoch  174  is =  1.1575211155489784\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  175  -----\n",
      "Loss epoch  175  is =  1.157432730937305\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  176  -----\n",
      "Loss epoch  176  is =  1.1573445116301608\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  177  -----\n",
      "Loss epoch  177  is =  1.1572564572791502\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  178  -----\n",
      "Loss epoch  178  is =  1.157168567536603\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  179  -----\n",
      "Loss epoch  179  is =  1.157080842055557\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  180  -----\n",
      "Loss epoch  180  is =  1.156993280489771\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  181  -----\n",
      "Loss epoch  181  is =  1.1569058824937115\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  182  -----\n",
      "Loss epoch  182  is =  1.1568186477225615\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  183  -----\n",
      "Loss epoch  183  is =  1.1567315758322156\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  184  -----\n",
      "Loss epoch  184  is =  1.156644666479275\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  185  -----\n",
      "Loss epoch  185  is =  1.1565579193210553\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  186  -----\n",
      "Loss epoch  186  is =  1.1564713340155766\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  187  -----\n",
      "Loss epoch  187  is =  1.1563849102215669\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  188  -----\n",
      "Loss epoch  188  is =  1.1562986475984591\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  189  -----\n",
      "Loss epoch  189  is =  1.1562125458063948\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  190  -----\n",
      "Loss epoch  190  is =  1.1561266045062195\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  191  -----\n",
      "Loss epoch  191  is =  1.156040823359479\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  192  -----\n",
      "Loss epoch  192  is =  1.1559552020284247\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  193  -----\n",
      "Loss epoch  193  is =  1.1558697401760072\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  194  -----\n",
      "Loss epoch  194  is =  1.1557844374658746\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  195  -----\n",
      "Loss epoch  195  is =  1.1556992935623744\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  196  -----\n",
      "Loss epoch  196  is =  1.1556143081305599\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  197  -----\n",
      "Loss epoch  197  is =  1.1555294808361671\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  198  -----\n",
      "Loss epoch  198  is =  1.1554448113456397\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  199  -----\n",
      "Loss epoch  199  is =  1.155360299326108\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  200  -----\n",
      "Loss epoch  200  is =  1.155275944445399\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  201  -----\n",
      "Loss epoch  201  is =  1.155191746372036\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  202  -----\n",
      "Loss epoch  202  is =  1.155107704775229\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  203  -----\n",
      "Loss epoch  203  is =  1.1550238193248783\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  204  -----\n",
      "Loss epoch  204  is =  1.1549400896915785\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  205  -----\n",
      "Loss epoch  205  is =  1.1548565155466084\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  206  -----\n",
      "Loss epoch  206  is =  1.1547730965619307\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  207  -----\n",
      "Loss epoch  207  is =  1.154689832410202\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  208  -----\n",
      "Loss epoch  208  is =  1.1546067227647592\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  209  -----\n",
      "Loss epoch  209  is =  1.1545237672996251\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  210  -----\n",
      "Loss epoch  210  is =  1.1544409656895027\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  211  -----\n",
      "Loss epoch  211  is =  1.1543583176097807\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  212  -----\n",
      "Loss epoch  212  is =  1.1542758227365257\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  213  -----\n",
      "Loss epoch  213  is =  1.154193480746485\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  214  -----\n",
      "Loss epoch  214  is =  1.1541112913170848\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  215  -----\n",
      "Loss epoch  215  is =  1.1540292541264277\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  216  -----\n",
      "Loss epoch  216  is =  1.1539473688532969\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  217  -----\n",
      "Loss epoch  217  is =  1.15386563517715\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  218  -----\n",
      "Loss epoch  218  is =  1.1537840527781105\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  219  -----\n",
      "Loss epoch  219  is =  1.1537026213369859\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  220  -----\n",
      "Loss epoch  220  is =  1.1536213405352498\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  221  -----\n",
      "Loss epoch  221  is =  1.1535402100550516\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  222  -----\n",
      "Loss epoch  222  is =  1.15345922957921\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  223  -----\n",
      "Loss epoch  223  is =  1.1533783987912074\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  224  -----\n",
      "Loss epoch  224  is =  1.1532977173752008\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  225  -----\n",
      "Loss epoch  225  is =  1.153217185016013\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  226  -----\n",
      "Loss epoch  226  is =  1.1531368013991274\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  227  -----\n",
      "Loss epoch  227  is =  1.1530565662106995\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  228  -----\n",
      "Loss epoch  228  is =  1.1529764791375448\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  229  -----\n",
      "Loss epoch  229  is =  1.1528965398671414\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  230  -----\n",
      "Loss epoch  230  is =  1.1528167480876292\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  231  -----\n",
      "Loss epoch  231  is =  1.1527371034878093\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  232  -----\n",
      "Loss epoch  232  is =  1.1526576057571436\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  233  -----\n",
      "Loss epoch  233  is =  1.1525782545857473\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  234  -----\n",
      "Loss epoch  234  is =  1.1524990496643956\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  235  -----\n",
      "Loss epoch  235  is =  1.152419990684523\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  236  -----\n",
      "Loss epoch  236  is =  1.1523410773382172\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  237  -----\n",
      "Loss epoch  237  is =  1.152262309318219\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  238  -----\n",
      "Loss epoch  238  is =  1.1521836863179211\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  239  -----\n",
      "Loss epoch  239  is =  1.1521052080313692\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  240  -----\n",
      "Loss epoch  240  is =  1.152026874153263\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  241  -----\n",
      "Loss epoch  241  is =  1.1519486843789448\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  242  -----\n",
      "Loss epoch  242  is =  1.1518706384044177\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  243  -----\n",
      "Loss epoch  243  is =  1.1517927359263203\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  244  -----\n",
      "Loss epoch  244  is =  1.1517149766419421\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  245  -----\n",
      "Loss epoch  245  is =  1.1516373602492238\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  246  -----\n",
      "Loss epoch  246  is =  1.1515598864467418\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  247  -----\n",
      "Loss epoch  247  is =  1.1514825549337173\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  248  -----\n",
      "Loss epoch  248  is =  1.151405365410019\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  249  -----\n",
      "Loss epoch  249  is =  1.151328317576153\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  250  -----\n",
      "Loss epoch  250  is =  1.1512514111332628\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  251  -----\n",
      "Loss epoch  251  is =  1.1511746457831322\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  252  -----\n",
      "Loss epoch  252  is =  1.1510980212281912\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  253  -----\n",
      "Loss epoch  253  is =  1.1510215371714994\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  254  -----\n",
      "Loss epoch  254  is =  1.150945193316755\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  255  -----\n",
      "Loss epoch  255  is =  1.150868989368284\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  256  -----\n",
      "Loss epoch  256  is =  1.150792925031054\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  257  -----\n",
      "Loss epoch  257  is =  1.1507170000106617\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  258  -----\n",
      "Loss epoch  258  is =  1.1506412140133397\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  259  -----\n",
      "Loss epoch  259  is =  1.1505655667459467\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  260  -----\n",
      "Loss epoch  260  is =  1.150490057915967\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  261  -----\n",
      "Loss epoch  261  is =  1.1504146872315166\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  262  -----\n",
      "Loss epoch  262  is =  1.1503394544013474\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  263  -----\n",
      "Loss epoch  263  is =  1.1502643591348263\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  264  -----\n",
      "Loss epoch  264  is =  1.1501894011419527\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  265  -----\n",
      "Loss epoch  265  is =  1.1501145801333403\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  266  -----\n",
      "Loss epoch  266  is =  1.150039895820235\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  267  -----\n",
      "Loss epoch  267  is =  1.1499653479145016\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  268  -----\n",
      "Loss epoch  268  is =  1.1498909361286234\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  269  -----\n",
      "Loss epoch  269  is =  1.1498166601757083\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  270  -----\n",
      "Loss epoch  270  is =  1.1497425197694773\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  271  -----\n",
      "Loss epoch  271  is =  1.149668514624272\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  272  -----\n",
      "Loss epoch  272  is =  1.1495946444550504\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  273  -----\n",
      "Loss epoch  273  is =  1.1495209089773826\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  274  -----\n",
      "Loss epoch  274  is =  1.1494473079074603\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  275  -----\n",
      "Loss epoch  275  is =  1.1493738409620806\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  276  -----\n",
      "Loss epoch  276  is =  1.1493005078586613\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  277  -----\n",
      "Loss epoch  277  is =  1.149227308315225\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  278  -----\n",
      "Loss epoch  278  is =  1.1491542420504044\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  279  -----\n",
      "Loss epoch  279  is =  1.1490813087834422\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  280  -----\n",
      "Loss epoch  280  is =  1.1490085082341883\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  281  -----\n",
      "Loss epoch  281  is =  1.1489358401231011\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  282  -----\n",
      "Loss epoch  282  is =  1.1488633041712473\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  283  -----\n",
      "Loss epoch  283  is =  1.1487909001002943\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  284  -----\n",
      "Loss epoch  284  is =  1.1487186276325125\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  285  -----\n",
      "Loss epoch  285  is =  1.1486464864907742\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  286  -----\n",
      "Loss epoch  286  is =  1.148574476398558\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  287  -----\n",
      "Loss epoch  287  is =  1.1485025970799434\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  288  -----\n",
      "Loss epoch  288  is =  1.148430848259602\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  289  -----\n",
      "Loss epoch  289  is =  1.148359229662809\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  290  -----\n",
      "Loss epoch  290  is =  1.1482877410154344\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  291  -----\n",
      "Loss epoch  291  is =  1.1482163820439468\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  292  -----\n",
      "Loss epoch  292  is =  1.1481451524754065\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  293  -----\n",
      "Loss epoch  293  is =  1.148074052037476\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  294  -----\n",
      "Loss epoch  294  is =  1.1480030804583985\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  295  -----\n",
      "Loss epoch  295  is =  1.1479322374670173\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  296  -----\n",
      "Loss epoch  296  is =  1.1478615227927655\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  297  -----\n",
      "Loss epoch  297  is =  1.1477909361656617\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  298  -----\n",
      "Loss epoch  298  is =  1.1477204773163197\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  299  -----\n",
      "Loss epoch  299  is =  1.147650145975937\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  300  -----\n",
      "Loss epoch  300  is =  1.1475799418762989\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  301  -----\n",
      "Loss epoch  301  is =  1.1475098647497737\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  302  -----\n",
      "Loss epoch  302  is =  1.1474399143293146\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  303  -----\n",
      "Loss epoch  303  is =  1.1473700903484643\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  304  -----\n",
      "Loss epoch  304  is =  1.1473003925413392\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  305  -----\n",
      "Loss epoch  305  is =  1.1472308206426443\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  306  -----\n",
      "Loss epoch  306  is =  1.147161374387659\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  307  -----\n",
      "Loss epoch  307  is =  1.1470920535122433\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  308  -----\n",
      "Loss epoch  308  is =  1.147022857752837\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  309  -----\n",
      "Loss epoch  309  is =  1.1469537868464577\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  310  -----\n",
      "Loss epoch  310  is =  1.1468848405306973\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  311  -----\n",
      "Loss epoch  311  is =  1.146816018543718\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  312  -----\n",
      "Loss epoch  312  is =  1.1467473206242649\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  313  -----\n",
      "Loss epoch  313  is =  1.1466787465116512\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  314  -----\n",
      "Loss epoch  314  is =  1.1466102959457602\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  315  -----\n",
      "Loss epoch  315  is =  1.1465419686670477\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  316  -----\n",
      "Loss epoch  316  is =  1.1464737644165395\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  317  -----\n",
      "Loss epoch  317  is =  1.1464056829358265\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  318  -----\n",
      "Loss epoch  318  is =  1.1463377239670751\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  319  -----\n",
      "Loss epoch  319  is =  1.1462698872530064\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  320  -----\n",
      "Loss epoch  320  is =  1.1462021725369198\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  321  -----\n",
      "Loss epoch  321  is =  1.1461345795626712\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  322  -----\n",
      "Loss epoch  322  is =  1.146067108074679\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  323  -----\n",
      "Loss epoch  323  is =  1.1459997578179275\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  324  -----\n",
      "Loss epoch  324  is =  1.1459325285379602\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  325  -----\n",
      "Loss epoch  325  is =  1.145865419980879\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  326  -----\n",
      "Loss epoch  326  is =  1.1457984318933507\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  327  -----\n",
      "Loss epoch  327  is =  1.1457315640225945\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  328  -----\n",
      "Loss epoch  328  is =  1.1456648161163874\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  329  -----\n",
      "Loss epoch  329  is =  1.1455981879230681\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  330  -----\n",
      "Loss epoch  330  is =  1.145531679191522\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  331  -----\n",
      "Loss epoch  331  is =  1.1454652896711919\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  332  -----\n",
      "Loss epoch  332  is =  1.1453990191120735\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  333  -----\n",
      "Loss epoch  333  is =  1.1453328672647165\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  334  -----\n",
      "Loss epoch  334  is =  1.1452668338802179\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  335  -----\n",
      "Loss epoch  335  is =  1.1452009187102232\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  336  -----\n",
      "Loss epoch  336  is =  1.1451351215069294\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  337  -----\n",
      "Loss epoch  337  is =  1.1450694420230805\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  338  -----\n",
      "Loss epoch  338  is =  1.1450038800119675\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  339  -----\n",
      "Loss epoch  339  is =  1.144938435227426\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  340  -----\n",
      "Loss epoch  340  is =  1.1448731074238385\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  341  -----\n",
      "Loss epoch  341  is =  1.1448078963561257\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  342  -----\n",
      "Loss epoch  342  is =  1.1447428017797507\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  343  -----\n",
      "Loss epoch  343  is =  1.1446778234507244\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  344  -----\n",
      "Loss epoch  344  is =  1.1446129611255942\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  345  -----\n",
      "Loss epoch  345  is =  1.144548214561446\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  346  -----\n",
      "Loss epoch  346  is =  1.1444835835159037\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  347  -----\n",
      "Loss epoch  347  is =  1.1444190677471344\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  348  -----\n",
      "Loss epoch  348  is =  1.1443546670138298\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  349  -----\n",
      "Loss epoch  349  is =  1.1442903810752294\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  350  -----\n",
      "Loss epoch  350  is =  1.1442262096910951\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  351  -----\n",
      "Loss epoch  351  is =  1.1441621526217318\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  352  -----\n",
      "Loss epoch  352  is =  1.1440982096279708\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  353  -----\n",
      "Loss epoch  353  is =  1.1440343804711752\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  354  -----\n",
      "Loss epoch  354  is =  1.1439706649132402\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  355  -----\n",
      "Loss epoch  355  is =  1.1439070627165864\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  356  -----\n",
      "Loss epoch  356  is =  1.143843573644164\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  357  -----\n",
      "Loss epoch  357  is =  1.1437801974594493\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  358  -----\n",
      "Loss epoch  358  is =  1.1437169339264455\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  359  -----\n",
      "Loss epoch  359  is =  1.1436537828096829\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  360  -----\n",
      "Loss epoch  360  is =  1.143590743874212\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  361  -----\n",
      "Loss epoch  361  is =  1.143527816885609\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  362  -----\n",
      "Loss epoch  362  is =  1.1434650016099668\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  363  -----\n",
      "Loss epoch  363  is =  1.143402297813902\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  364  -----\n",
      "Loss epoch  364  is =  1.1433397052645529\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  365  -----\n",
      "Loss epoch  365  is =  1.1432772237295763\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  366  -----\n",
      "Loss epoch  366  is =  1.1432148529771418\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  367  -----\n",
      "Loss epoch  367  is =  1.14315259277594\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  368  -----\n",
      "Loss epoch  368  is =  1.1430904428951758\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  369  -----\n",
      "Loss epoch  369  is =  1.1430284031045712\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  370  -----\n",
      "Loss epoch  370  is =  1.1429664731743563\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  371  -----\n",
      "Loss epoch  371  is =  1.1429046528752793\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  372  -----\n",
      "Loss epoch  372  is =  1.1428429419786008\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  373  -----\n",
      "Loss epoch  373  is =  1.1427813402560814\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  374  -----\n",
      "Loss epoch  374  is =  1.1427198474800042\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  375  -----\n",
      "Loss epoch  375  is =  1.142658463423155\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  376  -----\n",
      "Loss epoch  376  is =  1.1425971878588237\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  377  -----\n",
      "Loss epoch  377  is =  1.1425360205608164\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  378  -----\n",
      "Loss epoch  378  is =  1.1424749613034348\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  379  -----\n",
      "Loss epoch  379  is =  1.1424140098614908\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  380  -----\n",
      "Loss epoch  380  is =  1.1423531660102961\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  381  -----\n",
      "Loss epoch  381  is =  1.1422924295256698\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  382  -----\n",
      "Loss epoch  382  is =  1.1422318001839293\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  383  -----\n",
      "Loss epoch  383  is =  1.14217127776189\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  384  -----\n",
      "Loss epoch  384  is =  1.1421108620368714\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  385  -----\n",
      "Loss epoch  385  is =  1.1420505527866918\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  386  -----\n",
      "Loss epoch  386  is =  1.1419903497896586\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  387  -----\n",
      "Loss epoch  387  is =  1.1419302528245896\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  388  -----\n",
      "Loss epoch  388  is =  1.1418702616707872\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  389  -----\n",
      "Loss epoch  389  is =  1.1418103761080511\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  390  -----\n",
      "Loss epoch  390  is =  1.1417505959166756\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  391  -----\n",
      "Loss epoch  391  is =  1.1416909208774473\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  392  -----\n",
      "Loss epoch  392  is =  1.1416313507716422\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  393  -----\n",
      "Loss epoch  393  is =  1.141571885381031\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  394  -----\n",
      "Loss epoch  394  is =  1.1415125244878688\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  395  -----\n",
      "Loss epoch  395  is =  1.1414532678749034\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  396  -----\n",
      "Loss epoch  396  is =  1.1413941153253675\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  397  -----\n",
      "Loss epoch  397  is =  1.1413350666229831\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  398  -----\n",
      "Loss epoch  398  is =  1.141276121551958\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  399  -----\n",
      "Loss epoch  399  is =  1.1412172798969769\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  400  -----\n",
      "Loss epoch  400  is =  1.1411585414432162\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  401  -----\n",
      "Loss epoch  401  is =  1.1410999059763338\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  402  -----\n",
      "Loss epoch  402  is =  1.1410413732824658\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  403  -----\n",
      "Loss epoch  403  is =  1.140982943148234\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  404  -----\n",
      "Loss epoch  404  is =  1.1409246153607362\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  405  -----\n",
      "Loss epoch  405  is =  1.140866389707549\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  406  -----\n",
      "Loss epoch  406  is =  1.1408082659767271\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  407  -----\n",
      "Loss epoch  407  is =  1.1407502439568042\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  408  -----\n",
      "Loss epoch  408  is =  1.1406923234367847\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  409  -----\n",
      "Loss epoch  409  is =  1.1406345042061525\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  410  -----\n",
      "Loss epoch  410  is =  1.1405767860548635\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  411  -----\n",
      "Loss epoch  411  is =  1.1405191687733474\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  412  -----\n",
      "Loss epoch  412  is =  1.1404616521525024\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  413  -----\n",
      "Loss epoch  413  is =  1.140404235983703\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  414  -----\n",
      "Loss epoch  414  is =  1.140346920058788\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  415  -----\n",
      "Loss epoch  415  is =  1.1402897041700684\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  416  -----\n",
      "Loss epoch  416  is =  1.1402325881103228\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  417  -----\n",
      "Loss epoch  417  is =  1.1401755716727946\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  418  -----\n",
      "Loss epoch  418  is =  1.1401186546512012\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  419  -----\n",
      "Loss epoch  419  is =  1.1400618368397137\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  420  -----\n",
      "Loss epoch  420  is =  1.1400051180329727\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  421  -----\n",
      "Loss epoch  421  is =  1.1399484980260854\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  422  -----\n",
      "Loss epoch  422  is =  1.139891976614614\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  423  -----\n",
      "Loss epoch  423  is =  1.139835553594588\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  424  -----\n",
      "Loss epoch  424  is =  1.1397792287624948\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  425  -----\n",
      "Loss epoch  425  is =  1.1397230019152824\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  426  -----\n",
      "Loss epoch  426  is =  1.1396668728503538\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  427  -----\n",
      "Loss epoch  427  is =  1.1396108413655723\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  428  -----\n",
      "Loss epoch  428  is =  1.1395549072592566\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  429  -----\n",
      "Loss epoch  429  is =  1.139499070330178\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  430  -----\n",
      "Loss epoch  430  is =  1.1394433303775702\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  431  -----\n",
      "Loss epoch  431  is =  1.1393876872011157\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  432  -----\n",
      "Loss epoch  432  is =  1.1393321406009456\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  433  -----\n",
      "Loss epoch  433  is =  1.139276690377649\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  434  -----\n",
      "Loss epoch  434  is =  1.1392213363322656\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  435  -----\n",
      "Loss epoch  435  is =  1.1391660782662802\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  436  -----\n",
      "Loss epoch  436  is =  1.1391109159816286\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  437  -----\n",
      "Loss epoch  437  is =  1.1390558492806973\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  438  -----\n",
      "Loss epoch  438  is =  1.1390008779663159\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  439  -----\n",
      "Loss epoch  439  is =  1.1389460018417612\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  440  -----\n",
      "Loss epoch  440  is =  1.138891220710758\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  441  -----\n",
      "Loss epoch  441  is =  1.1388365343774687\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  442  -----\n",
      "Loss epoch  442  is =  1.1387819426465056\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  443  -----\n",
      "Loss epoch  443  is =  1.138727445322918\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  444  -----\n",
      "Loss epoch  444  is =  1.1386730422122038\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  445  -----\n",
      "Loss epoch  445  is =  1.1386187331202928\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  446  -----\n",
      "Loss epoch  446  is =  1.1385645178535593\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  447  -----\n",
      "Loss epoch  447  is =  1.1385103962188146\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  448  -----\n",
      "Loss epoch  448  is =  1.138456368023307\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  449  -----\n",
      "Loss epoch  449  is =  1.1384024330747258\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  450  -----\n",
      "Loss epoch  450  is =  1.1383485911811873\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  451  -----\n",
      "Loss epoch  451  is =  1.1382948421512509\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  452  -----\n",
      "Loss epoch  452  is =  1.138241185793908\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  453  -----\n",
      "Loss epoch  453  is =  1.1381876219185818\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  454  -----\n",
      "Loss epoch  454  is =  1.1381341503351259\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  455  -----\n",
      "Loss epoch  455  is =  1.1380807708538274\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  456  -----\n",
      "Loss epoch  456  is =  1.138027483285404\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  457  -----\n",
      "Loss epoch  457  is =  1.1379742874410002\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  458  -----\n",
      "Loss epoch  458  is =  1.1379211831321911\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  459  -----\n",
      "Loss epoch  459  is =  1.137868170170979\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  460  -----\n",
      "Loss epoch  460  is =  1.1378152483697932\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  461  -----\n",
      "Loss epoch  461  is =  1.137762417541484\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  462  -----\n",
      "Loss epoch  462  is =  1.1377096774993347\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  463  -----\n",
      "Loss epoch  463  is =  1.1376570280570448\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  464  -----\n",
      "Loss epoch  464  is =  1.1376044690287412\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  465  -----\n",
      "Loss epoch  465  is =  1.1375520002289696\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  466  -----\n",
      "Loss epoch  466  is =  1.1374996214726996\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  467  -----\n",
      "Loss epoch  467  is =  1.1374473325753196\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  468  -----\n",
      "Loss epoch  468  is =  1.1373951333526389\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  469  -----\n",
      "Loss epoch  469  is =  1.1373430236208792\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  470  -----\n",
      "Loss epoch  470  is =  1.1372910031966867\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  471  -----\n",
      "Loss epoch  471  is =  1.1372390718971233\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  472  -----\n",
      "Loss epoch  472  is =  1.1371872295396632\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  473  -----\n",
      "Loss epoch  473  is =  1.137135475942198\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  474  -----\n",
      "Loss epoch  474  is =  1.137083810923032\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  475  -----\n",
      "Loss epoch  475  is =  1.13703223430088\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  476  -----\n",
      "Loss epoch  476  is =  1.1369807458948766\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  477  -----\n",
      "Loss epoch  477  is =  1.136929345524558\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  478  -----\n",
      "Loss epoch  478  is =  1.136878033009879\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  479  -----\n",
      "Loss epoch  479  is =  1.1368268081711954\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  480  -----\n",
      "Loss epoch  480  is =  1.1367756708292764\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  481  -----\n",
      "Loss epoch  481  is =  1.136724620805303\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  482  -----\n",
      "Loss epoch  482  is =  1.1366736579208538\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  483  -----\n",
      "Loss epoch  483  is =  1.1366227819979213\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  484  -----\n",
      "Loss epoch  484  is =  1.1365719928588947\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  485  -----\n",
      "Loss epoch  485  is =  1.1365212903265764\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  486  -----\n",
      "Loss epoch  486  is =  1.136470674224166\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  487  -----\n",
      "Loss epoch  487  is =  1.1364201443752693\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  488  -----\n",
      "Loss epoch  488  is =  1.1363697006038869\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  489  -----\n",
      "Loss epoch  489  is =  1.1363193427344256\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  490  -----\n",
      "Loss epoch  490  is =  1.1362690705916931\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  491  -----\n",
      "Loss epoch  491  is =  1.1362188840008896\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  492  -----\n",
      "Loss epoch  492  is =  1.1361687827876197\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  493  -----\n",
      "Loss epoch  493  is =  1.1361187667778805\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  494  -----\n",
      "Loss epoch  494  is =  1.136068835798068\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  495  -----\n",
      "Loss epoch  495  is =  1.1360189896749704\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  496  -----\n",
      "Loss epoch  496  is =  1.135969228235774\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  497  -----\n",
      "Loss epoch  497  is =  1.1359195513080593\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  498  -----\n",
      "Loss epoch  498  is =  1.1358699587197962\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  499  -----\n",
      "Loss epoch  499  is =  1.1358204502993439\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  500  -----\n",
      "Loss epoch  500  is =  1.1357710258754583\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  501  -----\n",
      "Loss epoch  501  is =  1.135721685277283\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  502  -----\n",
      "Loss epoch  502  is =  1.1356724283343538\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  503  -----\n",
      "Loss epoch  503  is =  1.135623254876591\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  504  -----\n",
      "Loss epoch  504  is =  1.1355741647343034\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  505  -----\n",
      "Loss epoch  505  is =  1.135525157738186\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  506  -----\n",
      "Loss epoch  506  is =  1.1354762337193192\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  507  -----\n",
      "Loss epoch  507  is =  1.1354273925091707\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  508  -----\n",
      "Loss epoch  508  is =  1.1353786339395902\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  509  -----\n",
      "Loss epoch  509  is =  1.1353299578428127\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  510  -----\n",
      "Loss epoch  510  is =  1.1352813640514552\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  511  -----\n",
      "Loss epoch  511  is =  1.1352328523985138\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  512  -----\n",
      "Loss epoch  512  is =  1.1351844227173673\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  513  -----\n",
      "Loss epoch  513  is =  1.1351360748417765\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  514  -----\n",
      "Loss epoch  514  is =  1.135087808605876\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  515  -----\n",
      "Loss epoch  515  is =  1.1350396238441816\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  516  -----\n",
      "Loss epoch  516  is =  1.1349915203915875\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  517  -----\n",
      "Loss epoch  517  is =  1.13494349808336\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  518  -----\n",
      "Loss epoch  518  is =  1.134895556755146\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  519  -----\n",
      "Loss epoch  519  is =  1.1348476962429657\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  520  -----\n",
      "Loss epoch  520  is =  1.1347999163832125\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  521  -----\n",
      "Loss epoch  521  is =  1.1347522170126545\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  522  -----\n",
      "Loss epoch  522  is =  1.1347045979684314\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  523  -----\n",
      "Loss epoch  523  is =  1.1346570590880531\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  524  -----\n",
      "Loss epoch  524  is =  1.1346096002093973\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  525  -----\n",
      "Loss epoch  525  is =  1.1345622211707185\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  526  -----\n",
      "Loss epoch  526  is =  1.1345149218106358\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  527  -----\n",
      "Loss epoch  527  is =  1.1344677019681397\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  528  -----\n",
      "Loss epoch  528  is =  1.134420561482582\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  529  -----\n",
      "Loss epoch  529  is =  1.1343735001936885\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  530  -----\n",
      "Loss epoch  530  is =  1.134326517941544\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  531  -----\n",
      "Loss epoch  531  is =  1.1342796145666025\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  532  -----\n",
      "Loss epoch  532  is =  1.1342327899096807\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  533  -----\n",
      "Loss epoch  533  is =  1.1341860438119573\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  534  -----\n",
      "Loss epoch  534  is =  1.134139376114976\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  535  -----\n",
      "Loss epoch  535  is =  1.1340927866606398\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  536  -----\n",
      "Loss epoch  536  is =  1.1340462752912142\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  537  -----\n",
      "Loss epoch  537  is =  1.1339998418493216\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  538  -----\n",
      "Loss epoch  538  is =  1.1339534861779463\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  539  -----\n",
      "Loss epoch  539  is =  1.1339072081204304\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  540  -----\n",
      "Loss epoch  540  is =  1.1338610075204762\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  541  -----\n",
      "Loss epoch  541  is =  1.1338148842221336\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  542  -----\n",
      "Loss epoch  542  is =  1.1337688380698212\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  543  -----\n",
      "Loss epoch  543  is =  1.1337228689083048\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  544  -----\n",
      "Loss epoch  544  is =  1.1336769765827048\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  545  -----\n",
      "Loss epoch  545  is =  1.1336311609384946\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  546  -----\n",
      "Loss epoch  546  is =  1.1335854218215011\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  547  -----\n",
      "Loss epoch  547  is =  1.1335397590779097\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  548  -----\n",
      "Loss epoch  548  is =  1.1334941725542445\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  549  -----\n",
      "Loss epoch  549  is =  1.1334486620973903\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  550  -----\n",
      "Loss epoch  550  is =  1.1334032275545785\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  551  -----\n",
      "Loss epoch  551  is =  1.1333578687733845\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  552  -----\n",
      "Loss epoch  552  is =  1.133312585601735\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  553  -----\n",
      "Loss epoch  553  is =  1.1332673778879052\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  554  -----\n",
      "Loss epoch  554  is =  1.1332222454805165\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  555  -----\n",
      "Loss epoch  555  is =  1.1331771882285346\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  556  -----\n",
      "Loss epoch  556  is =  1.133132205981271\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  557  -----\n",
      "Loss epoch  557  is =  1.133087298588379\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  558  -----\n",
      "Loss epoch  558  is =  1.133042465899856\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  559  -----\n",
      "Loss epoch  559  is =  1.1329977077660425\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  560  -----\n",
      "Loss epoch  560  is =  1.1329530240376218\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  561  -----\n",
      "Loss epoch  561  is =  1.1329084145656145\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  562  -----\n",
      "Loss epoch  562  is =  1.1328638792013828\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  563  -----\n",
      "Loss epoch  563  is =  1.1328194177966309\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  564  -----\n",
      "Loss epoch  564  is =  1.132775030203398\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  565  -----\n",
      "Loss epoch  565  is =  1.1327307162740603\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  566  -----\n",
      "Loss epoch  566  is =  1.1326864758613366\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  567  -----\n",
      "Loss epoch  567  is =  1.1326423088182755\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  568  -----\n",
      "Loss epoch  568  is =  1.1325982149982647\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  569  -----\n",
      "Loss epoch  569  is =  1.1325541942550237\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  570  -----\n",
      "Loss epoch  570  is =  1.1325102464426098\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  571  -----\n",
      "Loss epoch  571  is =  1.1324663714154088\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  572  -----\n",
      "Loss epoch  572  is =  1.132422569028144\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  573  -----\n",
      "Loss epoch  573  is =  1.1323788391358653\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  574  -----\n",
      "Loss epoch  574  is =  1.1323351815939553\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  575  -----\n",
      "Loss epoch  575  is =  1.1322915962581257\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  576  -----\n",
      "Loss epoch  576  is =  1.1322480829844215\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  577  -----\n",
      "Loss epoch  577  is =  1.13220464162921\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  578  -----\n",
      "Loss epoch  578  is =  1.1321612720491943\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  579  -----\n",
      "Loss epoch  579  is =  1.1321179741013956\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  580  -----\n",
      "Loss epoch  580  is =  1.1320747476431678\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  581  -----\n",
      "Loss epoch  581  is =  1.1320315925321855\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  582  -----\n",
      "Loss epoch  582  is =  1.1319885086264532\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  583  -----\n",
      "Loss epoch  583  is =  1.131945495784295\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  584  -----\n",
      "Loss epoch  584  is =  1.1319025538643566\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  585  -----\n",
      "Loss epoch  585  is =  1.1318596827256138\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  586  -----\n",
      "Loss epoch  586  is =  1.1318168822273593\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  587  -----\n",
      "Loss epoch  587  is =  1.131774152229207\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  588  -----\n",
      "Loss epoch  588  is =  1.1317314925910895\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  589  -----\n",
      "Loss epoch  589  is =  1.131688903173261\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  590  -----\n",
      "Loss epoch  590  is =  1.1316463838362967\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  591  -----\n",
      "Loss epoch  591  is =  1.131603934441084\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  592  -----\n",
      "Loss epoch  592  is =  1.1315615548488345\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  593  -----\n",
      "Loss epoch  593  is =  1.131519244921069\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  594  -----\n",
      "Loss epoch  594  is =  1.131477004519632\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  595  -----\n",
      "Loss epoch  595  is =  1.1314348335066742\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  596  -----\n",
      "Loss epoch  596  is =  1.1313927317446686\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  597  -----\n",
      "Loss epoch  597  is =  1.1313506990963973\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  598  -----\n",
      "Loss epoch  598  is =  1.1313087354249556\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  599  -----\n",
      "Loss epoch  599  is =  1.1312668405937516\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  600  -----\n",
      "Loss epoch  600  is =  1.1312250144665075\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  601  -----\n",
      "Loss epoch  601  is =  1.1311832569072517\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  602  -----\n",
      "Loss epoch  602  is =  1.1311415677803265\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  603  -----\n",
      "Loss epoch  603  is =  1.1310999469503789\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  604  -----\n",
      "Loss epoch  604  is =  1.1310583942823702\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  605  -----\n",
      "Loss epoch  605  is =  1.1310169096415648\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  606  -----\n",
      "Loss epoch  606  is =  1.1309754928935332\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  607  -----\n",
      "Loss epoch  607  is =  1.1309341439041578\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  608  -----\n",
      "Loss epoch  608  is =  1.1308928625396235\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  609  -----\n",
      "Loss epoch  609  is =  1.1308516486664195\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  610  -----\n",
      "Loss epoch  610  is =  1.1308105021513402\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  611  -----\n",
      "Loss epoch  611  is =  1.1307694228614824\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  612  -----\n",
      "Loss epoch  612  is =  1.1307284106642463\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  613  -----\n",
      "Loss epoch  613  is =  1.1306874654273353\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  614  -----\n",
      "Loss epoch  614  is =  1.130646587018753\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  615  -----\n",
      "Loss epoch  615  is =  1.130605775306804\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  616  -----\n",
      "Loss epoch  616  is =  1.130565030160096\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  617  -----\n",
      "Loss epoch  617  is =  1.1305243514475305\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  618  -----\n",
      "Loss epoch  618  is =  1.1304837390383096\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  619  -----\n",
      "Loss epoch  619  is =  1.130443192801932\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  620  -----\n",
      "Loss epoch  620  is =  1.1304027126081955\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  621  -----\n",
      "Loss epoch  621  is =  1.130362298327196\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  622  -----\n",
      "Loss epoch  622  is =  1.13032194982932\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  623  -----\n",
      "Loss epoch  623  is =  1.1302816669852513\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  624  -----\n",
      "Loss epoch  624  is =  1.130241449665972\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  625  -----\n",
      "Loss epoch  625  is =  1.130201297742755\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  626  -----\n",
      "Loss epoch  626  is =  1.1301612110871646\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  627  -----\n",
      "Loss epoch  627  is =  1.1301211895710583\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  628  -----\n",
      "Loss epoch  628  is =  1.1300812330665853\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  629  -----\n",
      "Loss epoch  629  is =  1.1300413414461876\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  630  -----\n",
      "Loss epoch  630  is =  1.1300015145825926\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  631  -----\n",
      "Loss epoch  631  is =  1.1299617523488228\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  632  -----\n",
      "Loss epoch  632  is =  1.1299220546181858\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  633  -----\n",
      "Loss epoch  633  is =  1.1298824212642786\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  634  -----\n",
      "Loss epoch  634  is =  1.129842852160986\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  635  -----\n",
      "Loss epoch  635  is =  1.129803347182481\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  636  -----\n",
      "Loss epoch  636  is =  1.129763906203218\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  637  -----\n",
      "Loss epoch  637  is =  1.1297245290979416\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  638  -----\n",
      "Loss epoch  638  is =  1.1296852157416786\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  639  -----\n",
      "Loss epoch  639  is =  1.1296459660097415\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  640  -----\n",
      "Loss epoch  640  is =  1.1296067797777254\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  641  -----\n",
      "Loss epoch  641  is =  1.1295676569215067\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  642  -----\n",
      "Loss epoch  642  is =  1.129528597317249\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  643  -----\n",
      "Loss epoch  643  is =  1.1294896008413893\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  644  -----\n",
      "Loss epoch  644  is =  1.1294506673706508\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  645  -----\n",
      "Loss epoch  645  is =  1.129411796782037\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  646  -----\n",
      "Loss epoch  646  is =  1.1293729889528301\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  647  -----\n",
      "Loss epoch  647  is =  1.1293342437605867\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  648  -----\n",
      "Loss epoch  648  is =  1.1292955610831479\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  649  -----\n",
      "Loss epoch  649  is =  1.1292569407986282\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  650  -----\n",
      "Loss epoch  650  is =  1.1292183827854203\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  651  -----\n",
      "Loss epoch  651  is =  1.1291798869221923\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  652  -----\n",
      "Loss epoch  652  is =  1.1291414530878878\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  653  -----\n",
      "Loss epoch  653  is =  1.129103081161727\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  654  -----\n",
      "Loss epoch  654  is =  1.129064771023202\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  655  -----\n",
      "Loss epoch  655  is =  1.1290265225520792\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  656  -----\n",
      "Loss epoch  656  is =  1.1289883356283978\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  657  -----\n",
      "Loss epoch  657  is =  1.1289502101324689\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  658  -----\n",
      "Loss epoch  658  is =  1.128912145944876\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  659  -----\n",
      "Loss epoch  659  is =  1.12887414294647\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  660  -----\n",
      "Loss epoch  660  is =  1.1288362010183781\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  661  -----\n",
      "Loss epoch  661  is =  1.1287983200419922\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  662  -----\n",
      "Loss epoch  662  is =  1.1287604998989744\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  663  -----\n",
      "Loss epoch  663  is =  1.1287227404712563\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  664  -----\n",
      "Loss epoch  664  is =  1.128685041641036\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  665  -----\n",
      "Loss epoch  665  is =  1.1286474032907776\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  666  -----\n",
      "Loss epoch  666  is =  1.128609825303215\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  667  -----\n",
      "Loss epoch  667  is =  1.1285723075613432\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  668  -----\n",
      "Loss epoch  668  is =  1.1285348499484251\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  669  -----\n",
      "Loss epoch  669  is =  1.1284974523479898\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  670  -----\n",
      "Loss epoch  670  is =  1.1284601146438242\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  671  -----\n",
      "Loss epoch  671  is =  1.1284228367199851\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  672  -----\n",
      "Loss epoch  672  is =  1.1283856184607879\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  673  -----\n",
      "Loss epoch  673  is =  1.1283484597508122\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  674  -----\n",
      "Loss epoch  674  is =  1.128311360474896\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  675  -----\n",
      "Loss epoch  675  is =  1.12827432051814\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  676  -----\n",
      "Loss epoch  676  is =  1.1282373397659036\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  677  -----\n",
      "Loss epoch  677  is =  1.1282004181038088\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  678  -----\n",
      "Loss epoch  678  is =  1.1281635554177314\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  679  -----\n",
      "Loss epoch  679  is =  1.1281267515938076\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  680  -----\n",
      "Loss epoch  680  is =  1.1280900065184356\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  681  -----\n",
      "Loss epoch  681  is =  1.1280533200782645\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  682  -----\n",
      "Loss epoch  682  is =  1.1280166921602013\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  683  -----\n",
      "Loss epoch  683  is =  1.1279801226514081\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  684  -----\n",
      "Loss epoch  684  is =  1.1279436114393044\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  685  -----\n",
      "Loss epoch  685  is =  1.1279071584115619\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  686  -----\n",
      "Loss epoch  686  is =  1.1278707634561087\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  687  -----\n",
      "Loss epoch  687  is =  1.1278344264611253\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  688  -----\n",
      "Loss epoch  688  is =  1.1277981473150407\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  689  -----\n",
      "Loss epoch  689  is =  1.1277619259065428\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  690  -----\n",
      "Loss epoch  690  is =  1.127725762124564\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  691  -----\n",
      "Loss epoch  691  is =  1.1276896558582936\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  692  -----\n",
      "Loss epoch  692  is =  1.1276536069971637\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  693  -----\n",
      "Loss epoch  693  is =  1.1276176154308695\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  694  -----\n",
      "Loss epoch  694  is =  1.1275816810493382\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  695  -----\n",
      "Loss epoch  695  is =  1.1275458037427553\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  696  -----\n",
      "Loss epoch  696  is =  1.1275099834015534\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  697  -----\n",
      "Loss epoch  697  is =  1.1274742199164127\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  698  -----\n",
      "Loss epoch  698  is =  1.1274385131782565\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  699  -----\n",
      "Loss epoch  699  is =  1.127402863078255\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  700  -----\n",
      "Loss epoch  700  is =  1.1273672695078265\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  701  -----\n",
      "Loss epoch  701  is =  1.1273317323586316\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  702  -----\n",
      "Loss epoch  702  is =  1.1272962515225775\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  703  -----\n",
      "Loss epoch  703  is =  1.127260826891811\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  704  -----\n",
      "Loss epoch  704  is =  1.1272254583587245\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  705  -----\n",
      "Loss epoch  705  is =  1.1271901458159541\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  706  -----\n",
      "Loss epoch  706  is =  1.1271548891563758\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  707  -----\n",
      "Loss epoch  707  is =  1.127119688273107\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  708  -----\n",
      "Loss epoch  708  is =  1.1270845430595051\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  709  -----\n",
      "Loss epoch  709  is =  1.1270494534091708\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  710  -----\n",
      "Loss epoch  710  is =  1.1270144192159408\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  711  -----\n",
      "Loss epoch  711  is =  1.126979440373893\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  712  -----\n",
      "Loss epoch  712  is =  1.1269445167773402\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  713  -----\n",
      "Loss epoch  713  is =  1.1269096483208396\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  714  -----\n",
      "Loss epoch  714  is =  1.1268748348991764\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  715  -----\n",
      "Loss epoch  715  is =  1.1268400764073807\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  716  -----\n",
      "Loss epoch  716  is =  1.1268053727407141\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  717  -----\n",
      "Loss epoch  717  is =  1.126770723794675\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  718  -----\n",
      "Loss epoch  718  is =  1.126736129465\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  719  -----\n",
      "Loss epoch  719  is =  1.126701589647653\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  720  -----\n",
      "Loss epoch  720  is =  1.1266671042388368\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  721  -----\n",
      "Loss epoch  721  is =  1.126632673134984\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  722  -----\n",
      "Loss epoch  722  is =  1.1265982962327628\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  723  -----\n",
      "Loss epoch  723  is =  1.1265639734290716\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  724  -----\n",
      "Loss epoch  724  is =  1.1265297046210416\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  725  -----\n",
      "Loss epoch  725  is =  1.1264954897060346\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  726  -----\n",
      "Loss epoch  726  is =  1.126461328581642\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  727  -----\n",
      "Loss epoch  727  is =  1.1264272211456852\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  728  -----\n",
      "Loss epoch  728  is =  1.1263931672962146\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  729  -----\n",
      "Loss epoch  729  is =  1.1263591669315116\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  730  -----\n",
      "Loss epoch  730  is =  1.1263252199500808\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  731  -----\n",
      "Loss epoch  731  is =  1.1262913262506598\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  732  -----\n",
      "Loss epoch  732  is =  1.1262574857322092\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  733  -----\n",
      "Loss epoch  733  is =  1.1262236982939207\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  734  -----\n",
      "Loss epoch  734  is =  1.1261899638352035\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  735  -----\n",
      "Loss epoch  735  is =  1.1261562822557027\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  736  -----\n",
      "Loss epoch  736  is =  1.1261226534552786\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  737  -----\n",
      "Loss epoch  737  is =  1.1260890773340235\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  738  -----\n",
      "Loss epoch  738  is =  1.1260555537922468\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  739  -----\n",
      "Loss epoch  739  is =  1.126022082730485\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  740  -----\n",
      "Loss epoch  740  is =  1.1259886640494978\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  741  -----\n",
      "Loss epoch  741  is =  1.1259552976502643\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  742  -----\n",
      "Loss epoch  742  is =  1.125921983433988\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  743  -----\n",
      "Loss epoch  743  is =  1.1258887213020896\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  744  -----\n",
      "Loss epoch  744  is =  1.1258555111562125\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  745  -----\n",
      "Loss epoch  745  is =  1.1258223528982223\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  746  -----\n",
      "Loss epoch  746  is =  1.1257892464301984\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  747  -----\n",
      "Loss epoch  747  is =  1.1257561916544419\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  748  -----\n",
      "Loss epoch  748  is =  1.1257231884734744\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  749  -----\n",
      "Loss epoch  749  is =  1.1256902367900319\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  750  -----\n",
      "Loss epoch  750  is =  1.12565733650707\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  751  -----\n",
      "Loss epoch  751  is =  1.1256244875277577\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  752  -----\n",
      "Loss epoch  752  is =  1.125591689755483\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  753  -----\n",
      "Loss epoch  753  is =  1.1255589430938504\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  754  -----\n",
      "Loss epoch  754  is =  1.1255262474466747\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  755  -----\n",
      "Loss epoch  755  is =  1.125493602717993\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  756  -----\n",
      "Loss epoch  756  is =  1.1254610088120474\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  757  -----\n",
      "Loss epoch  757  is =  1.125428465633301\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  758  -----\n",
      "Loss epoch  758  is =  1.1253959730864238\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  759  -----\n",
      "Loss epoch  759  is =  1.125363531076303\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  760  -----\n",
      "Loss epoch  760  is =  1.125331139508035\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  761  -----\n",
      "Loss epoch  761  is =  1.1252987982869305\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  762  -----\n",
      "Loss epoch  762  is =  1.1252665073185064\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  763  -----\n",
      "Loss epoch  763  is =  1.1252342665084953\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  764  -----\n",
      "Loss epoch  764  is =  1.1252020757628347\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  765  -----\n",
      "Loss epoch  765  is =  1.1251699349876754\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  766  -----\n",
      "Loss epoch  766  is =  1.1251378440893753\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  767  -----\n",
      "Loss epoch  767  is =  1.1251058029745016\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  768  -----\n",
      "Loss epoch  768  is =  1.1250738115498253\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  769  -----\n",
      "Loss epoch  769  is =  1.125041869722329\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  770  -----\n",
      "Loss epoch  770  is =  1.1250099773992028\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  771  -----\n",
      "Loss epoch  771  is =  1.1249781344878391\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  772  -----\n",
      "Loss epoch  772  is =  1.1249463408958407\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  773  -----\n",
      "Loss epoch  773  is =  1.124914596531012\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  774  -----\n",
      "Loss epoch  774  is =  1.1248829013013621\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  775  -----\n",
      "Loss epoch  775  is =  1.1248512551151049\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  776  -----\n",
      "Loss epoch  776  is =  1.12481965788066\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  777  -----\n",
      "Loss epoch  777  is =  1.1247881095066483\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  778  -----\n",
      "Loss epoch  778  is =  1.1247566099018942\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  779  -----\n",
      "Loss epoch  779  is =  1.1247251589754237\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  780  -----\n",
      "Loss epoch  780  is =  1.1246937566364652\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  781  -----\n",
      "Loss epoch  781  is =  1.1246624027944492\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  782  -----\n",
      "Loss epoch  782  is =  1.1246310973590044\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  783  -----\n",
      "Loss epoch  783  is =  1.124599840239961\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  784  -----\n",
      "Loss epoch  784  is =  1.12456863134735\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  785  -----\n",
      "Loss epoch  785  is =  1.1245374705913982\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  786  -----\n",
      "Loss epoch  786  is =  1.1245063578825374\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  787  -----\n",
      "Loss epoch  787  is =  1.1244752931313917\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  788  -----\n",
      "Loss epoch  788  is =  1.1244442762487867\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  789  -----\n",
      "Loss epoch  789  is =  1.1244133071457427\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  790  -----\n",
      "Loss epoch  790  is =  1.1243823857334772\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  791  -----\n",
      "Loss epoch  791  is =  1.1243515119234064\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  792  -----\n",
      "Loss epoch  792  is =  1.124320685627142\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  793  -----\n",
      "Loss epoch  793  is =  1.1242899067564887\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  794  -----\n",
      "Loss epoch  794  is =  1.1242591752234472\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  795  -----\n",
      "Loss epoch  795  is =  1.1242284909402116\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  796  -----\n",
      "Loss epoch  796  is =  1.1241978538191706\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  797  -----\n",
      "Loss epoch  797  is =  1.1241672637729077\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  798  -----\n",
      "Loss epoch  798  is =  1.1241367207141983\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  799  -----\n",
      "Loss epoch  799  is =  1.124106224556011\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  800  -----\n",
      "Loss epoch  800  is =  1.1240757752115051\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  801  -----\n",
      "Loss epoch  801  is =  1.124045372594029\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  802  -----\n",
      "Loss epoch  802  is =  1.1240150166171303\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  803  -----\n",
      "Loss epoch  803  is =  1.123984707194539\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  804  -----\n",
      "Loss epoch  804  is =  1.1239544442401797\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  805  -----\n",
      "Loss epoch  805  is =  1.1239242276681638\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  806  -----\n",
      "Loss epoch  806  is =  1.1238940573927934\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  807  -----\n",
      "Loss epoch  807  is =  1.123863933328559\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  808  -----\n",
      "Loss epoch  808  is =  1.1238338553901412\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  809  -----\n",
      "Loss epoch  809  is =  1.1238038234924048\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  810  -----\n",
      "Loss epoch  810  is =  1.123773837550404\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  811  -----\n",
      "Loss epoch  811  is =  1.1237438974793814\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  812  -----\n",
      "Loss epoch  812  is =  1.1237140031947603\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  813  -----\n",
      "Loss epoch  813  is =  1.1236841546121563\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  814  -----\n",
      "Loss epoch  814  is =  1.1236543516473674\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  815  -----\n",
      "Loss epoch  815  is =  1.1236245942163778\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  816  -----\n",
      "Loss epoch  816  is =  1.1235948822353554\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  817  -----\n",
      "Loss epoch  817  is =  1.1235652156206513\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  818  -----\n",
      "Loss epoch  818  is =  1.1235355942888015\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  819  -----\n",
      "Loss epoch  819  is =  1.1235060181565262\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  820  -----\n",
      "Loss epoch  820  is =  1.1234764871407261\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  821  -----\n",
      "Loss epoch  821  is =  1.1234470011584867\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  822  -----\n",
      "Loss epoch  822  is =  1.1234175601270697\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  823  -----\n",
      "Loss epoch  823  is =  1.1233881639639247\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  824  -----\n",
      "Loss epoch  824  is =  1.123358812586681\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  825  -----\n",
      "Loss epoch  825  is =  1.1233295059131434\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  826  -----\n",
      "Loss epoch  826  is =  1.1233002438613038\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  827  -----\n",
      "Loss epoch  827  is =  1.123271026349328\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  828  -----\n",
      "Loss epoch  828  is =  1.1232418532955637\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  829  -----\n",
      "Loss epoch  829  is =  1.1232127246185364\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  830  -----\n",
      "Loss epoch  830  is =  1.1231836402369493\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  831  -----\n",
      "Loss epoch  831  is =  1.1231546000696875\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  832  -----\n",
      "Loss epoch  832  is =  1.1231256040358073\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  833  -----\n",
      "Loss epoch  833  is =  1.1230966520545453\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  834  -----\n",
      "Loss epoch  834  is =  1.123067744045314\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  835  -----\n",
      "Loss epoch  835  is =  1.1230388799277011\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  836  -----\n",
      "Loss epoch  836  is =  1.1230100596214723\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  837  -----\n",
      "Loss epoch  837  is =  1.122981283046565\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  838  -----\n",
      "Loss epoch  838  is =  1.1229525501230941\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  839  -----\n",
      "Loss epoch  839  is =  1.1229238607713472\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  840  -----\n",
      "Loss epoch  840  is =  1.1228952149117875\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  841  -----\n",
      "Loss epoch  841  is =  1.1228666124650502\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  842  -----\n",
      "Loss epoch  842  is =  1.1228380533519442\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  843  -----\n",
      "Loss epoch  843  is =  1.122809537493451\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  844  -----\n",
      "Loss epoch  844  is =  1.1227810648107202\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  845  -----\n",
      "Loss epoch  845  is =  1.122752635225081\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  846  -----\n",
      "Loss epoch  846  is =  1.1227242486580278\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  847  -----\n",
      "Loss epoch  847  is =  1.1226959050312264\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  848  -----\n",
      "Loss epoch  848  is =  1.1226676042665167\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  849  -----\n",
      "Loss epoch  849  is =  1.1226393462859041\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  850  -----\n",
      "Loss epoch  850  is =  1.1226111310115665\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  851  -----\n",
      "Loss epoch  851  is =  1.1225829583658513\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  852  -----\n",
      "Loss epoch  852  is =  1.1225548282712712\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  853  -----\n",
      "Loss epoch  853  is =  1.1225267406505102\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  854  -----\n",
      "Loss epoch  854  is =  1.1224986954264191\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  855  -----\n",
      "Loss epoch  855  is =  1.1224706925220183\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  856  -----\n",
      "Loss epoch  856  is =  1.12244273186049\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  857  -----\n",
      "Loss epoch  857  is =  1.122414813365189\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  858  -----\n",
      "Loss epoch  858  is =  1.1223869369596342\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  859  -----\n",
      "Loss epoch  859  is =  1.1223591025675101\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  860  -----\n",
      "Loss epoch  860  is =  1.1223313101126648\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  861  -----\n",
      "Loss epoch  861  is =  1.1223035595191138\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  862  -----\n",
      "Loss epoch  862  is =  1.1222758507110369\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  863  -----\n",
      "Loss epoch  863  is =  1.1222481836127776\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  864  -----\n",
      "Loss epoch  864  is =  1.1222205581488427\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  865  -----\n",
      "Loss epoch  865  is =  1.122192974243903\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  866  -----\n",
      "Loss epoch  866  is =  1.122165431822792\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  867  -----\n",
      "Loss epoch  867  is =  1.1221379308105066\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  868  -----\n",
      "Loss epoch  868  is =  1.122110471132205\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  869  -----\n",
      "Loss epoch  869  is =  1.1220830527132066\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  870  -----\n",
      "Loss epoch  870  is =  1.122055675478993\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  871  -----\n",
      "Loss epoch  871  is =  1.1220283393552073\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  872  -----\n",
      "Loss epoch  872  is =  1.1220010442676498\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  873  -----\n",
      "Loss epoch  873  is =  1.1219737901422868\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  874  -----\n",
      "Loss epoch  874  is =  1.1219465769052408\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  875  -----\n",
      "Loss epoch  875  is =  1.121919404482791\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  876  -----\n",
      "Loss epoch  876  is =  1.1218922728013807\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  877  -----\n",
      "Loss epoch  877  is =  1.12186518178761\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  878  -----\n",
      "Loss epoch  878  is =  1.121838131368235\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  879  -----\n",
      "Loss epoch  879  is =  1.121811121470171\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  880  -----\n",
      "Loss epoch  880  is =  1.1217841520204934\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  881  -----\n",
      "Loss epoch  881  is =  1.1217572229464299\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  882  -----\n",
      "Loss epoch  882  is =  1.1217303341753668\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  883  -----\n",
      "Loss epoch  883  is =  1.121703485634847\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  884  -----\n",
      "Loss epoch  884  is =  1.1216766772525686\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  885  -----\n",
      "Loss epoch  885  is =  1.121649908956387\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  886  -----\n",
      "Loss epoch  886  is =  1.1216231806743107\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  887  -----\n",
      "Loss epoch  887  is =  1.121596492334503\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  888  -----\n",
      "Loss epoch  888  is =  1.1215698438652808\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  889  -----\n",
      "Loss epoch  889  is =  1.1215432351951151\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  890  -----\n",
      "Loss epoch  890  is =  1.121516666252633\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  891  -----\n",
      "Loss epoch  891  is =  1.1214901369666133\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  892  -----\n",
      "Loss epoch  892  is =  1.1214636472659842\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  893  -----\n",
      "Loss epoch  893  is =  1.1214371970798325\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  894  -----\n",
      "Loss epoch  894  is =  1.12141078633739\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  895  -----\n",
      "Loss epoch  895  is =  1.1213844149680476\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  896  -----\n",
      "Loss epoch  896  is =  1.1213580829013403\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  897  -----\n",
      "Loss epoch  897  is =  1.1213317900669582\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  898  -----\n",
      "Loss epoch  898  is =  1.121305536394741\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  899  -----\n",
      "Loss epoch  899  is =  1.121279321814676\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  900  -----\n",
      "Loss epoch  900  is =  1.1212531462569029\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  901  -----\n",
      "Loss epoch  901  is =  1.1212270096517136\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  902  -----\n",
      "Loss epoch  902  is =  1.1212009119295427\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  903  -----\n",
      "Loss epoch  903  is =  1.121174853020978\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  904  -----\n",
      "Loss epoch  904  is =  1.1211488328567516\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  905  -----\n",
      "Loss epoch  905  is =  1.121122851367748\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  906  -----\n",
      "Loss epoch  906  is =  1.1210969084849955\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  907  -----\n",
      "Loss epoch  907  is =  1.1210710041396725\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  908  -----\n",
      "Loss epoch  908  is =  1.1210451382631035\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  909  -----\n",
      "Loss epoch  909  is =  1.1210193107867539\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  910  -----\n",
      "Loss epoch  910  is =  1.120993521642244\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  911  -----\n",
      "Loss epoch  911  is =  1.1209677707613355\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  912  -----\n",
      "Loss epoch  912  is =  1.120942058075935\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  913  -----\n",
      "Loss epoch  913  is =  1.120916383518093\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  914  -----\n",
      "Loss epoch  914  is =  1.1208907470200047\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  915  -----\n",
      "Loss epoch  915  is =  1.120865148514013\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  916  -----\n",
      "Loss epoch  916  is =  1.1208395879326039\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  917  -----\n",
      "Loss epoch  917  is =  1.1208140652084044\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  918  -----\n",
      "Loss epoch  918  is =  1.1207885802741835\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  919  -----\n",
      "Loss epoch  919  is =  1.1207631330628567\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  920  -----\n",
      "Loss epoch  920  is =  1.1207377235074818\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  921  -----\n",
      "Loss epoch  921  is =  1.1207123515412571\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  922  -----\n",
      "Loss epoch  922  is =  1.1206870170975218\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  923  -----\n",
      "Loss epoch  923  is =  1.1206617201097584\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  924  -----\n",
      "Loss epoch  924  is =  1.1206364605115906\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  925  -----\n",
      "Loss epoch  925  is =  1.1206112382367786\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  926  -----\n",
      "Loss epoch  926  is =  1.1205860532192295\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  927  -----\n",
      "Loss epoch  927  is =  1.1205609053929848\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  928  -----\n",
      "Loss epoch  928  is =  1.1205357946922294\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  929  -----\n",
      "Loss epoch  929  is =  1.1205107210512848\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  930  -----\n",
      "Loss epoch  930  is =  1.120485684404613\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  931  -----\n",
      "Loss epoch  931  is =  1.1204606846868144\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  932  -----\n",
      "Loss epoch  932  is =  1.1204357218326255\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  933  -----\n",
      "Loss epoch  933  is =  1.120410795776925\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  934  -----\n",
      "Loss epoch  934  is =  1.120385906454727\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  935  -----\n",
      "Loss epoch  935  is =  1.120361053801182\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  936  -----\n",
      "Loss epoch  936  is =  1.1203362377515764\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  937  -----\n",
      "Loss epoch  937  is =  1.1203114582413354\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  938  -----\n",
      "Loss epoch  938  is =  1.12028671520602\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  939  -----\n",
      "Loss epoch  939  is =  1.1202620085813257\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  940  -----\n",
      "Loss epoch  940  is =  1.120237338303086\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  941  -----\n",
      "Loss epoch  941  is =  1.120212704307267\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  942  -----\n",
      "Loss epoch  942  is =  1.120188106529971\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  943  -----\n",
      "Loss epoch  943  is =  1.120163544907436\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  944  -----\n",
      "Loss epoch  944  is =  1.1201390193760317\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  945  -----\n",
      "Loss epoch  945  is =  1.1201145298722626\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  946  -----\n",
      "Loss epoch  946  is =  1.1200900763327644\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  947  -----\n",
      "Loss epoch  947  is =  1.1200656586943127\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  948  -----\n",
      "Loss epoch  948  is =  1.1200412768938097\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  949  -----\n",
      "Loss epoch  949  is =  1.1200169308682912\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  950  -----\n",
      "Loss epoch  950  is =  1.1199926205549258\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  951  -----\n",
      "Loss epoch  951  is =  1.1199683458910157\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  952  -----\n",
      "Loss epoch  952  is =  1.1199441068139924\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  953  -----\n",
      "Loss epoch  953  is =  1.1199199032614193\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  954  -----\n",
      "Loss epoch  954  is =  1.1198957351709897\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  955  -----\n",
      "Loss epoch  955  is =  1.1198716024805304\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  956  -----\n",
      "Loss epoch  956  is =  1.1198475051279957\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  957  -----\n",
      "Loss epoch  957  is =  1.1198234430514706\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  958  -----\n",
      "Loss epoch  958  is =  1.11979941618917\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  959  -----\n",
      "Loss epoch  959  is =  1.1197754244794373\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  960  -----\n",
      "Loss epoch  960  is =  1.1197514678607448\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  961  -----\n",
      "Loss epoch  961  is =  1.1197275462716954\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  962  -----\n",
      "Loss epoch  962  is =  1.1197036596510195\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  963  -----\n",
      "Loss epoch  963  is =  1.1196798079375712\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  964  -----\n",
      "Loss epoch  964  is =  1.1196559910703385\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  965  -----\n",
      "Loss epoch  965  is =  1.1196322089884314\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  966  -----\n",
      "Loss epoch  966  is =  1.1196084616310944\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  967  -----\n",
      "Loss epoch  967  is =  1.1195847489376902\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  968  -----\n",
      "Loss epoch  968  is =  1.1195610708477142\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  969  -----\n",
      "Loss epoch  969  is =  1.1195374273007832\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  970  -----\n",
      "Loss epoch  970  is =  1.119513818236643\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  971  -----\n",
      "Loss epoch  971  is =  1.119490243595164\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  972  -----\n",
      "Loss epoch  972  is =  1.1194667033163408\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  973  -----\n",
      "Loss epoch  973  is =  1.1194431973402936\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  974  -----\n",
      "Loss epoch  974  is =  1.1194197256072702\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  975  -----\n",
      "Loss epoch  975  is =  1.119396288057637\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  976  -----\n",
      "Loss epoch  976  is =  1.1193728846318862\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  977  -----\n",
      "Loss epoch  977  is =  1.119349515270636\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  978  -----\n",
      "Loss epoch  978  is =  1.1193261799146268\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  979  -----\n",
      "Loss epoch  979  is =  1.1193028785047197\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  980  -----\n",
      "Loss epoch  980  is =  1.1192796109819019\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  981  -----\n",
      "Loss epoch  981  is =  1.1192563772872803\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  982  -----\n",
      "Loss epoch  982  is =  1.119233177362085\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  983  -----\n",
      "Loss epoch  983  is =  1.1192100111476684\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  984  -----\n",
      "Loss epoch  984  is =  1.1191868785855024\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  985  -----\n",
      "Loss epoch  985  is =  1.1191637796171834\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  986  -----\n",
      "Loss epoch  986  is =  1.1191407141844265\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  987  -----\n",
      "Loss epoch  987  is =  1.1191176822290676\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  988  -----\n",
      "Loss epoch  988  is =  1.1190946836930629\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  989  -----\n",
      "Loss epoch  989  is =  1.1190717185184904\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  990  -----\n",
      "Loss epoch  990  is =  1.119048786647546\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  991  -----\n",
      "Loss epoch  991  is =  1.1190258880225417\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  992  -----\n",
      "Loss epoch  992  is =  1.1190030225859144\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  993  -----\n",
      "Loss epoch  993  is =  1.118980190280219\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  994  -----\n",
      "Loss epoch  994  is =  1.1189573910481263\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  995  -----\n",
      "Loss epoch  995  is =  1.118934624832426\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  996  -----\n",
      "Loss epoch  996  is =  1.1189118915760259\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  997  -----\n",
      "Loss epoch  997  is =  1.1188891912219532\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  998  -----\n",
      "Loss epoch  998  is =  1.1188665237133497\n",
      "---------------------------------\n",
      "Started  0  -------------\n",
      "Training EPoch ----  999  -----\n",
      "Loss epoch  999  is =  1.118843888993475\n",
      "---------------------------------\n",
      "Started  0  -------------\n"
     ]
    }
   ],
   "source": [
    "fin_loss, fin_acc, fin_acc_test, fin_W, fin_op, fin_loss_test, out = train_my_model(N, H,epochs, chain, X_train, y_train, X_test, y_test, delta, lr, reg_lam);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x255eeb36e48>]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhV1b3/8fc3E4GMkIEEQgijEECmMMgkVaooVpza6rW2t/VqrbUObZ9b7+3ttePt/XmtUx1xrLZq61AnFGudAEEgIEOYEzAhTAmBQMKQcf3+2CcSMSEBcjjJzuf1PPs5yd4b8t1s/ZyVtddZy5xziIiIf4WFugAREQkuBb2IiM8p6EVEfE5BLyLicwp6ERGfU9CLiPhci0FvZk+aWYmZ5TVzfIiZLTazKjP76THHbjOztWaWZ2bPm1l0WxUuIiKt05oW/dPAzOMc3wvcDNzVeKeZ9Q7sz3HODQfCgStPrkwRETlZES2d4Jybb2ZZxzleApSY2axm/v6uZlYDdAN2tKao5ORkl5XV7I8UEZFjLF++fI9zLqWpYy0G/clyzm03s7uAIuAw8A/n3D+aO9/MrgeuB8jMzCQ3NzdYpYmI+I6ZFTZ3LGgPY82sOzAb6Af0AmLM7FvNne+cm+Ocy3HO5aSkNPmmJCIiJyGYo25mAFudc6XOuRrgFWBSEH+eiIg0IZhBXwRMNLNuZmbAucD6IP48ERFpQot99Gb2PDAdSDazYuAOIBLAOfeImaUBuUA8UG9mtwLZzrklZvYSsAKoBT4F5gTlKkREpFmtGXVzVQvHdwEZzRy7A++NQUREQkSfjBUR8TkFvYiIz/kq6P/43mbytu8PdRkiIu2Kb4K+/FA1zy8t4uuPLGZe3s5QlyMi0m74JugTu0Xx6k2TGZIexw1/XsEf39uM1sMVEfFR0AOkxkXz/HUTuXR0b/7w7iZufmElR2rqQl2WiEhIBW2um1CJjgzn7m+MZFDPWP7vnY0UlR1kzrdz6BmvGZJFpHPyVYu+gZlx4/SBPPqtsWwuqeTiBxayurg81GWJiISEL4O+wXnD0nj5B5OICAvjG48u5o1VrZolWUTEV3wd9ABD0+N57abJDO+VwI+e/5Tfv72e2rr6UJclInLa+D7oAZJju/DcdRO5ekImj360hX99ahl7D1aHuiwRkdOiUwQ9QFREGL+7dAR3Xn4mSz/by9f+uFAfrhKRTqHTBH2Db4zrw4vfPwvnHJc/vIiXlxeHuiQRkaDqdEEPMLJPIq//aAqjMxP5yYur+O/X8qiuVb+9iPhTpwx68Prt/3ztBP5tSj+eWVzI1Y9/QsmBI6EuS0SkzXXaoAeICA/jvy7K5r4rR5G3/QAX3r+QRfl7Ql2WiEib6tRB32D2qN68+sPJJHSN4OonlnDfPzdTV695ckTEHxT0AWekxfH6TVO4ZFRv7vnnJr7z5FL2VFaFuiwRkVOmoG8kpksEd39jJP972QiWfraXC+9bwJItZaEuS0TklCjoj2FmXDk+k1dvnExMlwiueuwTHvwgn3p15YhIB6Wgb0Z2r3je+NEUZp3Zi/97ZyPf+9MydeWISIekoD+O2C4R3H/lKH5zyXAWFZRxwX0LmL+pNNRliYicEAV9C8yMayb25bUfTiaxayTffnIpv5u7jqpaLWgiIh2Dgr6VhqZ7XTnXTOzLYwu2ctlDiygorQx1WSIiLVLQn4DoyHB+c8lwHvt2DjvKD3PR/Qt5YWmR1qYVkXZNQX8Svprdk7dvmcbozERuf2UNN/5lBeWHNO2xiLRPCvqTlJYQzZ+vncDtFwzh3XW7ueC+BSzcrOkTRKT9UdCfgrAw44azB/DyDybRNSqcbz2xhDtey+NwtR7Uikj7oaBvAyP7JDL3R1P57uQs/rS4kAvvX8CKon2hLktEBFDQt5muUeHc8bVhPHfdBKpr67ni4UXcOW+D5rkXkZBT0LexSQOSmXfrVK4Ym8FDHxYw+8GPWb/zQKjLEpFOTEEfBHHRkdx5xUge/3YOpRVVXPzAQh76MJ/aOrXuReT0U9AH0Yzsnvzjtml8Nbsnd87byKUPLWLdDrXuReT0ajHozexJMysxs7xmjg8xs8VmVmVmPz3mWKKZvWRmG8xsvZmd1VaFdxQ9YqJ46OqxPHT1GHbuP8zFDyzkD//YqCkUROS0aU2L/mlg5nGO7wVuBu5q4th9wDzn3BBgJLD+RAv0iwtHpPPubWdz8ahe/PH9fC66f6FG5ojIadFi0Dvn5uOFeXPHS5xzy4CaxvvNLB6YBjwROK/aOVd+auV2bN1jorj7G6N46rvjOFhVy+UPL+I3b67jUHVtqEsTER8LZh99f6AUeMrMPjWzx80sprmTzex6M8s1s9zSUn9PBfyVM1J557ZpXD0hkycWbmXmvQu0KLmIBE0wgz4CGAM87JwbDRwEbm/uZOfcHOdcjnMuJyUlJYhltQ9x0ZH89pIR/PX6iYQZ/MvjS/jJ31ZRpsVNRKSNBTPoi4Fi59ySwPcv4QW/NDKhfxLzbp3GjdMH8NrK7Zx790f8dVmRli4UkTYTtKB3zu0CtpnZGYFd5wLrgvXzOrLoyHD+feYQ3rplKoNT4/jZy2v45pzFbNpdEerSRMQHrKW51M3seWA6kAzsBu4AIgGcc4+YWRqQC8QD9UAlkO2cO2Bmo4DHgShgC/Bd51yLQ01ycnJcbm7uyV5Th+ac48Xlxfz+rfVUHKnlumn9ufmcQXSNCg91aSLSjpnZcudcTpPH2uOiGZ056BvsPVjN/769nr/lFpPRvSu/nj2Mc4b0DHVZItJOHS/o9cnYdqpHTBR3XjGSv14/ka6R4Xzv6VyueyaXorJDoS5NRDoYBX07N6F/EnNvnsrtFwxhUf4eZtzzEX/4x0aNvReRVlPQdwBREWHccPYA3v/pdGaNSOeP7+cz4w8f8ebqHVqvVkRapKDvQHrGR3PPN0fx0g1n0T0mipue+5Qr53yiaZBF5LgU9B1QTlYPXr9pCv9z6Qg27a5g1v0LuOO1PC1QLiJNUtB3UOFhxr9MyOSDn07nWxP78uwnhUy/60Oe+ngrNZr3XkQaUdB3cIndovj17OHMvXkqw3sl8Ks31nHePfN5Z+0u9d+LCKCg942h6fE8e+14nvrXcYSHGd9/djnfnPMJq4s79YShIoKC3lfMjK8MSWXeLVP57SXDKSip5OIHPubWFz5le/nhUJcnIiGiT8b6WMWRGh7+sIAnFm4F4Nop/fjB9AHERUeGuDIRaWuaAqGT215+mP+bt4FXV+6ge7dIbpw+kGvO6kt0pObPEfELTYHQyfVO7Mq9V47mjZumMLx3Ar97az1fuetD/rqsiFqN0BHxPQV9JzIiI4Fnr53Ac9dNoGd8ND97eQ3n3Tuft9bs1AgdER9T0HdCkwYk8/cbJ/HoNWMJN+PGv6xg9oMfs2BzqQJfxIcU9J2UmXH+sDTm3TqNu74+krLKaq55YilXP76E5YXNrgUvIh2QHsYKAFW1dTy3pIgHP8hnT2U1Uwclc+uMwYzt2z3UpYlIK2jUjbTaoepa/vxJIY9+tIWygwp8kY5CQS8n7FB1Lc8uLuTR+VvYe7CaaYNTuHXGIMZkKvBF2iMFvZy0g1W1PPtJIXMCgX92IPBHK/BF2hUFvZyyg1W1PLO4kDnzC9h3qIapg5K5cfpAJvbvgZmFujyRTk9BL22moYX/+IKt7KmsYnRmIj+cPpBzhqQSFqbAFwkVBb20uSM1dbyYu41H52+heN9hzugZx41fGcCsEelEhGvUrsjppqCXoKmpq+fN1Tt46IMCNpdUktmjG98/uz+Xj8nQXDoip5GCXoKuvt7xz/W7efDDAlZtKyc1rgvXTunHVRMyiddsmSJBp6CX08Y5x+KCMh76sICF+XuIiQrnm+My+e7kLPr06Bbq8kR8S0EvIZG3fT+PL9jCm6t3Uu8cF4xI57qp/RnVJzHUpYn4joJeQmrn/sM8/fFnPLe0iIojtYzL6s6/Te3PjKE9CddIHZE2oaCXdqGyqpa/LtvGkwu3sr38MFlJ3bh2Sj8uH5tBt6iIUJcn0qEp6KVdqa2rZ97aXTy2YCurtpUTFx3BN3L6cM3EvmQlx4S6PJEOSUEv7ZJzjhVF+3h6USFvr9lJnXNMH5zCdyZlMW1Qij6AJXICFPTS7u0+cITnlhTx3NIiSiuq6JccwzUT+3JFToaGZ4q0goJeOozq2nreztvJnxZ9xoqicrpFhXP5mAy+fVZfBvWMC3V5Iu2Wgl46pDXF+/nT4s94fdUOqmvrGd+vB/8yPpOZw9P0qVuRY5xS0JvZk8BFQIlzbngTx4cATwFjgJ875+465ng4kAtsd85d1JqCFfTSWFllFS8uL+b5pUUUlh0isVskl4/J4KrxmQxMjQ11eSLtwqkG/TSgEnimmaBPBfoClwD7mgj6HwM5QLyCXk5Ffb1j8ZYynltSxDtrd1Fb7xjfrwdXT8jk/GFq5Uvndrygb3HwsnNuvpllHed4CVBiZrOa+MEZwCzgd8CPW1uwSFPCwozJA5OZPDCZ0ooqXgq08m95YSXdA638K9XKF/mSYH9K5V7g34EWn6KZ2fXA9QCZmZlBLks6upS4Lvxg+gC+P60/iwrKeG5pIU8v+ozHF25lbN/ufH1sBrPOTCdOI3ZEghf0ZtbQr7/czKa3dL5zbg4wB7yum2DVJf4SFmZMGZTMlEFeK//lFcW8mLuN219Zwy/fWMuFw9O5YmwGE/snaVy+dFrBbNFPBi42swuBaCDezP7snPtWEH+mdGIpcV244Wyvlb9yWzkvLi/mjVU7eOXT7WR078rlYzK4YmyGZtGUTqdVwysDffRvNvUwttE5vwQqj30YGzg2HfipHsbK6Xakpo531u7ipeXFLMzfg3NwVv8kvp6TwczhaZpjR3zjVEfdPA9MB5KB3cAdQCSAc+4RM0vDGz4ZD9TjjdDJds4daPR3TEdBLyG2vfwwrywv5qUVxRSWHaJbVDjnZfdk9ujeTB2YrCUQpUPTB6ZEGnHOseyzfby6cjtzV+9k/+EakmKiuOjMdC4Z3ZtRfRIxU3++dCwKepFmVNXW8dHGUl5buYN31++muraevkndmD2qN5eM6kX/FA3VlI5BQS/SCgeO1DAvbxevrdzOooIynIMzMxKYPao3s0akk5YQHeoSRZqloBc5QbsPHOGNVTv4+6fbWbvDe9w0Lqs7s0akc8GIdHrGK/SlfVHQi5yCgtJK3lq9k7lrdrJhVwVmMC6rBxedmc7M4Wmkxin0JfQU9CJtJL+kgrmrdzF3zQ427a7EDCb068GsEenMHJ5OSlyXUJconZSCXiQINu2uYG6gpZ9fUkmYwYR+ScwcnsZ5w3qSntA11CVKJ6KgFwki5xybdlcyd81O5q7eQUHpQQBGZiRw3rA0zh/WkwEpsRqyKUGloBc5jfJLKvnHul28s3Y3q7aVA9A/OYbzhnkt/VEZiZp3R9qcgl4kRHbtP8K7gdD/ZEsZtfWO1LgufDW7J+cPS2Ni/ySiIvSJXDl1CnqRdmD/oRre37ibf6zdzYcbSzlcU0dclwimDU7hK0NSmX5GCsmxepgrJ0dBL9LOHKmpY+HmPby7bjcfbCyhpKIKMxjVJ5FzzkjlnKGpZKfHq19fWk1BL9KO1dc71u08wHvrS3h/w25WFe8HIC0+mq8MSeXcIalMHphM1ygtlSjNU9CLdCAlFUf4cGMpH2woYf6mUg5W1xEVEcakAUl85YxUpg1OISupm1r78gUKepEOqrq2nqVb9/L+Bq+1/1nZIQD69OjKtEEpTBucwqQBSVoyURT0In7x2Z6DzN9cyvxNe1hcsIeD1XWEhxljMhM/D/7hvRMI1/DNTkdBL+JD1bX1rCjax/xNpSzYvIc1272+/e7dIpk8MJlpg1OYNihFs252Egp6kU6grLKKhfl7+CgQ/KUVVQAMTI1l0oAkJg1IYmL/JBK7RYW4UgkGBb1IJ+OcY8OuChZsLuXj/DKWfbaXQ9V1mEF2ejyTByZz1oAkxmf1IKaL1s31AwW9SCdXXVvP6uJyFhWUsahgDysKy6muqycizBjZJzHQ4k9mdGYi0ZEaxtkRKehF5AsOV9exvHAfiwr2sKigjNXF5dQ76BIRxti+3ZnQL4nx/Xoo+DuQ4wW9fmcT6YS6RoUzZVAyUwYlA94yiku37GVRQRmLt5Rx73ubcA4iw42RGYmM69eD8f16MLZvd+I1lLPDUYteRL5k/+EalhfuZcnWvSzdupc1xfuprXeEGWT3imd8VhLj+3VnXFYPkjQ/T7ugrhsROSWHqmtZWVT+efCvKNpHVW094I3qGd+vBzl9uzO2b3cye+hTu6GgoBeRNlVdW8+a7eUs3bqPpVvLyP1sHxVVtQAkx0YxOtML/TGZ3TkzI0H9/KeBgl5Egqqu3rG5pILlhftYXriPT4vK2brHW2krIswY1juBMZmJjA20+rXMYttT0IvIaVdWWcWnReUsL/LCf3VxOUdqvO6e9IRoxgRa/KMzE8lOj1er/xRp1I2InHZJsV2Ykd2TGdk9Aaipq2f9zgOsKNzH8qJyVhTuY+7qnYDX6h+SHsfIjERv65PIwNRYzdnTRtSiF5GQ2bX/CCu3lbO6uJxVxeWsLt5PxRGvr79bVDjDeycwqk8iZ2YkMDIjkYzuXfWgtxlq0YtIu5SWEM3MhDRmDk8DvEVYtpYd9IJ/235Wbivn6UWfUR0Y4dMjJoqRGQmM7OO1/If1jic1TpO2tURBLyLtRliYMSAllgEpsVw6OgPwRvhs3FXBquJyVm3zWv0fbtpMQ2dEalwXhvdOYHiveLJ7JTC8dzy9E9Xyb0xBLyLtWlREGCMyEhiRkcC3JvYF4GBVLXnb97N2xwHyduxn7fYDfLSplLp6L/0Tu0UyvFcCw3rFMyzwJpCVFENYJ+3zV9CLSIcT0yWCCf2TmNA/6fN9R2rq2LCrIvAGsJ+87Qd46uPPqK7zun1iosLJ7hXPsMAbQHaveAamxtIlwv+jfRT0IuIL0ZHhjOqTyKg+iZ/vq6mrZ/PuSvJ27GfdjgPkbd/P33K3cai6DoDwMGNASgxD0uIZkh7H0MBrWny0r7p+Wgx6M3sSuAgocc4Nb+L4EOApYAzwc+fcXYH9fYBngDSgHpjjnLuvDWsXETmuyPAwsgOt9wZ19Y6tew6yYdcBNuysYMOuAywv3Mfrq3Z8fk5C10iGpMUxND2eIWlxDEmPZ3DPWLpFdcy2cYvDK81sGlAJPNNM0KcCfYFLgH2Ngj4dSHfOrTCzOGA5cIlzbl1LRWl4pYicbvsP17BpdwUbdh5g/S7vdeOuCg4GWv9mkJUU4wV/mhf8g3rGkZXUjYjwsBBXf4rDK51z880s6zjHS4ASM5t1zP6dwM7A1xVmth7oDbQY9CIip1tC10jGZfVgXFaPz/fV1zuK9x1mfaPW/4ZdFcxbu+vzUT+R4Ub/5FgG9YxlUGpc4A0glr5JMUS2gzcAOE199IE3itHAkuOccz1wPUBmZubpKEtE5LjCwozMpG5kJnXj/GFpn+8/VF1LQclBNpdUsGl3JZt3V7C6eD9z1+z8whtAv+QYBvWMY3BqHIN6xjI4RG8AQQ96M4sFXgZudc4daO4859wcYA54XTfBrktE5GR1i4r4fMhnY4er6ygorWTTbu8NIL+kgjXF+3mrqTeA1DgGpMYyICWGASmx9E+JCdozgKAGvZlF4oX8X5xzrwTzZ4mIhFrXwLQNw3s3/QbQ+DeAvB37eTtvJ/WNmrUDU2N597ZpbT7iJ2hBb16lTwDrnXN3B+vniIi0d829ARypqaOw7BD5JZUUlFZyqLouKMM6WzO88nlgOpBsZsXAHUAkgHPuETNLA3KBeKDezG4FsoEzgWuANWa2MvDX/adz7q02vwoRkQ4oOjKcM9LiOCMtLqg/pzWjbq5q4fguIKOJQwsB/3ziQESkg2ofY39ERCRoFPQiIj6noBcR8TkFvYiIzynoRUR8TkEvIuJzCnoREZ9T0IuI+JyCXkTE5xT0IiI+p6AXEfE5Bb2IiM8p6EVEfE5BLyLicwp6ERGfU9CLiPicgl5ExOcU9CIiPqegFxHxOQW9iIjPKehFRHxOQS8i4nMKehERn/NX0K9eDbW1oa5CRKRd8U/Q79sHZ58NOTmwbFmoqxERaTf8E/SJifDEE1BaChMmwC23QEVFqKsSEQk5/wS9GVx2GaxbBzfeCH/8I2Rnw+uvh7oyEZGQ8k/QN0hIgAcegEWLvFb+7Nlw+eWwfXuoKxMRCQn/BX2DiRNhxQr4/e/hrbdg6FC45x6oqQl1ZSIip5V/gx4gMhJuvx3y8mDyZPjxj2HUKHj//VBXJiJy2vg76BsMGOC16l97DQ4fhnPPha9/HYqKQl2ZiEjQdY6gB+9h7cUXw9q18Otfw5tvwpAh8NvfwpEjoa5ORCRoOk/QN+jaFX7xC9iwAS680Ps6Oxv+9jdwLtTViYi0uRaD3syeNLMSM8tr5vgQM1tsZlVm9tNjjs00s41mlm9mt7dV0W2ib1946SV4912IjYVvfhMmTYKPPw51ZSIibao1LfqngZnHOb4XuBm4q/FOMwsHHgQuALKBq8ws++TKDKIZM+DTT70PWxUVwZQp3nDMzZtDXZmISJtoMeidc/Pxwry54yXOuWXAseMWxwP5zrktzrlq4AVg9qkUGzTh4fC978GmTV7//TvveN05t9wCe/aEujoRkVMSzD763sC2Rt8XB/Y1ycyuN7NcM8stLS0NYlnHERPj9dnn58O113ofvBowAH71KzhwIDQ1iYicomAGvTWxr9mnnc65Oc65HOdcTkpKShDLaoW0NHjkEVizxuva+eUvoV8/uPNOOHgwtLWJiJygYAZ9MdCn0fcZwI4g/ry2l50NL78MubneRGk/+5nXwr//fg3JFJEOI5hBvwwYZGb9zCwKuBLomDOMjR3rfeBq4UJvKoVbboFBg+DRR6GqKtTViYgcV2uGVz4PLAbOMLNiM7vWzG4wsxsCx9PMrBj4MfBfgXPinXO1wE3AO8B64G/OubXBu5TTYPJkb/qEf/4TMjLghhu8Fv5998GhQ6GuTkSkSeba4YeEcnJyXG5ubqjLOD7nvDH4v/sdzJ8PKSlw223wwx9CfHyoqxORTsbMljvncpo61vk+GdtWzOC88+Cjj2DBAq975z//EzIz4b//G8rKQl2hiAigoG8bU6bA2297D23PPRd+8xsv8G+6yRuqKSISQgr6tjR2rDdKZ+1ab0qFxx6DwYPh0ku9Vn877CYTEf9T0AdDdjY8+SQUFsLPf+6F/LRpMH48vPCCFj8RkdNKQR9MaWleN05RETz8sPfp2quu8kbq/P73UFIS6gpFpBNQ0J8O3bp5QzHXr/cWKx80yHtwm5EBV1/tzZipbh0RCRIF/ekUFgZf+xq89x6sWwc/+IG3AMqUKd4Sh48+CpWVoa5SRHxGQR8qQ4d6H7TasQPmzPHeBG64AXr18t4Ali1TK19E2oSCPtRiYuC662DFCli0CGbPhqef9h7cnnkm3H23+vJF5JQo6NsLMzjrLHj2Wdi1y5s9MyYGfvIT6N0bLrsM3ngDamtDXamIdDAK+vYoIQG+/3345BPIy/MmUfv4Y29x8z594NZbYelSde2ISKso6Nu7YcPgrruguBhefRUmTvSGak6Y4I3e+cUvvNE8IiLNUNB3FJGRXv/93/8Ou3d7a9xmZcH//I/3Aa3Ro72FUQoLQ12piLQzCvqOKDHRW+P2n//0Wvr33gtRUd7CKFlZMG6c94GsjRtDXamItAOapthP8vO9uXZeecXrwwevtX/ZZd42apT30FdEfOd40xQr6P1q2zavT/+VV7z58uvrvdb+7Nkwa5Y3906XLqGuUkTaiIK+syst9aZeeOUV71O5VVXe0M0ZM7zQv/BCbwiniHRYCno56uBB+OADmDvX27Zt8/aPHOmF/syZ3oieqKjQ1ikiJ0RBL01zzps7/623vND/+GOoq/Na+9OmeYuozJgBI0Z4UzSISLuloJfWKS+HDz/0RvO89x5s2ODtT072Qv/cc+Gcc6B/fz3UFWlnFPRycoqL4f33jwb/jh3e/vR0mDrVm3Vz6lSvxR8eHtpaRTo5Bb2cOue8Fn7DYugLF3oLqgDEx8OkSUeDPyfHm4NfRE4bBb0ER1GRF/gNwZ+X5+0PD/embhg/3tvGjYPhwyEiIrT1iviYgl5Oj717vamWly715tNfutTbB9C1K4wZ44X+uHHelA2DB6vLR6SNKOglNJyDLVu8wG8I/+XL4cgR73h0tNfSHznS20aN8ubgT0gIbd0iHZCCXtqPmhpvGcVVq2DlSu911SooKzt6TlaWF/zZ2d5KXEOHwpAhEBsbsrJF2jsFvbRvznkjehpCv2HbvNkb19+gT58vBv/QoXDGGdCzp4Z7Sqd3vKDX0zEJPTNvCobevb3pGBpUV0NBgTfffuPtscfg0KGj58XEeGP7BwzwtsZf9+3rTfEsEmp1dd5vrqWlX95KSrzXyEj4y1/a/Ecr6KX9ioo62oJvrL7eG+O/fj1s2uS9GWzZ4n09b97RZwDgPezNzPQCv0+fprfERP1GICfmyBEvtE9k27u3+VXhuneHlBTo1y8o5SropeMJC/PCOzMTzj//i8fq62HnzqPhX1DgbYWF3mcAtm//YncQeL8RZGR4od+rF6Sled1BDa8NX/fooakg/KSmBvbv97by8qa35kK78W+Ux+rWDZKSjm59+nivycmQmuoFeuMtKSnov3Uq6MVfwsKOdgNNm/bl43V13uLr27Y1vW3c6K3gVV395T8bEeH9j9qzp/ealOSF//G27t31+YG2VlsLlZVQUeFtDV8f+3q8AC8v9yb4O56wMO/+NQR2RoY3SKBxiDe1RUefnn+HE6D/AqVzCQ8/+kYwcWLT5zjnBcHu3d62a9cXXxu2/Hzv1/Hy8uMv1B4bC3FxrdtiY73PHERHe1vjr5vaunTxrikUXU/Oea3i421VVXD48Mlthw41HeaNu+aOJzzc65ZLSPBeExO9h/cNX7e0xcb6pktPQS9yLDOvJde9uze6pyV1dV7rce/eprd9+44GVsNWVPTF71sbXs0JC/N+cwgPb/614YskogwAAAYiSURBVOsGzrV+q6v7cogf2wV2Krp08d7UGm8xMd6bX0rKF98IG78293VsrPfnfRLUp6pVQW9mTwIXASXOueFNHDfgPuBC4BDwr865FYFjdwKz8NanfRe4xbXHMZ0iJys8/GhXzclq3B1x5EjrtsOHvde6Om+rrf3y1029wtEANGvdFh7u9SOf6NZUgB+7demiZx9B1toW/dPAA8AzzRy/ABgU2CYADwMTzGwSMBk4M3DeQuBs4MOTK1fEpyIijnYZiLSxVr2NOufmA3uPc8ps4Bnn+QRINLN0wAHRQBTQBYgEdp9aySIiciLa6vel3sC2Rt8XA72dc4uBD4Cdge0d59z6pv4CM7vezHLNLLe0tLSNyhIRkbYK+qaeeDgzGwgMBTLw3gzOMbMmxryBc26Ocy7HOZeTkpLSRmWJiEhbBX0x0KfR9xnADuBS4BPnXKVzrhJ4G2hmTJuIiARDWwX968C3zTMR2O+c2wkUAWebWYSZReI9iG2y60ZERIKjtcMrnwemA8lmVgzcgfdgFefcI8BbeEMr8/GGV3438EdfAs4B1uA9mJ3nnHujDesXEZEWtCronXNXtXDcAT9sYn8d8P2TK01ERNqCPqUgIuJz7XLhETMrBQpP8o8nA3vasJyOQNfcOeia/e9Urrevc67JIYvtMuhPhZnlNrfKil/pmjsHXbP/Bet61XUjIuJzCnoREZ/zY9DPCXUBIaBr7hx0zf4XlOv1XR+9iIh8kR9b9CIi0oiCXkTE53wT9GY208w2mlm+md0e6nraipn1MbMPzGy9ma01s1sC+3uY2btmtjnw2j2w38zs/sC/w2ozGxPaKzh5ZhZuZp+a2ZuB7/uZ2ZLANf/VzKIC+7sEvs8PHM8KZd0ny8wSzewlM9sQuN9n+f0+m9ltgf+u88zseTOL9tt9NrMnzazEzPIa7Tvh+2pm3wmcv9nMvnMiNfgi6M0sHHgQb6WrbOAqM8sObVVtphb4iXNuKN7Mnz8MXNvtwHvOuUHAe4Hv4YurfV2Pt9pXR3ULX5wE7/8B9wSueR9wbWD/tcA+59xA4J7AeR3RfXjzQQ0BRuJdu2/vs5n1Bm4GcgJLlIYDV+K/+/w0MPOYfSd0X82sB94cYxOA8cAdDW8OreKc6/AbcBbeoiYN3/8H8B+hritI1/oa8FVgI5Ae2JcObAx8/ShwVaPzPz+vI214U12/hzcp3pt4ax7sASKOvefAO8BZga8jAudZqK/hBK83Hth6bN1+vs8cXbCoR+C+vQmc78f7DGQBeSd7X4GrgEcb7f/CeS1tvmjR08wKVyGqJWgCv6qOBpYAPZ03FTSB19TAaX75t7gX+HegPvB9ElDunAusbv2F6/r8mgPH9wfO70j6A6XAU4HuqsfNLAYf32fn3HbgLrzpzHfi3bfl+Ps+NzjR+3pK99svQd/kClenvYogMrNY4GXgVufcgeOd2sS+DvVvYWYXASXOueWNdzdxqmvFsY4iAhgDPOycGw0c5Oiv803p8Ncc6HqYDfQDegExeF0Xx/LTfW5Jc9d4Stful6BvboUrXwgs2vIy8Bfn3CuB3bsDC7ATeC0J7PfDv8Vk4GIz+wx4Aa/75l68RecbptZufF2fX3PgeALHX8y+PSoGip1zSwLfv4QX/H6+zzOArc65UudcDfAKMAl/3+cGJ3pfT+l++yXolwGDAk/ro/Ae6Lwe4prahJkZ8ASw3jl3d6NDrwMNT96/g9d337C/qdW+Ogzn3H845zKcc1l49/J959zVeAvNXxE47dhrbvi3uCJwfodq6TnndgHbzOyMwK5zgXX4+D7jddlMNLNugf/OG67Zt/e5kRO9r+8A55lZ98BvQucF9rVOqB9StOHDjguBTUAB8PNQ19OG1zUF71e01cDKwHYhXt/ke8DmwGuPwPmGNwKpAG9lr5xQX8MpXv904M3A1/2BpXgrmb0IdAnsjw58nx843j/UdZ/ktY4CcgP3+lWgu9/vM/ArYAOQBzwLdPHbfQaex3sGUYPXMr/2ZO4r8L3AtecD3z2RGjQFgoiIz/ml60ZERJqhoBcR8TkFvYiIzynoRUR8TkEvIuJzCnoREZ9T0IuI+Nz/BydVYp4PZ08MAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(epochs)\n",
    "plt.plot(x, fin_loss)\n",
    "plt.plot(x, fin_loss_test, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x255efc870c8>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAATm0lEQVR4nO3dcZBd533W8e8TqbKJaEiCt5BIciSbTUGJi5Je5ATSknHtVK4hcmfSQWomCMggPFjjJmmnlgfPhJi/8IDtMmjSaECBgToCQgcWU6qBUDpjWlxdUSW27AitlTTaKOBNo8aTBmop/vHHHrnX0vXuWWnl9b76fmbu7H1/73vuvu+eneeePffePakqJEntet1yT0CSdGUZ9JLUOINekhpn0EtS4wx6SWrc6uWewIWuu+662rhx43JPQ5JWlCNHjnyzqibG9b3mgn7jxo0Mh8PlnoYkrShJfueV+jx1I0mNM+glqXEGvSQ1zqCXpMYZ9JLUuF5Bn2RbkuNJppPsnWfch5JUksFI7b5uu+NJfnwpJi1J6m/Bt1cmWQXsA24DZoDDSaaq6ukLxn0/cA/wxEhtM7ADeAfwVuC/JHl7VX1v6ZYgSZpPn/fRbwWmq+okQJKDwHbg6QvG/T3gQeDnRmrbgYNV9QfAV5JMd4/3m5c78XE+9R+O8fTp56/EQ0vSFbf5rW/gk3/5HUv+uH1O3awDTo20Z7raS5K8C9hQVY8tdttu+91JhkmGs7OzvSYuSeqnzxF9xtReulpJktcBDwN/bbHbvlSo2g/sBxgMBpd8JZQr8UwoSStdn6CfATaMtNcDp0fa3w+8E/hvSQD+JDCV5IM9tpUkXWF9Tt0cBiaTbEqyhrkXV6fOd1bVt6vquqraWFUbgf8BfLCqht24HUmuSbIJmAR+a8lXIUl6RQse0VfVuSR7gEPAKuBAVR1L8gAwrKqpebY9luRfM/fC7Tngbt9xI0mvrrzWLg4+GAzK/14pSYuT5EhVDcb1+clYSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN6xX0SbYlOZ5kOsneMf13JXkyydEkjyfZ3NXXJPls1/fFJO9f4vlLkhawYNAnWQXsA24HNgM7zwf5iEer6qaq2gI8CDzU1f8mQFXdBNwG/MMk/hUhSa+iPqG7FZiuqpNV9QJwENg+OqCqnh9prgXOX3F8M/CFbsxzwO8BYy9eK0m6MvoE/Trg1Eh7pqu9TJK7kzzL3BH9PV35i8D2JKuTbAJ+GNgwZtvdSYZJhrOzs4tdgyRpHn2CPmNqdVGhal9V3QjcC9zflQ8w98QwBB4BfgM4N2bb/VU1qKrBxMRE37lLknpY3WPMDC8/Cl8PnJ5n/EHg0wBVdQ74+PmOJL8BnFj8NCVJl6rPEf1hYDLJpiRrgB3A1OiAJJMjzTvowjzJ65Os7e7fBpyrqqeXZOaSpF4WPKKvqnNJ9gCHgFXAgao6luQBYFhVU8CeJLcCZ4EzwK5u8x8ADiV5Efg68JErsQhJ0itL1UWn25fVYDCo4XC43NOQpBUlyZGqGvuuRt/TLkmNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY3rFfRJtiU5nmQ6yd4x/XcleTLJ0SSPJ9nc1b8vyT/v+p5Jct9SL0CSNL8Fgz7JKmAfcDuwGdh5PshHPFpVN1XVFuBB4KGu/lPANVV1E/DDwN9KsnGJ5i5J6qHPEf1WYLqqTlbVC8BBYPvogKp6fqS5Fjh/IdoC1iZZDfwR4AVgdKwk6Qpb3WPMOuDUSHsGuPnCQUnuBj4BrAFu6cqfZ+5J4RvA64GPV9W3LmfCkqTF6XNEnzG1uqhQta+qbgTuBe7vyluB7wFvBTYBP5vkhou+QbI7yTDJcHZ2tvfkJUkL6xP0M8CGkfZ64PQ84w8Cd3b3fxr41ao6W1XPAf8dGFy4QVXtr6pBVQ0mJib6zVyS1EufoD8MTCbZlGQNsAOYGh2QZHKkeQdworv/NeCWzFkLvAf48uVPW5LU14Ln6KvqXJI9wCFgFXCgqo4leQAYVtUUsCfJrcBZ4Aywq9t8H/BZ4CnmTgF9tqq+dAXWIUl6Bam66HT7shoMBjUcDpd7GpK0oiQ5UlUXnRoHPxkrSc0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxvYI+ybYkx5NMJ9k7pv+uJE8mOZrk8SSbu/qHu9r524tJtiz1IiRJr2zBoE+yirmLfN8ObAZ2ng/yEY9W1U1VtQV4EHgIoKp+qaq2dPWPAF+tqqNLugJJ0rz6HNFvBaar6mRVvQAcBLaPDqiq50eaa4FxVxzfCXzuUicqSbo0q3uMWQecGmnPADdfOCjJ3cAngDXALWMe569wwRPEyLa7gd0A119/fY8pSZL66nNEnzG1i47Yq2pfVd0I3Avc/7IHSG4GvltVT437BlW1v6oGVTWYmJjoMSVJUl99gn4G2DDSXg+cnmf8QeDOC2o78LSNJC2LPkF/GJhMsinJGuZCe2p0QJLJkeYdwImRvtcBP8XcE4Ak6VW24Dn6qjqXZA9wCFgFHKiqY0keAIZVNQXsSXIrcBY4A+waeYgfBWaq6uTST1+StJBUjXuDzPIZDAY1HA6XexqStKIkOVJVg3F9fjJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjegV9km1JjieZTrJ3TP9dSZ5McjTJ40k2j/T9UJLfTHKsG3PtUi5AkjS/BYM+ySpgH3A7sBnYORrknUer6qaq2gI8CDzUbbsa+JfAXVX1DuD9zF1AXJL0KulzRL8VmK6qk1X1AnAQ2D46oKqeH2muBc5fcfwDwJeq6ovduN+tqu9d/rQlSX31Cfp1wKmR9kxXe5kkdyd5lrkj+nu68tuBSnIoyf9M8vPjvkGS3UmGSYazs7OLW4EkaV59gj5janVRoWpfVd0I3Avc35VXA+8DPtx9/ckkPzZm2/1VNaiqwcTERO/JS5IW1ifoZ4ANI+31wOl5xh8E7hzZ9ter6ptV9V3gV4B3X8pEJUmXpk/QHwYmk2xKsgbYAUyNDkgyOdK8AzjR3T8E/FCS13cvzP5F4OnLn7Ykqa/VCw2oqnNJ9jAX2quAA1V1LMkDwLCqpoA9SW5l7h01Z4Bd3bZnkjzE3JNFAb9SVf/xCq1FkjRGqi463b6sBoNBDYfD5Z6GJK0oSY5U1WBcn5+MlaTGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuF5Bn2RbkuNJppPsHdN/V5InkxxN8niSzV19Y5L/29WPJvnFpV6AJGl+C14cPMkqYB9wGzADHE4yVVVPjwx7tKp+sRv/QeAhYFvX92xVbVnaaUuS+upzRL8VmK6qk1X1AnAQ2D46oKqeH2muBV5bVxyXpKtYn6BfB5waac90tZdJcneSZ4EHgXtGujYl+e0kv57kR8Z9gyS7kwyTDGdnZxcxfUnSQvoEfcbULjpir6p9VXUjcC9wf1f+BnB9Vb0L+ATwaJI3jNl2f1UNqmowMTHRf/aSpAX1CfoZYMNIez1wep7xB4E7AarqD6rqd7v7R4Bngbdf2lQlSZeiT9AfBiaTbEqyBtgBTI0OSDI50rwDONHVJ7oXc0lyAzAJnFyKiUuS+lnwXTdVdS7JHuAQsAo4UFXHkjwADKtqCtiT5FbgLHAG2NVt/qPAA0nOAd8D7qqqb12JhUiSxkvVa+sNMoPBoIbD4XJPQ5JWlCRHqmowrs9PxkpS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjegV9km1JjieZTrJ3TP9dSZ5McjTJ40k2X9B/fZLvJPm5pZq4JKmfBYO+u7j3PuB2YDOw88IgBx6tqpuqagvwIPDQBf0PA/9pCeYrSVqkPkf0W4HpqjpZVS8AB4HtowOq6vmR5lrgpQvRJrkTOAkcu/zpSpIWq0/QrwNOjbRnutrLJLk7ybPMHdHf09XWAvcCn5rvGyTZnWSYZDg7O9t37pKkHvoEfcbU6qJC1b6qupG5YL+/K38KeLiqvjPfN6iq/VU1qKrBxMREjylJkvpa3WPMDLBhpL0eOD3P+IPAp7v7NwMfSvIg8EbgxST/r6r+8aVMVpK0eH2C/jAwmWQT8HVgB/DTowOSTFbVia55B3ACoKp+ZGTM3wW+Y8hL0qtrwaCvqnNJ9gCHgFXAgao6luQBYFhVU8CeJLcCZ4EzwK4rOWlJUn+puuh0+7IaDAY1HA6XexqStKIkOVJVg3F9fjJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjegV9km1JjieZTrJ3TP9dSZ5McjTJ40k2d/WtXe1oki8m+cmlXoAkaX4LBn2SVcA+4HZgM7DzfJCPeLSqbqqqLcCDwENd/Slg0NW3AZ9JsuAFySVJS6fPEf1WYLqqTlbVC8BBYPvogKp6fqS5Fqiu/t2qOtfVrz1flyS9evocXa8DTo20Z4CbLxyU5G7gE8Aa4JaR+s3AAeBtwEdGgn90293AboDrr79+EdOXJC2kzxF9xtQuOjKvqn1VdSNwL3D/SP2JqnoH8OeA+5JcO2bb/VU1qKrBxMRE/9lLkhbUJ+hngA0j7fXA6XnGHwTuvLBYVc8Avw+8czETlCRdnj5BfxiYTLIpyRpgBzA1OiDJ5EjzDuBEV990/sXXJG8DfhD46hLMW5LU04Ln6KvqXJI9wCFgFXCgqo4leQAYVtUUsCfJrcBZ4Aywq9v8fcDeJGeBF4G/XVXfvBILkSSNl6rX1hthBoNBDYfD5Z6GJK0oSY5U1WBcn5+MlaTGGfSS1DiDXpIaZ9BLUuPa+r8zH/sYHD263LOQpEuzZQs88siSP6xH9JLUuLaO6K/AM6EkrXQe0UtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa95r7f/RJZoHfuYyHuA64mi5ucrWtF1zz1cI1L87bqmrsRbdfc0F/uZIMX+mf77foalsvuOarhWteOp66kaTGGfSS1LgWg37/ck/gVXa1rRdc89XCNS+R5s7RS5JersUjeknSCINekhrXTNAn2ZbkeJLpJHuXez5LJcmGJL+W5Jkkx5L8TFd/c5L/nORE9/VNXT1J/lH3c/hSkncv7wouTZJVSX47yWNde1OSJ7r1/qska7r6NV17uuvfuJzzvhxJ3pjk80m+3O3v914F+/nj3e/1U0k+l+Ta1vZ1kgNJnkvy1Eht0fs1ya5u/IkkuxYzhyaCPskqYB9wO7AZ2Jlk8/LOasmcA362qv4M8B7g7m5te4EvVNUk8IWuDXM/g8nuthv49Ks/5SXxM8AzI+2/DzzcrfcM8NGu/lHgTFX9KeDhbtxK9QvAr1bVnwb+LHPrb3Y/J1kH3AMMquqdwCpgB+3t638GbLugtqj9muTNwCeBm4GtwCfPPzn0UlUr/ga8Fzg00r4PuG+553WF1vrvgduA48BbutpbgOPd/c8AO0fGvzRupdyA9d0v/y3AY0CY+7Tg6gv3N3AIeG93f3U3Lsu9hktY8xuAr1w498b38zrgFPDmbt89Bvx4i/sa2Ag8dan7FdgJfGak/rJxC92aOKLnD39hzpvpak3p/lR9F/AE8Ceq6hsA3dcf6Ia18LN4BPh54MWu/ceB36uqc117dE0vrbfr/3Y3fqW5AZgFPtudsvonSdbS8H6uqq8D/wD4GvAN5vbdEdrf17D4/XpZ+7uVoM+YWlPvG03yR4F/C3ysqp6fb+iY2or5WST5S8BzVXVktDxmaPXoW0lWA+8GPl1V7wJ+nz/8c36cFb/u7tTDdmAT8FZgLXOnLi7U2r6ezyut8bLW3krQzwAbRtrrgdPLNJcll+T7mAv5X6qqX+7K/yfJW7r+twDPdfWV/rP4C8AHk3wVOMjc6ZtHgDcmWd2NGV3TS+vt+v8Y8K1Xc8JLZAaYqaonuvbnmQv+VvczwK3AV6pqtqrOAr8M/Hna39ew+P16Wfu7laA/DEx2r9avYe4FnallntOSSBLgnwLPVNVDI11TwPlX3ncxd+7+fP2vdq/evwf49vk/EVeCqrqvqtZX1Ubm9uN/raoPA78GfKgbduF6z/8cPtSNX3FHeVX1v4FTSX6wK/0Y8DSN7ufO14D3JHl993t+fs1N7+vOYvfrIeADSd7U/SX0ga7Wz3K/SLGEL3b8BPC/gGeBv7Pc81nCdb2PuT/RvgQc7W4/wdy5yS8AJ7qvb+7Gh7l3ID0LPMncOxqWfR2XuPb3A491928AfguYBv4NcE1Xv7ZrT3f9Nyz3vC9jvVuAYbev/x3wptb3M/Ap4MvAU8C/AK5pbV8Dn2PuNYizzB2Zf/RS9ivwN7q1TwN/fTFz8F8gSFLjWjl1I0l6BQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJatz/B3p/9ntXpjg3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, fin_acc, \"r\")\n",
    "plt.plot(x, fin_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56     2\n",
       "104    1\n",
       "69     2\n",
       "55     2\n",
       "132    1\n",
       "      ..\n",
       "71     2\n",
       "106    1\n",
       "14     0\n",
       "92     2\n",
       "102    1\n",
       "Name: Species, Length: 135, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y.astype('int') #convert output to integers 0-9\n",
    "X_data = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.T\n",
    "#X_val = X_val.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [784, 50, 10]\n",
    "H = len(N)-1\n",
    "W = initialise(N, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = [relu, softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "lr = 0.5\n",
    "delta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EPoch ----  0  -----\n",
      "Loss epoch  0  is =  6.164257768858461\n",
      "---------------------------------\n",
      "Started  0  -------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-49a3be565cdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfin_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfin_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfin_acc_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfin_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfin_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfin_loss_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_my_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-df18e7b5e465>\u001b[0m in \u001b[0;36mtrain_my_model\u001b[1;34m(N, H, epochs, chain, X_train, y_train, X_test, y_test, pert, lr)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m           \u001b[0mDw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mApplyPert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m           \u001b[0moutput_pert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mForwardPassgen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m           \u001b[0mloss_pert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_pert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m           \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss_pert\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c7fbab02a33a>\u001b[0m in \u001b[0;36mForwardPassgen\u001b[1;34m(chain, W, input, H)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mz1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fin_loss, fin_acc, fin_acc_test, fin_W, fin_op, fin_loss_test, out = train_my_model(N, H,epochs, chain, X_train, y_train, X_val, y_val, delta, lr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4bbe7d57c95a32d1f644ee2367dd137d325d38ab01c1d3e852db70e81865468"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
