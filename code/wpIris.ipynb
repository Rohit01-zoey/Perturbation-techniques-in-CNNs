{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize, minmax_scale, scale\n",
    "from sklearn.metrics import confusion_matrix, log_loss, mean_squared_error\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\perturbation_on_chip_learning\\\\Perturbation-techniques-in-CNNs\\\\data\\\\Iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    1\n",
       "146    1\n",
       "147    1\n",
       "148    1\n",
       "149    1\n",
       "Name: Species, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.Species.replace({\"Iris-setosa\" : 0, \"Iris-virginica\" : 1, \"Iris-versicolor\":2})\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
       "0              5.1           3.5            1.4           0.2\n",
       "1              4.9           3.0            1.4           0.2\n",
       "2              4.7           3.2            1.3           0.2\n",
       "3              4.6           3.1            1.5           0.2\n",
       "4              5.0           3.6            1.4           0.2\n",
       "..             ...           ...            ...           ...\n",
       "145            6.7           3.0            5.2           2.3\n",
       "146            6.3           2.5            5.0           1.9\n",
       "147            6.5           3.0            5.2           2.0\n",
       "148            6.2           3.4            5.4           2.3\n",
       "149            5.9           3.0            5.1           1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xn = df.iloc[:, 1:5]\n",
    "Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yn = df.Species.replace({\"Iris-setosa\" : 0, \"Iris-virginica\" : 1, \"Iris-versicolor\":2})\n",
    "yn = yn.to_numpy()\n",
    "yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xn = df.iloc[:, 1:5]\n",
    "Xn = Xn.to_numpy()\n",
    "Xn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
       "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
       "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
       "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
       "       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
       "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
       "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
       "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
       "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
       "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
       "       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n",
       "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
       "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
       "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
       "       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n",
       "       [0.25      , 0.875     , 0.08474576, 0.        ],\n",
       "       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
       "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
       "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
       "       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n",
       "       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n",
       "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
       "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
       "       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
       "       [0.75      , 0.5       , 0.62711864, 0.54166667],\n",
       "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
       "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
       "       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n",
       "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
       "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
       "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
       "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
       "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
       "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
       "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
       "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
       "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
       "       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n",
       "       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n",
       "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
       "       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n",
       "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
       "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
       "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
       "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
       "       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
       "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
       "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
       "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
       "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
       "       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n",
       "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
       "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
       "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
       "       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
       "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
       "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
       "       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n",
       "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
       "       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n",
       "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
       "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
       "       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n",
       "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
       "       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n",
       "       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n",
       "       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
       "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
       "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
       "       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
       "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
       "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
       "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
       "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
       "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
       "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
       "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
       "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
       "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
       "       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n",
       "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
       "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
       "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
       "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
       "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
       "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
       "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
       "       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n",
       "       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
       "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
       "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n",
       "       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n",
       "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
       "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
       "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
       "       [0.5       , 0.25      , 0.77966102, 0.54166667],\n",
       "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
       "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
       "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
       "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
       "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
       "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
       "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
       "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
       "       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n",
       "       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n",
       "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
       "       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xn = minmax_scale(Xn, feature_range=(0, 1), copy=False)\n",
    "Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(Xn, yn, test_size=0.1, random_state=42) #split the data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 135) (4, 15)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_init(seed=2):\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  W1 = np.random.rand(50,4) - 0.5\n",
    "  b1 = np.random.rand(50,1) - 0.5\n",
    "  W2 = np.random.rand(3,50) - 0.5 \n",
    "  b2 = np.random.rand(3,1) - 0.5 \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 3)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "def crossEntropy(y,y_pre):\n",
    "  loss=-np.sum(np.multiply(y, np.log(y_pre)), axis = 0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2\n",
    "  A2 = softmax(Z2)\n",
    "  \n",
    "\n",
    "  return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "\n",
    "\n",
    "  return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(Z1, A1, Z2, A2, W1, W2, X, y):\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ2 = (A2 - Y)\n",
    "  \n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T)\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1)\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis=1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(9): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*15: (j+1)*15].T,Y[j*15: (j+1)*15]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "\n",
    "      dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 55.55555555555556\n",
      "Val accuracy: 53.333333333333336\n",
      "Iteration: 2\n",
      "Train accuracy: 64.44444444444444\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 3\n",
      "Train accuracy: 70.37037037037037\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 4\n",
      "Train accuracy: 76.29629629629629\n",
      "Val accuracy: 80.0\n",
      "Iteration: 5\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 6\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 7\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 8\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 9\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 10\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 11\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 12\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 13\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 14\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 15\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 16\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 17\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 18\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 19\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 20\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 21\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 22\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 23\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 24\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 25\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 26\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 27\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 28\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 29\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 30\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 31\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 32\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 33\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 34\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 35\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 36\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 37\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 38\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 39\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 40\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 41\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 42\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 43\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 44\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 45\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 46\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 80.0\n",
      "Iteration: 47\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 48\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 49\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 50\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 51\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 52\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 53\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 54\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 55\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 56\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 57\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 58\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 59\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 60\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 61\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 62\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 63\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 64\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 65\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 66\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 67\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 68\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 69\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 70\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 71\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 72\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 73\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 74\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 75\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 76\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 77\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 78\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 79\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 80\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 81\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 82\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 83\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 84\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 85\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 86\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 87\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 88\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 89\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 90\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 91\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 92\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 93\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 94\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 95\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 96\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 97\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 98\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 99\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 100\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, train_accBP, val_accBP, train_loss, val_loss, sum_weights = batch_grad_descent(x_train,y_train,iter=100, lr=0.01, print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WP(W1, b1, W2, b2, pert, lossBeforePert, X, y):\n",
    "    m = y.shape[0] #m is the number of training examples\n",
    "    Y = one_hot_encoding(y)\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    lossAfterPertW1 = np.zeros_like(W1)\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1pert = W1.copy()\n",
    "            W1pert[i, j] += pert\n",
    "            _, _, _, A2pert = forward(X, W1pert, b1, W2, b2)\n",
    "            lossAfterPertW1[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    dW1 = 1/m * (lossAfterPertW1-lossBeforePert)/pert\n",
    "\n",
    "\n",
    "    db1 = np.zeros_like(b1)\n",
    "    lossAfterPertb1 = np.zeros_like(b1)\n",
    "    for i in range(b1.shape[0]):\n",
    "        b1pert = b1.copy()\n",
    "        b1pert[i]+=pert\n",
    "        _, _, _, A2pert = forward(X, W1, b1pert, W2, b2)\n",
    "        lossAfterPertb1[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    db1 = 1/m * (lossAfterPertb1-lossBeforePert)/pert\n",
    "\n",
    "    \n",
    "    dW2 = np.zeros_like(W2)\n",
    "    lossAfterPertW2 = np.zeros_like(W2)\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2pert = W2.copy()\n",
    "            W2pert[i, j] += pert\n",
    "            _, _, _, A2pert = forward(X, W1, b1, W2pert, b2)\n",
    "            lossAfterPertW2[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    dW2 = 1/m * (lossAfterPertW2-lossBeforePert)/pert\n",
    "\n",
    "\n",
    "    db2 = np.zeros_like(b2)\n",
    "    lossAfterPertb2 = np.zeros_like(b2)\n",
    "    for i in range(b2.shape[0]):\n",
    "        b2pert = b2.copy()\n",
    "        b2pert[i]+=pert\n",
    "        _, _, _, A2pert = forward(X, W1, b1, W2, b2pert)\n",
    "        lossAfterPertb2[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    db2 = 1/m * (lossAfterPertb2-lossBeforePert)/pert\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentWP(X,Y,iter, lr, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(9): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*15: (j+1)*15].T,Y[j*15: (j+1)*15]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "      print(f\"BP Iter {i+1} -> sub iter {j} : {accuracy(predictions(A2), Y1)}\", end = \"\\r\", flush = True)\n",
    "\n",
    "      #dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "      loss = np.sum(crossEntropy(one_hot_encoding(Y1), A2))\n",
    "      dW1, db1, dW2, db2 = WP(W1, b1, W2, b2, pert=pert, lossBeforePert=loss, X=X1, y=Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr = lr)\n",
    "\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1 sub iter 8 : 60.066666666666666\n",
      "Train accuracy: 55.55555555555556\n",
      "Val accuracy: 53.333333333333336\n",
      "Iteration: 2 sub iter 8 : 73.333333333333336\n",
      "Train accuracy: 63.70370370370371\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 3 sub iter 8 : 80.033333333333336\n",
      "Train accuracy: 66.66666666666666\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 4 sub iter 8 : 73.333333333333336\n",
      "Train accuracy: 64.44444444444444\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 5 sub iter 8 : 66.666666666666666\n",
      "Train accuracy: 68.14814814814815\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 6 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 70.37037037037037\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 7 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 73.33333333333333\n",
      "Val accuracy: 60.0\n",
      "Iteration: 8 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 73.33333333333333\n",
      "Val accuracy: 53.333333333333336\n",
      "Iteration: 9 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 76.29629629629629\n",
      "Val accuracy: 60.0\n",
      "Iteration: 10 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.03703703703704\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 11 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 12 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 13 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 14 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 15 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 16 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 17 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 18 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 19 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 20 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 21 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 22 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 23 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 24 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 25 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 26 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 27 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 28 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 29 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 30 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 31 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 32 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 33 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 34 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 35 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 36 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 37 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 78.51851851851852\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 38 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 79.25925925925927\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 39 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 79.25925925925927\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 40 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 79.25925925925927\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 41 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 79.25925925925927\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 42 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 79.25925925925927\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 43 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 80.0\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 44 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 80.0\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 45 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 80.0\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 46 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 80.0\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 47 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 80.0\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 48 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 80.74074074074075\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 49 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 50 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 51 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 52 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 53 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 54 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 55 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 56 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 57 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 58 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 59 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 60 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 61 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 62 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 63 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 64 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 65 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 66 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 67 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 68 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 69 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 70 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 71 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 72 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 73 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 74 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 75 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 76 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 77 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 78 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 79 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 80 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 81 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 82 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 83 sub iter 8 : 93.33333333333333\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 84 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 85 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 86 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 87 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 88 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 89 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 90 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 91 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 92 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 93 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 94 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 95 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 96 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 97 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 98 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 99 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 100 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n"
     ]
    }
   ],
   "source": [
    "batch_grad_descentWP(x_train,y_train,iter=100, lr=0.01, pert=1, print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1 sub iter 8 : 60.033333333333336\n",
      "Train accuracy: 56.2962962962963\n",
      "Val accuracy: 53.333333333333336\n",
      "Iteration: 2 sub iter 8 : 66.666666666666666\n",
      "Train accuracy: 67.4074074074074\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 3 sub iter 8 : 80.033333333333336\n",
      "Train accuracy: 69.62962962962963\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 4 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 74.07407407407408\n",
      "Val accuracy: 80.0\n",
      "Iteration: 5 sub iter 8 : 80.066666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 66.66666666666666\n",
      "Iteration: 6 sub iter 8 : 80.03333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 7 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 8 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 9 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 10 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 11 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 12 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 13 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 14 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 15 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 16 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 17 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 18 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 19 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 20 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 21 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 22 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 23 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 24 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 25 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 26 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 27 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 28 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 29 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 30 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 31 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 32 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 33 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 34 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 35 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 36 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 37 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 38 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 39 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 40 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 41 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 42 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.92592592592592\n",
      "Val accuracy: 80.0\n",
      "Iteration: 43 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 44 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 45 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 46 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 47 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 48 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 49 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 50 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 51 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 52 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 53 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 80.0\n",
      "Iteration: 54 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 55 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 56 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 57 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 58 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 59 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 60 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 61 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 62 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 63 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 64 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 65 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 66 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 67 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 68 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 69 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 70 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 71 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 72 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 73 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 74 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 75 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 76 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 77 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 78 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 79 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 80 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 81 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 82 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 83 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 84 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 85 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 86 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 87 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 88 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 89 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 90 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 91 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 92 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 93 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 94 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 95 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 96 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 97 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 98 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 99 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 100 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Params Initialised\n",
      "Iteration: 1 sub iter 8 : 60.033333333333336\n",
      "Train accuracy: 55.55555555555556\n",
      "Val accuracy: 53.333333333333336\n",
      "Iteration: 2 sub iter 8 : 66.666666666666666\n",
      "Train accuracy: 64.44444444444444\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 3 sub iter 8 : 80.033333333333336\n",
      "Train accuracy: 70.37037037037037\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 4 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 75.55555555555556\n",
      "Val accuracy: 80.0\n",
      "Iteration: 5 sub iter 8 : 80.066666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 6 sub iter 8 : 80.03333333333333\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 7 sub iter 8 : 80.03333333333333\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 8 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 9 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 10 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 11 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 12 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 13 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 14 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 15 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 16 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 17 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 18 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 19 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 20 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 21 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 22 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 23 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 24 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 25 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 26 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 27 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 28 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 29 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 30 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 31 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 32 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 33 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 34 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 35 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 36 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 37 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 38 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 39 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 40 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 41 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 42 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 43 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 44 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 45 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 46 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 47 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 48 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 49 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 50 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 51 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 52 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 53 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 54 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 55 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 56 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 57 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 58 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 59 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 60 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 61 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 62 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 63 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 64 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 65 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 66 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 67 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 68 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 69 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 70 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 71 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 72 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 73 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 74 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 75 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 76 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 77 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 78 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 79 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 80 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 81 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 82 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 83 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 84 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 85 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 86 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 87 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 88 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 89 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 90 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 91 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 92 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 93 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 94 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 95 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 96 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 97 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 98 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 99 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 100 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Params Initialised\n",
      "Iteration: 1 sub iter 8 : 60.033333333333336\n",
      "Train accuracy: 55.55555555555556\n",
      "Val accuracy: 53.333333333333336\n",
      "Iteration: 2 sub iter 8 : 66.666666666666666\n",
      "Train accuracy: 64.44444444444444\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 3 sub iter 8 : 80.066666666666676\n",
      "Train accuracy: 70.37037037037037\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 4 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 76.29629629629629\n",
      "Val accuracy: 80.0\n",
      "Iteration: 5 sub iter 8 : 80.066666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 6 sub iter 8 : 80.03333333333333\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 7 sub iter 8 : 80.03333333333333\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 8 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 9 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 10 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 11 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 12 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 13 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 14 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 15 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 16 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 17 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 18 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 19 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 20 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 21 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 22 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 23 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 24 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 25 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 26 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 27 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 28 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 29 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 30 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 31 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 32 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 33 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 34 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 35 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 36 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 37 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 38 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 39 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 40 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 41 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 42 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 43 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 44 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 45 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 46 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 80.0\n",
      "Iteration: 47 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 48 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 49 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 50 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 51 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 52 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 53 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 54 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 55 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 56 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 57 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 58 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 59 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 60 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 61 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 62 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 63 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 64 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 65 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 66 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 67 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 68 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 69 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 70 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 71 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 72 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 73 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 74 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 75 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 76 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 77 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 78 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 79 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 80 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 81 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 82 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 83 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 84 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 85 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 86 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 87 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 88 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 89 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 90 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 91 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 92 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 93 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 94 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 95 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 96 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 97 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 98 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 99 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 100 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Params Initialised\n",
      "Iteration: 1 sub iter 8 : 60.033333333333336\n",
      "Train accuracy: 55.55555555555556\n",
      "Val accuracy: 53.333333333333336\n",
      "Iteration: 2 sub iter 8 : 66.666666666666666\n",
      "Train accuracy: 64.44444444444444\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 3 sub iter 8 : 80.066666666666676\n",
      "Train accuracy: 70.37037037037037\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 4 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 76.29629629629629\n",
      "Val accuracy: 80.0\n",
      "Iteration: 5 sub iter 8 : 80.066666666666666\n",
      "Train accuracy: 77.77777777777779\n",
      "Val accuracy: 73.33333333333333\n",
      "Iteration: 6 sub iter 8 : 80.03333333333333\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 7 sub iter 8 : 80.03333333333333\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 8 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 9 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 10 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 11 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 12 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 13 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 14 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 15 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 16 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 81.48148148148148\n",
      "Val accuracy: 80.0\n",
      "Iteration: 17 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 18 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 19 sub iter 8 : 66.66666666666666\n",
      "Train accuracy: 82.22222222222221\n",
      "Val accuracy: 80.0\n",
      "Iteration: 20 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 21 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 22 sub iter 8 : 73.33333333333333\n",
      "Train accuracy: 82.96296296296296\n",
      "Val accuracy: 80.0\n",
      "Iteration: 23 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 24 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 25 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 26 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 83.7037037037037\n",
      "Val accuracy: 80.0\n",
      "Iteration: 27 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 28 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 84.44444444444444\n",
      "Val accuracy: 80.0\n",
      "Iteration: 29 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 30 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 31 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 32 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 33 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 34 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 35 sub iter 8 : 80.06666666666667\n",
      "Train accuracy: 85.18518518518519\n",
      "Val accuracy: 80.0\n",
      "Iteration: 36 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 37 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 38 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 39 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 40 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 41 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 42 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 43 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 86.66666666666667\n",
      "Val accuracy: 80.0\n",
      "Iteration: 44 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 45 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 87.4074074074074\n",
      "Val accuracy: 80.0\n",
      "Iteration: 46 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 80.0\n",
      "Iteration: 47 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 48 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 49 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 80.0\n",
      "Iteration: 50 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 51 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 52 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 53 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 86.66666666666667\n",
      "Iteration: 54 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 55 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 56 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 57 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.14814814814815\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 58 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 59 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 60 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 61 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 62 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 63 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 64 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 65 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 66 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 67 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 68 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 69 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 70 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 71 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 72 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 73 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 88.88888888888889\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 74 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 75 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 76 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 77 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 78 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 79 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 89.62962962962962\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 80 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 81 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 82 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 83 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 84 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 85 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 86 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 87 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 90.37037037037037\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 88 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 89 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 90 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 91 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 92 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 93 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 94 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.11111111111111\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 95 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 96 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 97 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 98 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 99 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n",
      "Iteration: 100 sub iter 8 : 86.66666666666667\n",
      "Train accuracy: 91.85185185185185\n",
      "Val accuracy: 93.33333333333333\n"
     ]
    }
   ],
   "source": [
    "pertList = [0.1, 0.01, 0.001, 0.0001]\n",
    "trainAccWP = []\n",
    "valAccWP = []\n",
    "for pert in pertList:\n",
    "    _, _, _, _, train_acc, val_acc, train_loss, val_loss, sum_weights = batch_grad_descentWP(x_train,y_train,100, 0.01, pert = pert, print_op=1);\n",
    "    trainAccWP.append(train_acc)\n",
    "    valAccWP.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy vs epochs for different techiques')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "format = ['r.', 'gx', 'md', 'c^']\n",
    "col = ['r', 'g', 'm', 'c']\n",
    "leg = []\n",
    "for _ in range(len(pertList)):\n",
    "    plt.plot(trainAccWP[_], format[_])\n",
    "    leg.append(f\"Weigth perturbation for {pertList[_]}\")\n",
    "plt.plot(train_accBP, 'kh')\n",
    "leg.append(\"Baseline back propagation\")\n",
    "for _ in range(len(pertList)):\n",
    "    plt.plot(trainAccWP[_], col[_], linewidth=0.2, alpha = 0.3)\n",
    "plt.plot(train_accBP, 'k', linewidth=0.2, alpha = 0.3)\n",
    "plt.legend(leg)\n",
    "plt.xlabel(\"Epochs >>>>>>>>>>\", size = 15)\n",
    "plt.ylabel(\"Accuracy %\", size = 15)\n",
    "plt.title(\"Accuracy vs epochs for different techiques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
