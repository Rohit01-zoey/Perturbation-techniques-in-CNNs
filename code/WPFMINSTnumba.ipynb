{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:14:52.736946Z",
     "iopub.status.busy": "2022-09-30T16:14:52.736567Z",
     "iopub.status.idle": "2022-09-30T16:14:54.875744Z",
     "shell.execute_reply": "2022-09-30T16:14:54.874499Z",
     "shell.execute_reply.started": "2022-09-30T16:14:52.736873Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numba import njit, cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:14:54.878872Z",
     "iopub.status.busy": "2022-09-30T16:14:54.878130Z",
     "iopub.status.idle": "2022-09-30T16:16:19.902309Z",
     "shell.execute_reply": "2022-09-30T16:16:19.901480Z",
     "shell.execute_reply.started": "2022-09-30T16:14:54.878872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:19.903625Z",
     "iopub.status.busy": "2022-09-30T16:16:19.903330Z",
     "iopub.status.idle": "2022-09-30T16:16:19.907863Z",
     "shell.execute_reply": "2022-09-30T16:16:19.906850Z",
     "shell.execute_reply.started": "2022-09-30T16:16:19.903612Z"
    }
   },
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:19.910904Z",
     "iopub.status.busy": "2022-09-30T16:16:19.910598Z",
     "iopub.status.idle": "2022-09-30T16:16:21.223369Z",
     "shell.execute_reply": "2022-09-30T16:16:21.222313Z",
     "shell.execute_reply.started": "2022-09-30T16:16:19.910877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the functions(GENERAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.224889Z",
     "iopub.status.busy": "2022-09-30T16:16:21.224637Z",
     "iopub.status.idle": "2022-09-30T16:16:21.230775Z",
     "shell.execute_reply": "2022-09-30T16:16:21.229732Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.224864Z"
    }
   },
   "outputs": [],
   "source": [
    "def params_init_seeded(seed=2):\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  W1 = np.random.rand(20,784) - 0.5\n",
    "  b1 = np.random.rand(20,1) - 0.5\n",
    "  W2 = np.random.rand(10,20) - 0.5 \n",
    "  b2 = np.random.rand(10,1) - 0.5 \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.232264Z",
     "iopub.status.busy": "2022-09-30T16:16:21.232024Z",
     "iopub.status.idle": "2022-09-30T16:16:21.237423Z",
     "shell.execute_reply": "2022-09-30T16:16:21.236690Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.232241Z"
    }
   },
   "outputs": [],
   "source": [
    "def params_init():\n",
    "\n",
    "  W1 = np.random.rand(20,784) - 0.5\n",
    "  b1 = np.random.rand(20,1) - 0.5\n",
    "  W2 = np.random.rand(10,20) - 0.5 \n",
    "  b2 = np.random.rand(10,1) - 0.5 \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other generic functions egs-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.238646Z",
     "iopub.status.busy": "2022-09-30T16:16:21.238321Z",
     "iopub.status.idle": "2022-09-30T16:16:21.243353Z",
     "shell.execute_reply": "2022-09-30T16:16:21.242380Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.238615Z"
    }
   },
   "outputs": [],
   "source": [
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.244769Z",
     "iopub.status.busy": "2022-09-30T16:16:21.244517Z",
     "iopub.status.idle": "2022-09-30T16:16:21.251797Z",
     "shell.execute_reply": "2022-09-30T16:16:21.250781Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.244746Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "def crossEntropy(y,y_pre):\n",
    "  loss=-np.sum(np.multiply(y, np.log(y_pre + 10**(-16))), axis = 0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.253296Z",
     "iopub.status.busy": "2022-09-30T16:16:21.253012Z",
     "iopub.status.idle": "2022-09-30T16:16:21.258874Z",
     "shell.execute_reply": "2022-09-30T16:16:21.257822Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.253258Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2\n",
    "  A2 = softmax(Z2)\n",
    "  \n",
    "\n",
    "  return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.263773Z",
     "iopub.status.busy": "2022-09-30T16:16:21.263297Z",
     "iopub.status.idle": "2022-09-30T16:16:21.269045Z",
     "shell.execute_reply": "2022-09-30T16:16:21.268183Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.263702Z"
    }
   },
   "outputs": [],
   "source": [
    "def param_update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "\n",
    "\n",
    "  return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.270136Z",
     "iopub.status.busy": "2022-09-30T16:16:21.269901Z",
     "iopub.status.idle": "2022-09-30T16:16:21.276415Z",
     "shell.execute_reply": "2022-09-30T16:16:21.275409Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.270113Z"
    }
   },
   "outputs": [],
   "source": [
    "def backprop(Z1, A1, Z2, A2, W1, W2, X, y):\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ2 = (A2 - Y)\n",
    "  \n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T)\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1)\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis=1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.277660Z",
     "iopub.status.busy": "2022-09-30T16:16:21.277429Z",
     "iopub.status.idle": "2022-09-30T16:16:21.289874Z",
     "shell.execute_reply": "2022-09-30T16:16:21.288644Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.277637Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      #X1, Y1 = X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "\n",
    "      dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "      if (i+1)%20==0:\n",
    "        plt.figure()\n",
    "        plt.subplot(2,2 ,1)\n",
    "        plt.hist(W1.flatten(), bins = 30)\n",
    "        plt.subplot(2,2 ,2)\n",
    "        plt.hist(W2.flatten(), bins = 30)\n",
    "        plt.subplot(2,2, 3)\n",
    "        plt.hist(b1.flatten(), bins = 30)\n",
    "        plt.subplot(2,2, 4)\n",
    "        plt.hist(b2.flatten(), bins = 30)\n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 64.52698412698412\n",
      "Val accuracy: 65.17142857142856\n",
      "Iteration: 2\n",
      "Train accuracy: 68.88888888888889\n",
      "Val accuracy: 69.17142857142858\n",
      "Iteration: 3\n",
      "Train accuracy: 71.2031746031746\n",
      "Val accuracy: 71.67142857142858\n",
      "Iteration: 4\n",
      "Train accuracy: 72.99365079365079\n",
      "Val accuracy: 73.65714285714286\n",
      "Iteration: 5\n",
      "Train accuracy: 74.71904761904761\n",
      "Val accuracy: 75.35714285714286\n",
      "Iteration: 6\n",
      "Train accuracy: 76.16190476190476\n",
      "Val accuracy: 76.57142857142857\n",
      "Iteration: 7\n",
      "Train accuracy: 77.15079365079364\n",
      "Val accuracy: 77.51428571428572\n",
      "Iteration: 8\n",
      "Train accuracy: 77.92222222222223\n",
      "Val accuracy: 78.17142857142856\n",
      "Iteration: 9\n",
      "Train accuracy: 78.57936507936508\n",
      "Val accuracy: 78.7\n",
      "Iteration: 10\n",
      "Train accuracy: 79.15238095238095\n",
      "Val accuracy: 79.17142857142856\n",
      "Iteration: 11\n",
      "Train accuracy: 79.5968253968254\n",
      "Val accuracy: 79.62857142857143\n",
      "Iteration: 12\n",
      "Train accuracy: 79.97460317460317\n",
      "Val accuracy: 79.87142857142857\n",
      "Iteration: 13\n",
      "Train accuracy: 80.35238095238095\n",
      "Val accuracy: 80.15714285714286\n",
      "Iteration: 14\n",
      "Train accuracy: 80.64761904761905\n",
      "Val accuracy: 80.5\n",
      "Iteration: 15\n",
      "Train accuracy: 80.96507936507936\n",
      "Val accuracy: 80.82857142857142\n",
      "Iteration: 16\n",
      "Train accuracy: 81.18730158730159\n",
      "Val accuracy: 81.21428571428572\n",
      "Iteration: 17\n",
      "Train accuracy: 81.46190476190476\n",
      "Val accuracy: 81.52857142857142\n",
      "Iteration: 18\n",
      "Train accuracy: 81.67777777777778\n",
      "Val accuracy: 81.78571428571428\n",
      "Iteration: 19\n",
      "Train accuracy: 81.91587301587302\n",
      "Val accuracy: 81.91428571428571\n",
      "Iteration: 20\n",
      "Train accuracy: 82.1968253968254\n",
      "Val accuracy: 82.18571428571428\n",
      "Iteration: 21\n",
      "Train accuracy: 82.37142857142857\n",
      "Val accuracy: 82.32857142857142\n",
      "Iteration: 22\n",
      "Train accuracy: 82.56031746031745\n",
      "Val accuracy: 82.39999999999999\n",
      "Iteration: 23\n",
      "Train accuracy: 82.70793650793651\n",
      "Val accuracy: 82.45714285714286\n",
      "Iteration: 24\n",
      "Train accuracy: 82.84920634920636\n",
      "Val accuracy: 82.54285714285714\n",
      "Iteration: 25\n",
      "Train accuracy: 83.0\n",
      "Val accuracy: 82.64285714285714\n",
      "Iteration: 26\n",
      "Train accuracy: 83.16190476190476\n",
      "Val accuracy: 82.84285714285714\n",
      "Iteration: 27\n",
      "Train accuracy: 83.26984126984127\n",
      "Val accuracy: 82.95714285714286\n",
      "Iteration: 28\n",
      "Train accuracy: 83.36666666666666\n",
      "Val accuracy: 83.12857142857143\n",
      "Iteration: 29\n",
      "Train accuracy: 83.4984126984127\n",
      "Val accuracy: 83.27142857142857\n",
      "Iteration: 30\n",
      "Train accuracy: 83.57936507936508\n",
      "Val accuracy: 83.35714285714285\n",
      "Iteration: 31\n",
      "Train accuracy: 83.67777777777778\n",
      "Val accuracy: 83.48571428571428\n",
      "Iteration: 32\n",
      "Train accuracy: 83.76825396825397\n",
      "Val accuracy: 83.62857142857143\n",
      "Iteration: 33\n",
      "Train accuracy: 83.85873015873015\n",
      "Val accuracy: 83.68571428571428\n",
      "Iteration: 34\n",
      "Train accuracy: 83.94126984126984\n",
      "Val accuracy: 83.8\n",
      "Iteration: 35\n",
      "Train accuracy: 84.0047619047619\n",
      "Val accuracy: 83.89999999999999\n",
      "Iteration: 36\n",
      "Train accuracy: 84.12380952380953\n",
      "Val accuracy: 84.0\n",
      "Iteration: 37\n",
      "Train accuracy: 84.20793650793651\n",
      "Val accuracy: 84.05714285714285\n",
      "Iteration: 38\n",
      "Train accuracy: 84.31746031746032\n",
      "Val accuracy: 84.11428571428571\n",
      "Iteration: 39\n",
      "Train accuracy: 84.39365079365079\n",
      "Val accuracy: 84.2\n",
      "Iteration: 40\n",
      "Train accuracy: 84.44920634920635\n",
      "Val accuracy: 84.22857142857143\n",
      "Iteration: 41\n",
      "Train accuracy: 84.54444444444445\n",
      "Val accuracy: 84.31428571428572\n",
      "Iteration: 42\n",
      "Train accuracy: 84.61111111111111\n",
      "Val accuracy: 84.34285714285714\n",
      "Iteration: 43\n",
      "Train accuracy: 84.66825396825396\n",
      "Val accuracy: 84.37142857142858\n",
      "Iteration: 44\n",
      "Train accuracy: 84.72857142857143\n",
      "Val accuracy: 84.44285714285714\n",
      "Iteration: 45\n",
      "Train accuracy: 84.78730158730158\n",
      "Val accuracy: 84.5\n",
      "Iteration: 46\n",
      "Train accuracy: 84.86190476190475\n",
      "Val accuracy: 84.62857142857143\n",
      "Iteration: 47\n",
      "Train accuracy: 84.9047619047619\n",
      "Val accuracy: 84.7\n",
      "Iteration: 48\n",
      "Train accuracy: 84.97460317460317\n",
      "Val accuracy: 84.78571428571429\n",
      "Iteration: 49\n",
      "Train accuracy: 85.04126984126984\n",
      "Val accuracy: 84.78571428571429\n",
      "Iteration: 50\n",
      "Train accuracy: 85.0904761904762\n",
      "Val accuracy: 84.87142857142858\n",
      "Iteration: 51\n",
      "Train accuracy: 85.15714285714286\n",
      "Val accuracy: 84.89999999999999\n",
      "Iteration: 52\n",
      "Train accuracy: 85.22222222222223\n",
      "Val accuracy: 84.92857142857143\n",
      "Iteration: 53\n",
      "Train accuracy: 85.27936507936508\n",
      "Val accuracy: 85.08571428571429\n",
      "Iteration: 54\n",
      "Train accuracy: 85.32063492063492\n",
      "Val accuracy: 85.14285714285714\n",
      "Iteration: 55\n",
      "Train accuracy: 85.35079365079366\n",
      "Val accuracy: 85.18571428571428\n",
      "Iteration: 56\n",
      "Train accuracy: 85.39047619047619\n",
      "Val accuracy: 85.27142857142857\n",
      "Iteration: 57\n",
      "Train accuracy: 85.43174603174603\n",
      "Val accuracy: 85.32857142857144\n",
      "Iteration: 58\n",
      "Train accuracy: 85.47460317460317\n",
      "Val accuracy: 85.39999999999999\n",
      "Iteration: 59\n",
      "Train accuracy: 85.51111111111112\n",
      "Val accuracy: 85.39999999999999\n",
      "Iteration: 60\n",
      "Train accuracy: 85.56666666666666\n",
      "Val accuracy: 85.42857142857143\n",
      "Iteration: 61\n",
      "Train accuracy: 85.615873015873\n",
      "Val accuracy: 85.57142857142857\n",
      "Iteration: 62\n",
      "Train accuracy: 85.68095238095238\n",
      "Val accuracy: 85.64285714285714\n",
      "Iteration: 63\n",
      "Train accuracy: 85.7063492063492\n",
      "Val accuracy: 85.64285714285714\n",
      "Iteration: 64\n",
      "Train accuracy: 85.73015873015873\n",
      "Val accuracy: 85.65714285714286\n",
      "Iteration: 65\n",
      "Train accuracy: 85.77777777777777\n",
      "Val accuracy: 85.67142857142858\n",
      "Iteration: 66\n",
      "Train accuracy: 85.83333333333333\n",
      "Val accuracy: 85.64285714285714\n",
      "Iteration: 67\n",
      "Train accuracy: 85.87777777777778\n",
      "Val accuracy: 85.72857142857143\n",
      "Iteration: 68\n",
      "Train accuracy: 85.93015873015874\n",
      "Val accuracy: 85.74285714285715\n",
      "Iteration: 69\n",
      "Train accuracy: 85.97777777777777\n",
      "Val accuracy: 85.8\n",
      "Iteration: 70\n",
      "Train accuracy: 86.02063492063492\n",
      "Val accuracy: 85.82857142857144\n",
      "Iteration: 71\n",
      "Train accuracy: 86.07301587301586\n",
      "Val accuracy: 85.88571428571429\n",
      "Iteration: 72\n",
      "Train accuracy: 86.0968253968254\n",
      "Val accuracy: 85.9\n",
      "Iteration: 73\n",
      "Train accuracy: 86.12698412698413\n",
      "Val accuracy: 85.91428571428571\n",
      "Iteration: 74\n",
      "Train accuracy: 86.15555555555555\n",
      "Val accuracy: 85.95714285714286\n",
      "Iteration: 75\n",
      "Train accuracy: 86.18412698412699\n",
      "Val accuracy: 86.0\n",
      "Iteration: 76\n",
      "Train accuracy: 86.22380952380952\n",
      "Val accuracy: 86.08571428571429\n",
      "Iteration: 77\n",
      "Train accuracy: 86.25079365079365\n",
      "Val accuracy: 86.12857142857143\n",
      "Iteration: 78\n",
      "Train accuracy: 86.28095238095239\n",
      "Val accuracy: 86.11428571428571\n",
      "Iteration: 79\n",
      "Train accuracy: 86.31746031746032\n",
      "Val accuracy: 86.14285714285714\n",
      "Iteration: 80\n",
      "Train accuracy: 86.33333333333333\n",
      "Val accuracy: 86.15714285714286\n",
      "Iteration: 81\n",
      "Train accuracy: 86.35238095238094\n",
      "Val accuracy: 86.15714285714286\n",
      "Iteration: 82\n",
      "Train accuracy: 86.38412698412698\n",
      "Val accuracy: 86.18571428571428\n",
      "Iteration: 83\n",
      "Train accuracy: 86.4126984126984\n",
      "Val accuracy: 86.2\n",
      "Iteration: 84\n",
      "Train accuracy: 86.44603174603175\n",
      "Val accuracy: 86.2\n",
      "Iteration: 85\n",
      "Train accuracy: 86.46666666666667\n",
      "Val accuracy: 86.31428571428572\n",
      "Iteration: 86\n",
      "Train accuracy: 86.4857142857143\n",
      "Val accuracy: 86.27142857142857\n",
      "Iteration: 87\n",
      "Train accuracy: 86.51269841269841\n",
      "Val accuracy: 86.27142857142857\n",
      "Iteration: 88\n",
      "Train accuracy: 86.53492063492064\n",
      "Val accuracy: 86.32857142857144\n",
      "Iteration: 89\n",
      "Train accuracy: 86.57619047619048\n",
      "Val accuracy: 86.32857142857144\n",
      "Iteration: 90\n",
      "Train accuracy: 86.59206349206349\n",
      "Val accuracy: 86.34285714285714\n",
      "Iteration: 91\n",
      "Train accuracy: 86.62380952380953\n",
      "Val accuracy: 86.37142857142858\n",
      "Iteration: 92\n",
      "Train accuracy: 86.65238095238095\n",
      "Val accuracy: 86.41428571428571\n",
      "Iteration: 93\n",
      "Train accuracy: 86.68095238095238\n",
      "Val accuracy: 86.45714285714286\n",
      "Iteration: 94\n",
      "Train accuracy: 86.70476190476191\n",
      "Val accuracy: 86.4857142857143\n",
      "Iteration: 95\n",
      "Train accuracy: 86.72063492063492\n",
      "Val accuracy: 86.5\n",
      "Iteration: 96\n",
      "Train accuracy: 86.76666666666667\n",
      "Val accuracy: 86.5142857142857\n",
      "Iteration: 97\n",
      "Train accuracy: 86.78888888888889\n",
      "Val accuracy: 86.47142857142858\n",
      "Iteration: 98\n",
      "Train accuracy: 86.8111111111111\n",
      "Val accuracy: 86.45714285714286\n",
      "Iteration: 99\n",
      "Train accuracy: 86.83650793650793\n",
      "Val accuracy: 86.5\n",
      "Iteration: 100\n",
      "Train accuracy: 86.86507936507937\n",
      "Val accuracy: 86.52857142857144\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUTklEQVR4nO3df6zldX3n8eerIJio24IzSxEYr2woXfpHwdxQU5qm9UeLsAFM3QaSRehixqaQaGKyGdc/trExRdPa3U0bu6MSaWoRfxFpdWtBMMZE0BmWyq+lDHSMMxkZUKO4m9AdfO8f5zvt4c69c8+5c7/ne879PB/Jyf2e7/me831/z/2c1/3cz/n+SFUhSdrafmroAiRJ/TPsJakBhr0kNcCwl6QGGPaS1ICThy4AYNu2bbW0tDR0Gdqi9u7d+2xVbR9i3bZt9Wmatj0XYb+0tMSePXuGLkNbVJJvD7Vu27b6NE3bdhhHWiHJLUkOJ3l4bN7vJzmY5MHudtmQNUrTMuylY30cuHSV+X9SVRd2ty/OuCbphBj20gpV9VXg+0PXIW2muRiz13SWdn3hmHn7b758gEqac1OStwF7gHdX1Q9WWyjJTmAnwI4dO2ZYXhs2s/2vfK2t/DmyZy9N5sPAvwEuBA4Bf7zWglW1u6qWq2p5+/ZBdgKSjmHPXuvyPwmoqqePTif5CPA3A5YjTc2wH9A8huhqNQmSnFlVh7q7bwEePt7y0rwx7Bsyj39c5lGS24BfA7YlOQD8F+DXklwIFLAfeMdQ9UkbYdhvYZP00u3JH6uqrlll9sdmXoi0iQz7nsy6F21oa95NsueL7bg/hv2ccahFUh/c9VKSGmDPfgH4r62kE2XYz5ChLWkohv0GOK4+2R+u1t4TaZ6tO2af5Pyx07o+mORHSd51vFO+JnlPkn1JHk/ym/1ugiRpPev27KvqcUbnAyHJScBB4A7gdxid8vWPxpdPcgFwNfALwKuAu5P8XFW9sLmlzxeHaKTp+JmZrWn3xnkD8GRVHe/qKFcCn6yq56vqH4F9wMUbLVCSdOKmDfurgdvG7t+U5FvdlX1O6+adBXxnbJkD3bwXSbIzyZ4ke5555pkpy5AkTWPisE9yCnAF8Olu1sSnfF2Np4GVpNmZpmf/ZuCBo6d6raqnq+qFqvoJ8BH+ZajmIHDO2PPO7uZJkgYyTdhfw9gQTpIzxx4bP+XrncDVSU5N8hrgPOAbJ1qoJGnjJtrPPsnLgDfx4tO6fnC1U75W1SNJPgU8ChwBbtzqe+JIerF53NNmHmuapYnCvqr+D/DKFfOuPc7y7wfef2KlSZI2iydCk6QGeLqEdbT+r5+krcGevSQ1wLCXpAY4jCNpy3H49Vj27CWpAYa9JDXAsJdW6E7sdzjJw2PzTk9yV5Inup+nHe81pHlj2EvH+jhw6Yp5u4AvV9V5wJe7+9LCMOylFarqq8D3V8y+Eri1m74VuGqWNUknyr1x1JuVe0Qs+DVpz6iqQ930d4Ez1lowyU5gJ8COHTtmUNqwttKeL1v5+tL27KUpVVUxOgHgWo97rQbNHcNemszTR0/r3f08PHA90lQMe2kydwLXddPXAZ8fsBZpaoa9tEKS24CvA+cnOZDkBuBm4E1JngDe2N2XFoZf0EorVNU1azz0hpkWIm0ie/aS1IBJL0u4H3gOeAE4UlXLSU4HbgeWGF2W8Ler6gdJAvw34DLg/wLXV9UDm196P7bSbmRSH+bxMzKPNc2baXr2v15VF1bVcnd/rSMK38zoIuPnMdrX+MObVawkaWNOZBhnrSMKrwT+okbuA37m6C5rkqRhTBr2Bfxdkr3d0YGw9hGFZwHfGXvugW7eiyTZmWRPkj3PPPPMBkqXJE1q0r1xfqWqDib518BdSf73+INVVUnWPKJwNVW1G9gNsLy8PNVzJUnTmahnX1UHu5+HgTuAi1n7iMKDwDljTz+7mydJGsi6YZ/kZUlecXQa+A3gYdY+ovBO4G0ZeR3ww7HhHknSACYZxjkDuGO0RyUnA39VVX+b5JvAp7qjC78N/Ha3/BcZ7Xa5j9Gul7+z6VVLkqaybthX1VPAL64y/3usckRhd0bAGzelOknSpvAIWklqgGEvSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJaoBhL0kN8ILjUqNWu5Tf/psvX3eZ1k3yvs0je/aS1ADDXpIa4DCONIUk+4HngBeAI1W1PGxF0mQMe2l6v15Vzw5dhDQNh3EkqQH27KXpFPB3SQr4H1W1e+UCSXYCOwF27Ngx4/JOjHvfbF327KXp/EpVvRZ4M3Bjkl9duUBV7a6q5apa3r59++wrlFYxyQXHz0lyb5JHkzyS5J3d/N9PcjDJg93tsrHnvCfJviSPJ/nNPjdAmqWqOtj9PAzcAVw8bEXSZCYZxjkCvLuqHkjyCmBvkru6x/6kqv5ofOEkFwBXA78AvAq4O8nPVdULm1m4NGtJXgb8VFU9103/BvC+gcuSJjLJBccPAYe66eeSPAacdZynXAl8sqqeB/4xyT5GvZ+vb0K9WmCLeuThmDOAO5LA6LPzV1X1t8OWJE1mqi9okywBFwH3A5cANyV5G7CHUe//B4z+ENw39rQDrPLHYZG/xFKbquop4BeHrkPaiInDPsnLgc8C76qqHyX5MPAHjPZO+APgj4H/OOnrdXsx7AZYXl6uaYqWpHmyCP+1TrQ3TpKXMAr6T1TV5wCq6umqeqGqfgJ8hH/5ouogcM7Y08/u5kmSBjLJ3jgBPgY8VlUfGpt/5thibwEe7qbvBK5OcmqS1wDnAd/YvJIlSdOaZBjnEuBa4KEkD3bz/jNwTZILGQ3j7AfeAVBVjyT5FPAooz15bnRPHEka1iR743wNyCoPffE4z3k/8P4TqEuStIk8glaSGuC5cSTpOGZ5vqA+9+qxZy9JDTDsJakBTQ/jeDpXSa2wZy9JDTDsJakBhr0kNaDpMXtpq/L7KK1kz16SGmDYS1IDDHtJaoBhL0kN8AtaDWoRrvAjbQWGvTSQlX/oJv0j5542i2GS3+8sf5cO40hSA5rq2dsjktSq3nr2SS5N8niSfUl29bUeaZZs11pUvfTsk5wE/BnwJuAA8M0kd1bVo32sT1vLRsey+2a71iLraxjnYmBfVT0FkOSTwJWMLkI+Ew7ZqAeDt2tpo/oK+7OA74zdPwD80vgCSXYCO7u7P07yeE+1TGsb8OzQRUxp0Wqeqt584ITX9+oTfoWRdds1bLxt5wNz93ucp3rmqRbYQD0bbcfrPG/itj3YF7RVtRvYPdT615JkT1UtD13HNBat5kWrd1obbdvz9r7MUz3zVAvMXz2T6OsL2oPAOWP3z+7mSYvMdq2F1VfYfxM4L8lrkpwCXA3c2dO6pFmxXWth9TKMU1VHktwEfAk4Cbilqh7pY109mLuhpQksWs2LVi8wk3Y9b+/LPNUzT7XA/NWzrlTV0DVIknrm6RIkqQGGvSQ1oPmwT3J6kruSPNH9PG2N5V5I8mB3m/mXcusdpp/k1CS3d4/fn2Rp1jWuqGe9eq9P8szYe/r2IeocUpJ/n+SRJD9JsuZufLM6RcM8fBbmrZ1vqXZcVU3fgA8Cu7rpXcAH1ljuxwPWeBLwJHAucArw98AFK5b5PeDPu+mrgdvnvN7rgT8d+vc/5A34t8D5wFeA5Y2+l5tYz6CfhXlr51utHTffs2d0uPut3fStwFXDlbKmfz5Mv6r+CTh6mP648e34DPCGJJlhjeMmqbd5VfVYVa13dO0s38uhPwvz1s63VDs27OGMqjrUTX8XOGON5V6aZE+S+5JcNZvS/tlqh+mftdYyVXUE+CHwyplUd6xJ6gX4rSTfSvKZJOes8rgmfy83w9CfhXlr51uqHTdxPvskdwM/u8pD7x2/U1WVZK19UV9dVQeTnAvck+Shqnpys2ttyF8Dt1XV80newai39vqBa9p0x2t7VfX5eapn/I6fhYktTDtuIuyr6o1rPZbk6SRnVtWhJGcCh9d4jYPdz6eSfAW4iNF43ixMcpj+0WUOJDkZ+Gnge7Mp7xjr1ltV47V9lNF48ZZzvLY3oU09RcOcfxbmrZ1vqXbsMM7ocPfruunrgGN6W0lOS3JqN70NuITZntZ2ksP0x7fjrcA91X2DNIB16+3C5KgrgMdmWN8imeUpGob+LMxbO99a7Xjob4iHvjEa7/sy8ARwN3B6N38Z+Gg3/cvAQ4y+jX8IuGGAOi8D/oFRD+q93bz3AVd00y8FPg3sA74BnDvw+7pevX8IPNK9p/cCPz90WxjgPXoLo3Hg54GngS91818FfPF472VP9Qz+WZi3dr6V2rGnS5CkBjiMI0kNMOwlqQGGvSQ1YC52vdy2bVstLS0NXYa2qL179z5bVduHWLdtW32apm3PRdgvLS2xZ8+eocvQFpXk20Ot27atPk3TtnsbxklyUpL/leRv+lqHNEtJzklyb5JHu7NVvnPomqRJ9dmzfyejAwz+VY/rkGbpCPDuqnogySuAvUnuqqpZHmAnbUgvPfskZwOXMzp8WNoSqupQVT3QTT/HqDPT10nJpE3VV8/+vwL/CXjFWgsk2QnsBNixY0dPZWirWtr1hRfd33/z5TNdf3fRjIuA+1d5bMu07ZXvM8z+vdbm2PSefZJ/Bxyuqr3HW66qdlfVclUtb98+yI4S0oYkeTnwWeBdVfWjlY/btjWP+hjGuQS4Isl+Rif7f32Sv+xhPdLMJXkJo6D/RFV9buh6pEltethX1Xuq6uyqWmJ0lrh7quo/bPZ6pFnrroj0MeCxqvrQ0PVI0/AIWmlylwDXMvpv9egFpi8buihpEr0eVFVVX2F0MWVp4VXV14ChrusrnRB79pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAb2EfZJzktyb5NEkjyR5Zx/rkWYtyS1JDid5eOhapGn01bM/Ary7qi4AXgfcmOSCntYlzdLHgUuHLkKaVi9hX1WHquqBbvo54DHgrD7WJc1SVX0V+P7QdUjTOrnvFSRZAi4C7l8xfyewE2DHjh19lwHA0q4vvOj+/psvn8l617KyHth4TZO81mauT2ubtG1P0h436/c669/9Rtc3j9syS31uW69f0CZ5OfBZ4F1V9aPxx6pqd1UtV9Xy9u3b+yxDminbtuZRb2Gf5CWMgv4TVfW5vtYjSVpfX3vjBPgY8FhVfaiPdUiSJtdXz/4S4Frg9Uke7G6X9bQuaWaS3AZ8HTg/yYEkNwxdkzSJXr6graqvAenjtaUhVdU1Q9cgbYRH0EpSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIaYNhLUgMMe0lqgGEvSQ0w7CWpAYa9JDXAsJekBhj2ktQAw16SGmDYS1IDerlS1WZa2vWFY+btv/nydZfZrPWtXFffJtneE3mtIU26bUP/DqStyJ69JDXAsJekBhj2ktQAw16SGmDYS1IDDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIa0FvYJ7k0yeNJ9iXZ1dd6pFmyXWtR9RL2SU4C/gx4M3ABcE2SC/pYlzQrtmstsr569hcD+6rqqar6J+CTwJU9rUuaFdu1FlaqavNfNHkrcGlVvb27fy3wS1V109gyO4Gd3d3zgcdXealtwLObXuB8amlbYbbb++qq2n6iLzJJu+7mT9K2N2qR2smi1LoodcKxtU7ctk/up571VdVuYPfxlkmyp6qWZ1TSoFraVtja2ztJ296oRXrfFqXWRakTTqzWvoZxDgLnjN0/u5snLTLbtRZWX2H/TeC8JK9JcgpwNXBnT+uSZsV2rYXVyzBOVR1JchPwJeAk4JaqemQDL9XLv8JzqqVthQXc3k1s1ydikd63Ral1UeqEE6i1ly9oJUnzxSNoJakBhr0kNWCuwj7J6UnuSvJE9/O0NZZ7IcmD3W2hviBb73D7JKcmub17/P4kSwOUuSkm2Nbrkzwz9rt8+xB1zrN5/0wsUntelPaY5JYkh5M8vMbjSfLfu+34VpLXTvTCVTU3N+CDwK5uehfwgTWW+/HQtW5w+04CngTOBU4B/h64YMUyvwf8eTd9NXD70HX3uK3XA386dK3zfJvnz8QitedFao/ArwKvBR5e4/HLgP8JBHgdcP8krztXPXtGh57f2k3fClw1XCm9mORw+/H34DPAG5JkhjVuFk8tsDnm+TOxSO15YdpjVX0V+P5xFrkS+IsauQ/4mSRnrve68xb2Z1TVoW76u8AZayz30iR7ktyX5KrZlLYpzgK+M3b/QDdv1WWq6gjwQ+CVM6luc02yrQC/1f0r+pkk56zyeOvm+TOxSO15K7XHSbflRWZ+uoQkdwM/u8pD7x2/U1WVZK39Ql9dVQeTnAvck+Shqnpys2tV7/4auK2qnk/yDkY9wNcPXNPM+ZmYG1u6Pc487KvqjWs9luTpJGdW1aHu35LDa7zGwe7nU0m+AlzEaDxu3k1yuP3RZQ4kORn4aeB7sylvU627rVU1vl0fZTQ+3ZwF/kwsUnveSu1xQ6ftmLdhnDuB67rp64DPr1wgyWlJTu2mtwGXAI/OrMITM8nh9uPvwVuBe6r7VmbBrLutK8YZrwAem2F9i2KePxOL1J63Unu8E3hbt1fO64Afjg31rW3ob55XfMv8SuDLwBPA3cDp3fxl4KPd9C8DDzH6Nv0h4Iah655yGy8D/oFRr+u93bz3AVd00y8FPg3sA74BnDt0zT1u6x8Cj3S/y3uBnx+65nm7zftnYpHa86K0R+A24BDw/xiNx98A/C7wu93jYXQRnSe73/fyJK/r6RIkqQHzNowjSeqBYS9JDTDsJakBhr0kNcCwl6QGGPaS1ADDXpIa8P8Bb1r1wFHwQjkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD7CAYAAACIYvgKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAATsklEQVR4nO3dYaxk5V3H8e9PkDZRY6G7rghsb4mkSkxs8YYS25ja1gZow2JaG3hhl0q9bSwxJr5wtYk19IXURE0bmzYr3QBGt1S06TZdRUolxESQxbSwiMhCaNjNloVisE0jFfz7Ys7S8TJz79y5M+ecufv9JJM5c865c/4z9zn3d5/nnDmTqkKSdGr7ga4LkCR1zzCQJBkGkiTDQJKEYSBJwjCQJDFBGCTZl+REksND885KckeSR5v7M5v5SfLJJEeSPJDkoqGf2d2s/2iS3fN5OZKkaUzSM7gJuHTVvD3AnVV1AXBn8xjgMuCC5rYCfBoG4QF8FHgjcDHw0ZMBIknq3unrrVBVdydZWjV7F/CWZvpm4C7gd5r5t9Tgk2z3JHlVkrObde+oqmcBktzBIGD2r7Xtbdu21dLS6k1Ls3P//fc/U1Xb296ubVvzNE27XjcMxthRVceb6W8CO5rpc4Anh9Y72swbN39NS0tLHDp0aMoSpfUl+UYX27Vta56madebPoDc9AJmdk2LJCtJDiU59PTTT8/qaSVJa5g2DJ5qhn9o7k80848B5w2td24zb9z8l6mqvVW1XFXL27e33nuXpFPStGFwADh5RtBu4ItD89/XnFV0CfBcM5x0O/COJGc2B47f0cyTJPXAuscMkuxncAB4W5KjDM4KugH4fJJrgW8A721WPwhcDhwBvgu8H6Cqnk3yMeC+Zr3rTx5M1nwt7fnyy+Y9ccM7N7yO1JZp26PteHMmOZvo6jGL3jZi3QI+POZ59gH7NlSdJKkVfgJZkjT1qaVq2Sy7wKOea7117G5LW5thsIVM8kdekkYxDDSRSYPGHoS0mAyDHvA/ekldMwwWmCEiaVY8m0iSZBhIkgwDSRIeM2id4/yS+sgwmDP/+G8dSfYB7wJOVNXPNPP+APh14OT11n+vqg52U6E0PYeJpMndxMu/AhbgT6vq9c3NINBCsmegmdrKl7EY8xWw0pZgz0DavOuSPJBkX/N9HdLCsWcgbc6ngY8x+OrXjwF/DPzaqBWTrAArADt37myrvi3L43GzZc9A2oSqeqqqXqyq/wX+HLh4jXX9Slf1lmEgbcLJ7wJv/DJwuKtapM1wmEia0JivgH1LktczGCZ6AvhgV/VJm2EYSBMa8xWwn229EGkODANJM+EX0i82w2CGPLtB0qLyALIkyZ6B5suhA2kx2DOQJNkzkNQee4r9Zc9AkmQYSJIMA0kShoEkCQ8gS+pYHz+sOUlNW+3Atz0DSZJhIEkyDCRJGAaSJAwDSRKeTSTpFNfHs5m6YM9AmlCSfUlOJDk8NO+sJHckebS5P7PLGqVpGQbS5G4CLl01bw9wZ1VdANzZPJYWjsNEm2D38tRSVXcnWVo1exfwlmb6ZuAu4Hfaq0qaDXsG0ubsqKrjzfQ3gR1dFiNNa1M9gyRPAN8GXgReqKrlJGcBtwJLwBPAe6vqP5ME+ARwOfBd4Jqq+tfNbF/qk6qqJDVueZIVYAVg586drdWl77M3P94sega/WFWvr6rl5vG4MdTLgAua2wrw6RlsW+raU0nOBmjuT4xbsar2VtVyVS1v3769tQKlScxjmGgXg7FTmvsrh+bfUgP3AK86uRNJC+wAsLuZ3g18scNapKltNgwK+Ick9zddYBg/hnoO8OTQzx5t5kkLIcl+4J+B1yU5muRa4Abgl5I8Cry9eSwtnM2eTfTmqjqW5MeAO5L8+/DC9cZQR3FcVX1VVVePWfS2VguR5mBTYVBVx5r7E0m+AFxMM4ZaVcdXjaEeA84b+vFzm3mrn3MvsBdgeXl5Q0EiaWua9sCvB4wnN/UwUZIfSvIjJ6eBdwCHGT+GegB4XwYuAZ4bGk6SJHVoMz2DHcAXBmeMcjrwV1X190nuAz7fjKd+A3hvs/5BBqeVHmFwaun7N7FtSdIMTR0GVfU48LMj5n+LEWOoVVXAh6fdniRpfvwEsiTJMJAkGQaSJLxqqTow6nS/J254ZweVSDrJnoEkyTCQJBkGkiQ8ZiBpAquP83iMZ7RFfp/sGUiSDANJkmEgScIwkCThAWRJc7SVv09gq702w0CagSRPAN8GXgReqKrlbiuSNsYwkGbnF6vqma6LkKbhMQNJkmEgzUgB/5Dk/iQrXRcjbZTDRNJsvLmqjiX5MeCOJP9eVXcPr9CExArAzp07u6hRGsuegTQDVXWsuT8BfAG4eMQ6e6tquaqWt2/f3naJ0poMA2mTkvxQkh85OQ28AzjcbVXSxjhMJG3eDuALSWCwT/1VVf19tyVJG2MYTGirfcBEs1NVjwM/23Ud0mYYBtIpbpEvu6zZ8ZiBJMkwkCQZBpIkDANJEoaBJAnPJpI0BU+1bt+8z/qyZyBJMgwkSQ4TqSf84JPULXsGkiR7BpI0L5McaO9LL9iegSTJMJAkGQaSJAwDSRIeQB7JT1eqS9OeZjurdmv7b1df3m/DQL00agfpy1kX0lbU+jBRkkuTPJLkSJI9bW9fmgfbtRZdqz2DJKcBnwJ+CTgK3JfkQFX9W5t1rNaXbpoWU1/btbQRbQ8TXQwcab5AnCSfA3YB7jRaV48vWWG71sJrOwzOAZ4cenwUeOPwCklWgJXm4XeSPNJSbdPYBjzTdRFr6Ht9sIka8/GZbP81M3iOdds1TN+213idffj99qEG6EcdrdawTrvYcLvu3QHkqtoL7O26jkkkOVRVy13XMU7f64PFqHFWZt22+/De9aGGvtTRhxqG6lja6M+1fQD5GHDe0ONzm3nSIrNda+G1HQb3ARckeW2SM4CrgAMt1yDNmu1aC6/VYaKqeiHJdcDtwGnAvqp6qM0aZqzvw1l9rw8Wo8Y1ddiu+/De9aEG6EcdfagBpqwjVTXrQiRJC8ZrE0mSDANJkmGwIUnOSnJHkkeb+zPHrPdikq81t7kfSFzvUghJXpHk1mb5vUmW5l3TFDVek+TpofftA23X2HdJfiXJQ0n+N8nYUxjneWmMrveBPrT1PrTlJPuSnEhyeMzyJPlkU+MDSS5a90mrytuEN+CPgD3N9B7g42PW+06LNZ0GPAacD5wBfB24cNU6vwF8ppm+Cri15fdtkhqvAf6s699xn2/ATwOvA+4Clqd9rzdZQ2f7QB/ael/aMvALwEXA4THLLwf+DghwCXDves9pz2BjdgE3N9M3A1d2V8pLXroUQlV9Dzh5KYRhw3XfBrwtSXpWo9ZRVQ9X1XqfWp73e93lPtCHtt6LtlxVdwPPrrHKLuCWGrgHeFWSs9d6TsNgY3ZU1fFm+pvAjjHrvTLJoST3JLlyzjWNuhTCOePWqaoXgOeAV8+5rpHbb4yqEeDdTZf2tiTnjViu9U36Xk+ry32gD219UdryhttB7y5H0bUkXwF+fMSijww/qKpKMu683NdU1bEk5wNfTfJgVT0261q3mC8B+6vq+SQfZPDf3Vs7rql1a7W/qvpi1zUMP3AfGGsh27JhsEpVvX3csiRPJTm7qo43Xa4TY57jWHP/eJK7gDcwGGech0kuhXBynaNJTgd+FPjWnOoZZd0aq2q4nhsZjE2fctZqfxPa9KUxerwP9KGtL0pb3nA7cJhoYw4Au5vp3cDL/lNLcmaSVzTT24A3Md9LGU9yKYThut8DfLWao0wtWbfGVeOZVwAPt1jfVjLvS2N0uQ/0oa0vSls+ALyvOavoEuC5oeG90eZ5xHur3RiMPd4JPAp8BTirmb8M3NhM/zzwIIOzDB4Erm2hrsuB/2Dwn9dHmnnXA1c0068E/ho4AvwLcH4H7916Nf4h8FDzvv0j8FNd/777dgN+mcHY7/PAU8DtzfyfAA6u9V7PsIZO94E+tPU+tGVgP3Ac+J+mTVwLfAj4ULM8DL5w6bHmdzDy7LPhm5ejkCQ5TCRJMgwkSRgGkiR6fmrptm3bamlpqesytIXdf//9z1TV9ra3a9vWPE3TrlsNg+aTeLcw+NRiAXur6hPj1l9aWuLQoUNtladTUJJvrLN8H/Au4ERV/cyI5QE+weAMk+8C11TVv663Xdu25mm9dj1K28NELwC/XVUXMrh40oeTXNhyDdJG3ARcusbyy4ALmtsK8OkWapJmrtUwqKrjJ/9rqqpvM/gwxiyvmyLNVM3hgmBSH3V2ALm5zvgbgHu7qkGagXlfGE5qRScHkJP8MPA3wG9V1X+tWrbCoLvNzp07O6gOlvZ8+WXznrjhneuuN2qdtmua1XPPa1uzNM/3ZB4mbdvTtqs2f4/T7iOT1DTJz4zb3jQWpR3N++9N6z2DJD/IIAj+sqr+dvXyqtpbVctVtbx9e+sneUgbNfEFwWzb6rNWw6A58+KzwMNV9Sdtbluak41fEEzqobaHid4E/CrwYJKvNfN+r6oOtlyHNJEk+4G3ANuSHAU+CvwgQFV9BjjI4LTSIwxOLX1/N5VKm9NqGFTVPzG4mp60EKrq6nWWF/DhlsqR5sbLUUiSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEm0HAZJ9iU5keRwm9uVNiPJpUkeSXIkyZ4Ry69J8nSSrzW3D3RRp7QZbfcMbgIubXmb0tSSnAZ8CrgMuBC4OsmFI1a9tape39xubLVIaQZaDYOquht4ts1tSpt0MXCkqh6vqu8BnwN2dVyTNHMeM5DWdg7w5NDjo8281d6d5IEktyU5r53SpNk5vesCVkuyAqwA7Ny5c+x6S3u+/LJ5T9zwzrnVNWp7XVtd06Svv+vXMsn2R72WSX6u7XbR+BKwv6qeT/JB4GbgratXmrRtS13oXc+gqvZW1XJVLW/fvr3rcqRjwPB/+uc2815SVd+qquebhzcCPzfqiWzb6rPehYHUM/cBFyR5bZIzgKuAA8MrJDl76OEVwMMt1ifNRNunlu4H/hl4XZKjSa5tc/vSRlXVC8B1wO0M/sh/vqoeSnJ9kiua1X4zyUNJvg78JnBNN9VK02v1mEFVXd3m9qRZqKqDwMFV835/aPp3gd9tuy5plhwmkiQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkuggDJJcmuSRJEeS7Gl7+9JGrddmk7wiya3N8nuTLHVQprQprYZBktOATwGXARcCVye5sM0apI2YsM1eC/xnVf0k8KfAx9utUtq8tnsGFwNHqurxqvoe8DlgV8s1SBsxSZvdBdzcTN8GvC1JWqxR2rS2w+Ac4Mmhx0ebeVJfTdJmX1qnql4AngNe3Up10oykqtrbWPIe4NKq+kDz+FeBN1bVdUPrrAArzcPXAY/MYNPbgGdm8Dxd8jXMx2uqavu4hRO22cPNOkebx4816zyz6rnm0ban0cffA1jXRq1V15rtepTTN1/PhhwDzht6fG4z7yVVtRfYO8uNJjlUVcuzfM62+Ro6s26bHVrnaJLTgR8FvrX6iebRtqfR19+DdW3MrOtqe5joPuCCJK9NcgZwFXCg5RqkjZikzR4AdjfT7wG+Wm12uaUZaLVnUFUvJLkOuB04DdhXVQ+1WYO0EePabJLrgUNVdQD4LPAXSY4AzzIIDGmhtD1MRFUdBA62vNnOu+Yz4GvoyKg2W1W/PzT938CvtF3XJvT192BdGzPb4XR7s5IkL0chSdqaYZDkrCR3JHm0uT9zzHovJvlac+vFgeytcOmDCV7DNUmeHnrvP9BFnaeKPu0PfW7ffWy3SfYlOdGcvjxqeZJ8sqn5gSQXTb2xqtpyN+CPgD3N9B7g42PW+07Xta6q5zTgMeB84Azg68CFq9b5DeAzzfRVwK1d1z3Fa7gG+LOuaz1Vbn3ZH/rcvvvaboFfAC4CDo9Zfjnwd0CAS4B7p93WluwZ8P8vD3AzcGV3pWzIVrj0gZcc6Z++7A99bt+9bLdVdTeDM9TG2QXcUgP3AK9KcvY029qqYbCjqo43098EdoxZ75VJDiW5J8mV7ZS2pq1w6YNJLzny7qZbe1uS80Ys1+z0ZX/oc/te1HY7s0v8tH5q6awk+Qrw4yMWfWT4QVVVknGnTL2mqo4lOR/4apIHq+qxWdeql/kSsL+qnk/yQQb/Cb6145oWmvtDK7Z0u13YMKiqt49bluSpJGdX1fGmy3RizHMca+4fT3IX8AYG44ZdmdmlDzo0ySVHhuu9kcGYtjZhQfaHPrfvRW23k7ynE9mqw0TDlwfYDXxx9QpJzkzyimZ6G/Am4N9aq3C0rXDpg3Vfw6oxzSuAh1us71TUl/2hz+17UdvtAeB9zVlFlwDPDQ0JbkybR8ZbPAL/auBO4FHgK8BZzfxl4MZm+ueBBxmcNfAgcG3Xddf3zw74Dwb/kX2kmXc9cEUz/Urgr4EjwL8A53dd8xSv4Q+Bh5r3/h+Bn+q65q1869P+0Of23cd2C+wHjgP/w+B4wLXAh4APNcvD4MuXHmt+b8vTbstPIEuStuwwkSRpAwwDSZJhIEkyDCRJGAaSJAwDSRKGgSQJw0CSBPwfnwM+OEMqgXwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAATF0lEQVR4nO3db6xkd13H8ffHViBRIi17XWvpcq1p0MZEbG5KI4YoILYloSWioQ9kMSULkSaa+GQNDzSYaDH+iQSDrrhpm2hBUcISkFIKTWNiC1uCdEvFbpsSdrN0W2qqxAQtfH0wZ8lw9869c++dOWfm/t6vZHLPnDk753vP/Z79zDm/mTOpKiRJ7fm+oQuQJA3DAJCkRhkAktQoA0CSGmUASFKjLhy6gM3s27evVldXhy5De9iDDz74dFWt9L1ee1vzNG1fL3QArK6ucvz48aHL0B6W5KtDrNfe1jxN29dbngJKcjTJ2SQnxuZdnOTuJI92Py/q5ifJe5OcTPKlJFeN/ZuD3fKPJjm4k19KkjQ704wB3AZcu27eYeCeqroCuKe7D3AdcEV3OwS8H0aBAfwu8ArgauB3z4WGJGkYWwZAVd0HPLNu9g3A7d307cCNY/PvqJH7gRcluQT4JeDuqnqmqv4TuJvzQ0WS1KOdjgHsr6oz3fTXgf3d9KXA18aWO9XNmzT/PEkOMTp64MCBAzssT5tZPfzx77n/xK2vH6gS6Xzr+xPs0XnZ9SBwVVWSmV1QqKqOAEcA1tbWvFBRD6bZ4dwppb1npwHwZJJLqupMd4rnbDf/NHDZ2HIv6eadBn5+3fx7d7hudXwlL2k3dhoAx4CDwK3dz4+Ozb8lyQcZDfg+24XEXcAfjA38vg74nZ2XrWlt9MpdkmCKAEhyJ6NX7/uSnGL0bp5bgb9PcjPwVeBXu8U/AVwPnAT+B/h1gKp6JsnvA5/vlnt3Va0fWJYk9WjLAKiqmyY89JoNli3gnROe5yhwdFvVSZLmZqE/Cdyyoc/vT3PqyIFhabkZAEtip/8hS9IkXg1UkhrlEUDPfJUuaVF4BCBJjTIAJKlRBoAkNcoAkKY04bsxfi/J6SRf7G7XD1mjtB0OAmumhv78wpzdBrwPuGPd/D+rqj/uvxxpdzwCkKY04bsxpKXlEYC0e7ckeQtwHPjt7kuPzrPXv+ui70+GT3O0ucePSHfNIwBpd94P/DjwcuAM8CeTFqyqI1W1VlVrKysrPZUnTWYASLtQVU9W1ber6jvAXzP6zmtpKRgA0i50X4h0zhuBE5OWlRaNYwBz5qUf9o4J343x80leDhTwBPD2oeqTtssAkKY04bsx/qb3QrQhX2xtn6eAJKlRBoAkNcoAkKRGGQCS1CgHgTVXfm+wtrKIg7etfILYIwBJapQBIEmNMgAkqVEGgCQ1ykFgSb1ZxAHflnkEIEmNMgAkqVEGgCQ1yjGAGfL8pqRl4hGAJDXKIwBJC8+j6/nwCECSGmUASFNKcjTJ2SQnxuZdnOTuJI92Py8askZpOwwAaXq3Adeum3cYuKeqrgDu6e5LS8EAkKZUVfcBz6ybfQNwezd9O3BjnzVJu+EgsLQ7+6vqTDf9dWD/pAWTHAIOARw4cKCH0jSNlgeYPQKQZqSqCqhNHj9SVWtVtbaystJjZdLGdhUASZ5I8lCSLyY53s3bcFAsI+9NcjLJl5JcNYtfQBrYk0kuAeh+nh24HmlqszgC+IWqenlVrXX3Jw2KXQdc0d0OAe+fwbqloR0DDnbTB4GPDliLtC3zOAU0aVDsBuCOGrkfeNG5V07SMkhyJ/CvwMuSnEpyM3Ar8ItJHgVe292XlsJuB4EL+FSSAv6qqo4weVDsUuBrY//2VDfvzNg8B8q0sKrqpgkPvabXQtS7jQaK98IXxe82AH6uqk4n+WHg7iT/Pv5gVVUXDlPrQuQIwNra2rb+rSRpers6BVRVp7ufZ4GPAFczeVDsNHDZ2D9/STdPkjSAHQdAkh9I8sJz08DrgBNMHhQ7BrylezfQNcCzY6eKJEk9280poP3AR5Kce56/q6pPJvk88PfdANlXgV/tlv8EcD1wEvgf4Nd3sW4tsb16PlVaNjsOgKp6HPjpDeZ/gw0GxboPybxzp+uTpEWyF17I+ElgSWqUASBJjTIAJKlRBoAkNcrLQe9Cy5eRlbT8DABJzfBF2/fyFJAkNcoAkKRGGQCS1CgDQJIa5SCwpC2tHzyd9pIHrQ267nQ7DcUjAElqlEcA0gwkeQL4b+DbwHNj35EtLSwDQJqdX6iqp4cuQpqWp4AkqVEGgDQbBXwqyYNJDm20QJJDSY4nOf7UU0/1XJ50PgNAmo2fq6qrgOuAdyZ51foFqupIVa1V1drKykr/FUrrGADSDFTV6e7nWeAjwNXDViRtzQCQdinJDyR54blp4HXAiWGrkrbmu4Ck3dsPfCQJjPapv6uqTw5bkrQ1A0Dapap6HPjpoeuQtstTQJLUKANAkhplAEhSowwASWqUg8BaCMt2GV1pLzAApIZsdH3+nYRta9f536lptve0f5N5vEjyFJAkNcoAkKRGeQpoSh7yStprPAKQpEZ5BCA1zndgtcsjAElqlAEgSY0yACSpUQaAJDXKQWAtpFl9YnUZTTMou9NPmO5k/ZqtRdq+BsAGFukPJEnz0nsAJLkW+HPgAuADVXVr3zVoOS3y2xXtay2jXscAklwA/AVwHXAlcFOSK/usQZo1+1rLqu8jgKuBk913qJLkg8ANwJd7ruN7eMpnOS3QOMFC9rW0lb4D4FLga2P3TwGvGF8gySHgUHf3m0m+AuwDnu6lwp2xvt2ZWX15z7b/yUtnsNot+xom9vaWpvyd9uU9g/6Nh+6xPbX+af7m65ZZv/6p+nrhBoGr6ghwZHxekuNVtTZQSVuyvt1Z9PpmZaPenpWht6HrX8719/05gNPAZWP3X9LNk5aZfa2l1HcAfB64IsmPJXke8GbgWM81SLNmX2sp9XoKqKqeS3ILcBejt8sdraqHp/inczlsniHr251Fr29Tu+jrWRp6G7r+JVx/qmrWhUiSloDXApKkRhkAktSohQyAJL+S5OEk30ky8a1NSa5N8pUkJ5Mc7rG+i5PcneTR7udFE5b7dpIvdre5DwputT2SPD/Jh7rHH0iyOu+atlHbW5M8Nba93tZXbcto6H1kqH1gyB4fuoeTHE1yNsmJCY8nyXu7+r6U5Kotn7SqFu4G/CTwMuBeYG3CMhcAjwGXA88D/g24sqf6/gg43E0fBt4zYblv9rjNttwewG8Af9lNvxn40ALV9lbgfUP33rLcht5HhtgHhuzxRehh4FXAVcCJCY9fD/wzEOAa4IGtnnMhjwCq6pGq2upTkt/9+H1V/S9w7uP3fbgBuL2bvh24saf1bmaa7TFe94eB1yTJgtSmbViAfWSIfWDIHh+8h6vqPuCZTRa5AbijRu4HXpTkks2ecyEDYEobffz+0p7Wvb+qznTTXwf2T1juBUmOJ7k/yY1zrmma7fHdZarqOeBZ4MVzrmva2gB+uTt0/XCSyzZ4XNszz31kiH1gyB5fhh7e9t97sEtBJPk08CMbPPSuqvpo3/Wst1l943eqqpJMei/tS6vqdJLLgc8keaiqHpt1rXvEx4A7q+pbSd7O6FXcqweuaVBD7yPuA9u2dD08WABU1Wt3+RRz/fj9ZvUleTLJJVV1pjvEOjvhOU53Px9Pci/wM4zOI87DNNvj3DKnklwI/BDwjTnVs63aqmq8jg8wOsfctKH3kQXcB4bs8WXo4W3/vZf5FNCQH78/Bhzspg8C570aS3JRkud30/uAVzLfywNPsz3G634T8JnqRo/mbMva1p2rfAPwSA917XXz3EeG2AeG7PFl6OFjwFu6dwNdAzw7dppuY/Masd7laPcbGZ2/+hbwJHBXN/9HgU+sG/X+D0avKN7VY30vBu4BHgU+DVzczV9j9G1QAD8LPMTo3QIPATf3UNd52wN4N/CGbvoFwD8AJ4HPAZf3uM22qu0PgYe77fVZ4CeG7sNFvg29jwy1DwzZ40P3MHAncAb4v+5vfzPwDuAd3eNh9MVEj3Xbe8N3h43fvBSEJDVqmU8BSZJ2wQCQpEYZAJLUqIX7Sshx+/btq9XV1aHL0B724IMPPl1VK32v197WPE3b1wsdAKurqxw/fnzoMrSHJfnqEOu1tzVP0/Z1r6eAklyW5LNJvtxdyfA3+1y/NA/2tZZV30cAzwG/XVVfSPJC4MEkd1fVPD8gJc2bfa2l1OsRQFWdqaovdNP/zeiTcn1dwE2aC/tay2rIi8GtMrouyAPr5h8CDgEcOHCg/8ImWD388fPmPXHr67e9zDxrmuW6Nvpd1tvp+vreTn2a1NfdY7329rJu52WtexkN8jbQJD8I/CPwW1X1X+OPVdWRqlqrqrWVld7fnCHt2GZ9Dfa2Fk/vAZDk+xntJH9bVf/U9/qlebCvtYz6fhdQgL8BHqmqP+1z3dK82NdaVn0fAbwS+DXg1WNfnHx9zzVIs2Zfayn1OghcVf/C6JKl0p5hX2tZeS0gSWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktSoXgMgydEkZ5Oc6HO90rzZ21pGfR8B3AZc2/M6pT7chr2tJdNrAFTVfcAzfa5T6oO9rWV04dAFrJfkEHAI4MCBAxOXWz388fPmPXHr6+dW17Q1LJppttMi/h6zrKnvvphkp729KPWPm+X+N83feuj9fej1b1TDLNa/cIPAVXWkqtaqam1lZWXocqSZsbe1aBYuACRJ/TAAJKlRfb8N9E7gX4GXJTmV5OY+1y/Ni72tZdTrIHBV3dTn+qS+2NtaRp4CkqRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmN6j0Aklyb5CtJTiY53Pf6pXmwr7WMeg2AJBcAfwFcB1wJ3JTkyj5rkGbNvtay6vsI4GrgZFU9XlX/C3wQuKHnGqRZs6+1lFJV/a0seRNwbVW9rbv/a8ArquqWsWUOAYe6uy8DvjKncvYBT8/puedl2WpehnpfWlUru3mCafq6mz/e2z8FnNjNemds0f5W1rO5reqZqq8vnF09s1FVR4Aj815PkuNVtTbv9czSstW8bPXO23hvL9q2sZ7N7dV6+j4FdBq4bOz+S7p50jKzr7WU+g6AzwNXJPmxJM8D3gwc67kGadbsay2lXk8BVdVzSW4B7gIuAI5W1cN91jBm7qeZ5mDZal62endkh329aNvGeja3J+vpdRBYkrQ4/CSwJDXKAJCkRjUTAEkuTnJ3kke7nxdNWO7bSb7Y3XofyNvqkgJJnp/kQ93jDyRZ7bvGDWraqua3JnlqbLu+bYg6h5TkV5I8nOQ7SSa+fa+vS0osyv6waP2+aL2c5GiSs0k2/MxIRt7b1fulJFdtawVV1cQN+CPgcDd9GHjPhOW+OWCNFwCPAZcDzwP+Dbhy3TK/AfxlN/1m4EMDb9dpan4r8L6he2Dg7fSTjD7YeC+wttNtOcN6Bt8fFq3fF7GXgVcBVwEnJjx+PfDPQIBrgAe28/zNHAEw+mj+7d307cCNw5Uy0TSXFBj/PT4MvCZJeqxxPS+DMIWqeqSqtvpUe5/bchH2h0Xr94Xr5aq6D3hmk0VuAO6okfuBFyW5ZNrnbykA9lfVmW7668D+Ccu9IMnxJPcnubGf0r7rUuBrY/dPdfM2XKaqngOeBV7cS3Ubm6ZmgF/uDlE/nOSyDR7X9NtyFhZhf1i0fl/GXt5VzyzcpSB2I8mngR/Z4KF3jd+pqkoy6f2vL62q00kuBz6T5KGqemzWtTbmY8CdVfWtJG9n9Iru1QPXNHOb9V9VfXSR6hm/4/6wLXuql/dUAFTVayc9luTJJJdU1ZnuEOnshOc43f18PMm9wM8wOi/Yh2kuKXBumVNJLgR+CPhGP+VtaMuaq2q8vg8wOv+852zWf1Oa6SUllmB/WLR+X8Ze3lXPtHQK6BhwsJs+CJz3iizJRUme303vA14JfLm3Cqe7pMD47/Em4DPVjQYNZMua152TfAPwSI/1LZM+LymxCPvDovX7MvbyMeAt3buBrgGeHTu1t7W+RrOHvjE6b3gP8CjwaeDibv4a8IFu+meBhxiN/j8E3DxAndcD/8HoVda7unnvBt7QTb8A+AfgJPA54PIF2LZb1fyHwMPddv0s8BND1zzANnojo/Oz3wKeBO7q5v8o8InNtuWc6lmI/WHR+n3Rehm4EzgD/F/XPzcD7wDe0T0eRl9G9Fj3N9rwHWaTbl4KQpIa1dIpIEnSGANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNer/Aa2ufNkYsNObAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _, _, _, train_accBP, val_accBP, train_loss, val_loss, sum_weights = batch_grad_descent(x_train,y_train,100, 0.1, print_op=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding variabilty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic functions related to variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.318303Z",
     "iopub.status.busy": "2022-09-30T16:16:21.318061Z",
     "iopub.status.idle": "2022-09-30T16:16:21.323827Z",
     "shell.execute_reply": "2022-09-30T16:16:21.322714Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.318279Z"
    }
   },
   "outputs": [],
   "source": [
    "#helps in rounding to the nearest integer multiples of the chosen 'step' value!\n",
    "#add clipping here\n",
    "# def roundArbitrary(weightArray, step):\n",
    "#   weightArrayDiv = weightArray / step\n",
    "#   weightArrayDiv = np.round(weightArrayDiv)\n",
    "#   return weightArrayDiv*step\n",
    "def roundArbitrary(weightArray, step, wRange):#updates function with clipping\n",
    "    #wRange is added for the clipping component \n",
    "\n",
    "    weightArrayDiv = np.clip(weightArray, a_min = -wRange, a_max = wRange)\n",
    "    weightArrayDiv = weightArrayDiv / step\n",
    "    weightArrayDiv = np.round(weightArrayDiv)\n",
    "    return weightArrayDiv*step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.325588Z",
     "iopub.status.busy": "2022-09-30T16:16:21.325284Z",
     "iopub.status.idle": "2022-09-30T16:16:21.330316Z",
     "shell.execute_reply": "2022-09-30T16:16:21.329353Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.325564Z"
    }
   },
   "outputs": [],
   "source": [
    "def getVth(mu, sigma, shape):\n",
    "  #last dimension represents the binary rep for each weight\n",
    "  print(shape)\n",
    "  return np.random.normal(loc=mu, scale=sigma, size=shape) #each bit is represented by an sram so we need those many vth values for each mosfet in this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.331790Z",
     "iopub.status.busy": "2022-09-30T16:16:21.331462Z",
     "iopub.status.idle": "2022-09-30T16:16:21.339043Z",
     "shell.execute_reply": "2022-09-30T16:16:21.337665Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.331790Z"
    }
   },
   "outputs": [],
   "source": [
    "def initMosParam(shape, mu, sigma, vDD, precision, k=100):\n",
    "    #modelling both Ion and Ioff  = I0*exp(Vgs-Vth/(eta*kB*T)),\n",
    "    dim1, dim2 = shape\n",
    "    sizeI = (dim1, dim2, precision)\n",
    "    I0On = 1e+06\n",
    "    I0Off = I0On/k\n",
    "    #eta = \n",
    "    #kB = 1.3806452e10-23\n",
    "    #T = 300\n",
    "    VT = 0.026*1.5#should be eqaul to eta x kB x T\n",
    "\n",
    "    #Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "    Vth = np.random.normal(loc=mu, scale=sigma, size = sizeI)\n",
    "    Vth[Vth<=0] = 10e-10\n",
    "\n",
    "    #iOn = ((vDD - Vth)**2)*I0On#scaling the current according to Ioff values arbitraryfor now!!\n",
    "    iOn = I0On * np.exp((0 - Vth)/(VT))\n",
    "    #iOn = I0On * np.ones_like(Vth)\n",
    "\n",
    "\n",
    "\n",
    "    #iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "    iOnNominal = np.sum(iOn)/(dim1*dim2*precision)\n",
    "    Vth = np.random.normal(loc=mu, scale=sigma, size = sizeI)\n",
    "    #iOff = np.random.uniform(low=0, high=1e-8, size = sizeI)#no negative value\n",
    "    iOff = I0Off * np.exp((0 - Vth)/(VT))\n",
    "    #iOff = I0Off * np.ones_like(Vth)\n",
    "    return (iOn, iOnNominal, iOff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.340845Z",
     "iopub.status.busy": "2022-09-30T16:16:21.340536Z",
     "iopub.status.idle": "2022-09-30T16:16:21.345805Z",
     "shell.execute_reply": "2022-09-30T16:16:21.344585Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.340845Z"
    }
   },
   "outputs": [],
   "source": [
    "def clippedWeight(weightArray, precision, step, discreteSteps):\n",
    "    return np.multiply(np.sign(weightArray), np.digitize(np.abs(weightArray), discreteSteps))*step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.347840Z",
     "iopub.status.busy": "2022-09-30T16:16:21.347478Z",
     "iopub.status.idle": "2022-09-30T16:16:21.355371Z",
     "shell.execute_reply": "2022-09-30T16:16:21.354484Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.347784Z"
    }
   },
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, currents, precision, step, discreteSteps, wRange):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  #clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "  #updating the above to the actual round function.\n",
    "  clippedWeightIndexArray = (roundArbitrary(weightArray, step, wRange)/step).astype(np.int64)\n",
    "  clippedWeightIndexArray = np.abs(clippedWeightIndexArray)\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  \n",
    "  iOn, iOnNominal, iOff = currents\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray +=  np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel)\n",
    "\n",
    "  \n",
    "  analogWeightArray  = np.multiply(np.sign(weightArray), analogWeightArray)\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.356547Z",
     "iopub.status.busy": "2022-09-30T16:16:21.356293Z",
     "iopub.status.idle": "2022-09-30T16:16:21.362317Z",
     "shell.execute_reply": "2022-09-30T16:16:21.361644Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.356522Z"
    }
   },
   "outputs": [],
   "source": [
    "# refer to https://stackoverflow.com/questions/9958506/element-wise-string-concatenation-in-numpy\n",
    "\n",
    "## NEED TO CORRECT IT FOR A NEGATIVE VALUES OF THE WEIGHT ARRAY!!!!\n",
    "def weigthBitArray(weightArray, precision, discreteSteps):\n",
    "    clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "\n",
    "    iniArr = np.where(np.bitwise_and(clippedWeightIndexArray, 2**0)>=1, '1', '0') #array of bits at level 0 (2^0)\n",
    "\n",
    "    for j in range(1, precision):\n",
    "        iniArr = np.core.defchararray.add(np.where(np.bitwise_and(clippedWeightIndexArray, 2**j)>=1, '1', '0'), iniArr)\n",
    "    return iniArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T03:01:20.817692Z",
     "iopub.status.busy": "2022-09-30T03:01:20.817202Z",
     "iopub.status.idle": "2022-09-30T03:01:20.841149Z",
     "shell.execute_reply": "2022-09-30T03:01:20.839373Z",
     "shell.execute_reply.started": "2022-09-30T03:01:20.817653Z"
    }
   },
   "outputs": [],
   "source": [
    "def batchGDCompOCUp(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "  W1comp = []\n",
    "\n",
    "  W1, b1, W2, b2 = params_init_seeded(seed=2)\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n",
    "  \n",
    "\n",
    "  W1bpvar, b1bpvar, W2bpvar, b2bpvar = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n",
    "  W1bpvar = roundArbitrary(W1bpvar, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1bpvar = roundArbitrary(b1bpvar, step, wRange)\n",
    "  W2bpvar = roundArbitrary(W2bpvar, step, wRange)\n",
    "  b2bpvar = roundArbitrary(b2bpvar, step, wRange)\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((20, 784), mu, sigma, vDD, precision, k =onoff)#k = Ion/Ioff\n",
    "  b1Currents = initMosParam((20, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W2Currents = initMosParam((10, 20) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b2Currents = initMosParam((10, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      \n",
    "\n",
    "      \n",
    "      W1varocwp = weightTransformWithVariability(W1bpvar, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varocwp = weightTransformWithVariability(b1bpvar, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varocwp = weightTransformWithVariability(W2bpvar, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varocwp = weightTransformWithVariability(b2bpvar, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      #doing the weight perturbation pass first\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1varocwp, b1varocwp, W2varocwp,b2varocwp) \n",
    "      #print(A2)\n",
    "      print(f\"BP with Var Iter {i} -> sub iter {j} : {round(accuracy(predictions(A2), Y1), 3)}\", end = \"\\r\", flush = True)\n",
    "      #lossBeforePert = np.sum(crossEntropy(one_hot_encoding(Y1), A2))\n",
    "\n",
    "\n",
    "      dW1bpvar, db1bpvar, dW2bpvar, db2bpvar = backprop(Z1, A1, Z2, A2, W1varocwp, W2varocwp, X1, Y1)\n",
    "      #print(dW1wp)\n",
    "      #print(W1roundwp/step)\n",
    "\n",
    "      # dW1roundbpvar = roundArbitrary(dW1bpvar, step, wRange)#weights have to maintained as their digitized versions\n",
    "      # db1roundbpvar = roundArbitrary(db1bpvar, step, wRange)\n",
    "      # dW2roundbpvar = roundArbitrary(dW2bpvar, step, wRange)\n",
    "      # db2roundbpvar = roundArbitrary(db2bpvar, step, wRange)\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      W1bpvar, b1bpvar, W2bpvar, b2bpvar = param_update(W1bpvar, b1bpvar, W2bpvar,b2bpvar, dW1bpvar, db1bpvar, dW2bpvar, db2bpvar, lr = lrNP)\n",
    "      W1bpvar = roundArbitrary(W1bpvar, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1bpvar = roundArbitrary(b1bpvar, step, wRange)\n",
    "      W2bpvar = roundArbitrary(W2bpvar, step, wRange)\n",
    "      b2bpvar = roundArbitrary(b2bpvar, step, wRange)\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1bp, b1bp, W2bp, b2bp) \n",
    "      print(f\"                                                                   BP(Software) Iter {i+1} -> sub iter {j} : {accuracy(predictions(A2), Y1)}\", end = \"\\r\", flush = True)\n",
    "\n",
    "      #dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "      #loss = np.sum(crossEntropy(one_hot_encoding(Y1), A2))\n",
    "      #dW1quantwp, db1quantwp, dW2quantwp, db2quantwp = WP(W1quantwp, b1quantwp, W2quantwp, b2quantwp, pert=pert, lossBeforePert=loss, X=X1, y=Y1)\n",
    "      dW1bp, db1bp, dW2bp, db2bp = backprop(Z1, A1, Z2, A2, W1bp, W2bp, X1, Y1)\n",
    "      W1bp, b1bp, W2bp, b2bp = param_update(W1bp, b1bp, W2bp, b2bp,  dW1bp,  db1bp, dW2bp, db2bp,  lr = lrNP)\n",
    "\n",
    "\n",
    "    # #   dW1quantround = roundArbitrary(dW1quantwp, step, wRange)#weights have to maintained as their digitized versions\n",
    "    # #   db1quantround = roundArbitrary(db1quantwp, step, wRange)\n",
    "    # #   dW2quantround = roundArbitrary(dW2quantwp, step, wRange)\n",
    "    # #   db2quantround = roundArbitrary(db2quantwp, step, wRange)\n",
    "\n",
    "\n",
    "    \n",
    "      # if (i==0 or i==1)and (j==0 or j==1):\n",
    "        \n",
    "      #   plt.figure(figsize=(16, 16))\n",
    "      #   plt.gcf().set_dpi(300)\n",
    "      #   plt.suptitle(f\"Intial model plots at iter = {i} epcoch {j} prec = {precision} sig = {sigma}  rat = {onoff}\")\n",
    "      #   plt.subplot(2,2,1)\n",
    "      #   plt.plot(W1bp.flatten(), W1bpvar.flatten(), '.')\n",
    "      #   plt.plot(W1bp.flatten(), W1bp.flatten(), 'r', alpha = 0.3)\n",
    "      #   plt.xlim([-1, 1])\n",
    "      #   plt.ylim([-1, 1])\n",
    "      #   plt.title(f\"Software W1 vs W1 calc with variability included \")\n",
    "      #   plt.xlabel(\"Sofware\")\n",
    "      #   plt.ylabel(\"Var\")\n",
    "      #   plt.subplot(2,2,2)\n",
    "      #   plt.plot(dW1bp.flatten(), dW1bpvar.flatten(), '.')\n",
    "      #   plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'r', alpha = 0.3)\n",
    "      #   #plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'g^', alpha = 0.3)\n",
    "      #   plt.xlim([-0.07, 0.07])\n",
    "      #   plt.ylim([-0.07, 0.07])\n",
    "      #   plt.title(f\"Weight updates in software vs variability\")\n",
    "      #   plt.xlabel(\"Sofware\")\n",
    "      #   plt.ylabel(\"Var\")\n",
    "      #   plt.subplot(2,2,3)\n",
    "      #   plt.plot(dW1bp.flatten(), dW1roundbpvar.flatten(), '.')\n",
    "      #   plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'r', alpha = 0.3)\n",
    "      #   plt.xlim([-0.07, 0.07])\n",
    "      #   plt.ylim([-0.07, 0.07])\n",
    "      #   plt.title(f\"Weight updates in software vs variability(dW1 rounded)\")\n",
    "      #   plt.xlabel(\"Sofware\")\n",
    "      #   plt.ylabel(\"Var\")\n",
    "      #   plt.subplot(2,2,4)\n",
    "      #   plt.plot(dW1bpvar.flatten(), dW1roundbpvar.flatten(), 'k.', alpha = 0.3)\n",
    "      #   plt.xlim([-0.07, 0.07])\n",
    "      #   plt.ylim([-0.07, 0.07])\n",
    "      #   plt.title(f\"dW1 vs its quantization\")\n",
    "      #   plt.xlabel(\"Sofware\")\n",
    "      #   plt.ylabel(\"Var\")\n",
    "    \n",
    "\n",
    "    # if i==iter-1:\n",
    "\n",
    "    #   plt.figure(figsize=(16, 8))\n",
    "    #   plt.gcf().set_dpi(300)\n",
    "    #   plt.suptitle(f\"Final trained model plots prec = {precision} sig = {sigma}  rat = {onoff}\")\n",
    "    #   plt.subplot(121)\n",
    "    #   plt.plot(W1bp.flatten(), W1bpvar.flatten(), '.')\n",
    "    #   plt.plot(W1bp.flatten(), W1bp.flatten(), 'r', alpha = 0.3)\n",
    "    #   plt.xlim([-1, 1])\n",
    "    #   plt.ylim([-1, 1])\n",
    "    #   plt.title(f\"Final weights\")\n",
    "    #   plt.xlabel(\"Sofware\")\n",
    "    #   plt.ylabel(\"Var\")\n",
    "    #   plt.subplot(122)\n",
    "    #   plt.plot(dW1bp.flatten(), dW1roundbpvar.flatten(), '.')\n",
    "    #   plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'r', alpha = 0.3)\n",
    "    #   plt.xlim([-0.07, 0.07])\n",
    "    #   plt.ylim([-0.07, 0.07])\n",
    "    #   plt.title(f\"Final weight updates dW1\")\n",
    "    #   plt.xlabel(\"Sofware\")\n",
    "    #   plt.ylabel(\"Var\")\n",
    "\n",
    "\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "\n",
    "      _, _, _, A2_train = forward(X, W1bp, b1bp, W2bp, b2bp)\n",
    "      _, _, _, A2_train_bp = forward(X, weightTransformWithVariability(W1bpvar, W1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b1bpvar, b1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(W2bpvar, W2Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b2bpvar, b2Currents, precision, step, discreteSteps, wRange))\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append([train_score, accuracy(predictions(A2_train_bp), Y)])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,  A2_val = forward(x_val, W1bp, b1bp, W2bp, b2bp)\n",
    "      _, _, _, A2_val_bp = forward(x_val,weightTransformWithVariability(W1bpvar, W1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b1bpvar, b1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(W2bpvar, W2Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b2bpvar, b2Currents, precision, step, discreteSteps, wRange))\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append([ val_score, accuracy(predictions(A2_val_bp), y_val)])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy(software BP): {round(train_score, 3)}::Val accuracy: {round(val_score, 3)}:#########################:Train Acc BP var::{round(accuracy(predictions(A2_train_bp), Y), 3)} Val Acc BP var::{round(accuracy(predictions(A2_val_bp), y_val), 3)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::58.34 Val Acc BP var::58.557\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::64.973 Val Acc BP var::64.057\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::59.962 Val Acc BP var::59.9\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::65.189 Val Acc BP var::64.4\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::64.724 Val Acc BP var::64.543\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::68.162 Val Acc BP var::68.371\n"
     ]
    }
   ],
   "source": [
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "iter=1\n",
    "lrBP=0.1\n",
    "lrNP=0.1\n",
    "pert=0.1\n",
    "mu = 0.7\n",
    "sigma = 0.07943282\n",
    "vDD = 5\n",
    "precision = 16#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "batchGDCompOCUp(x_train,y_train,60, lrBP, lrNP, step, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff = 10000,print_op=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for sig = 0.07943282347242814 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::51.792 Val Acc BP var::51.686\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::52.975 Val Acc BP var::52.557\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::52.41 Val Acc BP var::51.3\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::56.567 Val Acc BP var::55.914\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::55.306 Val Acc BP var::54.843\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::56.733 Val Acc BP var::56.457\n",
      "Training for sig = 0.07943282347242814 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::52.298 Val Acc BP var::52.114\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::59.003 Val Acc BP var::59.671\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::50.108 Val Acc BP var::50.029\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::60.613 Val Acc BP var::60.371\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::61.021 Val Acc BP var::60.543\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::57.768 Val Acc BP var::58.143\n",
      "Training for sig = 0.07943282347242814 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::60.521 Val Acc BP var::60.243\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::62.24 Val Acc BP var::63.157\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::64.663 Val Acc BP var::64.471\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::58.998 Val Acc BP var::60.214\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::50.678 Val Acc BP var::49.957\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::63.668 Val Acc BP var::63.629\n",
      "Training for sig = 0.07943282347242814 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::57.302 Val Acc BP var::57.271\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::63.475 Val Acc BP var::63.129\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::58.206 Val Acc BP var::58.029\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::60.529 Val Acc BP var::60.714\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::68.671 Val Acc BP var::67.614\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::67.884 Val Acc BP var::67.214\n",
      "Training for sig = 0.07943282347242814 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::56.197 Val Acc BP var::56.257\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::60.841 Val Acc BP var::61.043\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::69.181 Val Acc BP var::68.9\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::68.702 Val Acc BP var::68.629\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::64.005 Val Acc BP var::64.186\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::71.484 Val Acc BP var::71.029\n",
      "Training for sig = 0.0630957344480193 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::62.438 Val Acc BP var::61.443\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::62.265 Val Acc BP var::61.729\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::59.251 Val Acc BP var::58.729\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::63.73 Val Acc BP var::63.443\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::57.803 Val Acc BP var::57.857\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::64.973 Val Acc BP var::64.1\n",
      "Training for sig = 0.0630957344480193 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::66.152 Val Acc BP var::65.843\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::65.694 Val Acc BP var::64.986\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::58.895 Val Acc BP var::59.371\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::64.66 Val Acc BP var::64.514\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::67.405 Val Acc BP var::66.586\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::58.727 Val Acc BP var::58.457\n",
      "Training for sig = 0.0630957344480193 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::69.34 Val Acc BP var::68.914\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::69.716 Val Acc BP var::69.157\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::68.913 Val Acc BP var::68.129\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::64.448 Val Acc BP var::63.871\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::70.981 Val Acc BP var::70.6\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::67.392 Val Acc BP var::67.414\n",
      "Training for sig = 0.0630957344480193 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::65.6 Val Acc BP var::65.643\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::69.181 Val Acc BP var::69.457\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::71.122 Val Acc BP var::70.871\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::73.551 Val Acc BP var::73.857\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::69.267 Val Acc BP var::70.071\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::76.278 Val Acc BP var::75.843\n",
      "Training for sig = 0.0630957344480193 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::57.454 Val Acc BP var::57.114\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::67.614 Val Acc BP var::67.714\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::71.629 Val Acc BP var::70.771\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::71.079 Val Acc BP var::70.886\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::70.808 Val Acc BP var::71.029\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::75.673 Val Acc BP var::75.586\n",
      "Training for sig = 0.0501187233627272 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::68.681 Val Acc BP var::67.6\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::66.002 Val Acc BP var::65.4\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::60.06 Val Acc BP var::60.657\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::65.471 Val Acc BP var::65.957\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::69.671 Val Acc BP var::68.929\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::62.803 Val Acc BP var::63.171\n",
      "Training for sig = 0.0501187233627272 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::66.981 Val Acc BP var::67.157\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::71.175 Val Acc BP var::70.643\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::71.619 Val Acc BP var::71.929\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::76.365 Val Acc BP var::75.9\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::72.405 Val Acc BP var::72.214\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::73.54 Val Acc BP var::73.843\n",
      "Training for sig = 0.0501187233627272 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::72.049 Val Acc BP var::71.786\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::69.167 Val Acc BP var::69.014\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::69.943 Val Acc BP var::70.157\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::66.557 Val Acc BP var::66.586\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::75.302 Val Acc BP var::75.186\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::73.576 Val Acc BP var::73.914\n",
      "Training for sig = 0.0501187233627272 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::68.297 Val Acc BP var::68.114\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::67.876 Val Acc BP var::68.229\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::71.083 Val Acc BP var::70.629\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::74.354 Val Acc BP var::73.971\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::74.059 Val Acc BP var::73.486\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::72.894 Val Acc BP var::72.786\n",
      "Training for sig = 0.0501187233627272 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::65.514 Val Acc BP var::65.814\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::72.184 Val Acc BP var::72.229\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::72.943 Val Acc BP var::72.786\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::72.289 Val Acc BP var::72.757\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::73.319 Val Acc BP var::72.257\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::76.956 Val Acc BP var::76.686\n",
      "Training for sig = 0.03981071705534969 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::66.968 Val Acc BP var::66.771\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::73.954 Val Acc BP var::73.443\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::70.816 Val Acc BP var::70.686\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::69.303 Val Acc BP var::68.571\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::75.902 Val Acc BP var::75.057\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::66.881 Val Acc BP var::66.729\n",
      "Training for sig = 0.03981071705534969 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::69.224 Val Acc BP var::69.271\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::74.794 Val Acc BP var::74.443\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::70.783 Val Acc BP var::69.929\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::77.784 Val Acc BP var::77.2\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::78.714 Val Acc BP var::77.929\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::80.416 Val Acc BP var::79.471\n",
      "Training for sig = 0.03981071705534969 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::73.283 Val Acc BP var::72.671\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::70.076 Val Acc BP var::69.671\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::78.335 Val Acc BP var::78.714\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::80.478 Val Acc BP var::80.157\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::77.671 Val Acc BP var::77.586\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::81.49 Val Acc BP var::81.057\n",
      "Training for sig = 0.03981071705534969 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::71.187 Val Acc BP var::70.986\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::76.086 Val Acc BP var::74.957\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::78.79 Val Acc BP var::78.486\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::79.495 Val Acc BP var::78.943\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::78.916 Val Acc BP var::77.6\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::79.138 Val Acc BP var::78.543\n",
      "Training for sig = 0.03981071705534969 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::69.71 Val Acc BP var::69.314\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::78.495 Val Acc BP var::77.529\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::79.31 Val Acc BP var::78.586\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::75.244 Val Acc BP var::75.071\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::79.011 Val Acc BP var::79.214\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::80.111 Val Acc BP var::79.643\n",
      "Training for sig = 0.031622776601683764 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::71.511 Val Acc BP var::72.114\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::78.435 Val Acc BP var::78.586\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::75.873 Val Acc BP var::76.086\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::80.363 Val Acc BP var::80.443\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::79.824 Val Acc BP var::80.2\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::79.732 Val Acc BP var::79.1\n",
      "Training for sig = 0.031622776601683764 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::73.121 Val Acc BP var::73.614\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::77.205 Val Acc BP var::77.357\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::77.286 Val Acc BP var::76.871\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::74.195 Val Acc BP var::74.443\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::77.77 Val Acc BP var::78.029\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::78.397 Val Acc BP var::78.771\n",
      "Training for sig = 0.031622776601683764 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::73.608 Val Acc BP var::72.371\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::76.827 Val Acc BP var::77.0\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::79.225 Val Acc BP var::79.343\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::80.595 Val Acc BP var::80.514\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::81.029 Val Acc BP var::81.271\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::81.308 Val Acc BP var::81.214\n",
      "Training for sig = 0.031622776601683764 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::76.959 Val Acc BP var::76.429\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::79.525 Val Acc BP var::79.114\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::79.522 Val Acc BP var::79.757\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::82.227 Val Acc BP var::81.986\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::80.802 Val Acc BP var::80.886\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::79.129 Val Acc BP var::79.271\n",
      "Training for sig = 0.031622776601683764 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::71.781 Val Acc BP var::71.6\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::77.621 Val Acc BP var::77.243\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::79.654 Val Acc BP var::79.2\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::79.135 Val Acc BP var::78.457\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::81.121 Val Acc BP var::81.1\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::80.668 Val Acc BP var::80.457\n",
      "Training for sig = 0.02511886431509577 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::72.041 Val Acc BP var::72.157\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::76.2 Val Acc BP var::76.2\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::79.163 Val Acc BP var::79.057\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::77.93 Val Acc BP var::78.443\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::78.875 Val Acc BP var::78.9\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::80.465 Val Acc BP var::80.043\n",
      "Training for sig = 0.02511886431509577 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::76.035 Val Acc BP var::76.329\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::73.854 Val Acc BP var::74.729\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::76.305 Val Acc BP var::77.157\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::76.544 Val Acc BP var::76.814\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::80.083 Val Acc BP var::80.114\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::81.686 Val Acc BP var::81.157\n",
      "Training for sig = 0.02511886431509577 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::77.402 Val Acc BP var::76.586\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::81.249 Val Acc BP var::81.157\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::78.817 Val Acc BP var::79.214\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::81.251 Val Acc BP var::80.757\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::82.425 Val Acc BP var::82.371\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::81.349 Val Acc BP var::81.214\n",
      "Training for sig = 0.02511886431509577 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::77.759 Val Acc BP var::77.871\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::81.489 Val Acc BP var::81.471\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::82.31 Val Acc BP var::81.729\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::82.357 Val Acc BP var::82.171\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::83.135 Val Acc BP var::82.457\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.017 Val Acc BP var::83.557\n",
      "Training for sig = 0.02511886431509577 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::75.008 Val Acc BP var::75.1\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::77.678 Val Acc BP var::77.743\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::80.546 Val Acc BP var::80.3\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::82.083 Val Acc BP var::81.6\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::81.678 Val Acc BP var::81.886\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::83.465 Val Acc BP var::83.1\n",
      "Training for sig = 0.019952623149688768 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::71.933 Val Acc BP var::71.557\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::77.451 Val Acc BP var::77.2\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::80.754 Val Acc BP var::80.443\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::80.816 Val Acc BP var::79.857\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::81.33 Val Acc BP var::81.129\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::82.198 Val Acc BP var::81.814\n",
      "Training for sig = 0.019952623149688768 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::73.31 Val Acc BP var::73.886\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::76.341 Val Acc BP var::76.943\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::81.321 Val Acc BP var::80.814\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::81.856 Val Acc BP var::81.971\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::81.371 Val Acc BP var::81.614\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::81.879 Val Acc BP var::81.571\n",
      "Training for sig = 0.019952623149688768 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::78.365 Val Acc BP var::78.086\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::80.67 Val Acc BP var::80.757\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::82.237 Val Acc BP var::82.143\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::82.705 Val Acc BP var::83.071\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::81.078 Val Acc BP var::81.514\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::83.743 Val Acc BP var::83.857\n",
      "Training for sig = 0.019952623149688768 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::78.719 Val Acc BP var::78.471\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::82.01 Val Acc BP var::81.786\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::82.348 Val Acc BP var::82.057\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::83.181 Val Acc BP var::83.286\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::84.237 Val Acc BP var::83.957\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.386 Val Acc BP var::83.986\n",
      "Training for sig = 0.019952623149688768 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::78.759 Val Acc BP var::78.914\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::81.13 Val Acc BP var::80.543\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::83.379 Val Acc BP var::83.1\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::83.629 Val Acc BP var::83.457\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::84.198 Val Acc BP var::83.957\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.71 Val Acc BP var::84.543\n",
      "Training for sig = 0.01584893192461111 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::74.894 Val Acc BP var::74.9\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::78.875 Val Acc BP var::78.371\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::78.787 Val Acc BP var::79.271\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::77.151 Val Acc BP var::77.629\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::80.548 Val Acc BP var::80.714\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::79.059 Val Acc BP var::78.614\n",
      "Training for sig = 0.01584893192461111 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::78.73 Val Acc BP var::78.886\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::77.097 Val Acc BP var::77.571\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::81.967 Val Acc BP var::82.057\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::82.411 Val Acc BP var::82.571\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::83.578 Val Acc BP var::83.686\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::82.156 Val Acc BP var::82.5\n",
      "Training for sig = 0.01584893192461111 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::79.114 Val Acc BP var::79.243\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::82.013 Val Acc BP var::82.114\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::82.313 Val Acc BP var::82.271\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::82.671 Val Acc BP var::82.629\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::83.205 Val Acc BP var::83.057\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.397 Val Acc BP var::84.057\n",
      "Training for sig = 0.01584893192461111 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::77.651 Val Acc BP var::77.557\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::82.008 Val Acc BP var::81.529\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::81.354 Val Acc BP var::80.729\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::83.662 Val Acc BP var::83.371\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::84.549 Val Acc BP var::84.071\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.846 Val Acc BP var::84.143\n",
      "Training for sig = 0.01584893192461111 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::79.502 Val Acc BP var::79.814\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::81.716 Val Acc BP var::81.7\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::82.835 Val Acc BP var::82.929\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::84.29 Val Acc BP var::84.229\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::84.341 Val Acc BP var::84.143\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::85.281 Val Acc BP var::84.971\n",
      "Training for sig = 0.012589254117941649 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::77.708 Val Acc BP var::77.686\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::80.003 Val Acc BP var::80.557\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::81.184 Val Acc BP var::81.643\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::81.641 Val Acc BP var::81.314\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::80.917 Val Acc BP var::81.1\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::81.956 Val Acc BP var::81.929\n",
      "Training for sig = 0.012589254117941649 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::74.689 Val Acc BP var::75.286\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::81.471 Val Acc BP var::81.671\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::82.876 Val Acc BP var::83.157\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::83.602 Val Acc BP var::83.671\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::84.13 Val Acc BP var::83.986\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.14 Val Acc BP var::84.214\n",
      "Training for sig = 0.012589254117941649 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::79.646 Val Acc BP var::79.771\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::81.789 Val Acc BP var::81.757\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::80.776 Val Acc BP var::80.471\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::83.081 Val Acc BP var::82.7\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::83.051 Val Acc BP var::83.1\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.73 Val Acc BP var::84.586\n",
      "Training for sig = 0.012589254117941649 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::79.803 Val Acc BP var::79.657\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::82.34 Val Acc BP var::82.243\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::82.835 Val Acc BP var::82.657\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::84.449 Val Acc BP var::83.857\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::84.602 Val Acc BP var::84.257\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::84.837 Val Acc BP var::84.3\n",
      "Training for sig = 0.012589254117941649 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 78.578::Val accuracy: 78.857:#########################:Train Acc BP var::80.527 Val Acc BP var::80.043\n",
      "Iteration: 20::Train accuracy(software BP): 82.095::Val accuracy: 82.243:#########################:Train Acc BP var::83.017 Val Acc BP var::83.029\n",
      "Iteration: 30::Train accuracy(software BP): 83.667::Val accuracy: 83.414:#########################:Train Acc BP var::83.295 Val Acc BP var::83.557\n",
      "Iteration: 40::Train accuracy(software BP): 84.637::Val accuracy: 84.171:#########################:Train Acc BP var::83.89 Val Acc BP var::83.943\n",
      "Iteration: 50::Train accuracy(software BP): 85.27::Val accuracy: 84.814:#########################:Train Acc BP var::84.911 Val Acc BP var::84.529\n",
      "Iteration: 60::Train accuracy(software BP): 85.73::Val accuracy: 85.443:#########################:Train Acc BP var::85.184 Val Acc BP var::84.786\n"
     ]
    }
   ],
   "source": [
    "#Sigma = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "#nf = [10, 100, 1000, 10000, 100000]\n",
    "#prec = np.array([2,4,6,7, 8,9, 10, 11,12,14, 16])\n",
    "prec= np.array([10, 11,12,14, 16])\n",
    "trainAccSoft, trainAccVari1, valAccSoft, valAccVari1 = np.zeros((len(Sigma), len(prec))), np.zeros((len(Sigma), len(prec))), np.zeros((len(Sigma), len(prec))), np.zeros((len(Sigma), len(prec)))\n",
    "for j in range(len(Sigma)):\n",
    "    for k in range(len(prec)):\n",
    "        noOfLevels = 2**prec[k]- 1 #no of levels of quantization\n",
    "        #step = round(wRange/noOfLevels, precision)\n",
    "        step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "        #discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "        discreteSteps = [step*i for i in range(0, noOfLevels)] \n",
    "        print(f\"Training for sig = {Sigma[j]} prec = {prec[k]} step= {step}\")\n",
    "        trainAccVar, valAccVar,_, _, _ = batchGDCompOCUp(x_train,y_train,60, lrBP, lrNP, step, mu, Sigma[j], vDD, prec[k], step, discreteSteps, wRange, onoff = 10000,print_op=10);\n",
    "        trainAccSoft[j, k] = trainAccVar[-1][0]\n",
    "        trainAccVari1[j, k] = trainAccVar[-1][1]\n",
    "        valAccSoft[j, k] = valAccVar[-1][0]\n",
    "        valAccVari1[j, k] = valAccVar[-1][1]\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56.73333333, 57.76825397, 63.66825397, 67.88412698, 71.48412698],\n",
       "       [64.97301587, 58.72698413, 67.39206349, 76.27777778, 75.67301587],\n",
       "       [62.8031746 , 73.53968254, 73.57619048, 72.89365079, 76.95555556],\n",
       "       [66.88095238, 80.41587302, 81.49047619, 79.13809524, 80.11111111],\n",
       "       [79.73174603, 78.3968254 , 81.30793651, 79.12857143, 80.66825397],\n",
       "       [80.46507937, 81.68571429, 81.34920635, 84.01746032, 83.46507937],\n",
       "       [82.1984127 , 81.87936508, 83.74285714, 84.38571429, 84.70952381],\n",
       "       [79.05873016, 82.15555556, 84.3968254 , 84.84603175, 85.28095238],\n",
       "       [81.95555556, 84.13968254, 84.73015873, 84.83650794, 85.18412698]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAccVari1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56.45714286, 58.14285714, 63.62857143, 67.21428571, 71.02857143],\n",
       "       [64.1       , 58.45714286, 67.41428571, 75.84285714, 75.58571429],\n",
       "       [63.17142857, 73.84285714, 73.91428571, 72.78571429, 76.68571429],\n",
       "       [66.72857143, 79.47142857, 81.05714286, 78.54285714, 79.64285714],\n",
       "       [79.1       , 78.77142857, 81.21428571, 79.27142857, 80.45714286],\n",
       "       [80.04285714, 81.15714286, 81.21428571, 83.55714286, 83.1       ],\n",
       "       [81.81428571, 81.57142857, 83.85714286, 83.98571429, 84.54285714],\n",
       "       [78.61428571, 82.5       , 84.05714286, 84.14285714, 84.97142857],\n",
       "       [81.92857143, 84.21428571, 84.58571429, 84.3       , 84.78571429]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valAccVari1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NODe perturbatiion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, W1, W2, b1, b2, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #print(W2[3])\n",
    "  #first approximate dZ3\n",
    "  m = Y1.shape[0]\n",
    "  #print(Z3.shape)\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z2pert[i, :] +=  pert\n",
    "    A2pert = softmax(Z2pert)\n",
    "    #lossArrayAfterPertZ3[i, :] += np.sum(np.square(A3pert-one_hot_encoding(Y1)), axis=0)\n",
    "    lossArrayAfterPertZ2[i, :] += crossEntropy(one_hot_encoding(Y1), A2pert)\n",
    "\n",
    "\n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T)\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "    A1pert = relu(Z1pert)\n",
    "    \n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = softmax(Z2pert)\n",
    "    \n",
    "    #lossArrayAfterPertZ1[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "    lossArrayAfterPertZ1[i, :] += crossEntropy(one_hot_encoding(Y1), A2pert)\n",
    "\n",
    "    \n",
    "  #print(lossArrayAfterPertZ1)\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:37:38.911161Z",
     "iopub.status.busy": "2022-09-30T16:37:38.910809Z",
     "iopub.status.idle": "2022-09-30T16:37:38.942710Z",
     "shell.execute_reply": "2022-09-30T16:37:38.941425Z",
     "shell.execute_reply.started": "2022-09-30T16:37:38.911134Z"
    }
   },
   "outputs": [],
   "source": [
    "def batchGDCompOCNP(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "  W1comp = []\n",
    "\n",
    "  W1, b1, W2, b2 = params_init_seeded(seed=2)\n",
    "  #W1, b1, W2, b2 = params_init()\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n",
    "  \n",
    "\n",
    "  W1bpvar, b1bpvar, W2bpvar, b2bpvar = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n",
    "  W1bpvar = roundArbitrary(W1bpvar, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1bpvar = roundArbitrary(b1bpvar, step, wRange)\n",
    "  W2bpvar = roundArbitrary(W2bpvar, step, wRange)\n",
    "  b2bpvar = roundArbitrary(b2bpvar, step, wRange)\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((20, 784), mu, sigma, vDD, precision, k =onoff)#k = Ion/Ioff\n",
    "  b1Currents = initMosParam((20, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W2Currents = initMosParam((10, 20) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b2Currents = initMosParam((10, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      \n",
    "\n",
    "      \n",
    "      W1varocwp = weightTransformWithVariability(W1bpvar, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varocwp = weightTransformWithVariability(b1bpvar, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varocwp = weightTransformWithVariability(W2bpvar, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varocwp = weightTransformWithVariability(b2bpvar, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      #doing the weight perturbation pass first\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1varocwp, b1varocwp, W2varocwp,b2varocwp) \n",
    "      #print(A2)\n",
    "      print(f\"BP with Var Iter {i} -> sub iter {j} : {round(accuracy(predictions(A2), Y1), 3)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = crossEntropy(one_hot_encoding(Y1), A2)\n",
    "\n",
    "\n",
    "      dW1bpvar, db1bpvar, dW2bpvar, db2bpvar = NP(pert, lossBeforePert, Z1, A1, Z2, A2, W1varocwp, W2varocwp, b1varocwp, b2varocwp, X1, Y1)\n",
    "      #print(dW1wp)\n",
    "      #print(W1roundwp/step)\n",
    "\n",
    "      # dW1roundbpvar = roundArbitrary(dW1bpvar, step, wRange)#weights have to maintained as their digitized versions\n",
    "      # db1roundbpvar = roundArbitrary(db1bpvar, step, wRange)\n",
    "      # dW2roundbpvar = roundArbitrary(dW2bpvar, step, wRange)\n",
    "      # db2roundbpvar = roundArbitrary(db2bpvar, step, wRange)\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      W1bpvar, b1bpvar, W2bpvar, b2bpvar = param_update(W1bpvar, b1bpvar, W2bpvar,b2bpvar, dW1bpvar, db1bpvar, dW2bpvar, db2bpvar, lr = lrNP)\n",
    "      W1bpvar = roundArbitrary(W1bpvar, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1bpvar = roundArbitrary(b1bpvar, step, wRange)\n",
    "      W2bpvar = roundArbitrary(W2bpvar, step, wRange)\n",
    "      b2bpvar = roundArbitrary(b2bpvar, step, wRange)\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "\n",
    "      # Z1, A1, Z2, A2 = forward(X1, W1bp, b1bp, W2bp, b2bp) \n",
    "      # print(f\"                                                                   NP(Software) Iter {i+1} -> sub iter {j} : {accuracy(predictions(A2), Y1)}\", end = \"\\r\", flush = True)\n",
    "\n",
    "      # #dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "      # loss = crossEntropy(one_hot_encoding(Y1), A2)\n",
    "      # #dW1quantwp, db1quantwp, dW2quantwp, db2quantwp = WP(W1quantwp, b1quantwp, W2quantwp, b2quantwp, pert=pert, lossBeforePert=loss, X=X1, y=Y1)\n",
    "      # dW1bp, db1bp, dW2bp, db2bp = NP(pert, loss, Z1, A1, Z2, A2, W1bp, W2bp, b1bp, b2bp, X1, Y1)\n",
    "      # W1bp, b1bp, W2bp, b2bp = param_update(W1bp, b1bp, W2bp, b2bp,  dW1bp,  db1bp, dW2bp, db2bp,  lr = lrNP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # #   dW1quantround = roundArbitrary(dW1quantwp, step, wRange)#weights have to maintained as their digitized versions\n",
    "#     # #   db1quantround = roundArbitrary(db1quantwp, step, wRange)\n",
    "#     # #   dW2quantround = roundArbitrary(dW2quantwp, step, wRange)\n",
    "#     # #   db2quantround = roundArbitrary(db2quantwp, step, wRange)\n",
    "      \n",
    "    #   if (i==0 or i==1)and (j==0 or j==1):\n",
    "        \n",
    "    #     plt.figure(figsize=(16, 16))\n",
    "    #     plt.gcf().set_dpi(300)\n",
    "    #     plt.suptitle(f\"Intial model plots at iter = {i} epcoch {j} prec = {precision} sig = {sigma}  rat = {onoff}\")\n",
    "    #     plt.subplot(2,2,1)\n",
    "    #     plt.plot(W1bp.flatten(), W1bpvar.flatten(), '.')\n",
    "    #     plt.plot(W1bp.flatten(), W1bp.flatten(), 'r', alpha = 0.3)\n",
    "    #     plt.xlim([-1, 1])\n",
    "    #     plt.ylim([-1, 1])\n",
    "    #     plt.title(f\"Software W1 vs W1 calc with variability included \")\n",
    "    #     plt.xlabel(\"Sofware\")\n",
    "    #     plt.ylabel(\"Var\")\n",
    "    #     plt.subplot(2,2,2)\n",
    "    #     plt.plot(dW1bp.flatten(), dW1bpvar.flatten(), '.')\n",
    "    #     plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'r', alpha = 0.3)\n",
    "    #     #plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'g^', alpha = 0.3)\n",
    "    #     plt.xlim([-0.07, 0.07])\n",
    "    #     plt.ylim([-0.07, 0.07])\n",
    "    #     plt.title(f\"Weight updates in software vs variability\")\n",
    "    #     plt.xlabel(\"Sofware\")\n",
    "    #     plt.ylabel(\"Var\")\n",
    "    #     plt.subplot(2,2,3)\n",
    "    #     plt.plot(dW1bp.flatten(), dW1roundbpvar.flatten(), '.')\n",
    "    #     plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'r', alpha = 0.3)\n",
    "    #     plt.xlim([-0.07, 0.07])\n",
    "    #     plt.ylim([-0.07, 0.07])\n",
    "    #     plt.title(f\"Weight updates in software vs variability(dW1 rounded)\")\n",
    "    #     plt.xlabel(\"Sofware\")\n",
    "    #     plt.ylabel(\"Var\")\n",
    "    #     plt.subplot(2,2,4)\n",
    "    #     plt.plot(dW1bpvar.flatten(), dW1roundbpvar.flatten(), 'k.', alpha = 0.3)\n",
    "    #     plt.xlim([-0.07, 0.07])\n",
    "    #     plt.ylim([-0.07, 0.07])\n",
    "    #     plt.title(f\"dW1 vs its quantization\")\n",
    "    #     plt.xlabel(\"Sofware\")\n",
    "    #     plt.ylabel(\"Var\")\n",
    "    \n",
    "\n",
    "    # if i==iter-1:\n",
    "\n",
    "    #   plt.figure(figsize=(16, 8))\n",
    "    #   plt.gcf().set_dpi(300)\n",
    "    #   plt.suptitle(\"Final trained model plots prec = {precision} sig = {sigma}  rat = {onoff}\")\n",
    "    #   plt.subplot(121)\n",
    "    #   plt.plot(W1bp.flatten(), W1bpvar.flatten(), '.')\n",
    "    #   plt.plot(W1bp.flatten(), W1bp.flatten(), 'r', alpha = 0.3)\n",
    "    #   plt.xlim([-1, 1])\n",
    "    #   plt.ylim([-1, 1])\n",
    "    #   plt.title(f\"Final weights\")\n",
    "    #   plt.xlabel(\"Sofware\")\n",
    "    #   plt.ylabel(\"Var\")\n",
    "    #   plt.subplot(122)\n",
    "    #   plt.plot(dW1bp.flatten(), dW1roundbpvar.flatten(), '.')\n",
    "    #   plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'r', alpha = 0.3)\n",
    "    #   plt.xlim([-0.07, 0.07])\n",
    "    #   plt.ylim([-0.07, 0.07])\n",
    "    #   plt.title(f\"Final weight updates dW1\")\n",
    "    #   plt.xlabel(\"Sofware\")\n",
    "    #   plt.ylabel(\"Var\")\n",
    "\n",
    "\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "\n",
    "      _, _, _, A2_train = forward(X, W1bp, b1bp, W2bp, b2bp)\n",
    "      _, _, _, A2_train_bp = forward(X, weightTransformWithVariability(W1bpvar, W1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b1bpvar, b1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(W2bpvar, W2Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b2bpvar, b2Currents, precision, step, discreteSteps, wRange))\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append([train_score, accuracy(predictions(A2_train_bp), Y)])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,  A2_val = forward(x_val, W1bp, b1bp, W2bp, b2bp)\n",
    "      _, _, _, A2_val_bp = forward(x_val,weightTransformWithVariability(W1bpvar, W1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b1bpvar, b1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(W2bpvar, W2Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b2bpvar, b2Currents, precision, step, discreteSteps, wRange))\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append([ val_score, accuracy(predictions(A2_val_bp), y_val)])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy(software BP): {round(train_score, 3)}::Val accuracy: {round(val_score, 3)}:#########################:Train Acc BP var::{round(accuracy(predictions(A2_train_bp), Y), 3)} Val Acc BP var::{round(accuracy(predictions(A2_val_bp), y_val), 3)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:46:57.780807Z",
     "iopub.status.busy": "2022-09-30T16:46:57.780443Z",
     "iopub.status.idle": "2022-09-30T16:51:17.874196Z",
     "shell.execute_reply": "2022-09-30T16:51:17.873366Z",
     "shell.execute_reply.started": "2022-09-30T16:46:57.780780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::60.441 Val Acc BP var::60.529\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::65.863 Val Acc BP var::65.1\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.913 Val Acc BP var::66.729\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::62.014 Val Acc BP var::62.314\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::62.833 Val Acc BP var::62.986\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::69.263 Val Acc BP var::68.286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[12.2, 60.44126984126984],\n",
       "  [12.2, 65.86349206349207],\n",
       "  [12.2, 66.91269841269842],\n",
       "  [12.2, 62.01428571428571],\n",
       "  [12.2, 62.83333333333333],\n",
       "  [12.2, 69.26349206349207]],\n",
       " [[12.971428571428573, 60.52857142857143],\n",
       "  [12.971428571428573, 65.10000000000001],\n",
       "  [12.971428571428573, 66.72857142857143],\n",
       "  [12.971428571428573, 62.31428571428571],\n",
       "  [12.971428571428573, 62.98571428571429],\n",
       "  [12.971428571428573, 68.28571428571428]],\n",
       " [],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "iter=1\n",
    "lrBP=0.1\n",
    "lrNP=0.1\n",
    "pert=0.1\n",
    "mu = 0.7\n",
    "sigma = 0.07943282\n",
    "vDD = 5\n",
    "precision = 16#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "batchGDCompOCNP(x_train,y_train,60, lrBP, lrNP, 0.00001, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff = 10000, print_op=10)\n",
    "#(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for sig = 0.07943282347242814 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::48.732 Val Acc BP var::47.529\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::56.333 Val Acc BP var::56.014\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::57.36 Val Acc BP var::56.657\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::53.821 Val Acc BP var::54.043\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::62.827 Val Acc BP var::62.0\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::62.814 Val Acc BP var::61.986\n",
      "Training for sig = 0.07943282347242814 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::43.708 Val Acc BP var::42.829\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::50.489 Val Acc BP var::50.5\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::59.267 Val Acc BP var::58.471\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::54.932 Val Acc BP var::55.329\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::54.717 Val Acc BP var::55.7\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::56.532 Val Acc BP var::57.7\n",
      "Training for sig = 0.07943282347242814 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::59.002 Val Acc BP var::58.743\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::61.125 Val Acc BP var::62.157\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::65.563 Val Acc BP var::64.386\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.602 Val Acc BP var::66.486\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.075 Val Acc BP var::67.114\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::67.562 Val Acc BP var::66.457\n",
      "Training for sig = 0.07943282347242814 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::59.16 Val Acc BP var::59.986\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::57.138 Val Acc BP var::57.386\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::58.957 Val Acc BP var::58.871\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::61.659 Val Acc BP var::61.0\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.935 Val Acc BP var::65.829\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.144 Val Acc BP var::65.329\n",
      "Training for sig = 0.07943282347242814 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::57.433 Val Acc BP var::57.871\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::56.076 Val Acc BP var::55.886\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::67.225 Val Acc BP var::67.043\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.381 Val Acc BP var::66.286\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.668 Val Acc BP var::69.814\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::69.848 Val Acc BP var::68.943\n",
      "Training for sig = 0.0630957344480193 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::52.292 Val Acc BP var::52.471\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::61.641 Val Acc BP var::60.686\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::65.892 Val Acc BP var::65.043\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::55.373 Val Acc BP var::55.643\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::57.576 Val Acc BP var::57.857\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::57.378 Val Acc BP var::57.771\n",
      "Training for sig = 0.0630957344480193 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::63.462 Val Acc BP var::63.586\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.725 Val Acc BP var::65.914\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::63.032 Val Acc BP var::63.086\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.248 Val Acc BP var::70.686\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::65.921 Val Acc BP var::65.557\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::67.895 Val Acc BP var::67.343\n",
      "Training for sig = 0.0630957344480193 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::64.721 Val Acc BP var::64.671\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::71.711 Val Acc BP var::71.1\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::67.646 Val Acc BP var::67.014\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.378 Val Acc BP var::68.557\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::65.479 Val Acc BP var::66.0\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.825 Val Acc BP var::66.729\n",
      "Training for sig = 0.0630957344480193 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::64.646 Val Acc BP var::64.986\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::71.13 Val Acc BP var::69.771\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.167 Val Acc BP var::75.014\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.535 Val Acc BP var::68.214\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::69.449 Val Acc BP var::68.971\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.187 Val Acc BP var::74.7\n",
      "Training for sig = 0.0630957344480193 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::65.759 Val Acc BP var::65.157\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.024 Val Acc BP var::67.043\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.567 Val Acc BP var::68.271\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::71.329 Val Acc BP var::70.829\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.619 Val Acc BP var::70.786\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.724 Val Acc BP var::70.671\n",
      "Training for sig = 0.0501187233627272 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::63.67 Val Acc BP var::62.9\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::56.757 Val Acc BP var::57.186\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.908 Val Acc BP var::68.5\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::65.629 Val Acc BP var::65.171\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::73.692 Val Acc BP var::72.986\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::71.887 Val Acc BP var::71.314\n",
      "Training for sig = 0.0501187233627272 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.916 Val Acc BP var::69.914\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.057 Val Acc BP var::74.014\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.711 Val Acc BP var::77.186\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::69.781 Val Acc BP var::70.114\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::73.437 Val Acc BP var::73.129\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.157 Val Acc BP var::78.557\n",
      "Training for sig = 0.0501187233627272 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::63.135 Val Acc BP var::62.671\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.006 Val Acc BP var::67.9\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.713 Val Acc BP var::68.5\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.692 Val Acc BP var::70.629\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.862 Val Acc BP var::75.214\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::74.5 Val Acc BP var::74.886\n",
      "Training for sig = 0.0501187233627272 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::61.192 Val Acc BP var::61.586\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.006 Val Acc BP var::67.614\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.478 Val Acc BP var::69.757\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.965 Val Acc BP var::67.457\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.141 Val Acc BP var::75.529\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.5 Val Acc BP var::75.271\n",
      "Training for sig = 0.0501187233627272 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.229 Val Acc BP var::68.6\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::72.824 Val Acc BP var::72.086\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::73.611 Val Acc BP var::73.2\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.884 Val Acc BP var::70.257\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.913 Val Acc BP var::76.571\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.679 Val Acc BP var::77.886\n",
      "Training for sig = 0.03981071705534969 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::64.905 Val Acc BP var::65.2\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.156 Val Acc BP var::69.986\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.624 Val Acc BP var::68.771\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.662 Val Acc BP var::70.443\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::69.365 Val Acc BP var::69.143\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::71.54 Val Acc BP var::70.929\n",
      "Training for sig = 0.03981071705534969 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.544 Val Acc BP var::68.114\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::72.438 Val Acc BP var::72.343\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.479 Val Acc BP var::70.429\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.284 Val Acc BP var::77.586\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.062 Val Acc BP var::76.657\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.214 Val Acc BP var::77.829\n",
      "Training for sig = 0.03981071705534969 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.357 Val Acc BP var::74.986\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.586 Val Acc BP var::75.7\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::68.554 Val Acc BP var::69.286\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.603 Val Acc BP var::77.643\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.386 Val Acc BP var::79.971\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.003 Val Acc BP var::79.586\n",
      "Training for sig = 0.03981071705534969 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::73.829 Val Acc BP var::73.243\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::73.365 Val Acc BP var::72.529\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.441 Val Acc BP var::75.729\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.163 Val Acc BP var::75.429\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.351 Val Acc BP var::77.771\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.656 Val Acc BP var::81.657\n",
      "Training for sig = 0.03981071705534969 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::73.86 Val Acc BP var::73.243\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.219 Val Acc BP var::77.629\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.602 Val Acc BP var::78.757\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.2 Val Acc BP var::74.886\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.873 Val Acc BP var::78.743\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.159 Val Acc BP var::80.214\n",
      "Training for sig = 0.031622776601683764 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.479 Val Acc BP var::76.429\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.792 Val Acc BP var::76.343\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.178 Val Acc BP var::79.814\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.965 Val Acc BP var::79.714\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.362 Val Acc BP var::80.4\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.176 Val Acc BP var::80.3\n",
      "Training for sig = 0.031622776601683764 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::66.406 Val Acc BP var::67.143\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.63 Val Acc BP var::77.943\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.724 Val Acc BP var::76.871\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.119 Val Acc BP var::75.771\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::72.962 Val Acc BP var::72.043\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.622 Val Acc BP var::80.214\n",
      "Training for sig = 0.031622776601683764 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.456 Val Acc BP var::76.743\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.254 Val Acc BP var::77.471\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.216 Val Acc BP var::76.443\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.637 Val Acc BP var::80.9\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.089 Val Acc BP var::79.729\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.765 Val Acc BP var::81.729\n",
      "Training for sig = 0.031622776601683764 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.19 Val Acc BP var::76.386\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.998 Val Acc BP var::78.429\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.025 Val Acc BP var::79.914\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.589 Val Acc BP var::80.014\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.368 Val Acc BP var::82.471\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.759 Val Acc BP var::82.214\n",
      "Training for sig = 0.031622776601683764 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::74.051 Val Acc BP var::73.971\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.697 Val Acc BP var::79.1\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.417 Val Acc BP var::75.4\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.251 Val Acc BP var::81.629\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.794 Val Acc BP var::81.786\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.833 Val Acc BP var::81.114\n",
      "Training for sig = 0.02511886431509577 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.295 Val Acc BP var::75.343\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.789 Val Acc BP var::78.857\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.184 Val Acc BP var::79.857\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.113 Val Acc BP var::80.714\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.16 Val Acc BP var::79.929\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.137 Val Acc BP var::81.143\n",
      "Training for sig = 0.02511886431509577 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.514 Val Acc BP var::76.186\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.362 Val Acc BP var::76.7\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::74.224 Val Acc BP var::74.486\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.481 Val Acc BP var::76.7\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.549 Val Acc BP var::81.1\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.505 Val Acc BP var::81.614\n",
      "Training for sig = 0.02511886431509577 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.156 Val Acc BP var::77.086\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.533 Val Acc BP var::79.771\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.587 Val Acc BP var::80.1\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.221 Val Acc BP var::81.243\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.092 Val Acc BP var::79.1\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.929 Val Acc BP var::80.4\n",
      "Training for sig = 0.02511886431509577 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.99 Val Acc BP var::77.886\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.335 Val Acc BP var::80.086\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.13 Val Acc BP var::81.586\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.195 Val Acc BP var::77.186\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.519 Val Acc BP var::82.514\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.662 Val Acc BP var::84.514\n",
      "Training for sig = 0.02511886431509577 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.713 Val Acc BP var::75.914\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.483 Val Acc BP var::80.214\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.414 Val Acc BP var::80.143\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.968 Val Acc BP var::82.643\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.056 Val Acc BP var::82.629\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.284 Val Acc BP var::84.157\n",
      "Training for sig = 0.019952623149688768 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::72.013 Val Acc BP var::72.086\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.483 Val Acc BP var::78.071\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.841 Val Acc BP var::78.243\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::76.784 Val Acc BP var::75.971\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.749 Val Acc BP var::77.471\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.1 Val Acc BP var::81.386\n",
      "Training for sig = 0.019952623149688768 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::70.363 Val Acc BP var::70.786\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.184 Val Acc BP var::79.771\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.8 Val Acc BP var::80.2\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.481 Val Acc BP var::82.471\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.978 Val Acc BP var::83.471\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.633 Val Acc BP var::83.171\n",
      "Training for sig = 0.019952623149688768 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.127 Val Acc BP var::78.429\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.506 Val Acc BP var::80.486\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.5 Val Acc BP var::82.457\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.56 Val Acc BP var::82.686\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.47 Val Acc BP var::83.5\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.959 Val Acc BP var::83.114\n",
      "Training for sig = 0.019952623149688768 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.103 Val Acc BP var::79.086\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.365 Val Acc BP var::80.671\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.03 Val Acc BP var::80.9\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.03 Val Acc BP var::82.729\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.149 Val Acc BP var::82.929\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.786 Val Acc BP var::83.214\n",
      "Training for sig = 0.019952623149688768 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.173 Val Acc BP var::78.743\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.178 Val Acc BP var::82.0\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.557 Val Acc BP var::82.814\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.217 Val Acc BP var::82.886\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.413 Val Acc BP var::82.1\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.625 Val Acc BP var::84.329\n",
      "Training for sig = 0.01584893192461111 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::72.203 Val Acc BP var::71.986\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::74.967 Val Acc BP var::75.3\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.727 Val Acc BP var::79.371\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.202 Val Acc BP var::78.414\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.859 Val Acc BP var::79.714\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.898 Val Acc BP var::81.771\n",
      "Training for sig = 0.01584893192461111 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.257 Val Acc BP var::77.357\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.943 Val Acc BP var::80.729\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.265 Val Acc BP var::82.329\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.216 Val Acc BP var::83.657\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.43 Val Acc BP var::83.357\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.286 Val Acc BP var::84.029\n",
      "Training for sig = 0.01584893192461111 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.467 Val Acc BP var::79.014\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.465 Val Acc BP var::81.4\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.835 Val Acc BP var::82.557\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.689 Val Acc BP var::83.629\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.968 Val Acc BP var::83.814\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.811 Val Acc BP var::83.571\n",
      "Training for sig = 0.01584893192461111 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.96 Val Acc BP var::80.057\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.038 Val Acc BP var::81.814\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.286 Val Acc BP var::82.514\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.21 Val Acc BP var::83.671\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.817 Val Acc BP var::84.2\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.737 Val Acc BP var::84.357\n",
      "Training for sig = 0.01584893192461111 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.559 Val Acc BP var::79.857\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.589 Val Acc BP var::81.9\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.781 Val Acc BP var::82.8\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.827 Val Acc BP var::83.3\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.346 Val Acc BP var::84.157\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.778 Val Acc BP var::83.843\n",
      "Training for sig = 0.012589254117941649 prec = 10 step= 0.0009775171065493646\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::75.559 Val Acc BP var::75.771\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.17 Val Acc BP var::78.629\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.517 Val Acc BP var::81.329\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.789 Val Acc BP var::81.886\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.197 Val Acc BP var::82.186\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.543 Val Acc BP var::82.743\n",
      "Training for sig = 0.012589254117941649 prec = 11 step= 0.0004885197850512946\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::77.643 Val Acc BP var::78.129\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.483 Val Acc BP var::80.843\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::81.419 Val Acc BP var::81.7\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.638 Val Acc BP var::83.657\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.875 Val Acc BP var::83.657\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.97 Val Acc BP var::83.771\n",
      "Training for sig = 0.012589254117941649 prec = 12 step= 0.0002442002442002442\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::78.894 Val Acc BP var::79.2\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.033 Val Acc BP var::81.771\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.66 Val Acc BP var::82.371\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.719 Val Acc BP var::82.757\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.086 Val Acc BP var::84.1\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.444 Val Acc BP var::84.329\n",
      "Training for sig = 0.012589254117941649 prec = 14 step= 6.103888176768602e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::80.352 Val Acc BP var::80.514\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.459 Val Acc BP var::82.243\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.762 Val Acc BP var::83.086\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.371 Val Acc BP var::84.129\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.873 Val Acc BP var::84.514\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::85.252 Val Acc BP var::84.786\n",
      "Training for sig = 0.012589254117941649 prec = 16 step= 1.5259021896696422e-05\n",
      "Params Initialised\n",
      "Iteration: 10::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::79.524 Val Acc BP var::79.443\n",
      "Iteration: 20::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::82.024 Val Acc BP var::82.086\n",
      "Iteration: 30::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.824 Val Acc BP var::83.643\n",
      "Iteration: 40::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::83.795 Val Acc BP var::84.129\n",
      "Iteration: 50::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::84.729 Val Acc BP var::84.814\n",
      "Iteration: 60::Train accuracy(software BP): 12.2::Val accuracy: 12.971:#########################:Train Acc BP var::85.383 Val Acc BP var::84.957\n"
     ]
    }
   ],
   "source": [
    "#Sigma = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "#nf = [10, 100, 1000, 10000, 100000]\n",
    "#prec = np.array([2,4,6,7, 8,9, 10, 11,12,14, 16])\n",
    "prec= np.array([10, 11,12,14, 16])\n",
    "trainAccSoftNP, trainAccVari1NP, valAccSoftNP, valAccVari1NP = np.zeros((len(Sigma), len(prec))), np.zeros((len(Sigma), len(prec))), np.zeros((len(Sigma), len(prec))), np.zeros((len(Sigma), len(prec)))\n",
    "for j in range(len(Sigma)):\n",
    "    for k in range(len(prec)):\n",
    "        noOfLevels = 2**prec[k]- 1 #no of levels of quantization\n",
    "        #step = round(wRange/noOfLevels, precision)\n",
    "        step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "        #discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "        discreteSteps = [step*i for i in range(0, noOfLevels)] \n",
    "        print(f\"Training for sig = {Sigma[j]} prec = {prec[k]} step= {step}\")\n",
    "        trainAccVar, valAccVar,_, _, _ = batchGDCompOCNP(x_train,y_train,60, lrBP, lrNP, 0.00001, mu, Sigma[j], vDD, prec[k], step, discreteSteps, wRange, onoff = 10000, print_op=10)\n",
    "        trainAccSoftNP[j, k] = trainAccVar[-1][0]\n",
    "        trainAccVari1NP[j, k] = trainAccVar[-1][1]\n",
    "        valAccSoftNP[j, k] = valAccVar[-1][0]\n",
    "        valAccVari1NP[j, k] = valAccVar[-1][1]\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[62.81428571, 56.53174603, 67.56190476, 66.14444444, 69.84761905],\n",
       "       [57.37777778, 67.8952381 , 66.82539683, 75.18730159, 70.72380952],\n",
       "       [71.88730159, 79.15714286, 74.5       , 75.5       , 78.67936508],\n",
       "       [71.53968254, 78.21428571, 80.0031746 , 81.65555556, 80.15873016],\n",
       "       [80.17619048, 80.62222222, 81.76507937, 82.75873016, 81.83333333],\n",
       "       [81.13650794, 81.5047619 , 79.92857143, 84.66190476, 84.28412698],\n",
       "       [82.1       , 82.63333333, 82.95873016, 83.78571429, 84.62539683],\n",
       "       [81.8984127 , 84.28571429, 83.81111111, 84.73650794, 83.77777778],\n",
       "       [82.54285714, 83.96984127, 84.44444444, 85.25238095, 85.38253968]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAccVari1NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61.98571429, 57.7       , 66.45714286, 65.32857143, 68.94285714],\n",
       "       [57.77142857, 67.34285714, 66.72857143, 74.7       , 70.67142857],\n",
       "       [71.31428571, 78.55714286, 74.88571429, 75.27142857, 77.88571429],\n",
       "       [70.92857143, 77.82857143, 79.58571429, 81.65714286, 80.21428571],\n",
       "       [80.3       , 80.21428571, 81.72857143, 82.21428571, 81.11428571],\n",
       "       [81.14285714, 81.61428571, 80.4       , 84.51428571, 84.15714286],\n",
       "       [81.38571429, 83.17142857, 83.11428571, 83.21428571, 84.32857143],\n",
       "       [81.77142857, 84.02857143, 83.57142857, 84.35714286, 83.84285714],\n",
       "       [82.74285714, 83.77142857, 84.32857143, 84.78571429, 84.95714286]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valAccVari1NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WPwithVarUp(W1, b1, W2, b2, pert, lossBeforePert, X, y, precision, step, discreteSteps, wRange,  W1Var, b1Var, W2Var, b2Var, W1Currents, b1Currents, W2Currents, b2Currents):\n",
    "    #assert pert==step #to get increments of '1' in the bit pattern of the weight array\n",
    "    #!here we chnage perturb only one value\n",
    "    m = y.shape[0] #m is the number of training examples\n",
    "    Y = one_hot_encoding(y)\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    lossAfterPertW1 = np.zeros_like(W1)\n",
    "    \n",
    "    W1pert = W1.copy()\n",
    "    W1pert += pert\n",
    "    W1pertArrTr = weightTransformWithVariability(W1pert, W1Currents, precision, step, discreteSteps, wRange)\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1pertLoop = W1Var.copy()\n",
    "            W1pertLoop[i, j] = W1pertArrTr[i, j]\n",
    "            #print(W1pertLoop - W1Var)\n",
    "            #plt.figure()\n",
    "            #plt.plot(W1Var.flatten(), W1pertLoop.flatten(), '.')\n",
    "            #plt.title(\"W1 comp\")\n",
    "            #do the transform only on that perturbed weight and place it in the correct place\n",
    "            #W1pertArrTr = weightTransformWithVariability(W1pert, W1Currents, precision, step, discreteSteps, wRange)\n",
    "            _, _, _, A2pert = forward(X, W1pertLoop, b1Var, W2Var, b2Var)\n",
    "            #print(A2pert)\n",
    "            lossAfterPertW1[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    dW1 = 1/m * (lossAfterPertW1-lossBeforePert)/pert\n",
    "    #print(lossAfterPertW1-lossBeforePert)\n",
    "\n",
    "\n",
    "    db1 = np.zeros_like(b1)\n",
    "    lossAfterPertb1 = np.zeros_like(b1)\n",
    "\n",
    "    b1pert = b1.copy()\n",
    "    b1pert +=pert\n",
    "    b1pertArrTr = weightTransformWithVariability(b1pert, b1Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "    for i in range(b1.shape[0]):\n",
    "        b1pertLoop = b1Var.copy()\n",
    "        b1pertLoop[i]=b1pertArrTr[i]\n",
    "        #b1pertArrTr = weightTransformWithVariability(b1pert, b1Currents, precision, step, discreteSteps, wRange)\n",
    "        _, _, _, A2pert = forward(X, W1Var, b1pertLoop, W2Var, b2Var)\n",
    "        lossAfterPertb1[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    db1 = 1/m * (lossAfterPertb1-lossBeforePert)/pert\n",
    "\n",
    "    \n",
    "    dW2 = np.zeros_like(W2)\n",
    "    lossAfterPertW2 = np.zeros_like(W2)\n",
    "\n",
    "    W2pert = W2.copy()\n",
    "    W2pert += pert\n",
    "    W2pertArrTr = weightTransformWithVariability(W2pert, W2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2pertLoop = W2Var.copy()\n",
    "            W2pertLoop[i, j] = W2pertArrTr[i, j]\n",
    "            #W2pertArrTr = weightTransformWithVariability(W2pert, W2Currents, precision, step, discreteSteps, wRange)\n",
    "            _, _, _, A2pert = forward(X, W1Var, b1Var, W2pertLoop, b2Var)\n",
    "            lossAfterPertW2[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    dW2 = 1/m * (lossAfterPertW2-lossBeforePert)/pert\n",
    "    #print(lossAfterPertW2)\n",
    "\n",
    "\n",
    "    db2 = np.zeros_like(b2)\n",
    "    lossAfterPertb2 = np.zeros_like(b2)\n",
    "\n",
    "    b2pert = b2.copy()\n",
    "    b2pert += pert\n",
    "    b2pertArrTr = weightTransformWithVariability(b2pert, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "\n",
    "    for i in range(b2.shape[0]):\n",
    "        b2pertLoop = b2Var.copy()\n",
    "        b2pertLoop[i] = b2pertArrTr[i]\n",
    "        #b2pertArrTr = weightTransformWithVariability(b2pert, b2Currents, precision, step, discreteSteps, wRange)\n",
    "        _, _, _, A2pert = forward(X, W1Var, b1Var, W2Var, b2pertLoop)\n",
    "        lossAfterPertb2[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "    db2 = 1/m * (lossAfterPertb2-lossBeforePert)/pert\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDCompOCUp(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, wRange, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "  W1comp = []\n",
    "\n",
    "  W1, b1, W2, b2 = params_init()\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp= W1.copy(), b1.copy(), W2.copy(), b2.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "  W1bp = roundArbitrary(W1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1bp = roundArbitrary(b1bp, step, wRange)\n",
    "  W2bp = roundArbitrary(W2bp, step, wRange)\n",
    "  b2bp = roundArbitrary(b2bp, step, wRange)\n",
    "  \n",
    "\n",
    "  W1wp, b1wp, W2wp, b2wp = W1.copy(), b1.copy(), W2.copy(), b2.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "  W1wp = roundArbitrary(W1wp, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1wp = roundArbitrary(b1wp, step, wRange)\n",
    "  W2wp = roundArbitrary(W2wp, step, wRange)\n",
    "  b2wp = roundArbitrary(b2wp, step, wRange)\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((20, 784), mu, sigma, vDD, precision, k =10000)#k = Ion/Ioff\n",
    "  b1Currents = initMosParam((20, 1), mu, sigma, vDD, precision, k =10000)\n",
    "  W2Currents = initMosParam((10, 20) ,mu, sigma, vDD, precision, k =10000)\n",
    "  b2Currents = initMosParam((10, 1), mu, sigma, vDD, precision, k =10000)\n",
    "\n",
    "\n",
    "\n",
    "  # dW1Currents = initMosParam((20, 784), mu, sigma, vDD, precision)\n",
    "  # db1Currents = initMosParam((20, 1), mu, sigma, vDD, precision)\n",
    "  # dW2Currents = initMosParam((10, 20) ,mu, sigma, vDD, precision)\n",
    "  # db2Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    # w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    # b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    # w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    # b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    # w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    # b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    #print(cosine_similarity(W1bp.reshape(1,-1), W1wp.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      \n",
    "      W1varocwp = weightTransformWithVariability(W1wp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varocwp = weightTransformWithVariability(b1wp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varocwp = weightTransformWithVariability(W2wp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varocwp = weightTransformWithVariability(b2wp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      #W1comp.append([W1np[0, 0], W1varocwp[0, 0]])\n",
    "      #plt.figure()\n",
    "      #plt.plot(W1wp.flatten(), W1varocwp.flatten(), '.')\n",
    "      #print(W1varocwp)\n",
    "      #doing the weight perturbation pass first\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1varocwp, b1varocwp, W2varocwp,b2varocwp) \n",
    "      #print(A2)\n",
    "      print(f\"WP Iter {i} -> sub iter {j} : {round(accuracy(predictions(A2), Y1), 3)}\", end = \"\\r\", flush = True)\n",
    "      #lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "      lossBeforePert = np.sum(crossEntropy(one_hot_encoding(Y1), A2))\n",
    "\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      #dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varocnp, W2varocnp, W3varocnp,b1varocnp, b2varocnp, b3varocnp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      #dW1wp, db1wp, dW2wp, db2wp = WP(W1varocwp, b1varocwp, W2varocwp, b2varocwp, pert, lossBeforePert, X1, Y1)\n",
    "      dW1wp, db1wp, dW2wp, db2wp = WPwithVarUp(W1wp, b1wp, W2wp, b2wp, pert, lossBeforePert, X1, Y1, precision, step, discreteSteps,  wRange, W1varocwp, b1varocwp, W2varocwp, b2varocwp, W1Currents, b1Currents, W2Currents, b2Currents)\n",
    "      #print(dW1wp)\n",
    "      #print(W1roundwp/step)\n",
    "\n",
    "      # dW1roundwp = roundArbitrary(dW1wp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      # db1roundwp = roundArbitrary(db1wp, step, wRange)\n",
    "      # dW2roundwp = roundArbitrary(dW2wp, step, wRange)\n",
    "      # db2roundwp = roundArbitrary(db2wp, step, wRange)\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      W1wp, b1wp, W2wp, b2wp = param_update(W1wp, b1wp, W2wp,b2wp, dW1wp, db1wp, dW2wp, db2wp, lr = lrNP)\n",
    "      W1wp = roundArbitrary(W1wp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1wp = roundArbitrary(b1wp, step, wRange)\n",
    "      W2wp = roundArbitrary(W2wp, step, wRange)\n",
    "      b2wp = roundArbitrary(b2wp, step, wRange)\n",
    "      #print(dW1wp/step)\n",
    "      #print(W1wp)\n",
    "      ###print(f\"Max W1={np.max(np.abs(W1np))},b1={np.max(np.abs(b1np))},W2={np.max(np.abs(W2np))},b2={np.max(np.abs(b2np))},W3={np.max(np.abs(W3np))},b3={np.max(np.abs(b3np))}\", end= 'r', flush=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "\n",
    "      W1varocbp = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varocbp = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varocbp = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varocbp = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "      \n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1varocbp, b1varocbp, W2varocbp,b2varocbp)\n",
    "      #print(A2) \n",
    "      print(f\"                                                                        BP Iter {i} -> sub iter {j} : {round(accuracy(predictions(A2), Y1),3)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      dW1bp, db1bp, dW2bp, db2bp = backprop(Z1, A1, Z2, A2, W1varocbp, W2varocbp, X1, Y1)\n",
    "      #print(dW1bp)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      # dW1roundbp = roundArbitrary(dW1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      # db1roundbp = roundArbitrary(db1bp, step, wRange)\n",
    "      # dW2roundbp = roundArbitrary(dW2bp, step, wRange)\n",
    "      # db2roundbp = roundArbitrary(db2bp, step, wRange)\n",
    "\n",
    "\n",
    "      W1bp, b1bp, W2bp, b2bp = param_update(W1bp, b1bp, W2bp, b2bp, dW1bp, db1bp, dW2bp, db2bp,  lr = lrBP)\n",
    "      W1bp = roundArbitrary(W1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1bp = roundArbitrary(b1bp, step, wRange)\n",
    "      W2bp = roundArbitrary(W2bp, step, wRange)\n",
    "      b2bp = roundArbitrary(b2bp, step, wRange)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "      # plt.figure()\n",
    "      # plt.plot(dW1bp.flatten(), dW1wp.flatten(), '.')\n",
    "      # plt.title(f\"Iter {j}\")\n",
    "      #plt.plot(dW1roundbp.flatten(), dW1roundwp.flatten(), '.', alpha = 0.3)\n",
    "\n",
    "\n",
    "    #lrNP = lrNP*np.exp(-0.1)\n",
    "    #lrBP = lrBP*np.exp(-0.1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      # plt.figure()\n",
    "      # plt.plot(W1bp.flatten())\n",
    "      # plt.plot(W1wp.flatten())\n",
    "      # plt.legend([\"BP\", \"Wp\"])\n",
    "      _, _, _, A2_train = forward(X, weightTransformWithVariability(W1wp, W1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b1wp, b1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(W2wp, W2Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b2wp, b2Currents, precision, step, discreteSteps, wRange))\n",
    "      _, _, _, A2_train_bp = forward(X, weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange), weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps, wRange))\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A2_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,  A2_val = forward(x_val, W1wp, b1wp, W2wp, b2wp)\n",
    "      _, _, _, A2_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A2_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {round(train_score, 3)}::Val accuracy: {round(val_score, 3)}:#########################:Train Acc BP::{round(accuracy(predictions(A2_train_bp), Y), 3)} Val Acc BP::{round(accuracy(predictions(A2_val_bp), y_val), 3)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 57.483::Val accuracy: 52.2:#########################:Train Acc BP::64.748 Val Acc BP::64.1\n",
      "Iteration: 2::Train accuracy: 65.063::Val accuracy: 61.143:#########################:Train Acc BP::69.176 Val Acc BP::67.171\n",
      "Iteration: 3::Train accuracy: 63.565::Val accuracy: 59.429:#########################:Train Acc BP::70.817 Val Acc BP::69.329\n",
      "Iteration: 4::Train accuracy: 68.406::Val accuracy: 65.643:#########################:Train Acc BP::71.125 Val Acc BP::70.571\n",
      "Iteration: 5::Train accuracy: 69.824::Val accuracy: 63.586:#########################:Train Acc BP::74.381 Val Acc BP::72.714\n",
      "Iteration: 6::Train accuracy: 70.173::Val accuracy: 60.014:#########################:Train Acc BP::74.686 Val Acc BP::73.757\n",
      "Iteration: 7::Train accuracy: 71.324::Val accuracy: 62.257:#########################:Train Acc BP::75.505 Val Acc BP::75.157\n",
      "Iteration: 8::Train accuracy: 65.816::Val accuracy: 64.6:#########################:Train Acc BP::76.175 Val Acc BP::74.7\n",
      "Iteration: 9::Train accuracy: 68.77::Val accuracy: 66.086:#########################:Train Acc BP::76.524 Val Acc BP::75.3\n",
      "Iteration: 10::Train accuracy: 71.802::Val accuracy: 67.9:#########################:Train Acc BP::76.892 Val Acc BP::76.157\n",
      "Iteration: 11::Train accuracy: 72.008::Val accuracy: 67.271:#########################:Train Acc BP::78.698 Val Acc BP::77.343\n",
      "Iteration: 12::Train accuracy: 74.29::Val accuracy: 68.9:#########################:Train Acc BP::78.111 Val Acc BP::77.414\n",
      "Iteration: 13::Train accuracy: 74.871::Val accuracy: 68.757:#########################:Train Acc BP::78.933 Val Acc BP::77.957\n",
      "Iteration: 14::Train accuracy: 72.205::Val accuracy: 60.014:#########################:Train Acc BP::77.779 Val Acc BP::77.257\n",
      "Iteration: 15::Train accuracy: 73.198::Val accuracy: 58.043:#########################:Train Acc BP::78.902 Val Acc BP::77.714\n",
      "Iteration: 16::Train accuracy: 67.237::Val accuracy: 52.657:#########################:Train Acc BP::78.608 Val Acc BP::77.543\n",
      "Iteration: 17::Train accuracy: 71.61::Val accuracy: 55.229:#########################:Train Acc BP::79.868 Val Acc BP::77.729\n",
      "Iteration: 18::Train accuracy: 73.376::Val accuracy: 56.9:#########################:Train Acc BP::80.276 Val Acc BP::75.686\n",
      "Iteration: 19::Train accuracy: 73.568::Val accuracy: 55.543:#########################:Train Acc BP::80.494 Val Acc BP::75.2\n",
      "Iteration: 20::Train accuracy: 74.627::Val accuracy: 57.743:#########################:Train Acc BP::80.213 Val Acc BP::74.786\n",
      "Iteration: 21::Train accuracy: 73.141::Val accuracy: 64.329:#########################:Train Acc BP::79.694 Val Acc BP::74.429\n",
      "Iteration: 22::Train accuracy: 68.11::Val accuracy: 62.4:#########################:Train Acc BP::80.714 Val Acc BP::75.086\n",
      "Iteration: 23::Train accuracy: 72.014::Val accuracy: 62.729:#########################:Train Acc BP::80.968 Val Acc BP::74.9\n",
      "Iteration: 24::Train accuracy: 71.295::Val accuracy: 55.743:#########################:Train Acc BP::80.356 Val Acc BP::74.814\n",
      "Iteration: 25::Train accuracy: 70.806::Val accuracy: 58.929:#########################:Train Acc BP::80.686 Val Acc BP::75.057\n",
      "Iteration: 26::Train accuracy: 72.063::Val accuracy: 55.243:#########################:Train Acc BP::81.46 Val Acc BP::74.614\n",
      "Iteration: 27::Train accuracy: 72.886::Val accuracy: 56.714:#########################:Train Acc BP::82.562 Val Acc BP::76.157\n",
      "Iteration: 28::Train accuracy: 73.205::Val accuracy: 58.571:#########################:Train Acc BP::82.649 Val Acc BP::76.371\n",
      "Iteration: 29::Train accuracy: 73.335::Val accuracy: 56.586:#########################:Train Acc BP::82.724 Val Acc BP::76.3\n",
      "Iteration: 30::Train accuracy: 68.686::Val accuracy: 52.557:#########################:Train Acc BP::81.713 Val Acc BP::76.7\n",
      "Iteration: 31::Train accuracy: 71.859::Val accuracy: 56.157:#########################:Train Acc BP::81.127 Val Acc BP::76.7\n",
      "Iteration: 32::Train accuracy: 72.138::Val accuracy: 58.386:#########################:Train Acc BP::82.221 Val Acc BP::77.171\n",
      "Iteration: 33::Train accuracy: 72.379::Val accuracy: 59.971:#########################:Train Acc BP::82.386 Val Acc BP::77.157\n",
      "Iteration: 34::Train accuracy: 73.089::Val accuracy: 62.271:#########################:Train Acc BP::82.398 Val Acc BP::77.114\n",
      "Iteration: 35::Train accuracy: 73.779::Val accuracy: 61.814:#########################:Train Acc BP::82.844 Val Acc BP::77.429\n",
      "Iteration: 36::Train accuracy: 73.502::Val accuracy: 58.786:#########################:Train Acc BP::82.538 Val Acc BP::77.357\n",
      "Iteration: 37::Train accuracy: 71.303::Val accuracy: 58.6:#########################:Train Acc BP::82.668 Val Acc BP::77.5\n",
      "Iteration: 38::Train accuracy: 71.886::Val accuracy: 57.257:#########################:Train Acc BP::82.941 Val Acc BP::77.571\n",
      "Iteration: 39::Train accuracy: 72.125::Val accuracy: 57.529:#########################:Train Acc BP::82.997 Val Acc BP::77.857\n",
      "Iteration: 40::Train accuracy: 72.086::Val accuracy: 59.229:#########################:Train Acc BP::82.47 Val Acc BP::77.757\n",
      "Iteration: 41::Train accuracy: 70.79::Val accuracy: 56.143:#########################:Train Acc BP::81.592 Val Acc BP::77.786\n",
      "Iteration: 42::Train accuracy: 71.93::Val accuracy: 57.186:#########################:Train Acc BP::82.197 Val Acc BP::77.886\n",
      "Iteration: 43::Train accuracy: 72.417::Val accuracy: 57.4:#########################:Train Acc BP::83.078 Val Acc BP::78.2\n",
      "Iteration: 44::Train accuracy: 72.052::Val accuracy: 55.543:#########################:Train Acc BP::83.087 Val Acc BP::78.543\n",
      "Iteration: 45::Train accuracy: 72.64::Val accuracy: 55.843:#########################:Train Acc BP::83.114 Val Acc BP::78.686\n",
      "Iteration: 46::Train accuracy: 73.038::Val accuracy: 57.057:#########################:Train Acc BP::83.132 Val Acc BP::78.671\n",
      "Iteration: 47::Train accuracy: 73.189::Val accuracy: 56.329:#########################:Train Acc BP::83.017 Val Acc BP::78.843\n",
      "Iteration: 48::Train accuracy: 73.306::Val accuracy: 55.829:#########################:Train Acc BP::83.249 Val Acc BP::78.871\n",
      "Iteration: 49::Train accuracy: 73.867::Val accuracy: 56.043:#########################:Train Acc BP::84.237 Val Acc BP::79.429\n",
      "Iteration: 50::Train accuracy: 73.957::Val accuracy: 56.014:#########################:Train Acc BP::83.773 Val Acc BP::79.214\n",
      "Iteration: 51::Train accuracy: 74.1::Val accuracy: 53.043:#########################:Train Acc BP::83.244 Val Acc BP::78.829\n",
      "Iteration: 52::Train accuracy: 73.373::Val accuracy: 52.871:#########################:Train Acc BP::83.352 Val Acc BP::79.029\n",
      "Iteration: 53::Train accuracy: 74.484::Val accuracy: 53.571:#########################:Train Acc BP::83.711 Val Acc BP::79.229\n",
      "Iteration: 54::Train accuracy: 74.31::Val accuracy: 53.729:#########################:Train Acc BP::84.365 Val Acc BP::80.057\n",
      "Iteration: 55::Train accuracy: 74.39::Val accuracy: 53.314:#########################:Train Acc BP::83.359 Val Acc BP::78.943\n",
      "Iteration: 56::Train accuracy: 73.059::Val accuracy: 52.1:#########################:Train Acc BP::84.173 Val Acc BP::79.6\n",
      "Iteration: 57::Train accuracy: 73.027::Val accuracy: 53.0:#########################:Train Acc BP::83.662 Val Acc BP::78.814\n",
      "Iteration: 58::Train accuracy: 73.005::Val accuracy: 52.6:#########################:Train Acc BP::84.265 Val Acc BP::79.257\n",
      "Iteration: 59::Train accuracy: 73.59::Val accuracy: 53.457:#########################:Train Acc BP::84.398 Val Acc BP::79.386\n",
      "Iteration: 60::Train accuracy: 73.389::Val accuracy: 54.414:#########################:Train Acc BP::84.1 Val Acc BP::78.943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[64.74761904761904, 57.48253968253968],\n",
       "  [69.17619047619048, 65.06349206349206],\n",
       "  [70.81746031746032, 63.56507936507937],\n",
       "  [71.12539682539682, 68.4063492063492],\n",
       "  [74.38095238095238, 69.82380952380952],\n",
       "  [74.68571428571428, 70.17301587301587],\n",
       "  [75.5047619047619, 71.32380952380953],\n",
       "  [76.17460317460318, 65.81587301587302],\n",
       "  [76.52380952380953, 68.76984126984127],\n",
       "  [76.89206349206349, 71.8015873015873],\n",
       "  [78.6984126984127, 72.0079365079365],\n",
       "  [78.11111111111111, 74.2904761904762],\n",
       "  [78.93333333333334, 74.87142857142857],\n",
       "  [77.77936507936508, 72.20476190476191],\n",
       "  [78.90158730158731, 73.1984126984127],\n",
       "  [78.6079365079365, 67.23650793650793],\n",
       "  [79.86825396825397, 71.60952380952381],\n",
       "  [80.27619047619048, 73.37619047619047],\n",
       "  [80.4936507936508, 73.56825396825397],\n",
       "  [80.21269841269842, 74.62698412698413],\n",
       "  [79.69365079365079, 73.14126984126985],\n",
       "  [80.71428571428572, 68.10952380952381],\n",
       "  [80.96825396825396, 72.01428571428572],\n",
       "  [80.35555555555555, 71.2952380952381],\n",
       "  [80.68571428571428, 70.80634920634921],\n",
       "  [81.46031746031746, 72.06349206349206],\n",
       "  [82.56190476190476, 72.88571428571429],\n",
       "  [82.64920634920635, 73.20476190476191],\n",
       "  [82.72380952380952, 73.33492063492064],\n",
       "  [81.71269841269842, 68.68571428571428],\n",
       "  [81.12698412698413, 71.85873015873015],\n",
       "  [82.22063492063492, 72.13809523809523],\n",
       "  [82.38571428571429, 72.37936507936507],\n",
       "  [82.3984126984127, 73.08888888888889],\n",
       "  [82.84444444444445, 73.77936507936508],\n",
       "  [82.53809523809524, 73.5015873015873],\n",
       "  [82.66825396825396, 71.3031746031746],\n",
       "  [82.94126984126984, 71.88571428571429],\n",
       "  [82.9968253968254, 72.12539682539682],\n",
       "  [82.46984126984127, 72.08571428571429],\n",
       "  [81.59206349206349, 70.7904761904762],\n",
       "  [82.1968253968254, 71.93015873015874],\n",
       "  [83.07777777777777, 72.41746031746031],\n",
       "  [83.08730158730158, 72.05238095238096],\n",
       "  [83.11428571428571, 72.63968253968254],\n",
       "  [83.13174603174603, 73.03809523809524],\n",
       "  [83.01746031746032, 73.1888888888889],\n",
       "  [83.24920634920635, 73.30634920634921],\n",
       "  [84.23650793650793, 73.86666666666667],\n",
       "  [83.77301587301588, 73.95714285714286],\n",
       "  [83.24444444444444, 74.1],\n",
       "  [83.35238095238095, 73.37301587301587],\n",
       "  [83.71111111111111, 74.48412698412699],\n",
       "  [84.36507936507937, 74.30952380952381],\n",
       "  [83.35873015873015, 74.39047619047619],\n",
       "  [84.17301587301587, 73.05873015873016],\n",
       "  [83.66190476190476, 73.02698412698413],\n",
       "  [84.26507936507936, 73.0047619047619],\n",
       "  [84.3984126984127, 73.5904761904762],\n",
       "  [84.1, 73.38888888888889]],\n",
       " [[64.1, 52.2],\n",
       "  [67.17142857142858, 61.142857142857146],\n",
       "  [69.32857142857142, 59.42857142857143],\n",
       "  [70.57142857142857, 65.64285714285715],\n",
       "  [72.71428571428571, 63.58571428571429],\n",
       "  [73.75714285714285, 60.014285714285705],\n",
       "  [75.15714285714286, 62.25714285714285],\n",
       "  [74.7, 64.60000000000001],\n",
       "  [75.3, 66.08571428571427],\n",
       "  [76.15714285714286, 67.9],\n",
       "  [77.34285714285714, 67.27142857142857],\n",
       "  [77.41428571428571, 68.89999999999999],\n",
       "  [77.95714285714286, 68.75714285714287],\n",
       "  [77.25714285714285, 60.014285714285705],\n",
       "  [77.71428571428571, 58.04285714285714],\n",
       "  [77.54285714285714, 52.65714285714286],\n",
       "  [77.72857142857143, 55.22857142857143],\n",
       "  [75.68571428571428, 56.89999999999999],\n",
       "  [75.2, 55.54285714285714],\n",
       "  [74.78571428571429, 57.74285714285714],\n",
       "  [74.42857142857143, 64.32857142857142],\n",
       "  [75.08571428571429, 62.4],\n",
       "  [74.9, 62.728571428571435],\n",
       "  [74.81428571428572, 55.74285714285714],\n",
       "  [75.05714285714285, 58.92857142857143],\n",
       "  [74.61428571428571, 55.24285714285714],\n",
       "  [76.15714285714286, 56.714285714285715],\n",
       "  [76.37142857142857, 58.57142857142858],\n",
       "  [76.3, 56.58571428571428],\n",
       "  [76.7, 52.55714285714286],\n",
       "  [76.7, 56.15714285714286],\n",
       "  [77.17142857142856, 58.385714285714286],\n",
       "  [77.15714285714286, 59.971428571428575],\n",
       "  [77.11428571428571, 62.271428571428565],\n",
       "  [77.42857142857143, 61.81428571428571],\n",
       "  [77.35714285714286, 58.785714285714285],\n",
       "  [77.5, 58.599999999999994],\n",
       "  [77.57142857142857, 57.25714285714286],\n",
       "  [77.85714285714286, 57.52857142857143],\n",
       "  [77.75714285714285, 59.22857142857143],\n",
       "  [77.78571428571428, 56.14285714285714],\n",
       "  [77.88571428571429, 57.18571428571428],\n",
       "  [78.2, 57.4],\n",
       "  [78.54285714285714, 55.54285714285714],\n",
       "  [78.68571428571428, 55.84285714285714],\n",
       "  [78.67142857142856, 57.057142857142864],\n",
       "  [78.84285714285714, 56.32857142857143],\n",
       "  [78.87142857142857, 55.82857142857143],\n",
       "  [79.42857142857143, 56.04285714285714],\n",
       "  [79.21428571428571, 56.01428571428572],\n",
       "  [78.82857142857142, 53.042857142857144],\n",
       "  [79.02857142857142, 52.87142857142857],\n",
       "  [79.22857142857143, 53.57142857142857],\n",
       "  [80.05714285714286, 53.72857142857143],\n",
       "  [78.94285714285715, 53.31428571428572],\n",
       "  [79.60000000000001, 52.1],\n",
       "  [78.81428571428572, 53.0],\n",
       "  [79.25714285714285, 52.6],\n",
       "  [79.38571428571429, 53.457142857142856],\n",
       "  [78.94285714285715, 54.41428571428572]],\n",
       " [],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "iter=1\n",
    "lrBP=0.1\n",
    "lrNP=0.1\n",
    "pert=0.1\n",
    "mu = 0.7\n",
    "#sigma = 0.07943282\n",
    "sigma = 0.01\n",
    "vDD = 5\n",
    "precision = 16#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "#batchGDCompOCNP(x_train,y_train,60, lrBP, lrNP, 0.00001, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff = 10000, print_op=10)\n",
    "batchGDCompOCUp(x_train,y_train,60, lrBP, lrNP, step, mu, sigma, vDD, precision, step, discreteSteps, wRange, print_op=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 hidden layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init(seed):\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  #print(\"BP \\n \", dZ3)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init(2)\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batchGradDescentWithVar(X,Y,iter, lr, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init(2)\n",
    "  \n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp= W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "\n",
    "  W1bp = roundArbitrary(W1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1bp = roundArbitrary(b1bp, step, wRange)\n",
    "  W2bp = roundArbitrary(W2bp, step, wRange)\n",
    "  b2bp = roundArbitrary(b2bp, step, wRange)\n",
    "  W3bp = roundArbitrary(W3bp, step, wRange)\n",
    "  b3bp = roundArbitrary(b3bp, step, wRange)\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision, k =onoff)#k = Ion/Ioff\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W3Currents = initMosParam((10, 50) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      W1varoc = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "      W3varoc = weightTransformWithVariability(W3bp, W3Currents, precision, step, discreteSteps, wRange)\n",
    "      b3varoc = weightTransformWithVariability(b3bp, b3Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, X1, Y1)\n",
    "\n",
    "      # dW1round = roundArbitrary(dW1, step, wRange)#weights have to maintained as their digitized versions\n",
    "      # db1round = roundArbitrary(db1, step, wRange)\n",
    "      # dW2round = roundArbitrary(dW2, step, wRange)\n",
    "      # db2round = roundArbitrary(db2, step, wRange)\n",
    "      # dW3round = roundArbitrary(dW3, step, wRange)\n",
    "      # db3round = roundArbitrary(db3, step, wRange)\n",
    "\n",
    "\n",
    "\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "      W1bp = roundArbitrary(W1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1bp = roundArbitrary(b1bp, step, wRange)\n",
    "      W2bp = roundArbitrary(W2bp, step, wRange)\n",
    "      b2bp = roundArbitrary(b2bp, step, wRange)\n",
    "      W3bp = roundArbitrary(W3bp, step, wRange)\n",
    "      b3bp = roundArbitrary(b3bp, step, wRange)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 69.55079365079365\n",
      "Val accuracy: 69.39999999999999\n",
      "Iteration: 2\n",
      "Train accuracy: 74.57619047619048\n",
      "Val accuracy: 74.34285714285714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m discreteSteps \u001b[39m=\u001b[39m [step\u001b[39m*\u001b[39mi \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, noOfLevels)] \u001b[39m#storing the values of the steps\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batchGradDescentWithVar(x_train,y_train,\u001b[39miter\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 34\u001b[0m in \u001b[0;36mbatchGradDescentWithVar\u001b[1;34m(X, Y, iter, lr, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=237'>238</a>\u001b[0m X1, Y1 \u001b[39m=\u001b[39m shuffle(X[:, j\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m: (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m]\u001b[39m.\u001b[39mT,Y[j\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m: (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m]) \u001b[39m#shuffle each batch\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m X1 \u001b[39m=\u001b[39m X1\u001b[39m.\u001b[39mT \u001b[39m#take transpose to match the sizes\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=243'>244</a>\u001b[0m W1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=244'>245</a>\u001b[0m b1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=245'>246</a>\u001b[0m W2varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 34\u001b[0m in \u001b[0;36mweightTransformWithVariability\u001b[1;34m(weightArray, currents, precision, step, discreteSteps, wRange)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m analogWeightArray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(weightArray, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m bitLevel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(precision):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   analogWeightArray \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39;49mwhere(np\u001b[39m.\u001b[39;49mbitwise_and(clippedWeightIndexArray, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbitLevel)\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbitLevel)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m analogWeightArray  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmultiply(np\u001b[39m.\u001b[39msign(weightArray), analogWeightArray)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y116sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m weightWithVariability \u001b[39m=\u001b[39m (analogWeightArray\u001b[39m/\u001b[39miOnNominal)\u001b[39m*\u001b[39mstep\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "iter=100\n",
    "lrBP=0.1\n",
    "lrNP=0.1\n",
    "pert=0.1\n",
    "mu = 0.7\n",
    "sigma = 0.0000001\n",
    "vDD = 5\n",
    "precision = 16#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "batchGradDescentWithVar(x_train,y_train,iter, 0.1, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff=10000, print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 67.43174603174603\n",
      "Val accuracy: 66.28571428571428\n",
      "Iteration: 2\n",
      "Train accuracy: 72.26825396825397\n",
      "Val accuracy: 71.97142857142858\n",
      "Iteration: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights \u001b[39m=\u001b[39m batch_grad_descent(x_train,y_train,\u001b[39m100\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 34\u001b[0m in \u001b[0;36mbatch_grad_descent\u001b[1;34m(X, Y, iter, lr, print_op)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIteration: \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m#obtain training loss\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m _, _, _, _, _, A3_train \u001b[39m=\u001b[39m forward(X, W1, b1, W2, b2, W3, b3)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39m#for i in range(0, Y.shape[0]):\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m  \u001b[39m# train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39m#train_loss.append(train_loss_score)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m \u001b[39m#print(f'Train Loss: {train_loss_score}')\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39m#obtain training accuracy\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m train_score \u001b[39m=\u001b[39m accuracy(predictions(A3_train), Y)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 34\u001b[0m in \u001b[0;36mforward\u001b[1;34m(x_train, W1, b1, W2, b2, W3, b3)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(x_train, W1, b1, W2, b2, W3, b3):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   \u001b[39m#print(\"Entered FP\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   Z1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W1,x_train) \u001b[39m+\u001b[39m b1 \u001b[39m#W1 is 50*784, x_train is 748*m, Z1 is 50*m\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   A1 \u001b[39m=\u001b[39m relu(Z1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   Z2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2,A1) \u001b[39m+\u001b[39m b2 \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   A2 \u001b[39m=\u001b[39m relu(Z2)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 34\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m   \u001b[39m#Z4 = np.matmul(W4,A3) + b4\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m   \u001b[39m#A4 = relu(Z4)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m   \u001b[39m#A2 is 10*m, final predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m   \u001b[39m# print(\"Fp Done\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m Z1, A1, Z2, A2, Z3, A3\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m    \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmaximum(x,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(Z):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y110sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m   \u001b[39m#return np.exp(Z) / np.sum(np.exp(Z),0)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batch_grad_descent(x_train,y_train,100, 0.1, print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #print(W2[3])\n",
    "  #first approximate dZ3\n",
    "  m = Y1.shape[0]\n",
    "  #print(Z3.shape)\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i, :] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #lossArrayAfterPertZ3[i, :] += np.sum(np.square(A3pert-one_hot_encoding(Y1)), axis=0)\n",
    "    lossArrayAfterPertZ3[i, :] += crossEntropy(one_hot_encoding(Y1), A3pert)\n",
    "\n",
    "\n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "\n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #lossArrayAfterPertZ2[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "    lossArrayAfterPertZ2[i, :] += crossEntropy(one_hot_encoding(Y1), A3pert)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "    A1pert = relu(Z1pert)\n",
    "    \n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    \n",
    "    #lossArrayAfterPertZ1[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "    lossArrayAfterPertZ1[i, :] += crossEntropy(one_hot_encoding(Y1), A3pert)\n",
    "\n",
    "    \n",
    "  #print(lossArrayAfterPertZ1)\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDCompNP(X,Y,iter, lrBP, lrNP, pert, seed=2,print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init(seed)\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "\n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1np, b1np, W2np,b2np, W3np, b3np) \n",
    "      print(f\"NP Iter {i+1} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "      lossBeforePert = crossEntropy(one_hot_encoding(Y1), A3)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1np, b1np, W2np, b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr = lrNP)\n",
    "      #print(W1)\n",
    "\n",
    "\n",
    "      #print(f\"Max W1={np.max(np.abs(W1np))},b1={np.max(np.abs(b1np))},W2={np.max(np.abs(W2np))},b2={np.max(np.abs(b2np))},W3={np.max(np.abs(W3np))},b3={np.max(np.abs(b3np))}\", end= 'r', flush=True)\n",
    "\n",
    "\n",
    "\n",
    "      ##print(f\"Max W1={np.max(np.abs(W1np))},b1={np.max(np.abs(b1np))},W2={np.max(np.abs(W2np))},b2={np.max(np.abs(b2np))},W3={np.max(np.abs(W3np))},b3={np.max(np.abs(b3np))}\", end= 'r', flush=True)\n",
    "      #doing the back propagation for the same data set sample\n",
    "      \n",
    "      \n",
    "      # Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1bp, b1bp, W2bp,b2bp, W3bp, b3bp) \n",
    "      # print(f\"                                                                        BP Iter {i+1} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      # #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      # dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1bp, W2bp, W3bp, X1, Y1)\n",
    "      # W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1bp, b1bp, W2bp, b2bp, W3bp, b3bp, dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp, lr = lrBP)\n",
    "      \n",
    "      \n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    #lrNP = lrNP*np.exp(-0.01)\n",
    "    #lrBP = lrBP*np.exp(-0.01)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      \n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train Acc BP::{accuracy(predictions(A3_train_bp), Y)} Val Acc BP::{accuracy(predictions(A3_val_bp), y_val)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 74.01111111111112::Val accuracy: 73.34285714285714::Train Acc BP::12.874603174603175 Val Acc BP::12.628571428571428\n",
      "NP Iter 2 -> sub iter 10 : 73.33333333333333\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batchGDCompNP(x_train,y_train,\u001b[39m100\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, \u001b[39m0.01\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 39\u001b[0m in \u001b[0;36mbatchGDCompNP\u001b[1;34m(X, Y, iter, lrBP, lrNP, pert, seed, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m lossBeforePert \u001b[39m=\u001b[39m crossEntropy(one_hot_encoding(Y1), A3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m dW1np, db1np, dW2np, db2np, dW3np, db3np \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m#print(f\"iter in iter{j}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m W1np, b1np, W2np, b2np, W3np, b3np \u001b[39m=\u001b[39m param_update(W1np, b1np, W2np, b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr \u001b[39m=\u001b[39m lrNP)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 39\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m Z1pert[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pert\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m A1pert \u001b[39m=\u001b[39m relu(Z1pert)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m Z2pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(W2,A1pert) \u001b[39m+\u001b[39m b2 \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y122sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m Z3pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W3,A2pert) \u001b[39m+\u001b[39m b3\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batchGDCompNP(x_train,y_train,100, 0.1, 0.1, 0.01, seed=2,print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGradDescentNPWithVar(X,Y,iter, lr, pert, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init(2)\n",
    "  \n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp= W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "\n",
    "  W1bp = roundArbitrary(W1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1bp = roundArbitrary(b1bp, step, wRange)\n",
    "  W2bp = roundArbitrary(W2bp, step, wRange)\n",
    "  b2bp = roundArbitrary(b2bp, step, wRange)\n",
    "  W3bp = roundArbitrary(W3bp, step, wRange)\n",
    "  b3bp = roundArbitrary(b3bp, step, wRange)\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision, k =onoff)#k = Ion/Ioff\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W3Currents = initMosParam((10, 50) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      W1varoc = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "      W3varoc = weightTransformWithVariability(W3bp, W3Currents, precision, step, discreteSteps, wRange)\n",
    "      b3varoc = weightTransformWithVariability(b3bp, b3Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc)\n",
    "\n",
    "\n",
    "\n",
    "      lossBeforePert = crossEntropy(one_hot_encoding(Y1), A3)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n",
    "\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, X1, Y1)\n",
    "\n",
    "      # dW1round = roundArbitrary(dW1, step, wRange)#weights have to maintained as their digitized versions\n",
    "      # db1round = roundArbitrary(db1, step, wRange)\n",
    "      # dW2round = roundArbitrary(dW2, step, wRange)\n",
    "      # db2round = roundArbitrary(db2, step, wRange)\n",
    "      # dW3round = roundArbitrary(dW3, step, wRange)\n",
    "      # db3round = roundArbitrary(db3, step, wRange)\n",
    "\n",
    "\n",
    "\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "      W1bp = roundArbitrary(W1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1bp = roundArbitrary(b1bp, step, wRange)\n",
    "      W2bp = roundArbitrary(W2bp, step, wRange)\n",
    "      b2bp = roundArbitrary(b2bp, step, wRange)\n",
    "      W3bp = roundArbitrary(W3bp, step, wRange)\n",
    "      b3bp = roundArbitrary(b3bp, step, wRange)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 39.523809523809526\n",
      "Val accuracy: 38.48571428571429\n",
      "Iteration: 2\n",
      "Train accuracy: 46.37777777777778\n",
      "Val accuracy: 44.6\n",
      "Iteration: 3\n",
      "Train accuracy: 36.94603174603174\n",
      "Val accuracy: 36.94285714285714\n",
      "Iteration: 4\n",
      "Train accuracy: 39.84761904761905\n",
      "Val accuracy: 39.81428571428572\n",
      "Iteration: 5\n",
      "Train accuracy: 32.87619047619047\n",
      "Val accuracy: 33.128571428571426\n",
      "Iteration: 6\n",
      "Train accuracy: 50.52857142857143\n",
      "Val accuracy: 49.58571428571428\n",
      "Iteration: 7\n",
      "Train accuracy: 22.434920634920637\n",
      "Val accuracy: 22.357142857142858\n",
      "Iteration: 8\n",
      "Train accuracy: 35.699999999999996\n",
      "Val accuracy: 35.08571428571429\n",
      "Iteration: 9\n",
      "Train accuracy: 31.25079365079365\n",
      "Val accuracy: 31.071428571428573\n",
      "Iteration: 10\n",
      "Train accuracy: 39.355555555555554\n",
      "Val accuracy: 38.57142857142858\n",
      "Iteration: 11\n",
      "Train accuracy: 39.2015873015873\n",
      "Val accuracy: 38.67142857142857\n",
      "Iteration: 12\n",
      "Train accuracy: 34.74285714285714\n",
      "Val accuracy: 34.785714285714285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m discreteSteps \u001b[39m=\u001b[39m [step\u001b[39m*\u001b[39mi \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, noOfLevels)] \u001b[39m#storing the values of the steps\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batchGradDescentNPWithVar(x_train,y_train,\u001b[39miter\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, \u001b[39m0.00001\u001b[39;49m, mu, Sigma[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], vDD, precision, step, discreteSteps, wRange, onoff\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 41\u001b[0m in \u001b[0;36mbatchGradDescentNPWithVar\u001b[1;34m(X, Y, iter, lr, pert, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m lossBeforePert \u001b[39m=\u001b[39m crossEntropy(one_hot_encoding(Y1), A3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m dW1, db1, dW2, db2, dW3, db3 \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, X1, Y1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# dW1round = roundArbitrary(dW1, step, wRange)#weights have to maintained as their digitized versions\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# dW3round = roundArbitrary(dW3, step, wRange)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# db3round = roundArbitrary(db3, step, wRange)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m W1bp, b1bp, W2bp, b2bp, W3bp, b3bp \u001b[39m=\u001b[39m param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1, db1, dW2, db2, dW3, db3, lr \u001b[39m=\u001b[39m lr)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 41\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m Z1pert[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pert\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m A1pert \u001b[39m=\u001b[39m relu(Z1pert)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m Z2pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(W2,A1pert) \u001b[39m+\u001b[39m b2 \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y114sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m Z3pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W3,A2pert) \u001b[39m+\u001b[39m b3\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "iter=100\n",
    "lrBP=0.1\n",
    "lrNP=0.1\n",
    "pert=0.1\n",
    "mu = 0.7\n",
    "sigma = 0.0000001\n",
    "vDD = 5\n",
    "precision = 14#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "batchGradDescentNPWithVar(x_train,y_train,iter, 0.1, 0.00001, mu, Sigma[-2], vDD, precision, step, discreteSteps, wRange, onoff=10000, print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 32.215873015873015\n",
      "Val accuracy: 31.757142857142856\n",
      "Iteration: 2\n",
      "Train accuracy: 28.936507936507937\n",
      "Val accuracy: 29.2\n",
      "Iteration: 3\n",
      "Train accuracy: 30.214285714285715\n",
      "Val accuracy: 29.214285714285715\n",
      "Iteration: 4\n",
      "Train accuracy: 27.990476190476187\n",
      "Val accuracy: 27.15714285714286\n",
      "Iteration: 5\n",
      "Train accuracy: 32.55555555555556\n",
      "Val accuracy: 32.25714285714286\n",
      "Iteration: 6\n",
      "Train accuracy: 31.855555555555554\n",
      "Val accuracy: 31.52857142857143\n",
      "Iteration: 7\n",
      "Train accuracy: 31.04126984126984\n",
      "Val accuracy: 30.542857142857144\n",
      "Iteration: 8\n",
      "Train accuracy: 31.56984126984127\n",
      "Val accuracy: 31.128571428571426\n",
      "Iteration: 9\n",
      "Train accuracy: 33.3063492063492\n",
      "Val accuracy: 33.128571428571426\n",
      "Iteration: 10\n",
      "Train accuracy: 31.890476190476193\n",
      "Val accuracy: 31.285714285714285\n",
      "Iteration: 11\n",
      "Train accuracy: 32.41269841269842\n",
      "Val accuracy: 31.771428571428572\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m discreteSteps \u001b[39m=\u001b[39m [step\u001b[39m*\u001b[39mi \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, noOfLevels)] \u001b[39m#storing the values of the steps\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batchGradDescentWithVar(x_train,y_train,\u001b[39miter\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, mu, Sigma[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m], vDD, precision, step, discreteSteps, wRange, onoff\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 42\u001b[0m in \u001b[0;36mbatchGradDescentWithVar\u001b[1;34m(X, Y, iter, lr, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff, print_op)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=237'>238</a>\u001b[0m X1, Y1 \u001b[39m=\u001b[39m shuffle(X[:, j\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m: (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m]\u001b[39m.\u001b[39mT,Y[j\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m: (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m]) \u001b[39m#shuffle each batch\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m X1 \u001b[39m=\u001b[39m X1\u001b[39m.\u001b[39mT \u001b[39m#take transpose to match the sizes\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=243'>244</a>\u001b[0m W1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=244'>245</a>\u001b[0m b1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=245'>246</a>\u001b[0m W2varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\WPFMINSTnumba.ipynb Cell 42\u001b[0m in \u001b[0;36mweightTransformWithVariability\u001b[1;34m(weightArray, currents, precision, step, discreteSteps, wRange)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m analogWeightArray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(weightArray, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m bitLevel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(precision):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   analogWeightArray \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39;49mwhere(np\u001b[39m.\u001b[39;49mbitwise_and(clippedWeightIndexArray, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbitLevel)\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbitLevel)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m analogWeightArray  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmultiply(np\u001b[39m.\u001b[39msign(weightArray), analogWeightArray)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/WPFMINSTnumba.ipynb#Y115sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m weightWithVariability \u001b[39m=\u001b[39m (analogWeightArray\u001b[39m/\u001b[39miOnNominal)\u001b[39m*\u001b[39mstep\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Sigma = 10**(-np.arange(1, 2, 0.1)[1:])\n",
    "iter=100\n",
    "lrBP=0.1\n",
    "lrNP=0.1\n",
    "pert=0.1\n",
    "mu = 0.7\n",
    "sigma = 0.0000001\n",
    "vDD = 5\n",
    "precision = 14#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "batchGradDescentWithVar(x_train,y_train,iter, 0.1, mu, Sigma[-2], vDD, precision, step, discreteSteps, wRange, onoff=10000, print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
