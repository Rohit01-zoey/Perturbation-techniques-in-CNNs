{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying the effect of variability on BP and WP\n",
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:14:52.736946Z",
     "iopub.status.busy": "2022-09-30T16:14:52.736567Z",
     "iopub.status.idle": "2022-09-30T16:14:54.875744Z",
     "shell.execute_reply": "2022-09-30T16:14:54.874499Z",
     "shell.execute_reply.started": "2022-09-30T16:14:52.736873Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numba import njit, cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:14:54.878872Z",
     "iopub.status.busy": "2022-09-30T16:14:54.878130Z",
     "iopub.status.idle": "2022-09-30T16:16:19.902309Z",
     "shell.execute_reply": "2022-09-30T16:16:19.901480Z",
     "shell.execute_reply.started": "2022-09-30T16:14:54.878872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "#x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "x, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:19.903625Z",
     "iopub.status.busy": "2022-09-30T16:16:19.903330Z",
     "iopub.status.idle": "2022-09-30T16:16:19.907863Z",
     "shell.execute_reply": "2022-09-30T16:16:19.906850Z",
     "shell.execute_reply.started": "2022-09-30T16:16:19.903612Z"
    }
   },
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:19.910904Z",
     "iopub.status.busy": "2022-09-30T16:16:19.910598Z",
     "iopub.status.idle": "2022-09-30T16:16:21.223369Z",
     "shell.execute_reply": "2022-09-30T16:16:21.222313Z",
     "shell.execute_reply.started": "2022-09-30T16:16:19.910877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training software BP with new subsampled version of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_init(newShape, midLayerSize, seed=2):\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  W1 = np.random.rand(midLayerSize,newShape) - 0.5\n",
    "  b1 = np.random.rand(midLayerSize,1) - 0.5\n",
    "  W2 = np.random.rand(10,midLayerSize) - 0.5 \n",
    "  b2 = np.random.rand(10,1) - 0.5 \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  #Z = Z-np.max(Z, axis=0)\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "def crossEntropy(y,y_pre):\n",
    "  loss=-np.sum(np.multiply(y, np.log(y_pre + 10e-16)), axis = 0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2\n",
    "  A2 = softmax(Z2)\n",
    "  \n",
    "\n",
    "  return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "\n",
    "\n",
    "  return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(Z1, A1, Z2, A2, W1, W2, X, y):\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ2 = (A2 - Y)\n",
    "  \n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T)\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1)\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis=1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(X,Y,batchsize,iter, lr, midLayerSize, seed = None, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "  newShape = X[:, 0].shape[0]\n",
    "  W1, b1, W2, b2 = params_init(newShape=newShape, midLayerSize = midLayerSize, seed = seed)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(63000//batchsize): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*batchsize: (j+1)*batchsize].T,Y[j*batchsize: (j+1)*batchsize]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "      #X1, Y1 = X, Y\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "\n",
    "      dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      train_loss.append(np.sum(crossEntropy(one_hot_encoding(Y), A2_train)))\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      val_loss.append(np.sum(crossEntropy(one_hot_encoding(y_val), A2_val)))\n",
    "      print(f'Val accuracy: {val_score} Val loss: {np.sum(crossEntropy(one_hot_encoding(y_val), A2_val))}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "      # plt.figure()\n",
    "      # plt.hist(W1.flatten())\n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 75.01746031746032\n",
      "Val accuracy: 74.35714285714286 Val loss: 4924.942876762841\n",
      "Iteration: 2\n",
      "Train accuracy: 78.63968253968254\n",
      "Val accuracy: 78.2 Val loss: 4264.667712969776\n",
      "Iteration: 3\n",
      "Train accuracy: 80.20793650793651\n",
      "Val accuracy: 79.5 Val loss: 3980.600674369738\n",
      "Iteration: 4\n",
      "Train accuracy: 81.1031746031746\n",
      "Val accuracy: 80.68571428571428 Val loss: 3816.2054320040274\n",
      "Iteration: 5\n",
      "Train accuracy: 81.81428571428572\n",
      "Val accuracy: 81.25714285714287 Val loss: 3685.011076244544\n",
      "Iteration: 6\n",
      "Train accuracy: 82.4063492063492\n",
      "Val accuracy: 81.87142857142857 Val loss: 3574.3633303950473\n",
      "Iteration: 7\n",
      "Train accuracy: 82.86349206349206\n",
      "Val accuracy: 82.32857142857142 Val loss: 3485.447623525683\n",
      "Iteration: 8\n",
      "Train accuracy: 83.21587301587302\n",
      "Val accuracy: 82.58571428571429 Val loss: 3408.9371287544027\n",
      "Iteration: 9\n",
      "Train accuracy: 83.54603174603174\n",
      "Val accuracy: 82.91428571428571 Val loss: 3351.440442210497\n",
      "Iteration: 10\n",
      "Train accuracy: 83.83650793650793\n",
      "Val accuracy: 83.24285714285715 Val loss: 3292.503455717872\n",
      "Iteration: 11\n",
      "Train accuracy: 84.0031746031746\n",
      "Val accuracy: 83.38571428571429 Val loss: 3248.1723302085466\n",
      "Iteration: 12\n",
      "Train accuracy: 84.19682539682539\n",
      "Val accuracy: 83.7 Val loss: 3211.8367599546827\n",
      "Iteration: 13\n",
      "Train accuracy: 84.35714285714285\n",
      "Val accuracy: 83.88571428571429 Val loss: 3180.8267081543563\n",
      "Iteration: 14\n",
      "Train accuracy: 84.47301587301588\n",
      "Val accuracy: 84.02857142857142 Val loss: 3155.8522092702665\n",
      "Iteration: 15\n",
      "Train accuracy: 84.6047619047619\n",
      "Val accuracy: 84.02857142857142 Val loss: 3129.9816627739074\n",
      "Iteration: 16\n",
      "Train accuracy: 84.74920634920635\n",
      "Val accuracy: 84.2 Val loss: 3110.4191413200733\n",
      "Iteration: 17\n",
      "Train accuracy: 84.86031746031746\n",
      "Val accuracy: 84.32857142857144 Val loss: 3095.148725445825\n",
      "Iteration: 18\n",
      "Train accuracy: 84.94761904761904\n",
      "Val accuracy: 84.42857142857143 Val loss: 3082.714000790382\n",
      "Iteration: 19\n",
      "Train accuracy: 85.04285714285714\n",
      "Val accuracy: 84.45714285714286 Val loss: 3067.4395408441123\n",
      "Iteration: 20\n",
      "Train accuracy: 85.17142857142858\n",
      "Val accuracy: 84.54285714285714 Val loss: 3050.7492736893264\n",
      "Iteration: 21\n",
      "Train accuracy: 85.24126984126984\n",
      "Val accuracy: 84.62857142857143 Val loss: 3041.0932812139636\n",
      "Iteration: 22\n",
      "Train accuracy: 85.32857142857144\n",
      "Val accuracy: 84.75714285714285 Val loss: 3027.2938245502514\n",
      "Iteration: 23\n",
      "Train accuracy: 85.35714285714285\n",
      "Val accuracy: 84.94285714285714 Val loss: 3013.6697131949986\n",
      "Iteration: 24\n",
      "Train accuracy: 85.44761904761906\n",
      "Val accuracy: 84.97142857142858 Val loss: 3004.54892114979\n",
      "Iteration: 25\n",
      "Train accuracy: 85.5\n",
      "Val accuracy: 84.92857142857143 Val loss: 2995.071856142115\n",
      "Iteration: 26\n",
      "Train accuracy: 85.56190476190476\n",
      "Val accuracy: 85.1 Val loss: 2982.105078940561\n",
      "Iteration: 27\n",
      "Train accuracy: 85.5920634920635\n",
      "Val accuracy: 85.2 Val loss: 2974.2293656548627\n",
      "Iteration: 28\n",
      "Train accuracy: 85.67936507936508\n",
      "Val accuracy: 85.11428571428571 Val loss: 2970.170321572299\n",
      "Iteration: 29\n",
      "Train accuracy: 85.73492063492063\n",
      "Val accuracy: 85.1 Val loss: 2964.521337017619\n",
      "Iteration: 30\n",
      "Train accuracy: 85.79841269841269\n",
      "Val accuracy: 85.18571428571428 Val loss: 2956.6252718067212\n",
      "Iteration: 31\n",
      "Train accuracy: 85.87142857142858\n",
      "Val accuracy: 85.15714285714286 Val loss: 2950.8765682444864\n",
      "Iteration: 32\n",
      "Train accuracy: 85.93650793650794\n",
      "Val accuracy: 85.21428571428571 Val loss: 2945.8331986416124\n",
      "Iteration: 33\n",
      "Train accuracy: 85.97301587301587\n",
      "Val accuracy: 85.24285714285715 Val loss: 2941.9440621315516\n",
      "Iteration: 34\n",
      "Train accuracy: 86.03015873015873\n",
      "Val accuracy: 85.12857142857143 Val loss: 2940.1804079133676\n",
      "Iteration: 35\n",
      "Train accuracy: 86.07301587301586\n",
      "Val accuracy: 85.17142857142858 Val loss: 2935.8505846942826\n",
      "Iteration: 36\n",
      "Train accuracy: 86.1\n",
      "Val accuracy: 85.24285714285715 Val loss: 2930.2268897026406\n",
      "Iteration: 37\n",
      "Train accuracy: 86.13492063492063\n",
      "Val accuracy: 85.22857142857143 Val loss: 2925.9968428273132\n",
      "Iteration: 38\n",
      "Train accuracy: 86.1952380952381\n",
      "Val accuracy: 85.3142857142857 Val loss: 2920.5393525453737\n",
      "Iteration: 39\n",
      "Train accuracy: 86.25873015873016\n",
      "Val accuracy: 85.38571428571429 Val loss: 2917.1417011341537\n",
      "Iteration: 40\n",
      "Train accuracy: 86.24920634920635\n",
      "Val accuracy: 85.3142857142857 Val loss: 2917.3379810897304\n",
      "Iteration: 41\n",
      "Train accuracy: 86.23968253968253\n",
      "Val accuracy: 85.2 Val loss: 2916.056276792841\n",
      "Iteration: 42\n",
      "Train accuracy: 86.31587301587301\n",
      "Val accuracy: 85.3142857142857 Val loss: 2909.0097115934154\n",
      "Iteration: 43\n",
      "Train accuracy: 86.29206349206349\n",
      "Val accuracy: 85.3 Val loss: 2909.7936528205155\n",
      "Iteration: 44\n",
      "Train accuracy: 86.34444444444445\n",
      "Val accuracy: 85.41428571428571 Val loss: 2902.3162859559457\n",
      "Iteration: 45\n",
      "Train accuracy: 86.32698412698413\n",
      "Val accuracy: 85.34285714285714 Val loss: 2903.956031043571\n",
      "Iteration: 46\n",
      "Train accuracy: 86.34126984126983\n",
      "Val accuracy: 85.3142857142857 Val loss: 2905.31572850079\n",
      "Iteration: 47\n",
      "Train accuracy: 86.39365079365079\n",
      "Val accuracy: 85.32857142857144 Val loss: 2901.4247118377316\n",
      "Iteration: 48\n",
      "Train accuracy: 86.4126984126984\n",
      "Val accuracy: 85.32857142857144 Val loss: 2900.345885061475\n",
      "Iteration: 49\n",
      "Train accuracy: 86.43492063492063\n",
      "Val accuracy: 85.25714285714285 Val loss: 2897.604982395186\n",
      "Iteration: 50\n",
      "Train accuracy: 86.45238095238095\n",
      "Val accuracy: 85.32857142857144 Val loss: 2897.873121320164\n",
      "Iteration: 51\n",
      "Train accuracy: 86.48730158730159\n",
      "Val accuracy: 85.3 Val loss: 2899.0104011016465\n",
      "Iteration: 52\n",
      "Train accuracy: 86.52857142857144\n",
      "Val accuracy: 85.39999999999999 Val loss: 2889.181539748475\n",
      "Iteration: 53\n",
      "Train accuracy: 86.56349206349206\n",
      "Val accuracy: 85.42857142857143 Val loss: 2887.5021529773685\n",
      "Iteration: 54\n",
      "Train accuracy: 86.57142857142858\n",
      "Val accuracy: 85.34285714285714 Val loss: 2883.232408149155\n",
      "Iteration: 55\n",
      "Train accuracy: 86.64126984126985\n",
      "Val accuracy: 85.48571428571428 Val loss: 2879.1385587420878\n",
      "Iteration: 56\n",
      "Train accuracy: 86.6126984126984\n",
      "Val accuracy: 85.47142857142858 Val loss: 2879.9371957892104\n",
      "Iteration: 57\n",
      "Train accuracy: 86.65873015873015\n",
      "Val accuracy: 85.47142857142858 Val loss: 2879.166753846417\n",
      "Iteration: 58\n",
      "Train accuracy: 86.68730158730159\n",
      "Val accuracy: 85.55714285714285 Val loss: 2874.830846330859\n",
      "Iteration: 59\n",
      "Train accuracy: 86.73650793650793\n",
      "Val accuracy: 85.5142857142857 Val loss: 2873.436066596303\n",
      "Iteration: 60\n",
      "Train accuracy: 86.76984126984128\n",
      "Val accuracy: 85.54285714285714 Val loss: 2871.275687499735\n",
      "Iteration: 61\n",
      "Train accuracy: 86.76984126984128\n",
      "Val accuracy: 85.52857142857142 Val loss: 2869.914867828815\n",
      "Iteration: 62\n",
      "Train accuracy: 86.78412698412698\n",
      "Val accuracy: 85.5 Val loss: 2867.9213568404257\n",
      "Iteration: 63\n",
      "Train accuracy: 86.79841269841269\n",
      "Val accuracy: 85.62857142857143 Val loss: 2867.1970642808833\n",
      "Iteration: 64\n",
      "Train accuracy: 86.83333333333333\n",
      "Val accuracy: 85.6 Val loss: 2866.1836931393323\n",
      "Iteration: 65\n",
      "Train accuracy: 86.84444444444445\n",
      "Val accuracy: 85.65714285714286 Val loss: 2865.9807893861616\n",
      "Iteration: 66\n",
      "Train accuracy: 86.83809523809524\n",
      "Val accuracy: 85.58571428571429 Val loss: 2866.62278106253\n",
      "Iteration: 67\n",
      "Train accuracy: 86.85238095238094\n",
      "Val accuracy: 85.61428571428571 Val loss: 2866.435900178423\n",
      "Iteration: 68\n",
      "Train accuracy: 86.87301587301587\n",
      "Val accuracy: 85.67142857142858 Val loss: 2865.169657071835\n",
      "Iteration: 69\n",
      "Train accuracy: 86.87301587301587\n",
      "Val accuracy: 85.67142857142858 Val loss: 2863.9893768656293\n",
      "Iteration: 70\n",
      "Train accuracy: 86.89365079365079\n",
      "Val accuracy: 85.62857142857143 Val loss: 2863.7271752175775\n",
      "Iteration: 71\n",
      "Train accuracy: 86.8984126984127\n",
      "Val accuracy: 85.68571428571428 Val loss: 2863.620598980988\n",
      "Iteration: 72\n",
      "Train accuracy: 86.9031746031746\n",
      "Val accuracy: 85.64285714285714 Val loss: 2864.853667009673\n",
      "Iteration: 73\n",
      "Train accuracy: 86.90793650793651\n",
      "Val accuracy: 85.67142857142858 Val loss: 2864.736444402337\n",
      "Iteration: 74\n",
      "Train accuracy: 86.94444444444444\n",
      "Val accuracy: 85.68571428571428 Val loss: 2865.7986015651823\n",
      "Iteration: 75\n",
      "Train accuracy: 86.94603174603175\n",
      "Val accuracy: 85.74285714285715 Val loss: 2866.8506811077996\n",
      "Iteration: 76\n",
      "Train accuracy: 86.96825396825398\n",
      "Val accuracy: 85.7 Val loss: 2865.8515121261335\n",
      "Iteration: 77\n",
      "Train accuracy: 87.00952380952381\n",
      "Val accuracy: 85.72857142857143 Val loss: 2865.205827182991\n",
      "Iteration: 78\n",
      "Train accuracy: 87.01746031746032\n",
      "Val accuracy: 85.74285714285715 Val loss: 2864.5956567874086\n",
      "Iteration: 79\n",
      "Train accuracy: 87.02857142857144\n",
      "Val accuracy: 85.81428571428572 Val loss: 2865.206444580078\n",
      "Iteration: 80\n",
      "Train accuracy: 87.01746031746032\n",
      "Val accuracy: 85.81428571428572 Val loss: 2868.976582028707\n",
      "Iteration: 81\n",
      "Train accuracy: 87.02222222222223\n",
      "Val accuracy: 85.77142857142857 Val loss: 2872.731575758597\n",
      "Iteration: 82\n",
      "Train accuracy: 87.115873015873\n",
      "Val accuracy: 85.75714285714285 Val loss: 2864.1511248823927\n",
      "Iteration: 83\n",
      "Train accuracy: 87.0984126984127\n",
      "Val accuracy: 85.8 Val loss: 2865.09606297162\n",
      "Iteration: 84\n",
      "Train accuracy: 87.14444444444445\n",
      "Val accuracy: 85.77142857142857 Val loss: 2864.8850208783774\n",
      "Iteration: 85\n",
      "Train accuracy: 87.14126984126985\n",
      "Val accuracy: 85.81428571428572 Val loss: 2865.1520775293693\n",
      "Iteration: 86\n",
      "Train accuracy: 87.17301587301587\n",
      "Val accuracy: 85.78571428571429 Val loss: 2865.31976348973\n",
      "Iteration: 87\n",
      "Train accuracy: 87.17936507936508\n",
      "Val accuracy: 85.8 Val loss: 2865.9448549654676\n",
      "Iteration: 88\n",
      "Train accuracy: 87.18571428571428\n",
      "Val accuracy: 85.87142857142858 Val loss: 2866.077819300136\n",
      "Iteration: 89\n",
      "Train accuracy: 87.23333333333333\n",
      "Val accuracy: 85.8 Val loss: 2865.844099236785\n",
      "Iteration: 90\n",
      "Train accuracy: 87.23492063492063\n",
      "Val accuracy: 85.85714285714286 Val loss: 2866.8251559062046\n",
      "Iteration: 91\n",
      "Train accuracy: 87.23968253968253\n",
      "Val accuracy: 85.7 Val loss: 2863.2939112734475\n",
      "Iteration: 92\n",
      "Train accuracy: 87.25238095238095\n",
      "Val accuracy: 85.72857142857143 Val loss: 2863.489729789464\n",
      "Iteration: 93\n",
      "Train accuracy: 87.27460317460317\n",
      "Val accuracy: 85.7 Val loss: 2865.195151743192\n",
      "Iteration: 94\n",
      "Train accuracy: 87.26984126984128\n",
      "Val accuracy: 85.68571428571428 Val loss: 2864.446115718414\n",
      "Iteration: 95\n",
      "Train accuracy: 87.27142857142857\n",
      "Val accuracy: 85.7 Val loss: 2865.6068960909643\n",
      "Iteration: 96\n",
      "Train accuracy: 87.27142857142857\n",
      "Val accuracy: 85.67142857142858 Val loss: 2870.9080784246107\n",
      "Iteration: 97\n",
      "Train accuracy: 87.25555555555556\n",
      "Val accuracy: 85.72857142857143 Val loss: 2873.0632757770836\n",
      "Iteration: 98\n",
      "Train accuracy: 87.2984126984127\n",
      "Val accuracy: 85.75714285714285 Val loss: 2869.0302146126346\n",
      "Iteration: 99\n",
      "Train accuracy: 87.31428571428572\n",
      "Val accuracy: 85.72857142857143 Val loss: 2869.615688223923\n",
      "Iteration: 100\n",
      "Train accuracy: 87.31269841269841\n",
      "Val accuracy: 85.77142857142857 Val loss: 2870.857724116565\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, train_accBP, val_accBP, train_loss, val_loss, sum_weights = batch_grad_descent(x_train,y_train,batchsize = 100,iter=100, lr=0.1,midLayerSize = 10, seed = 2,print_op=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x294cafeceb0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.318303Z",
     "iopub.status.busy": "2022-09-30T16:16:21.318061Z",
     "iopub.status.idle": "2022-09-30T16:16:21.323827Z",
     "shell.execute_reply": "2022-09-30T16:16:21.322714Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.318279Z"
    }
   },
   "outputs": [],
   "source": [
    "#helps in rounding to the nearest integer multiples of the chosen 'step' value!\n",
    "#add clipping here\n",
    "# def roundArbitrary(weightArray, step):\n",
    "#   weightArrayDiv = weightArray / step\n",
    "#   weightArrayDiv = np.round(weightArrayDiv)\n",
    "#   return weightArrayDiv*step\n",
    "def roundArbitrary(weightArray, step, wRange):#updates function with clipping\n",
    "    #wRange is added for the clipping component \n",
    "\n",
    "    weightArrayDiv = np.clip(weightArray, a_min = -wRange, a_max = wRange)\n",
    "    weightArrayDiv = weightArrayDiv / step\n",
    "    weightArrayDiv = np.round(weightArrayDiv)\n",
    "    return weightArrayDiv*step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.331790Z",
     "iopub.status.busy": "2022-09-30T16:16:21.331462Z",
     "iopub.status.idle": "2022-09-30T16:16:21.339043Z",
     "shell.execute_reply": "2022-09-30T16:16:21.337665Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.331790Z"
    }
   },
   "outputs": [],
   "source": [
    "def initMosParam(shape, mu, sigma, vDD, precision, k=100):\n",
    "    #modelling both Ion and Ioff  = I0*exp(Vgs-Vth/(eta*kB*T)),\n",
    "    dim1, dim2 = shape\n",
    "    sizeI = (dim1, dim2, precision)\n",
    "    I0On = 1e+06\n",
    "    I0Off = I0On/k\n",
    "    #eta = \n",
    "    #kB = 1.3806452e10-23\n",
    "    #T = 300\n",
    "    VT = 0.026*1.5#should be eqaul to eta x kB x T\n",
    "\n",
    "    #Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "    Vth = np.random.normal(loc=mu, scale=sigma, size = sizeI)\n",
    "    Vth[Vth<=0] = 10e-10\n",
    "\n",
    "    #iOn = ((vDD - Vth)**2)*I0On#scaling the current according to Ioff values arbitraryfor now!!\n",
    "    iOn = I0On * np.exp((0 - Vth)/(VT))\n",
    "    #iOn = I0On * np.ones_like(Vth)\n",
    "\n",
    "\n",
    "\n",
    "    #iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "    iOnNominal = np.sum(iOn)/(dim1*dim2*precision)\n",
    "    #Vth = np.random.normal(loc=mu, scale=sigma, size = sizeI)\n",
    "    #iOff = np.random.uniform(low=0, high=1e-8, size = sizeI)#no negative value\n",
    "    iOff = I0Off * np.exp((0 - Vth)/(VT))\n",
    "    #iOff = I0Off * np.ones_like(Vth)\n",
    "    return (iOn, iOnNominal, iOff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.347840Z",
     "iopub.status.busy": "2022-09-30T16:16:21.347478Z",
     "iopub.status.idle": "2022-09-30T16:16:21.355371Z",
     "shell.execute_reply": "2022-09-30T16:16:21.354484Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.347784Z"
    }
   },
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, currents, precision, step, discreteSteps, wRange):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  #clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "  #updating the above to the actual round function.\n",
    "  clippedWeightIndexArray = (roundArbitrary(weightArray, step, wRange)/step).astype(np.int64)\n",
    "  clippedWeightIndexArray = np.abs(clippedWeightIndexArray)\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  \n",
    "  iOn, iOnNominal, iOff = currents\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray +=  np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel)\n",
    "\n",
    "  \n",
    "  analogWeightArray  = np.multiply(np.sign(weightArray), analogWeightArray)\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WPwithVarUp(W1, b1, W2, b2, pert, lossBeforePert, X, y, precision, step, discreteSteps, wRange,  W1Var, b1Var, W2Var, b2Var, W1Currents, b1Currents, W2Currents, b2Currents):\n",
    "    #assert pert==step #to get increments of '1' in the bit pattern of the weight array\n",
    "    #!here we chnage perturb only one value\n",
    "    m = y.shape[0] #m is the number of training examples\n",
    "    Y = one_hot_encoding(y)\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    lossAfterPertW1 = np.zeros_like(W1)\n",
    "    \n",
    "    W1pert = W1.copy()\n",
    "    W1pert += pert\n",
    "    W1pertArrTr = weightTransformWithVariability(W1pert, W1Currents, precision, step, discreteSteps, wRange)\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1pertLoop = W1Var.copy()\n",
    "            W1pertLoop[i, j] = W1pertArrTr[i, j]\n",
    "            #print(W1pertLoop - W1Var)\n",
    "            #plt.figure()\n",
    "            #plt.plot(W1Var.flatten(), W1pertLoop.flatten(), '.')\n",
    "            #plt.title(\"W1 comp\")\n",
    "            #do the transform only on that perturbed weight and place it in the correct place\n",
    "            #W1pertArrTr = weightTransformWithVariability(W1pert, W1Currents, precision, step, discreteSteps, wRange)\n",
    "            _, _, _, A2pert = forward(X, W1pertLoop, b1Var, W2Var, b2Var)\n",
    "            #print(A2pert)\n",
    "            lossAfterPertW1[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "            dW1[i, j] = 1/m * (np.sum(crossEntropy(one_hot_encoding(y), A2pert))-lossBeforePert)/(W1pertArrTr[i,j] - W1Var[i,j]+ 10e-16)\n",
    "    #dW1 = 1/m * (lossAfterPertW1-lossBeforePert)/pert\n",
    "    #print(lossAfterPertW1-lossBeforePert)\n",
    "\n",
    "\n",
    "    db1 = np.zeros_like(b1)\n",
    "    lossAfterPertb1 = np.zeros_like(b1)\n",
    "\n",
    "    b1pert = b1.copy()\n",
    "    b1pert +=pert\n",
    "    b1pertArrTr = weightTransformWithVariability(b1pert, b1Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "    for i in range(b1.shape[0]):\n",
    "        b1pertLoop = b1Var.copy()\n",
    "        b1pertLoop[i]=b1pertArrTr[i]\n",
    "        #b1pertArrTr = weightTransformWithVariability(b1pert, b1Currents, precision, step, discreteSteps, wRange)\n",
    "        _, _, _, A2pert = forward(X, W1Var, b1pertLoop, W2Var, b2Var)\n",
    "        lossAfterPertb1[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "        db1[i] = 1/m * (np.sum(crossEntropy(one_hot_encoding(y), A2pert))-lossBeforePert)/(b1pertArrTr[i] - b1Var[i] + 10e-16)\n",
    "    #db1 = 1/m * (lossAfterPertb1-lossBeforePert)/pert\n",
    "\n",
    "    \n",
    "    dW2 = np.zeros_like(W2)\n",
    "    lossAfterPertW2 = np.zeros_like(W2)\n",
    "\n",
    "    W2pert = W2.copy()\n",
    "    W2pert += pert\n",
    "    W2pertArrTr = weightTransformWithVariability(W2pert, W2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2pertLoop = W2Var.copy()\n",
    "            W2pertLoop[i, j] = W2pertArrTr[i, j]\n",
    "            #W2pertArrTr = weightTransformWithVariability(W2pert, W2Currents, precision, step, discreteSteps, wRange)\n",
    "            _, _, _, A2pert = forward(X, W1Var, b1Var, W2pertLoop, b2Var)\n",
    "            lossAfterPertW2[i, j] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "            dW2[i, j] = 1/m * (np.sum(crossEntropy(one_hot_encoding(y), A2pert))-lossBeforePert)/(W2pertArrTr[i,j] - W2Var[i,j]+ 10e-16)\n",
    "    #dW2 = 1/m * (lossAfterPertW2-lossBeforePert)/pert\n",
    "    #print(lossAfterPertW2)\n",
    "\n",
    "\n",
    "    db2 = np.zeros_like(b2)\n",
    "    lossAfterPertb2 = np.zeros_like(b2)\n",
    "\n",
    "    b2pert = b2.copy()\n",
    "    b2pert += pert\n",
    "    b2pertArrTr = weightTransformWithVariability(b2pert, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "\n",
    "    for i in range(b2.shape[0]):\n",
    "        b2pertLoop = b2Var.copy()\n",
    "        b2pertLoop[i] = b2pertArrTr[i]\n",
    "        #b2pertArrTr = weightTransformWithVariability(b2pert, b2Currents, precision, step, discreteSteps, wRange)\n",
    "        _, _, _, A2pert = forward(X, W1Var, b1Var, W2Var, b2pertLoop)\n",
    "        lossAfterPertb2[i] = np.sum(crossEntropy(one_hot_encoding(y), A2pert))\n",
    "        db2[i] = 1/m * (np.sum(crossEntropy(one_hot_encoding(y), A2pert))-lossBeforePert)/(b2pertArrTr[i] - b2Var[i]+ 10e-16)\n",
    "\n",
    "    #db2 = 1/m * (lossAfterPertb2-lossBeforePert)/pert\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentBPWPComp(X,Y, batchsize,iter, lr, midLayerSize, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff,seed = None, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  newShape = X[:, 0].shape[0]\n",
    "  W1, b1, W2, b2 = params_init(newShape=newShape, midLayerSize = midLayerSize, seed = seed)\n",
    "  W1 = roundArbitrary(W1, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1 = roundArbitrary(b1, step, wRange)\n",
    "  W2 = roundArbitrary(W2, step, wRange)\n",
    "  b2 = roundArbitrary(b2, step, wRange)\n",
    "\n",
    "  #bp\n",
    "  W1bp = W1.copy()\n",
    "  b1bp = b1.copy()\n",
    "  W2bp = W2.copy()\n",
    "  b2bp = b2.copy()\n",
    "\n",
    "  #variability aware updates\n",
    "  W1wp = W1.copy()\n",
    "  b1wp = b1.copy()\n",
    "  W2wp = W2.copy()\n",
    "  b2wp = b2.copy()\n",
    "\n",
    "  disArr = np.array([(-1)*i for i in discreteSteps[::-1]] + discreteSteps)\n",
    "  \n",
    "  W1Currents = initMosParam((midLayerSize, newShape), mu, sigma, vDD, precision, k =onoff)#k = Ion/Ioff\n",
    "  b1Currents = initMosParam((midLayerSize, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W2Currents = initMosParam((10, midLayerSize) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b2Currents = initMosParam((10, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(63000//batchsize): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batchsize = 630\n",
    "      X1, Y1 = shuffle(X[:, j*batchsize: (j+1)*batchsize].T,Y[j*batchsize: (j+1)*batchsize]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "      #X1, Y1 = X, Y\n",
    "\n",
    "\n",
    "      #variability aware updates\n",
    "    \n",
    "      \n",
    "\n",
    "\n",
    "      #back propagation\n",
    "      W1varoc = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc) \n",
    "\n",
    "      dW1bp, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1varoc, W2varoc, X1, Y1)\n",
    "\n",
    "      W1bp, b1bp, W2bp, b2bp = param_update(W1varoc, b1varoc, W2varoc,b2varoc, dW1bp, db1, dW2, db2, lr = lr)\n",
    "\n",
    "      W1bp = roundArbitrary(W1bp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1bp = roundArbitrary(b1bp, step, wRange)\n",
    "      W2bp = roundArbitrary(W2bp, step, wRange)\n",
    "      b2bp = roundArbitrary(b2bp, step, wRange)\n",
    "\n",
    "\n",
    "      #weight perturbation\n",
    "      W1varoc = weightTransformWithVariability(W1wp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1wp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2wp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2wp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc) \n",
    "\n",
    "      #dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1varoc, W2varoc, X1, Y1)\n",
    "      lossBeforePert = np.sum(crossEntropy(one_hot_encoding(Y1), A2))\n",
    "      dW1wp, db1, dW2, db2 = WPwithVarUp(W1wp, b1wp, W2wp, b2wp, step, lossBeforePert, X1, Y1, precision, step, discreteSteps, wRange,  W1varoc, b1varoc, W2varoc, b2varoc, W1Currents, b1Currents, W2Currents, b2Currents)\n",
    "\n",
    "      W1wp, b1wp, W2wp, b2wp = param_update(W1varoc, b1varoc, W2varoc,b2varoc, dW1wp, db1, dW2, db2, lr = lr)\n",
    "\n",
    "      W1wp = roundArbitrary(W1wp, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1wp = roundArbitrary(b1wp, step, wRange)\n",
    "      W2wp = roundArbitrary(W2wp, step, wRange)\n",
    "      b2wp = roundArbitrary(b2wp, step, wRange)\n",
    "\n",
    "\n",
    "      # plt.figure()\n",
    "      # plt.plot(dW1bp.flatten(), dW1wp.flatten(), 'b.', label = \"comparison of weigts\")\n",
    "      # plt.plot(dW1bp.flatten(), dW1bp.flatten(), 'r.', label = \"one-one\")\n",
    "      # plt.figure()\n",
    "      # plt.plot(W1bp.flatten(), W1wp.flatten())\n",
    "      \n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "      \n",
    "\n",
    "      #obtain training loss\n",
    "      W1varoc = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "      _, _, _,  A2_train = forward(X, W1varoc, b1varoc, W2varoc, b2varoc)\n",
    "      _, _, _,   A2_val = forward(x_val, W1varoc, b1varoc, W2varoc, b2varoc)\n",
    "      bpAccTrainAcc = accuracy(predictions(A2_train), Y)\n",
    "      bpTrainLoss = np.sum(crossEntropy(one_hot_encoding(Y), A2_train))\n",
    "      bpAccValAcc = accuracy(predictions(A2_val), y_val)\n",
    "      bpValLoss = np.sum(crossEntropy(one_hot_encoding(y_val), A2_val))\n",
    "\n",
    "\n",
    "\n",
    "      W1varoc = weightTransformWithVariability(W1wp, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1wp, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2wp, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2wp, b2Currents, precision, step, discreteSteps, wRange)\n",
    "      _, _, _,  A2_train = forward(X, W1varoc, b1varoc, W2varoc, b2varoc)\n",
    "      _, _, _,   A2_val = forward(x_val, W1varoc, b1varoc, W2varoc, b2varoc)\n",
    "      wpAccTrainAcc = accuracy(predictions(A2_train), Y)\n",
    "      wpTrainLoss = np.sum(crossEntropy(one_hot_encoding(Y), A2_train))\n",
    "      wpAccValAcc = accuracy(predictions(A2_val), y_val)\n",
    "      wpValLoss = np.sum(crossEntropy(one_hot_encoding(y_val), A2_val))\n",
    "\n",
    "\n",
    "  \n",
    "      train_acc.append([bpAccTrainAcc, wpAccTrainAcc])\n",
    "      print(f'Training :: BP : {bpAccTrainAcc} :: WP : {wpAccTrainAcc}')\n",
    "\n",
    "\n",
    "      val_acc.append([bpAccValAcc, wpAccValAcc])\n",
    "      print(f'Validation Loss :: BP Loss : {bpValLoss} :: WP Loss : {wpValLoss}')\n",
    "      train_loss.append([bpTrainLoss, wpTrainLoss])\n",
    "      val_loss.append([bpValLoss, wpValLoss])\n",
    "\n",
    "      # plt.figure()\n",
    "      # plt.plot(dW1bu.flatten(), dW1bu.flatten(), alpha = 0.5)\n",
    "      # plt.plot(dW1bu.flatten(), actualW1vaUpdate.flatten(), '.')\n",
    "      # plt.xlabel(\"Blind weights\")\n",
    "      # plt.ylabel(\"VA weights\")\n",
    "\n",
    "    \n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentBPQuant(X,Y,batchsize,iter, lr, midLayerSize, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff,seed = None, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  newShape = X[:, 0].shape[0]\n",
    "  W1, b1, W2, b2 = params_init(newShape=newShape, midLayerSize = midLayerSize, seed = seed)\n",
    "  W1 = roundArbitrary(W1, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1 = roundArbitrary(b1, step, wRange)\n",
    "  W2 = roundArbitrary(W2, step, wRange)\n",
    "  b2 = roundArbitrary(b2, step, wRange)\n",
    "\n",
    "\n",
    "  disArr = np.array([(-1)*i for i in discreteSteps[::-1]] + discreteSteps)\n",
    "  \n",
    "  W1Currents = initMosParam((midLayerSize, newShape), mu, sigma, vDD, precision, k =onoff)#k = Ion/Ioff\n",
    "  b1Currents = initMosParam((midLayerSize, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "  W2Currents = initMosParam((10, midLayerSize) ,mu, sigma, vDD, precision, k =onoff)\n",
    "  b2Currents = initMosParam((10, 1), mu, sigma, vDD, precision, k =onoff)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(63000//batchsize): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*batchsize: (j+1)*batchsize].T,Y[j*batchsize: (j+1)*batchsize]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "      #X1, Y1 = X, Y\n",
    "\n",
    "\n",
    "      #variability aware updates\n",
    "    \n",
    "      \n",
    "\n",
    "\n",
    "      #blind updates\n",
    "      # W1varoc = weightTransformWithVariability(W1bu, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      # b1varoc = weightTransformWithVariability(b1bu, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      # W2varoc = weightTransformWithVariability(W2bu, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      # b2varoc = weightTransformWithVariability(b2bu, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "\n",
    "      dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2,b2, dW1, db1, dW2, db2, lr = lr)\n",
    "\n",
    "      W1 = roundArbitrary(W1, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1 = roundArbitrary(b1, step, wRange)\n",
    "      W2 = roundArbitrary(W2, step, wRange)\n",
    "      b2 = roundArbitrary(b2, step, wRange)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      buAccTrainAcc = accuracy(predictions(A2_train), Y)\n",
    "      buTrainLoss = np.sum(crossEntropy(one_hot_encoding(Y), A2_train))\n",
    "      buAccValAcc = accuracy(predictions(A2_val), y_val)\n",
    "      buValLoss = np.sum(crossEntropy(one_hot_encoding(y_val), A2_val))\n",
    "\n",
    "\n",
    "  \n",
    "      train_acc.append(buAccTrainAcc)\n",
    "      print(f'Training :: Blind : {buAccTrainAcc}')\n",
    "\n",
    "\n",
    "      val_acc.append(buAccValAcc)\n",
    "      print(f'Validation  :: Blind : {buAccValAcc} :: Blind Loss : {buValLoss}')\n",
    "\n",
    "      # plt.figure()\n",
    "      # plt.plot(dW1bu.flatten(), dW1bu.flatten(), alpha = 0.5)\n",
    "      # plt.plot(dW1bu.flatten(), actualW1vaUpdate.flatten(), '.')\n",
    "      # plt.xlabel(\"Blind weights\")\n",
    "      # plt.ylabel(\"VA weights\")\n",
    "\n",
    "    \n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 0.0000000001\n",
    "onoff = 10000\n",
    "vDD = 5\n",
    "precision = 12#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "iter = 100\n",
    "midLayerSize = 10 #10\n",
    "lr = 0.1\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: BP : 74.83333333333333 :: WP : 74.85555555555555\n",
      "Validation Loss :: BP Loss : 4960.717559187193 :: WP Loss : 4967.3858903204\n",
      "Iteration: 2\n",
      "Training :: BP : 78.62222222222222 :: WP : 78.54920634920634\n",
      "Validation Loss :: BP Loss : 4284.421561650732 :: WP Loss : 4284.906577065121\n",
      "Iteration: 3\n",
      "Training :: BP : 80.21587301587302 :: WP : 80.28253968253968\n",
      "Validation Loss :: BP Loss : 4000.461252421636 :: WP Loss : 3992.1808584564897\n",
      "Iteration: 4\n",
      "Training :: BP : 81.13650793650794 :: WP : 81.14444444444445\n",
      "Validation Loss :: BP Loss : 3833.9757708472357 :: WP Loss : 3820.3802427394326\n",
      "Iteration: 5\n",
      "Training :: BP : 81.73015873015873 :: WP : 81.7952380952381\n",
      "Validation Loss :: BP Loss : 3716.476436775404 :: WP Loss : 3694.409632374165\n",
      "Iteration: 6\n",
      "Training :: BP : 82.24603174603175 :: WP : 82.33015873015873\n",
      "Validation Loss :: BP Loss : 3618.6172842497745 :: WP Loss : 3597.3979158147863\n",
      "Iteration: 7\n",
      "Training :: BP : 82.70793650793651 :: WP : 82.68095238095238\n",
      "Validation Loss :: BP Loss : 3544.2980453895543 :: WP Loss : 3521.1485360156403\n",
      "Iteration: 8\n",
      "Training :: BP : 83.02698412698413 :: WP : 83.05079365079365\n",
      "Validation Loss :: BP Loss : 3486.5565776933845 :: WP Loss : 3462.4980281819\n",
      "Iteration: 9\n",
      "Training :: BP : 83.28253968253968 :: WP : 83.28412698412698\n",
      "Validation Loss :: BP Loss : 3432.814445789486 :: WP Loss : 3406.7954053406675\n",
      "Iteration: 10\n",
      "Training :: BP : 83.52222222222223 :: WP : 83.58571428571429\n",
      "Validation Loss :: BP Loss : 3383.3898841676223 :: WP Loss : 3359.4891735589545\n",
      "Iteration: 11\n",
      "Training :: BP : 83.71587301587302 :: WP : 83.78253968253968\n",
      "Validation Loss :: BP Loss : 3338.4676259681487 :: WP Loss : 3314.662950974698\n",
      "Iteration: 12\n",
      "Training :: BP : 83.91269841269842 :: WP : 83.92222222222222\n",
      "Validation Loss :: BP Loss : 3294.407784359443 :: WP Loss : 3276.1104211722427\n",
      "Iteration: 13\n",
      "Training :: BP : 84.0904761904762 :: WP : 84.10952380952381\n",
      "Validation Loss :: BP Loss : 3261.7172523502595 :: WP Loss : 3240.1895563970656\n",
      "Iteration: 14\n",
      "Training :: BP : 84.2015873015873 :: WP : 84.24285714285715\n",
      "Validation Loss :: BP Loss : 3233.585419328694 :: WP Loss : 3210.878399937248\n",
      "Iteration: 15\n",
      "Training :: BP : 84.32698412698413 :: WP : 84.3873015873016\n",
      "Validation Loss :: BP Loss : 3213.770036669141 :: WP Loss : 3187.710999102925\n",
      "Iteration: 16\n",
      "Training :: BP : 84.46349206349207 :: WP : 84.51746031746032\n",
      "Validation Loss :: BP Loss : 3190.0429095984173 :: WP Loss : 3160.753735318951\n",
      "Iteration: 17\n",
      "Training :: BP : 84.58888888888889 :: WP : 84.67777777777778\n",
      "Validation Loss :: BP Loss : 3170.6755817460953 :: WP Loss : 3141.4669869656072\n",
      "Iteration: 18\n",
      "Training :: BP : 84.71111111111111 :: WP : 84.73809523809524\n",
      "Validation Loss :: BP Loss : 3147.310286356279 :: WP Loss : 3125.3908284874174\n",
      "Iteration: 19\n",
      "Training :: BP : 84.77619047619046 :: WP : 84.77619047619046\n",
      "Validation Loss :: BP Loss : 3131.311066716521 :: WP Loss : 3110.923237376271\n",
      "Iteration: 20\n",
      "Training :: BP : 84.84285714285714 :: WP : 84.86031746031746\n",
      "Validation Loss :: BP Loss : 3118.6711184009764 :: WP Loss : 3097.5079510170217\n",
      "Iteration: 21\n",
      "Training :: BP : 84.90793650793651 :: WP : 84.93015873015874\n",
      "Validation Loss :: BP Loss : 3109.2582548278397 :: WP Loss : 3083.257148631093\n",
      "Iteration: 22\n",
      "Training :: BP : 84.9984126984127 :: WP : 84.97460317460317\n",
      "Validation Loss :: BP Loss : 3097.788739268005 :: WP Loss : 3071.1593866760645\n",
      "Iteration: 23\n",
      "Training :: BP : 85.06984126984128 :: WP : 85.03015873015873\n",
      "Validation Loss :: BP Loss : 3086.103339062681 :: WP Loss : 3060.657352445571\n",
      "Iteration: 24\n",
      "Training :: BP : 85.15555555555555 :: WP : 85.13015873015874\n",
      "Validation Loss :: BP Loss : 3077.2227951585646 :: WP Loss : 3050.469658098842\n",
      "Iteration: 25\n",
      "Training :: BP : 85.2063492063492 :: WP : 85.21111111111111\n",
      "Validation Loss :: BP Loss : 3071.5045451847564 :: WP Loss : 3040.5078579930628\n",
      "Iteration: 26\n",
      "Training :: BP : 85.28412698412698 :: WP : 85.26349206349207\n",
      "Validation Loss :: BP Loss : 3061.9492598231373 :: WP Loss : 3032.2474408793555\n",
      "Iteration: 27\n",
      "Training :: BP : 85.35079365079366 :: WP : 85.33968253968254\n",
      "Validation Loss :: BP Loss : 3051.9799678525023 :: WP Loss : 3025.1431031342427\n",
      "Iteration: 28\n",
      "Training :: BP : 85.42539682539683 :: WP : 85.4015873015873\n",
      "Validation Loss :: BP Loss : 3044.307622294784 :: WP Loss : 3017.867909494609\n",
      "Iteration: 29\n",
      "Training :: BP : 85.44444444444444 :: WP : 85.43809523809523\n",
      "Validation Loss :: BP Loss : 3040.4545044013807 :: WP Loss : 3016.4048841041604\n",
      "Iteration: 30\n",
      "Training :: BP : 85.54920634920636 :: WP : 85.48412698412699\n",
      "Validation Loss :: BP Loss : 3027.1599897046945 :: WP Loss : 3006.3965111883354\n",
      "Iteration: 31\n",
      "Training :: BP : 85.5920634920635 :: WP : 85.53968253968253\n",
      "Validation Loss :: BP Loss : 3022.671615901201 :: WP Loss : 3000.408911658507\n",
      "Iteration: 32\n",
      "Training :: BP : 85.63174603174603 :: WP : 85.58095238095238\n",
      "Validation Loss :: BP Loss : 3017.2228782313005 :: WP Loss : 2993.516191477096\n"
     ]
    }
   ],
   "source": [
    "trainBPWP = []\n",
    "valBPWP = []\n",
    "trainLossBPWP = []\n",
    "valLossBPWP = []\n",
    "for sigma in []:\n",
    "    W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss = batch_grad_descentBPWPComp(x_train,y_train,batchsize,iter, lr, midLayerSize, mu, sigma, vDD, precision, step, discreteSteps, wRange, onoff,seed = 2,print_op=1)\n",
    "    trainBPWP.append(train_acc)\n",
    "    valBPWP.append(val_acc)\n",
    "    trainLossBPWP.append(train_loss)\n",
    "    valLossBPWP.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
