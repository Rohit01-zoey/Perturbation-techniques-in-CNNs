{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying the effect of asymmetrical variability on back propagation\n",
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:14:52.736946Z",
     "iopub.status.busy": "2022-09-30T16:14:52.736567Z",
     "iopub.status.idle": "2022-09-30T16:14:54.875744Z",
     "shell.execute_reply": "2022-09-30T16:14:54.874499Z",
     "shell.execute_reply.started": "2022-09-30T16:14:52.736873Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numba import njit, cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:14:54.878872Z",
     "iopub.status.busy": "2022-09-30T16:14:54.878130Z",
     "iopub.status.idle": "2022-09-30T16:16:19.902309Z",
     "shell.execute_reply": "2022-09-30T16:16:19.901480Z",
     "shell.execute_reply.started": "2022-09-30T16:14:54.878872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False)\n",
    "#x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:19.903625Z",
     "iopub.status.busy": "2022-09-30T16:16:19.903330Z",
     "iopub.status.idle": "2022-09-30T16:16:19.907863Z",
     "shell.execute_reply": "2022-09-30T16:16:19.906850Z",
     "shell.execute_reply.started": "2022-09-30T16:16:19.903612Z"
    }
   },
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:19.910904Z",
     "iopub.status.busy": "2022-09-30T16:16:19.910598Z",
     "iopub.status.idle": "2022-09-30T16:16:21.223369Z",
     "shell.execute_reply": "2022-09-30T16:16:21.222313Z",
     "shell.execute_reply.started": "2022-09-30T16:16:19.910877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training software BP with new subsampled version of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_init(newShape, midLayerSize, seed=2):\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  W1 = np.random.rand(midLayerSize,newShape) - 0.5\n",
    "  b1 = np.random.rand(midLayerSize,1) - 0.5\n",
    "  W2 = np.random.rand(10,midLayerSize) - 0.5 \n",
    "  b2 = np.random.rand(10,1) - 0.5 \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  #Z = Z-np.max(Z, axis=0)\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "def crossEntropy(y,y_pre):\n",
    "  loss=-np.sum(np.multiply(y, np.log(y_pre + 10e-16)), axis = 0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2\n",
    "  A2 = softmax(Z2)\n",
    "  \n",
    "\n",
    "  return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "\n",
    "\n",
    "  return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(Z1, A1, Z2, A2, W1, W2, X, y):\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ2 = (A2 - Y)\n",
    "  \n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T)\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1)\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis=1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(X,Y,batchsize,iter, lr, midLayerSize, seed = None, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "  newShape = X[:, 0].shape[0]\n",
    "  W1, b1, W2, b2 = params_init(newShape=newShape, midLayerSize = midLayerSize, seed = seed)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(63000//batchsize): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*batchsize: (j+1)*batchsize].T,Y[j*batchsize: (j+1)*batchsize]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "      #X1, Y1 = X, Y\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1, b1, W2, b2) \n",
    "\n",
    "      dW1, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1, W2, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2 = param_update(W1, b1, W2, b2,  dW1, db1, dW2, db2,  lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _,  A2_train = forward(X, W1, b1, W2, b2)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A2_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _,   A2_val = forward(x_val, W1, b1, W2, b2)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A2_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score} Val loss: {np.sum(crossEntropy(one_hot_encoding(y_val), A2_val))}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "      # plt.figure()\n",
    "      # plt.hist(W1.flatten())\n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 65.67142857142856\n",
      "Val accuracy: 64.47142857142858 Val loss: 6653.281599688319\n",
      "Iteration: 2\n",
      "Train accuracy: 69.95396825396826\n",
      "Val accuracy: 69.19999999999999 Val loss: 5723.207730349599\n",
      "Iteration: 3\n",
      "Train accuracy: 71.65079365079366\n",
      "Val accuracy: 71.14285714285714 Val loss: 5342.6829474595215\n",
      "Iteration: 4\n",
      "Train accuracy: 72.63809523809523\n",
      "Val accuracy: 72.17142857142858 Val loss: 5110.70009579257\n",
      "Iteration: 5\n",
      "Train accuracy: 73.4904761904762\n",
      "Val accuracy: 73.12857142857143 Val loss: 4928.86631570027\n",
      "Iteration: 6\n",
      "Train accuracy: 74.28095238095239\n",
      "Val accuracy: 74.02857142857144 Val loss: 4764.495329503857\n",
      "Iteration: 7\n",
      "Train accuracy: 74.98412698412699\n",
      "Val accuracy: 74.7 Val loss: 4633.007240898238\n",
      "Iteration: 8\n",
      "Train accuracy: 75.53650793650793\n",
      "Val accuracy: 75.34285714285714 Val loss: 4518.873969677319\n",
      "Iteration: 9\n",
      "Train accuracy: 76.11904761904762\n",
      "Val accuracy: 76.0 Val loss: 4412.086344467167\n",
      "Iteration: 10\n",
      "Train accuracy: 76.7\n",
      "Val accuracy: 76.55714285714285 Val loss: 4310.537049999838\n",
      "Iteration: 11\n",
      "Train accuracy: 77.1126984126984\n",
      "Val accuracy: 76.87142857142857 Val loss: 4230.032288060676\n",
      "Iteration: 12\n",
      "Train accuracy: 77.53015873015873\n",
      "Val accuracy: 77.4 Val loss: 4154.030700237427\n",
      "Iteration: 13\n",
      "Train accuracy: 77.82222222222222\n",
      "Val accuracy: 77.78571428571428 Val loss: 4091.8385382482898\n",
      "Iteration: 14\n",
      "Train accuracy: 78.15555555555555\n",
      "Val accuracy: 78.12857142857142 Val loss: 4036.3405430874136\n",
      "Iteration: 15\n",
      "Train accuracy: 78.53015873015873\n",
      "Val accuracy: 78.47142857142858 Val loss: 3983.8920863269477\n",
      "Iteration: 16\n",
      "Train accuracy: 78.8063492063492\n",
      "Val accuracy: 78.84285714285714 Val loss: 3933.613848862102\n",
      "Iteration: 17\n",
      "Train accuracy: 79.11904761904762\n",
      "Val accuracy: 79.04285714285714 Val loss: 3882.786781089667\n",
      "Iteration: 18\n",
      "Train accuracy: 79.3984126984127\n",
      "Val accuracy: 79.32857142857142 Val loss: 3837.0580436439895\n",
      "Iteration: 19\n",
      "Train accuracy: 79.62698412698413\n",
      "Val accuracy: 79.57142857142857 Val loss: 3796.6516581844953\n",
      "Iteration: 20\n",
      "Train accuracy: 79.83333333333333\n",
      "Val accuracy: 79.77142857142857 Val loss: 3760.530172896212\n",
      "Iteration: 21\n",
      "Train accuracy: 80.05555555555556\n",
      "Val accuracy: 80.0 Val loss: 3723.678449934502\n",
      "Iteration: 22\n",
      "Train accuracy: 80.25555555555556\n",
      "Val accuracy: 80.15714285714286 Val loss: 3689.1803768703085\n",
      "Iteration: 23\n",
      "Train accuracy: 80.43809523809524\n",
      "Val accuracy: 80.25714285714287 Val loss: 3660.8332639625723\n",
      "Iteration: 24\n",
      "Train accuracy: 80.63174603174603\n",
      "Val accuracy: 80.44285714285714 Val loss: 3631.420879250113\n",
      "Iteration: 25\n",
      "Train accuracy: 80.84920634920636\n",
      "Val accuracy: 80.62857142857143 Val loss: 3601.5456766229972\n",
      "Iteration: 26\n",
      "Train accuracy: 81.0079365079365\n",
      "Val accuracy: 80.82857142857142 Val loss: 3576.359528293735\n",
      "Iteration: 27\n",
      "Train accuracy: 81.17460317460318\n",
      "Val accuracy: 80.94285714285714 Val loss: 3550.0803808164164\n",
      "Iteration: 28\n",
      "Train accuracy: 81.33174603174604\n",
      "Val accuracy: 81.04285714285714 Val loss: 3524.7694647016224\n",
      "Iteration: 29\n",
      "Train accuracy: 81.48571428571428\n",
      "Val accuracy: 81.17142857142858 Val loss: 3501.0235955713697\n",
      "Iteration: 30\n",
      "Train accuracy: 81.62539682539682\n",
      "Val accuracy: 81.27142857142857 Val loss: 3477.7671147903866\n",
      "Iteration: 31\n",
      "Train accuracy: 81.77301587301588\n",
      "Val accuracy: 81.37142857142857 Val loss: 3454.125911702793\n",
      "Iteration: 32\n",
      "Train accuracy: 81.92539682539682\n",
      "Val accuracy: 81.57142857142857 Val loss: 3433.667458655869\n",
      "Iteration: 33\n",
      "Train accuracy: 82.04603174603174\n",
      "Val accuracy: 81.64285714285714 Val loss: 3413.458089480729\n",
      "Iteration: 34\n",
      "Train accuracy: 82.15079365079364\n",
      "Val accuracy: 81.68571428571428 Val loss: 3395.821115857771\n",
      "Iteration: 35\n",
      "Train accuracy: 82.24761904761905\n",
      "Val accuracy: 81.72857142857143 Val loss: 3379.4387269487643\n",
      "Iteration: 36\n",
      "Train accuracy: 82.32539682539682\n",
      "Val accuracy: 81.77142857142857 Val loss: 3363.495388157323\n",
      "Iteration: 37\n",
      "Train accuracy: 82.42539682539683\n",
      "Val accuracy: 81.92857142857143 Val loss: 3349.266390252953\n",
      "Iteration: 38\n",
      "Train accuracy: 82.5079365079365\n",
      "Val accuracy: 82.01428571428572 Val loss: 3336.7242967550774\n",
      "Iteration: 39\n",
      "Train accuracy: 82.57301587301588\n",
      "Val accuracy: 82.14285714285714 Val loss: 3323.7561305715235\n",
      "Iteration: 40\n",
      "Train accuracy: 82.67619047619048\n",
      "Val accuracy: 82.22857142857143 Val loss: 3308.733153398722\n",
      "Iteration: 41\n",
      "Train accuracy: 82.75714285714287\n",
      "Val accuracy: 82.32857142857142 Val loss: 3296.3750241008884\n",
      "Iteration: 42\n",
      "Train accuracy: 82.86984126984127\n",
      "Val accuracy: 82.45714285714286 Val loss: 3283.44280016018\n",
      "Iteration: 43\n",
      "Train accuracy: 82.95238095238095\n",
      "Val accuracy: 82.51428571428572 Val loss: 3270.685419179892\n",
      "Iteration: 44\n",
      "Train accuracy: 83.01428571428572\n",
      "Val accuracy: 82.67142857142858 Val loss: 3258.370129911339\n",
      "Iteration: 45\n",
      "Train accuracy: 83.10634920634921\n",
      "Val accuracy: 82.75714285714287 Val loss: 3246.6822808087427\n",
      "Iteration: 46\n",
      "Train accuracy: 83.18730158730159\n",
      "Val accuracy: 82.8 Val loss: 3237.0928544666413\n",
      "Iteration: 47\n",
      "Train accuracy: 83.24603174603175\n",
      "Val accuracy: 82.85714285714286 Val loss: 3228.08508905245\n",
      "Iteration: 48\n",
      "Train accuracy: 83.31111111111112\n",
      "Val accuracy: 82.87142857142857 Val loss: 3219.443697086729\n",
      "Iteration: 49\n",
      "Train accuracy: 83.36031746031746\n",
      "Val accuracy: 82.97142857142858 Val loss: 3210.8510296988093\n",
      "Iteration: 50\n",
      "Train accuracy: 83.40793650793651\n",
      "Val accuracy: 83.0 Val loss: 3201.686524717009\n",
      "Iteration: 51\n",
      "Train accuracy: 83.44285714285714\n",
      "Val accuracy: 83.05714285714285 Val loss: 3194.5195949054487\n",
      "Iteration: 52\n",
      "Train accuracy: 83.50158730158729\n",
      "Val accuracy: 83.12857142857143 Val loss: 3185.7396786959757\n",
      "Iteration: 53\n",
      "Train accuracy: 83.53492063492064\n",
      "Val accuracy: 83.22857142857143 Val loss: 3178.1674999045536\n",
      "Iteration: 54\n",
      "Train accuracy: 83.6\n",
      "Val accuracy: 83.32857142857144 Val loss: 3170.0541541602433\n",
      "Iteration: 55\n",
      "Train accuracy: 83.67460317460318\n",
      "Val accuracy: 83.35714285714285 Val loss: 3161.952199209464\n",
      "Iteration: 56\n",
      "Train accuracy: 83.72222222222221\n",
      "Val accuracy: 83.44285714285714 Val loss: 3154.9001667361727\n",
      "Iteration: 57\n",
      "Train accuracy: 83.75396825396825\n",
      "Val accuracy: 83.48571428571428 Val loss: 3148.5603649759187\n",
      "Iteration: 58\n",
      "Train accuracy: 83.7968253968254\n",
      "Val accuracy: 83.55714285714285 Val loss: 3140.9130296863664\n",
      "Iteration: 59\n",
      "Train accuracy: 83.84920634920636\n",
      "Val accuracy: 83.58571428571429 Val loss: 3134.669124556377\n",
      "Iteration: 60\n",
      "Train accuracy: 83.88412698412698\n",
      "Val accuracy: 83.62857142857143 Val loss: 3128.18670027221\n",
      "Iteration: 61\n",
      "Train accuracy: 83.94126984126984\n",
      "Val accuracy: 83.65714285714286 Val loss: 3121.04989382021\n",
      "Iteration: 62\n",
      "Train accuracy: 83.98730158730159\n",
      "Val accuracy: 83.72857142857143 Val loss: 3114.486708411774\n",
      "Iteration: 63\n",
      "Train accuracy: 84.02380952380952\n",
      "Val accuracy: 83.82857142857144 Val loss: 3108.187170587327\n",
      "Iteration: 64\n",
      "Train accuracy: 84.07460317460318\n",
      "Val accuracy: 83.88571428571429 Val loss: 3102.4394350833545\n",
      "Iteration: 65\n",
      "Train accuracy: 84.14285714285714\n",
      "Val accuracy: 83.95714285714286 Val loss: 3095.5959335177226\n",
      "Iteration: 66\n",
      "Train accuracy: 84.18253968253968\n",
      "Val accuracy: 83.98571428571428 Val loss: 3090.6218124217876\n",
      "Iteration: 67\n",
      "Train accuracy: 84.21587301587302\n",
      "Val accuracy: 84.04285714285714 Val loss: 3084.394524003501\n",
      "Iteration: 68\n",
      "Train accuracy: 84.25396825396825\n",
      "Val accuracy: 84.08571428571429 Val loss: 3079.5453505553573\n",
      "Iteration: 69\n",
      "Train accuracy: 84.3031746031746\n",
      "Val accuracy: 84.07142857142857 Val loss: 3074.397941848066\n",
      "Iteration: 70\n",
      "Train accuracy: 84.31269841269841\n",
      "Val accuracy: 84.05714285714285 Val loss: 3068.9186829201894\n",
      "Iteration: 71\n",
      "Train accuracy: 84.35079365079365\n",
      "Val accuracy: 84.08571428571429 Val loss: 3063.736499692229\n",
      "Iteration: 72\n",
      "Train accuracy: 84.36666666666667\n",
      "Val accuracy: 84.12857142857143 Val loss: 3058.845623208188\n",
      "Iteration: 73\n",
      "Train accuracy: 84.4063492063492\n",
      "Val accuracy: 84.12857142857143 Val loss: 3054.369819470328\n",
      "Iteration: 74\n",
      "Train accuracy: 84.44285714285714\n",
      "Val accuracy: 84.17142857142858 Val loss: 3049.493994821357\n",
      "Iteration: 75\n",
      "Train accuracy: 84.48253968253968\n",
      "Val accuracy: 84.2 Val loss: 3044.9976774566762\n",
      "Iteration: 76\n",
      "Train accuracy: 84.4984126984127\n",
      "Val accuracy: 84.15714285714286 Val loss: 3040.029443288889\n",
      "Iteration: 77\n",
      "Train accuracy: 84.52063492063492\n",
      "Val accuracy: 84.21428571428572 Val loss: 3035.929610899033\n",
      "Iteration: 78\n",
      "Train accuracy: 84.55714285714285\n",
      "Val accuracy: 84.28571428571429 Val loss: 3031.217114091194\n",
      "Iteration: 79\n",
      "Train accuracy: 84.5873015873016\n",
      "Val accuracy: 84.31428571428572 Val loss: 3026.7854392226627\n",
      "Iteration: 80\n",
      "Train accuracy: 84.60793650793651\n",
      "Val accuracy: 84.38571428571429 Val loss: 3022.6964283394386\n",
      "Iteration: 81\n",
      "Train accuracy: 84.63015873015874\n",
      "Val accuracy: 84.44285714285714 Val loss: 3018.280610581076\n",
      "Iteration: 82\n",
      "Train accuracy: 84.64761904761905\n",
      "Val accuracy: 84.44285714285714 Val loss: 3014.518628712282\n",
      "Iteration: 83\n",
      "Train accuracy: 84.67619047619047\n",
      "Val accuracy: 84.47142857142858 Val loss: 3010.677797522992\n",
      "Iteration: 84\n",
      "Train accuracy: 84.6984126984127\n",
      "Val accuracy: 84.47142857142858 Val loss: 3007.2204694684897\n",
      "Iteration: 85\n",
      "Train accuracy: 84.71587301587302\n",
      "Val accuracy: 84.52857142857142 Val loss: 3002.5067302794396\n",
      "Iteration: 86\n",
      "Train accuracy: 84.76190476190476\n",
      "Val accuracy: 84.54285714285714 Val loss: 2998.9277866887687\n",
      "Iteration: 87\n",
      "Train accuracy: 84.78888888888889\n",
      "Val accuracy: 84.52857142857142 Val loss: 2994.891540903939\n",
      "Iteration: 88\n",
      "Train accuracy: 84.82063492063492\n",
      "Val accuracy: 84.57142857142857 Val loss: 2991.5216881241095\n",
      "Iteration: 89\n",
      "Train accuracy: 84.83492063492064\n",
      "Val accuracy: 84.58571428571429 Val loss: 2988.1730522332323\n",
      "Iteration: 90\n",
      "Train accuracy: 84.86031746031746\n",
      "Val accuracy: 84.64285714285714 Val loss: 2984.458320749562\n",
      "Iteration: 91\n",
      "Train accuracy: 84.8920634920635\n",
      "Val accuracy: 84.65714285714286 Val loss: 2980.872642419508\n",
      "Iteration: 92\n",
      "Train accuracy: 84.89999999999999\n",
      "Val accuracy: 84.68571428571428 Val loss: 2977.317993277332\n",
      "Iteration: 93\n",
      "Train accuracy: 84.91587301587302\n",
      "Val accuracy: 84.7 Val loss: 2974.0383938271266\n",
      "Iteration: 94\n",
      "Train accuracy: 84.92063492063492\n",
      "Val accuracy: 84.7 Val loss: 2970.9223771644038\n",
      "Iteration: 95\n",
      "Train accuracy: 84.92698412698412\n",
      "Val accuracy: 84.72857142857143 Val loss: 2967.9752766723877\n",
      "Iteration: 96\n",
      "Train accuracy: 84.96507936507936\n",
      "Val accuracy: 84.77142857142857 Val loss: 2965.32893597112\n",
      "Iteration: 97\n",
      "Train accuracy: 84.97777777777777\n",
      "Val accuracy: 84.78571428571429 Val loss: 2962.3223318744613\n",
      "Iteration: 98\n",
      "Train accuracy: 85.01746031746032\n",
      "Val accuracy: 84.84285714285714 Val loss: 2959.1938258636724\n",
      "Iteration: 99\n",
      "Train accuracy: 85.01904761904761\n",
      "Val accuracy: 84.84285714285714 Val loss: 2956.415498079619\n",
      "Iteration: 100\n",
      "Train accuracy: 85.02857142857142\n",
      "Val accuracy: 84.87142857142858 Val loss: 2953.738135418447\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, train_accBP, val_accBP, train_loss, val_loss, sum_weights = batch_grad_descent(x_train,y_train,batchsize = 630,iter=100, lr=0.1,midLayerSize = 10, seed = 47,print_op=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic functions related to variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.318303Z",
     "iopub.status.busy": "2022-09-30T16:16:21.318061Z",
     "iopub.status.idle": "2022-09-30T16:16:21.323827Z",
     "shell.execute_reply": "2022-09-30T16:16:21.322714Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.318279Z"
    }
   },
   "outputs": [],
   "source": [
    "#helps in rounding to the nearest integer multiples of the chosen 'step' value!\n",
    "#add clipping here\n",
    "# def roundArbitrary(weightArray, step):\n",
    "#   weightArrayDiv = weightArray / step\n",
    "#   weightArrayDiv = np.round(weightArrayDiv)\n",
    "#   return weightArrayDiv*step\n",
    "def roundArbitrary(weightArray, step, wRange):#updates function with clipping\n",
    "    #wRange is added for the clipping component \n",
    "\n",
    "    weightArrayDiv = np.clip(weightArray, a_min = -wRange, a_max = wRange)\n",
    "    weightArrayDiv = weightArrayDiv / step\n",
    "    weightArrayDiv = np.round(weightArrayDiv)\n",
    "    return weightArrayDiv*step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.331790Z",
     "iopub.status.busy": "2022-09-30T16:16:21.331462Z",
     "iopub.status.idle": "2022-09-30T16:16:21.339043Z",
     "shell.execute_reply": "2022-09-30T16:16:21.337665Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.331790Z"
    }
   },
   "outputs": [],
   "source": [
    "def initMosParam(shape, mu, sigma, vDD, precision, asym, k=10000):\n",
    "    #modelling both Ion and Ioff  = I0*exp(Vgs-Vth/(eta*kB*T)),\n",
    "    dim1, dim2 = shape\n",
    "    sizeI = (dim1, dim2, precision)\n",
    "    I0On = 1e+06\n",
    "    I0Off = I0On/k\n",
    "    #eta = \n",
    "    #kB = 1.3806452e10-23\n",
    "    #T = 300\n",
    "    VT = 0.026*1.5#should be eqaul to eta x kB x T\n",
    "\n",
    "    #Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "    Vth = np.random.normal(loc=mu, scale=sigma, size = sizeI)\n",
    "    Vth[Vth<=0] = 10e-10\n",
    "\n",
    "    #iOn = ((vDD - Vth)**2)*I0On#scaling the current according to Ioff values arbitraryfor now!!\n",
    "    iOn = I0On * np.exp((0 - Vth)/(VT))\n",
    "    #iOn = I0On * np.ones_like(Vth)\n",
    "\n",
    "\n",
    "\n",
    "    #iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "    iOnNominal = np.sum(iOn)/(dim1*dim2*precision)\n",
    "    #Vth = np.random.normal(loc=mu, scale=sigma, size = sizeI)\n",
    "    #iOff = np.random.uniform(low=0, high=1e-8, size = sizeI)#no negative value\n",
    "    iOff = I0Off * np.exp((0 - Vth)/(VT))\n",
    "    #iOff = I0Off * np.ones_like(Vth)\n",
    "\n",
    "\n",
    "    Vth_asym = np.random.normal(loc = Vth, scale = sigma/asym)\n",
    "    Vth_asym[Vth_asym<=0] = 10e-10\n",
    "    iOnAsym = I0On * np.exp((0 - Vth_asym)/(VT))\n",
    "    iOnNominalAsym = np.sum(iOnAsym)/(dim1*dim2*precision)\n",
    "    iOffAsym = I0Off * np.exp((0 - Vth_asym)/(VT))\n",
    "    #return these\n",
    "    return (iOn, iOnNominal, iOff), (iOnAsym, iOnNominalAsym, iOffAsym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T16:16:21.347840Z",
     "iopub.status.busy": "2022-09-30T16:16:21.347478Z",
     "iopub.status.idle": "2022-09-30T16:16:21.355371Z",
     "shell.execute_reply": "2022-09-30T16:16:21.354484Z",
     "shell.execute_reply.started": "2022-09-30T16:16:21.347784Z"
    }
   },
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, currents, precision, step, discreteSteps, wRange):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  #clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "  #updating the above to the actual round function.\n",
    "  clippedWeightIndexArray = (roundArbitrary(weightArray, step, wRange)/step).astype(np.int64)\n",
    "  clippedWeightIndexArray = np.abs(clippedWeightIndexArray)\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  \n",
    "  iOn, iOnNominal, iOff = currents\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray +=  np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel)\n",
    "\n",
    "  \n",
    "  analogWeightArray  = np.multiply(np.sign(weightArray), analogWeightArray)\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent_with_var(X,Y, batchsize,iter, lr, midLayerSize, mu, sigma, vDD, precision, step, discreteSteps, wRange, asym, onoff,seed = None, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  newShape = X[:, 0].shape[0]\n",
    "  W1, b1, W2, b2 = params_init(newShape=newShape, midLayerSize = midLayerSize, seed = seed)\n",
    "  W1 = roundArbitrary(W1, step, wRange)#weights have to maintained as their digitized versions\n",
    "  b1 = roundArbitrary(b1, step, wRange)\n",
    "  W2 = roundArbitrary(W2, step, wRange)\n",
    "  b2 = roundArbitrary(b2, step, wRange)\n",
    "\n",
    "  #blind updates\n",
    "  W1bu = W1.copy()\n",
    "  b1bu = b1.copy()\n",
    "  W2bu = W2.copy()\n",
    "  b2bu = b2.copy()\n",
    "\n",
    "\n",
    "  disArr = np.array([(-1)*i for i in discreteSteps[::-1]] + discreteSteps)\n",
    "  \n",
    "  W1Currents, W1CurrentsAsym = initMosParam((midLayerSize, newShape), mu, sigma, vDD, precision, asym = asym, k =onoff)#k = Ion/Ioff\n",
    "  b1Currents, b1CurrentsAsym = initMosParam((midLayerSize, 1), mu, sigma, vDD, precision, asym = asym,k =onoff)\n",
    "  W2Currents, W2CurrentsAsym = initMosParam((10, midLayerSize) ,mu, sigma, vDD, precision, asym = asym,k =onoff)\n",
    "  b2Currents, b2CurrentsAsym = initMosParam((10, 1), mu, sigma, vDD, precision, asym = asym,k =onoff)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(63000//batchsize): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batchsize = 630\n",
    "      X1, Y1 = shuffle(X[:, j*batchsize: (j+1)*batchsize].T,Y[j*batchsize: (j+1)*batchsize]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "      #X1, Y1 = X, Y\n",
    "\n",
    "\n",
    "      #variability aware updates\n",
    "    \n",
    "      \n",
    "\n",
    "\n",
    "      #blind updates\n",
    "      W1varoc = weightTransformWithVariability(W1bu, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1bu, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2bu, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2bu, b2Currents, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      W1varocAsym = weightTransformWithVariability(W1bu, W1CurrentsAsym, precision, step, discreteSteps, wRange)\n",
    "      b1varocAsym = weightTransformWithVariability(b1bu, b1CurrentsAsym, precision, step, discreteSteps, wRange)\n",
    "      W2varocAsym = weightTransformWithVariability(W2bu, W2CurrentsAsym, precision, step, discreteSteps, wRange)\n",
    "      b2varocAsym = weightTransformWithVariability(b2bu, b2CurrentsAsym, precision, step, discreteSteps, wRange)\n",
    "\n",
    "      Z1, A1, Z2, A2 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc) \n",
    "\n",
    "      dW1bu, db1, dW2, db2 = backprop(Z1, A1, Z2, A2, W1varocAsym, W2varocAsym, X1, Y1)\n",
    "\n",
    "      W1bu, b1bu, W2bu, b2bu = param_update(W1varoc, b1varoc, W2varoc,b2varoc, dW1bu, db1, dW2, db2, lr = lr)\n",
    "\n",
    "      W1bu = roundArbitrary(W1bu, step, wRange)#weights have to maintained as their digitized versions\n",
    "      b1bu = roundArbitrary(b1bu, step, wRange)\n",
    "      W2bu = roundArbitrary(W2bu, step, wRange)\n",
    "      b2bu = roundArbitrary(b2bu, step, wRange)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      W1varoc = weightTransformWithVariability(W1bu, W1Currents, precision, step, discreteSteps, wRange)\n",
    "      b1varoc = weightTransformWithVariability(b1bu, b1Currents, precision, step, discreteSteps, wRange)\n",
    "      W2varoc = weightTransformWithVariability(W2bu, W2Currents, precision, step, discreteSteps, wRange)\n",
    "      b2varoc = weightTransformWithVariability(b2bu, b2Currents, precision, step, discreteSteps, wRange)\n",
    "      _, _, _,  A2_train = forward(X, W1varoc, b1varoc, W2varoc, b2varoc)\n",
    "      _, _, _,   A2_val = forward(x_val, W1varoc, b1varoc, W2varoc, b2varoc)\n",
    "      buAccTrainAcc = accuracy(predictions(A2_train), Y)\n",
    "      buTrainLoss = np.sum(crossEntropy(one_hot_encoding(Y), A2_train))\n",
    "      buAccValAcc = accuracy(predictions(A2_val), y_val)\n",
    "      buValLoss = np.sum(crossEntropy(one_hot_encoding(y_val), A2_val))\n",
    "\n",
    "\n",
    "  \n",
    "      train_acc.append(buAccTrainAcc)\n",
    "      train_loss.append(buTrainLoss)\n",
    "      print(f'Training :: Blind : {buAccTrainAcc}')\n",
    "\n",
    "\n",
    "      val_acc.append(buAccValAcc)\n",
    "      val_loss.append(buValLoss)\n",
    "      print(f'Validation  :: Blind : {buAccValAcc} :: Blind Loss : {buValLoss}')\n",
    "\n",
    "      # plt.figure()\n",
    "      # plt.plot(dW1bu.flatten(), dW1bu.flatten(), alpha = 0.5)\n",
    "      # plt.plot(dW1bu.flatten(), actualW1vaUpdate.flatten(), '.')\n",
    "      # plt.xlabel(\"Blind weights\")\n",
    "      # plt.ylabel(\"VA weights\")\n",
    "\n",
    "    \n",
    "\n",
    "  return W1, b1, W2, b2, train_acc, val_acc, train_loss, val_loss, sum_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 0.001\n",
    "onoff = 10000\n",
    "asym = 1000000000000\n",
    "vDD = 5\n",
    "precision = 12#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "iter = 100\n",
    "midLayerSize = 10 #10\n",
    "lr = 0.1 #0.1\n",
    "batchsize = 630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.98571428571428\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 9025.102135733043\n",
      "Iteration: 2\n",
      "Training :: Blind : 58.12539682539683\n",
      "Validation  :: Blind : 58.4 :: Blind Loss : 8142.630078813778\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.56031746031746\n",
      "Validation  :: Blind : 59.18571428571428 :: Blind Loss : 7831.58427211157\n",
      "Iteration: 4\n",
      "Training :: Blind : 58.82222222222222\n",
      "Validation  :: Blind : 59.357142857142854 :: Blind Loss : 7754.658234508883\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.4936507936508\n",
      "Validation  :: Blind : 59.17142857142858 :: Blind Loss : 7777.486279363467\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.8952380952381\n",
      "Validation  :: Blind : 59.64285714285714 :: Blind Loss : 7740.1335879596545\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.73968253968253\n",
      "Validation  :: Blind : 59.42857142857143 :: Blind Loss : 7742.1563556262845\n",
      "Iteration: 8\n",
      "Training :: Blind : 59.112698412698414\n",
      "Validation  :: Blind : 59.5 :: Blind Loss : 7701.575842239116\n",
      "Iteration: 9\n",
      "Training :: Blind : 58.815873015873024\n",
      "Validation  :: Blind : 59.371428571428574 :: Blind Loss : 7698.725462377862\n",
      "Iteration: 10\n",
      "Training :: Blind : 59.13174603174603\n",
      "Validation  :: Blind : 59.42857142857143 :: Blind Loss : 7687.899868678802\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.72380952380952\n",
      "Validation  :: Blind : 60.02857142857143 :: Blind Loss : 7646.064049240345\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.871428571428574\n",
      "Validation  :: Blind : 60.08571428571429 :: Blind Loss : 7678.542669691679\n",
      "Iteration: 13\n",
      "Training :: Blind : 60.37936507936508\n",
      "Validation  :: Blind : 60.71428571428571 :: Blind Loss : 7579.532236771948\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.66825396825397\n",
      "Validation  :: Blind : 61.21428571428571 :: Blind Loss : 7519.166533543506\n",
      "Iteration: 15\n",
      "Training :: Blind : 60.868253968253974\n",
      "Validation  :: Blind : 61.0 :: Blind Loss : 7455.52926221859\n",
      "Iteration: 16\n",
      "Training :: Blind : 61.03968253968254\n",
      "Validation  :: Blind : 61.12857142857143 :: Blind Loss : 7432.137436408351\n",
      "Iteration: 17\n",
      "Training :: Blind : 60.91428571428571\n",
      "Validation  :: Blind : 60.94285714285714 :: Blind Loss : 7484.57512531326\n",
      "Iteration: 18\n",
      "Training :: Blind : 60.86349206349206\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7585.926283196618\n",
      "Iteration: 19\n",
      "Training :: Blind : 61.16984126984127\n",
      "Validation  :: Blind : 61.142857142857146 :: Blind Loss : 7565.621785830025\n",
      "Iteration: 20\n",
      "Training :: Blind : 61.24285714285714\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7544.974302337643\n",
      "Iteration: 21\n",
      "Training :: Blind : 61.469841269841275\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7503.898197915656\n",
      "Iteration: 22\n",
      "Training :: Blind : 61.50952380952381\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7503.445105740411\n",
      "Iteration: 23\n",
      "Training :: Blind : 61.43809523809524\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7515.917307743723\n",
      "Iteration: 24\n",
      "Training :: Blind : 61.457142857142856\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7513.099205693299\n",
      "Iteration: 25\n",
      "Training :: Blind : 61.577777777777776\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7493.3354311518415\n",
      "Iteration: 26\n",
      "Training :: Blind : 61.45555555555555\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7509.60448261606\n",
      "Iteration: 27\n",
      "Training :: Blind : 61.49206349206349\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7503.046560859191\n",
      "Iteration: 28\n",
      "Training :: Blind : 61.36507936507937\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7512.4508013471695\n",
      "Iteration: 29\n",
      "Training :: Blind : 61.22539682539683\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7523.998169696335\n",
      "Iteration: 30\n",
      "Training :: Blind : 61.319047619047616\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7515.491185792584\n",
      "Iteration: 31\n",
      "Training :: Blind : 61.27936507936508\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7524.098912098139\n",
      "Iteration: 32\n",
      "Training :: Blind : 61.1920634920635\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7537.819458028349\n",
      "Iteration: 33\n",
      "Training :: Blind : 61.3\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7523.550375796256\n",
      "Iteration: 34\n",
      "Training :: Blind : 61.31111111111112\n",
      "Validation  :: Blind : 61.32857142857143 :: Blind Loss : 7526.1313760927715\n",
      "Iteration: 35\n",
      "Training :: Blind : 61.24285714285714\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7527.300362184226\n",
      "Iteration: 36\n",
      "Training :: Blind : 61.26190476190476\n",
      "Validation  :: Blind : 61.21428571428571 :: Blind Loss : 7526.322623244967\n",
      "Iteration: 37\n",
      "Training :: Blind : 61.20476190476191\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7528.572313509912\n",
      "Iteration: 38\n",
      "Training :: Blind : 61.28095238095238\n",
      "Validation  :: Blind : 61.15714285714285 :: Blind Loss : 7527.307668939486\n",
      "Iteration: 39\n",
      "Training :: Blind : 61.20634920634921\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7532.665248438784\n",
      "Iteration: 40\n",
      "Training :: Blind : 61.107936507936515\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7542.072362588596\n",
      "Iteration: 41\n",
      "Training :: Blind : 61.142857142857146\n",
      "Validation  :: Blind : 61.1 :: Blind Loss : 7540.509682651782\n",
      "Iteration: 42\n",
      "Training :: Blind : 61.08253968253968\n",
      "Validation  :: Blind : 61.08571428571429 :: Blind Loss : 7538.859915977297\n",
      "Iteration: 43\n",
      "Training :: Blind : 61.21269841269841\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7530.11557273582\n",
      "Iteration: 44\n",
      "Training :: Blind : 61.27936507936508\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7524.942895554483\n",
      "Iteration: 45\n",
      "Training :: Blind : 61.32222222222222\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7522.58407274028\n",
      "Iteration: 46\n",
      "Training :: Blind : 61.247619047619054\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7535.216565317333\n",
      "Iteration: 47\n",
      "Training :: Blind : 61.24603174603175\n",
      "Validation  :: Blind : 61.08571428571429 :: Blind Loss : 7530.990653374525\n",
      "Iteration: 48\n",
      "Training :: Blind : 61.266666666666666\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7526.950763252318\n",
      "Iteration: 49\n",
      "Training :: Blind : 61.20476190476191\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7535.209149558319\n",
      "Iteration: 50\n",
      "Training :: Blind : 61.22539682539683\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7518.269681327099\n",
      "Iteration: 51\n",
      "Training :: Blind : 61.217460317460315\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7534.1287744184065\n",
      "Iteration: 52\n",
      "Training :: Blind : 61.17460317460317\n",
      "Validation  :: Blind : 61.1 :: Blind Loss : 7532.692359833842\n",
      "Iteration: 53\n",
      "Training :: Blind : 61.25873015873016\n",
      "Validation  :: Blind : 61.142857142857146 :: Blind Loss : 7522.893345135552\n",
      "Iteration: 54\n",
      "Training :: Blind : 61.196825396825396\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7532.772090975486\n",
      "Iteration: 55\n",
      "Training :: Blind : 61.144444444444446\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7538.875412397678\n",
      "Iteration: 56\n",
      "Training :: Blind : 61.265079365079366\n",
      "Validation  :: Blind : 61.15714285714285 :: Blind Loss : 7535.499720952537\n",
      "Iteration: 57\n",
      "Training :: Blind : 61.273015873015865\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7527.075584590221\n",
      "Iteration: 58\n",
      "Training :: Blind : 61.163492063492065\n",
      "Validation  :: Blind : 61.12857142857143 :: Blind Loss : 7541.952423788307\n",
      "Iteration: 59\n",
      "Training :: Blind : 61.12698412698413\n",
      "Validation  :: Blind : 61.0 :: Blind Loss : 7538.3467587619025\n",
      "Iteration: 60\n",
      "Training :: Blind : 61.28730158730159\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7517.736481654199\n",
      "Iteration: 61\n",
      "Training :: Blind : 61.18888888888889\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7533.794927433455\n",
      "Iteration: 62\n",
      "Training :: Blind : 61.25873015873016\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7529.476377615179\n",
      "Iteration: 63\n",
      "Training :: Blind : 61.24444444444445\n",
      "Validation  :: Blind : 61.25714285714285 :: Blind Loss : 7524.231852021089\n",
      "Iteration: 64\n",
      "Training :: Blind : 61.2015873015873\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7532.555140458595\n",
      "Iteration: 65\n",
      "Training :: Blind : 61.196825396825396\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7527.3139972903155\n",
      "Iteration: 66\n",
      "Training :: Blind : 61.28095238095238\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7519.245172317492\n",
      "Iteration: 67\n",
      "Training :: Blind : 61.17301587301587\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7526.913867074406\n",
      "Iteration: 68\n",
      "Training :: Blind : 61.249206349206354\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7518.919579965388\n",
      "Iteration: 69\n",
      "Training :: Blind : 61.199999999999996\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7527.148034258185\n",
      "Iteration: 70\n",
      "Training :: Blind : 61.14126984126984\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7535.402764054379\n",
      "Iteration: 71\n",
      "Training :: Blind : 61.27142857142858\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7529.1954312274065\n",
      "Iteration: 72\n",
      "Training :: Blind : 61.215873015873015\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7535.562979592735\n",
      "Iteration: 73\n",
      "Training :: Blind : 61.14761904761905\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7531.276306133783\n",
      "Iteration: 74\n",
      "Training :: Blind : 61.16031746031746\n",
      "Validation  :: Blind : 61.25714285714285 :: Blind Loss : 7520.038723042813\n",
      "Iteration: 75\n",
      "Training :: Blind : 61.196825396825396\n",
      "Validation  :: Blind : 61.31428571428571 :: Blind Loss : 7526.990777406024\n",
      "Iteration: 76\n",
      "Training :: Blind : 61.18412698412698\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7528.629659353761\n",
      "Iteration: 77\n",
      "Training :: Blind : 61.11746031746031\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7533.808298097967\n",
      "Iteration: 78\n",
      "Training :: Blind : 61.17142857142858\n",
      "Validation  :: Blind : 61.15714285714285 :: Blind Loss : 7530.33464005531\n",
      "Iteration: 79\n",
      "Training :: Blind : 61.13650793650793\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7529.919295298682\n",
      "Iteration: 80\n",
      "Training :: Blind : 61.092063492063495\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7529.944412957971\n",
      "Iteration: 81\n",
      "Training :: Blind : 61.08730158730159\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7526.971222144968\n",
      "Iteration: 82\n",
      "Training :: Blind : 61.2015873015873\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7531.029291465526\n",
      "Iteration: 83\n",
      "Training :: Blind : 61.13650793650793\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7527.02059557619\n",
      "Iteration: 84\n",
      "Training :: Blind : 61.10634920634921\n",
      "Validation  :: Blind : 61.042857142857144 :: Blind Loss : 7530.522796424381\n",
      "Iteration: 85\n",
      "Training :: Blind : 61.17619047619047\n",
      "Validation  :: Blind : 61.31428571428571 :: Blind Loss : 7524.0664375556435\n",
      "Iteration: 86\n",
      "Training :: Blind : 61.12222222222222\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7528.077405580818\n",
      "Iteration: 87\n",
      "Training :: Blind : 61.161904761904765\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7523.814197753754\n",
      "Iteration: 88\n",
      "Training :: Blind : 61.03809523809524\n",
      "Validation  :: Blind : 61.042857142857144 :: Blind Loss : 7537.357331938852\n",
      "Iteration: 89\n",
      "Training :: Blind : 61.16666666666667\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7529.578649961279\n",
      "Iteration: 90\n",
      "Training :: Blind : 61.115873015873014\n",
      "Validation  :: Blind : 61.31428571428571 :: Blind Loss : 7526.495536127305\n",
      "Iteration: 91\n",
      "Training :: Blind : 61.13015873015873\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7533.294281187056\n",
      "Iteration: 92\n",
      "Training :: Blind : 61.107936507936515\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7533.141197960105\n",
      "Iteration: 93\n",
      "Training :: Blind : 61.12539682539683\n",
      "Validation  :: Blind : 61.11428571428571 :: Blind Loss : 7531.674920176976\n",
      "Iteration: 94\n",
      "Training :: Blind : 61.18571428571429\n",
      "Validation  :: Blind : 61.25714285714285 :: Blind Loss : 7522.656318425731\n",
      "Iteration: 95\n",
      "Training :: Blind : 61.247619047619054\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7521.594078955395\n",
      "Iteration: 96\n",
      "Training :: Blind : 61.09523809523809\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7530.968759019878\n",
      "Iteration: 97\n",
      "Training :: Blind : 61.15873015873016\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7524.123727170072\n",
      "Iteration: 98\n",
      "Training :: Blind : 61.11746031746031\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7525.420533292947\n",
      "Iteration: 99\n",
      "Training :: Blind : 61.15873015873016\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7535.463978145426\n",
      "Iteration: 100\n",
      "Training :: Blind : 61.11428571428571\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7528.921765939598\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, train_acc_quant, val_acc_quant, train_loss, val_loss, sum_weights = batch_grad_descent_with_var(x_train,y_train,batchsize,iter, lr, midLayerSize, mu, sigma, vDD, precision, step, discreteSteps, wRange, asym, onoff,seed = 47,print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x227862b9450>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accBP, 'r', label = \"Back prop software\")\n",
    "plt.plot(train_acc_quant, 'b', label = \"variablity added\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different assymetries(12bits - 0.001var-10000onoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 0.001\n",
    "onoff = 10000\n",
    "asym = 100000\n",
    "vDD = 5\n",
    "precision = 12#setting the precision value of the calculations\n",
    "wRange = 1\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "#step = round(wRange/noOfLevels, precision)\n",
    "step =  wRange/noOfLevels#step size of each of the step after quantization\n",
    "#discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "discreteSteps = [step*i for i in range(0, noOfLevels)] #storing the values of the steps\n",
    "iter = 100\n",
    "midLayerSize = 10 #10\n",
    "lr = 0.1 #0.1\n",
    "batchsize = 630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.99047619047619\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 9035.551094042095\n",
      "Iteration: 2\n",
      "Training :: Blind : 57.960317460317455\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8157.605421035234\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.63809523809523\n",
      "Validation  :: Blind : 59.199999999999996 :: Blind Loss : 7829.209443023833\n",
      "Iteration: 4\n",
      "Training :: Blind : 58.56349206349206\n",
      "Validation  :: Blind : 59.0 :: Blind Loss : 7789.403957224436\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.74444444444444\n",
      "Validation  :: Blind : 59.357142857142854 :: Blind Loss : 7752.434348194175\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.939682539682536\n",
      "Validation  :: Blind : 59.55714285714285 :: Blind Loss : 7719.630647519686\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.62380952380952\n",
      "Validation  :: Blind : 59.457142857142856 :: Blind Loss : 7758.696337531579\n",
      "Iteration: 8\n",
      "Training :: Blind : 59.13333333333334\n",
      "Validation  :: Blind : 59.72857142857143 :: Blind Loss : 7701.0445209571735\n",
      "Iteration: 9\n",
      "Training :: Blind : 59.044444444444444\n",
      "Validation  :: Blind : 59.48571428571429 :: Blind Loss : 7713.685217228844\n",
      "Iteration: 10\n",
      "Training :: Blind : 59.46507936507936\n",
      "Validation  :: Blind : 59.94285714285714 :: Blind Loss : 7675.459583353153\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.42063492063492\n",
      "Validation  :: Blind : 59.77142857142857 :: Blind Loss : 7675.17926727691\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.56031746031746\n",
      "Validation  :: Blind : 59.75714285714285 :: Blind Loss : 7656.484431004321\n",
      "Iteration: 13\n",
      "Training :: Blind : 59.977777777777774\n",
      "Validation  :: Blind : 60.25714285714285 :: Blind Loss : 7601.884925393921\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.05873015873016\n",
      "Validation  :: Blind : 60.21428571428571 :: Blind Loss : 7580.58857855168\n",
      "Iteration: 15\n",
      "Training :: Blind : 59.958730158730155\n",
      "Validation  :: Blind : 60.14285714285714 :: Blind Loss : 7578.867950484315\n",
      "Iteration: 16\n",
      "Training :: Blind : 60.34285714285714\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7553.113601503831\n",
      "Iteration: 17\n",
      "Training :: Blind : 60.04126984126984\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7555.556654127311\n",
      "Iteration: 18\n",
      "Training :: Blind : 60.17142857142858\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7534.632312705612\n",
      "Iteration: 19\n",
      "Training :: Blind : 60.21428571428571\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7532.872709385536\n",
      "Iteration: 20\n",
      "Training :: Blind : 60.23333333333334\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7540.5006160054\n",
      "Iteration: 21\n",
      "Training :: Blind : 60.27619047619047\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7527.864251797895\n",
      "Iteration: 22\n",
      "Training :: Blind : 60.22063492063492\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7539.787290755288\n",
      "Iteration: 23\n",
      "Training :: Blind : 60.20793650793651\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7546.408461356508\n",
      "Iteration: 24\n",
      "Training :: Blind : 60.24761904761905\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7535.284277893978\n",
      "Iteration: 25\n",
      "Training :: Blind : 60.2031746031746\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7538.824584617015\n",
      "Iteration: 26\n",
      "Training :: Blind : 60.20793650793651\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7542.373349948817\n",
      "Iteration: 27\n",
      "Training :: Blind : 60.06984126984128\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7554.372192106797\n",
      "Iteration: 28\n",
      "Training :: Blind : 60.17142857142858\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7547.252326965336\n",
      "Iteration: 29\n",
      "Training :: Blind : 60.24285714285714\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7539.421847736561\n",
      "Iteration: 30\n",
      "Training :: Blind : 60.266666666666666\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7529.856231189661\n",
      "Iteration: 31\n",
      "Training :: Blind : 60.144444444444446\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7540.511583243595\n",
      "Iteration: 32\n",
      "Training :: Blind : 60.2031746031746\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7539.755268751102\n",
      "Iteration: 33\n",
      "Training :: Blind : 60.25714285714285\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7535.089430452156\n",
      "Iteration: 34\n",
      "Training :: Blind : 60.25714285714285\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7547.801853487224\n",
      "Iteration: 35\n",
      "Training :: Blind : 60.36349206349206\n",
      "Validation  :: Blind : 60.75714285714285 :: Blind Loss : 7531.153460638407\n",
      "Iteration: 36\n",
      "Training :: Blind : 60.25714285714285\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7532.800670177017\n",
      "Iteration: 37\n",
      "Training :: Blind : 60.2047619047619\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7529.1752355854205\n",
      "Iteration: 38\n",
      "Training :: Blind : 60.38888888888889\n",
      "Validation  :: Blind : 60.77142857142858 :: Blind Loss : 7522.034799597168\n",
      "Iteration: 39\n",
      "Training :: Blind : 60.25873015873016\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7553.562325568268\n",
      "Iteration: 40\n",
      "Training :: Blind : 60.339682539682535\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7532.923748155035\n",
      "Iteration: 41\n",
      "Training :: Blind : 60.25079365079365\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7542.282493547662\n",
      "Iteration: 42\n",
      "Training :: Blind : 60.23492063492063\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7533.97755051575\n",
      "Iteration: 43\n",
      "Training :: Blind : 60.22222222222222\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7531.743963844069\n",
      "Iteration: 44\n",
      "Training :: Blind : 60.18730158730159\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7534.509030605439\n",
      "Iteration: 45\n",
      "Training :: Blind : 60.26031746031746\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7531.128682851306\n",
      "Iteration: 46\n",
      "Training :: Blind : 60.03015873015873\n",
      "Validation  :: Blind : 60.24285714285714 :: Blind Loss : 7545.011722694273\n",
      "Iteration: 47\n",
      "Training :: Blind : 60.01111111111111\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7558.55860756802\n",
      "Iteration: 48\n",
      "Training :: Blind : 60.25873015873016\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7533.57012883407\n",
      "Iteration: 49\n",
      "Training :: Blind : 60.333333333333336\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7527.317081763346\n",
      "Iteration: 50\n",
      "Training :: Blind : 60.350793650793655\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7519.775506427102\n",
      "Iteration: 51\n",
      "Training :: Blind : 60.28730158730159\n",
      "Validation  :: Blind : 60.27142857142857 :: Blind Loss : 7535.3723309433135\n",
      "Iteration: 52\n",
      "Training :: Blind : 60.20634920634921\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7544.486866553725\n",
      "Iteration: 53\n",
      "Training :: Blind : 60.26190476190476\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7541.623173142007\n",
      "Iteration: 54\n",
      "Training :: Blind : 60.25079365079365\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7537.961578902836\n",
      "Iteration: 55\n",
      "Training :: Blind : 60.15238095238096\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7553.96022061443\n",
      "Iteration: 56\n",
      "Training :: Blind : 60.369841269841274\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7528.951151139271\n",
      "Iteration: 57\n",
      "Training :: Blind : 60.406349206349205\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7528.965873481226\n",
      "Iteration: 58\n",
      "Training :: Blind : 60.39523809523809\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7522.598870658314\n",
      "Iteration: 59\n",
      "Training :: Blind : 60.32698412698413\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7530.317403257333\n",
      "Iteration: 60\n",
      "Training :: Blind : 60.03492063492063\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7555.573314039644\n",
      "Iteration: 61\n",
      "Training :: Blind : 60.231746031746034\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7532.18336986631\n",
      "Iteration: 62\n",
      "Training :: Blind : 60.15396825396825\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7540.2554613370885\n",
      "Iteration: 63\n",
      "Training :: Blind : 60.22222222222222\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7540.952905078215\n",
      "Iteration: 64\n",
      "Training :: Blind : 60.30952380952381\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7527.976068035752\n",
      "Iteration: 65\n",
      "Training :: Blind : 60.38888888888889\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7525.883037578085\n",
      "Iteration: 66\n",
      "Training :: Blind : 60.37619047619047\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7524.8466275867395\n",
      "Iteration: 67\n",
      "Training :: Blind : 60.29523809523809\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7533.1303879815605\n",
      "Iteration: 68\n",
      "Training :: Blind : 60.37777777777777\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7537.727097133244\n",
      "Iteration: 69\n",
      "Training :: Blind : 60.1920634920635\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7530.403940923243\n",
      "Iteration: 70\n",
      "Training :: Blind : 60.349206349206355\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7524.704605087851\n",
      "Iteration: 71\n",
      "Training :: Blind : 60.20793650793651\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7545.362479295914\n",
      "Iteration: 72\n",
      "Training :: Blind : 60.26984126984127\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7529.912412288629\n",
      "Iteration: 73\n",
      "Training :: Blind : 60.212698412698415\n",
      "Validation  :: Blind : 60.285714285714285 :: Blind Loss : 7530.633811235422\n",
      "Iteration: 74\n",
      "Training :: Blind : 60.525396825396825\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7512.3400098703205\n",
      "Iteration: 75\n",
      "Training :: Blind : 60.1015873015873\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7540.991767572554\n",
      "Iteration: 76\n",
      "Training :: Blind : 60.27619047619047\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7529.574388412225\n",
      "Iteration: 77\n",
      "Training :: Blind : 60.03174603174604\n",
      "Validation  :: Blind : 60.199999999999996 :: Blind Loss : 7550.975386575932\n",
      "Iteration: 78\n",
      "Training :: Blind : 60.28888888888889\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7523.351783458027\n",
      "Iteration: 79\n",
      "Training :: Blind : 60.2984126984127\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7538.427930294154\n",
      "Iteration: 80\n",
      "Training :: Blind : 60.43174603174604\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7521.893086106996\n",
      "Iteration: 81\n",
      "Training :: Blind : 60.22222222222222\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7530.799634387707\n",
      "Iteration: 82\n",
      "Training :: Blind : 60.33492063492063\n",
      "Validation  :: Blind : 60.68571428571429 :: Blind Loss : 7528.03905743181\n",
      "Iteration: 83\n",
      "Training :: Blind : 60.31111111111112\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7531.584483529359\n",
      "Iteration: 84\n",
      "Training :: Blind : 60.25873015873016\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7537.977371897061\n",
      "Iteration: 85\n",
      "Training :: Blind : 60.30793650793651\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7529.09993447103\n",
      "Iteration: 86\n",
      "Training :: Blind : 60.099999999999994\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7543.890821268771\n",
      "Iteration: 87\n",
      "Training :: Blind : 60.24444444444445\n",
      "Validation  :: Blind : 60.27142857142857 :: Blind Loss : 7535.658073773363\n",
      "Iteration: 88\n",
      "Training :: Blind : 60.5\n",
      "Validation  :: Blind : 60.75714285714285 :: Blind Loss : 7518.643970335141\n",
      "Iteration: 89\n",
      "Training :: Blind : 60.36190476190476\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7525.127082524563\n",
      "Iteration: 90\n",
      "Training :: Blind : 60.10793650793651\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7544.3016265245205\n",
      "Iteration: 91\n",
      "Training :: Blind : 60.285714285714285\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7528.779463187868\n",
      "Iteration: 92\n",
      "Training :: Blind : 60.25238095238096\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7530.980272385986\n",
      "Iteration: 93\n",
      "Training :: Blind : 60.423809523809524\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7524.331212825995\n",
      "Iteration: 94\n",
      "Training :: Blind : 60.14761904761905\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7538.067618725108\n",
      "Iteration: 95\n",
      "Training :: Blind : 60.219047619047615\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7536.822342998231\n",
      "Iteration: 96\n",
      "Training :: Blind : 60.12857142857143\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7543.940432220694\n",
      "Iteration: 97\n",
      "Training :: Blind : 60.25555555555555\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7530.625306869796\n",
      "Iteration: 98\n",
      "Training :: Blind : 60.34285714285714\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7519.104773870935\n",
      "Iteration: 99\n",
      "Training :: Blind : 60.19523809523809\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7544.647501837893\n",
      "Iteration: 100\n",
      "Training :: Blind : 60.4\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7529.810142631824\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 53.05555555555556\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 9027.572157274035\n",
      "Iteration: 2\n",
      "Training :: Blind : 58.05555555555556\n",
      "Validation  :: Blind : 58.32857142857143 :: Blind Loss : 8149.433909497737\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.75714285714285\n",
      "Validation  :: Blind : 59.371428571428574 :: Blind Loss : 7818.736668601187\n",
      "Iteration: 4\n",
      "Training :: Blind : 58.83492063492064\n",
      "Validation  :: Blind : 59.41428571428572 :: Blind Loss : 7755.496869401058\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.94920634920635\n",
      "Validation  :: Blind : 59.62857142857143 :: Blind Loss : 7738.457514070397\n",
      "Iteration: 6\n",
      "Training :: Blind : 59.03809523809523\n",
      "Validation  :: Blind : 59.699999999999996 :: Blind Loss : 7710.910854896991\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.82539682539682\n",
      "Validation  :: Blind : 59.65714285714285 :: Blind Loss : 7735.320172377999\n",
      "Iteration: 8\n",
      "Training :: Blind : 58.93809523809524\n",
      "Validation  :: Blind : 59.48571428571429 :: Blind Loss : 7707.416969831331\n",
      "Iteration: 9\n",
      "Training :: Blind : 59.18730158730159\n",
      "Validation  :: Blind : 59.52857142857143 :: Blind Loss : 7674.170285154484\n",
      "Iteration: 10\n",
      "Training :: Blind : 58.838095238095235\n",
      "Validation  :: Blind : 59.11428571428572 :: Blind Loss : 7707.6927713280165\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.48571428571429\n",
      "Validation  :: Blind : 59.8 :: Blind Loss : 7645.101247331666\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.88888888888889\n",
      "Validation  :: Blind : 60.15714285714285 :: Blind Loss : 7616.889818702132\n",
      "Iteration: 13\n",
      "Training :: Blind : 60.077777777777776\n",
      "Validation  :: Blind : 60.25714285714285 :: Blind Loss : 7572.096693485897\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.27936507936508\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7549.556367825982\n",
      "Iteration: 15\n",
      "Training :: Blind : 60.16984126984127\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7554.888899205703\n",
      "Iteration: 16\n",
      "Training :: Blind : 60.31111111111112\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7548.565783728511\n",
      "Iteration: 17\n",
      "Training :: Blind : 59.887301587301586\n",
      "Validation  :: Blind : 60.24285714285714 :: Blind Loss : 7581.6570306846625\n",
      "Iteration: 18\n",
      "Training :: Blind : 60.12222222222222\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7558.18938619462\n",
      "Iteration: 19\n",
      "Training :: Blind : 60.01269841269842\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7560.073244955876\n",
      "Iteration: 20\n",
      "Training :: Blind : 60.098412698412695\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7562.1600274589255\n",
      "Iteration: 21\n",
      "Training :: Blind : 60.28095238095238\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7553.439906837041\n",
      "Iteration: 22\n",
      "Training :: Blind : 60.12698412698413\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7552.939020231321\n",
      "Iteration: 23\n",
      "Training :: Blind : 60.19523809523809\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7561.998154775615\n",
      "Iteration: 24\n",
      "Training :: Blind : 60.16984126984127\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7555.9855128037925\n",
      "Iteration: 25\n",
      "Training :: Blind : 60.199999999999996\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7546.519385050926\n",
      "Iteration: 26\n",
      "Training :: Blind : 60.096825396825395\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7570.231817534844\n",
      "Iteration: 27\n",
      "Training :: Blind : 59.97460317460317\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7568.365918779841\n",
      "Iteration: 28\n",
      "Training :: Blind : 60.42857142857143\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7549.493139359563\n",
      "Iteration: 29\n",
      "Training :: Blind : 60.06190476190476\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7561.7842483128115\n",
      "Iteration: 30\n",
      "Training :: Blind : 60.2031746031746\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7555.196198098384\n",
      "Iteration: 31\n",
      "Training :: Blind : 60.23333333333334\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7555.887835398552\n",
      "Iteration: 32\n",
      "Training :: Blind : 60.01111111111111\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7570.461214980418\n",
      "Iteration: 33\n",
      "Training :: Blind : 60.2015873015873\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7546.788691627103\n",
      "Iteration: 34\n",
      "Training :: Blind : 60.211111111111116\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7554.131636004019\n",
      "Iteration: 35\n",
      "Training :: Blind : 60.10476190476191\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7560.886879822716\n",
      "Iteration: 36\n",
      "Training :: Blind : 60.14761904761905\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7555.591801744811\n",
      "Iteration: 37\n",
      "Training :: Blind : 59.990476190476194\n",
      "Validation  :: Blind : 60.199999999999996 :: Blind Loss : 7564.687122385118\n",
      "Iteration: 38\n",
      "Training :: Blind : 60.02698412698413\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7565.283951486379\n",
      "Iteration: 39\n",
      "Training :: Blind : 60.16984126984127\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7549.74769688491\n",
      "Iteration: 40\n",
      "Training :: Blind : 59.93809523809524\n",
      "Validation  :: Blind : 60.21428571428571 :: Blind Loss : 7570.098128716085\n",
      "Iteration: 41\n",
      "Training :: Blind : 60.26031746031746\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7549.595225529827\n",
      "Iteration: 42\n",
      "Training :: Blind : 60.06666666666667\n",
      "Validation  :: Blind : 60.15714285714285 :: Blind Loss : 7558.1394877068105\n",
      "Iteration: 43\n",
      "Training :: Blind : 60.14761904761905\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7559.792843697839\n",
      "Iteration: 44\n",
      "Training :: Blind : 60.06190476190476\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7556.55635364368\n",
      "Iteration: 45\n",
      "Training :: Blind : 60.1920634920635\n",
      "Validation  :: Blind : 60.27142857142857 :: Blind Loss : 7559.2869443774325\n",
      "Iteration: 46\n",
      "Training :: Blind : 60.13333333333334\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7552.094981961109\n",
      "Iteration: 47\n",
      "Training :: Blind : 60.11904761904761\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7564.193177694517\n",
      "Iteration: 48\n",
      "Training :: Blind : 60.17936507936508\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7566.617634042534\n",
      "Iteration: 49\n",
      "Training :: Blind : 60.099999999999994\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7567.836094712957\n",
      "Iteration: 50\n",
      "Training :: Blind : 60.05873015873016\n",
      "Validation  :: Blind : 60.22857142857143 :: Blind Loss : 7563.172945295363\n",
      "Iteration: 51\n",
      "Training :: Blind : 60.24444444444445\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7551.288010665744\n",
      "Iteration: 52\n",
      "Training :: Blind : 60.12063492063492\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7562.77321337801\n",
      "Iteration: 53\n",
      "Training :: Blind : 60.458730158730155\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7541.962640062359\n",
      "Iteration: 54\n",
      "Training :: Blind : 60.29047619047619\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7552.772372812544\n",
      "Iteration: 55\n",
      "Training :: Blind : 60.17301587301588\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7556.953609493692\n",
      "Iteration: 56\n",
      "Training :: Blind : 60.1936507936508\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7555.714339443968\n",
      "Iteration: 57\n",
      "Training :: Blind : 60.006349206349206\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7573.102389178317\n",
      "Iteration: 58\n",
      "Training :: Blind : 60.111111111111114\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7564.892910076491\n",
      "Iteration: 59\n",
      "Training :: Blind : 60.13492063492063\n",
      "Validation  :: Blind : 60.27142857142857 :: Blind Loss : 7559.871230430697\n",
      "Iteration: 60\n",
      "Training :: Blind : 60.13492063492063\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7563.428446352106\n",
      "Iteration: 61\n",
      "Training :: Blind : 60.12698412698413\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7556.328937923034\n",
      "Iteration: 62\n",
      "Training :: Blind : 60.219047619047615\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7555.769890234305\n",
      "Iteration: 63\n",
      "Training :: Blind : 60.15555555555555\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7550.572805641446\n",
      "Iteration: 64\n",
      "Training :: Blind : 60.15238095238096\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7550.478795888075\n",
      "Iteration: 65\n",
      "Training :: Blind : 60.16031746031746\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7550.309756888713\n",
      "Iteration: 66\n",
      "Training :: Blind : 60.23333333333334\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7560.132863865685\n",
      "Iteration: 67\n",
      "Training :: Blind : 60.39047619047619\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7541.773092809742\n",
      "Iteration: 68\n",
      "Training :: Blind : 60.092063492063495\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7564.234057281861\n",
      "Iteration: 69\n",
      "Training :: Blind : 60.21428571428571\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7558.13784721135\n",
      "Iteration: 70\n",
      "Training :: Blind : 60.03492063492063\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7571.364937741287\n",
      "Iteration: 71\n",
      "Training :: Blind : 60.17619047619047\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7561.532247596278\n",
      "Iteration: 72\n",
      "Training :: Blind : 60.077777777777776\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7561.916595410254\n",
      "Iteration: 73\n",
      "Training :: Blind : 60.3984126984127\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7547.71518407497\n",
      "Iteration: 74\n",
      "Training :: Blind : 60.34603174603175\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7549.674431554498\n",
      "Iteration: 75\n",
      "Training :: Blind : 60.11746031746031\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7561.218968532863\n",
      "Iteration: 76\n",
      "Training :: Blind : 60.404761904761905\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7543.017136378003\n",
      "Iteration: 77\n",
      "Training :: Blind : 59.906349206349205\n",
      "Validation  :: Blind : 60.22857142857143 :: Blind Loss : 7582.6117110342275\n",
      "Iteration: 78\n",
      "Training :: Blind : 60.12698412698413\n",
      "Validation  :: Blind : 60.199999999999996 :: Blind Loss : 7563.521502528288\n",
      "Iteration: 79\n",
      "Training :: Blind : 60.16825396825397\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7563.19448627532\n",
      "Iteration: 80\n",
      "Training :: Blind : 60.17619047619047\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7554.271787418695\n",
      "Iteration: 81\n",
      "Training :: Blind : 60.13968253968254\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7559.105090543573\n",
      "Iteration: 82\n",
      "Training :: Blind : 60.25555555555555\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7545.645651409652\n",
      "Iteration: 83\n",
      "Training :: Blind : 60.15873015873016\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7558.7477772990715\n",
      "Iteration: 84\n",
      "Training :: Blind : 60.26031746031746\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7551.298654242101\n",
      "Iteration: 85\n",
      "Training :: Blind : 60.1015873015873\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7570.830639820172\n",
      "Iteration: 86\n",
      "Training :: Blind : 60.3\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7554.363895804481\n",
      "Iteration: 87\n",
      "Training :: Blind : 60.196825396825396\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7557.314511358536\n",
      "Iteration: 88\n",
      "Training :: Blind : 59.98095238095238\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7572.737164928414\n",
      "Iteration: 89\n",
      "Training :: Blind : 60.13809523809523\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7553.437932429158\n",
      "Iteration: 90\n",
      "Training :: Blind : 60.16190476190476\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7556.201111552957\n",
      "Iteration: 91\n",
      "Training :: Blind : 60.06666666666667\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7567.007031699564\n",
      "Iteration: 92\n",
      "Training :: Blind : 60.285714285714285\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7543.197003010562\n",
      "Iteration: 93\n",
      "Training :: Blind : 60.198412698412696\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7556.544239340112\n",
      "Iteration: 94\n",
      "Training :: Blind : 60.393650793650785\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7551.7567046740205\n",
      "Iteration: 95\n",
      "Training :: Blind : 60.46666666666667\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7541.161022961033\n",
      "Iteration: 96\n",
      "Training :: Blind : 60.22857142857143\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7558.396818756508\n",
      "Iteration: 97\n",
      "Training :: Blind : 60.38095238095238\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7548.415466331848\n",
      "Iteration: 98\n",
      "Training :: Blind : 60.11904761904761\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7559.5732391493075\n",
      "Iteration: 99\n",
      "Training :: Blind : 60.0015873015873\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7571.618217207235\n",
      "Iteration: 100\n",
      "Training :: Blind : 60.198412698412696\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7551.8327823937525\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.93174603174603\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 9028.829555072087\n",
      "Iteration: 2\n",
      "Training :: Blind : 58.08412698412698\n",
      "Validation  :: Blind : 58.128571428571426 :: Blind Loss : 8152.760183822334\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.82539682539682\n",
      "Validation  :: Blind : 59.457142857142856 :: Blind Loss : 7838.133085875052\n",
      "Iteration: 4\n",
      "Training :: Blind : 59.01269841269842\n",
      "Validation  :: Blind : 59.77142857142857 :: Blind Loss : 7750.104541515706\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.87936507936507\n",
      "Validation  :: Blind : 59.8 :: Blind Loss : 7751.430721975734\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.89206349206349\n",
      "Validation  :: Blind : 59.599999999999994 :: Blind Loss : 7753.294397326754\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.958730158730155\n",
      "Validation  :: Blind : 59.52857142857143 :: Blind Loss : 7741.053225313941\n",
      "Iteration: 8\n",
      "Training :: Blind : 59.06031746031746\n",
      "Validation  :: Blind : 59.67142857142858 :: Blind Loss : 7717.423910838686\n",
      "Iteration: 9\n",
      "Training :: Blind : 59.15714285714285\n",
      "Validation  :: Blind : 59.785714285714285 :: Blind Loss : 7721.001951377015\n",
      "Iteration: 10\n",
      "Training :: Blind : 59.49841269841269\n",
      "Validation  :: Blind : 59.9 :: Blind Loss : 7678.991321820693\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.98253968253968\n",
      "Validation  :: Blind : 60.12857142857143 :: Blind Loss : 7640.151234483321\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.884126984126986\n",
      "Validation  :: Blind : 60.099999999999994 :: Blind Loss : 7607.312524418567\n",
      "Iteration: 13\n",
      "Training :: Blind : 59.990476190476194\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7586.473751792064\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.26825396825397\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7553.990140183092\n",
      "Iteration: 15\n",
      "Training :: Blind : 60.1015873015873\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7569.785065455989\n",
      "Iteration: 16\n",
      "Training :: Blind : 60.3920634920635\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7541.683833824029\n",
      "Iteration: 17\n",
      "Training :: Blind : 60.477777777777774\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7522.558826426455\n",
      "Iteration: 18\n",
      "Training :: Blind : 60.320634920634916\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7532.222418017198\n",
      "Iteration: 19\n",
      "Training :: Blind : 60.37936507936508\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7533.440361907162\n",
      "Iteration: 20\n",
      "Training :: Blind : 60.43333333333333\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7517.703644466973\n",
      "Iteration: 21\n",
      "Training :: Blind : 60.333333333333336\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7533.968285225742\n",
      "Iteration: 22\n",
      "Training :: Blind : 60.33492063492063\n",
      "Validation  :: Blind : 60.68571428571429 :: Blind Loss : 7531.471821201061\n",
      "Iteration: 23\n",
      "Training :: Blind : 60.357142857142854\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7537.563385553371\n",
      "Iteration: 24\n",
      "Training :: Blind : 60.41746031746031\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7524.062451263828\n",
      "Iteration: 25\n",
      "Training :: Blind : 60.24126984126984\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7546.350062622414\n",
      "Iteration: 26\n",
      "Training :: Blind : 60.357142857142854\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7525.063374143383\n",
      "Iteration: 27\n",
      "Training :: Blind : 60.476190476190474\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7518.71419983789\n",
      "Iteration: 28\n",
      "Training :: Blind : 60.29047619047619\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7532.618113762926\n",
      "Iteration: 29\n",
      "Training :: Blind : 60.27460317460317\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7539.318451941545\n",
      "Iteration: 30\n",
      "Training :: Blind : 60.36031746031746\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7525.40181968777\n",
      "Iteration: 31\n",
      "Training :: Blind : 60.34285714285714\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7536.986746174022\n",
      "Iteration: 32\n",
      "Training :: Blind : 60.28888888888889\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7534.687458981691\n",
      "Iteration: 33\n",
      "Training :: Blind : 60.32539682539683\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7534.127418872087\n",
      "Iteration: 34\n",
      "Training :: Blind : 60.25873015873016\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7531.13357388431\n",
      "Iteration: 35\n",
      "Training :: Blind : 60.2920634920635\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7533.4684756030665\n",
      "Iteration: 36\n",
      "Training :: Blind : 60.27142857142857\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7536.711284991886\n",
      "Iteration: 37\n",
      "Training :: Blind : 60.41587301587301\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7534.640153284872\n",
      "Iteration: 38\n",
      "Training :: Blind : 60.368253968253974\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7527.820513814517\n",
      "Iteration: 39\n",
      "Training :: Blind : 60.26825396825397\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7539.938120923271\n",
      "Iteration: 40\n",
      "Training :: Blind : 60.45555555555555\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7522.505498375927\n",
      "Iteration: 41\n",
      "Training :: Blind : 60.34444444444445\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7528.569525225236\n",
      "Iteration: 42\n",
      "Training :: Blind : 60.27777777777777\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7539.824973498681\n",
      "Iteration: 43\n",
      "Training :: Blind : 60.27142857142857\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7536.8952283091385\n",
      "Iteration: 44\n",
      "Training :: Blind : 60.339682539682535\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7534.416558850697\n",
      "Iteration: 45\n",
      "Training :: Blind : 60.41269841269842\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7529.9273364915425\n",
      "Iteration: 46\n",
      "Training :: Blind : 60.41587301587301\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7526.366472351676\n",
      "Iteration: 47\n",
      "Training :: Blind : 60.29523809523809\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7533.029192996942\n",
      "Iteration: 48\n",
      "Training :: Blind : 60.27460317460317\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7532.69391246286\n",
      "Iteration: 49\n",
      "Training :: Blind : 60.39523809523809\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7528.037060298944\n",
      "Iteration: 50\n",
      "Training :: Blind : 60.41428571428571\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7534.100849752736\n",
      "Iteration: 51\n",
      "Training :: Blind : 60.439682539682536\n",
      "Validation  :: Blind : 60.71428571428571 :: Blind Loss : 7527.897849752095\n",
      "Iteration: 52\n",
      "Training :: Blind : 60.31428571428571\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7535.202936182236\n",
      "Iteration: 53\n",
      "Training :: Blind : 60.41587301587301\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7534.073354693557\n",
      "Iteration: 54\n",
      "Training :: Blind : 60.29523809523809\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7538.19398188329\n",
      "Iteration: 55\n",
      "Training :: Blind : 60.371428571428574\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7531.606877914641\n",
      "Iteration: 56\n",
      "Training :: Blind : 60.27142857142857\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7538.131949606983\n",
      "Iteration: 57\n",
      "Training :: Blind : 60.43492063492063\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7527.157082833766\n",
      "Iteration: 58\n",
      "Training :: Blind : 60.319047619047616\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7537.321718632112\n",
      "Iteration: 59\n",
      "Training :: Blind : 60.355555555555554\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7527.384525591483\n",
      "Iteration: 60\n",
      "Training :: Blind : 60.18888888888889\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7539.796604814221\n",
      "Iteration: 61\n",
      "Training :: Blind : 60.32698412698413\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7532.560654639976\n",
      "Iteration: 62\n",
      "Training :: Blind : 60.282539682539685\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7533.307241663741\n",
      "Iteration: 63\n",
      "Training :: Blind : 60.36349206349206\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7528.810709987491\n",
      "Iteration: 64\n",
      "Training :: Blind : 60.30952380952381\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7536.3093215989575\n",
      "Iteration: 65\n",
      "Training :: Blind : 60.238095238095234\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7539.100194200157\n",
      "Iteration: 66\n",
      "Training :: Blind : 60.29523809523809\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7537.6871956290215\n",
      "Iteration: 67\n",
      "Training :: Blind : 60.457142857142856\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7513.90420454592\n",
      "Iteration: 68\n",
      "Training :: Blind : 60.331746031746036\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7539.2027994116015\n",
      "Iteration: 69\n",
      "Training :: Blind : 60.42857142857143\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7524.063186854444\n",
      "Iteration: 70\n",
      "Training :: Blind : 60.36666666666667\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7527.041520244829\n",
      "Iteration: 71\n",
      "Training :: Blind : 60.18412698412698\n",
      "Validation  :: Blind : 60.27142857142857 :: Blind Loss : 7549.686940484832\n",
      "Iteration: 72\n",
      "Training :: Blind : 60.303174603174604\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7536.227703922188\n",
      "Iteration: 73\n",
      "Training :: Blind : 60.25714285714285\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7540.107699213004\n",
      "Iteration: 74\n",
      "Training :: Blind : 60.374603174603166\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7535.699768080434\n",
      "Iteration: 75\n",
      "Training :: Blind : 60.265079365079366\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7536.408848980928\n",
      "Iteration: 76\n",
      "Training :: Blind : 60.38095238095238\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7531.389467306336\n",
      "Iteration: 77\n",
      "Training :: Blind : 60.24603174603175\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7535.793217880473\n",
      "Iteration: 78\n",
      "Training :: Blind : 60.39682539682539\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7535.363190290234\n",
      "Iteration: 79\n",
      "Training :: Blind : 60.357142857142854\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7535.971586224509\n",
      "Iteration: 80\n",
      "Training :: Blind : 60.385714285714286\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7529.257622920135\n",
      "Iteration: 81\n",
      "Training :: Blind : 60.27460317460317\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7532.547933835172\n",
      "Iteration: 82\n",
      "Training :: Blind : 60.333333333333336\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7522.869233602947\n",
      "Iteration: 83\n",
      "Training :: Blind : 60.22380952380952\n",
      "Validation  :: Blind : 60.22857142857143 :: Blind Loss : 7542.516132406254\n",
      "Iteration: 84\n",
      "Training :: Blind : 60.32539682539683\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7539.088494865265\n",
      "Iteration: 85\n",
      "Training :: Blind : 60.368253968253974\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7525.408287402768\n",
      "Iteration: 86\n",
      "Training :: Blind : 60.32857142857143\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7536.483786906302\n",
      "Iteration: 87\n",
      "Training :: Blind : 60.371428571428574\n",
      "Validation  :: Blind : 60.68571428571429 :: Blind Loss : 7533.094397780207\n",
      "Iteration: 88\n",
      "Training :: Blind : 60.38888888888889\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7531.681260749786\n",
      "Iteration: 89\n",
      "Training :: Blind : 60.31269841269842\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7539.800829889334\n",
      "Iteration: 90\n",
      "Training :: Blind : 60.4\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7526.172100732578\n",
      "Iteration: 91\n",
      "Training :: Blind : 60.27936507936508\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7531.720007479105\n",
      "Iteration: 92\n",
      "Training :: Blind : 60.37301587301588\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7535.8272829897105\n",
      "Iteration: 93\n",
      "Training :: Blind : 60.477777777777774\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7514.857942875162\n",
      "Iteration: 94\n",
      "Training :: Blind : 60.34285714285714\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7548.062511177\n",
      "Iteration: 95\n",
      "Training :: Blind : 60.33492063492063\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7531.550948255102\n",
      "Iteration: 96\n",
      "Training :: Blind : 60.231746031746034\n",
      "Validation  :: Blind : 60.42857142857143 :: Blind Loss : 7540.498320577183\n",
      "Iteration: 97\n",
      "Training :: Blind : 60.38253968253968\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7528.79037958755\n",
      "Iteration: 98\n",
      "Training :: Blind : 60.18095238095238\n",
      "Validation  :: Blind : 60.357142857142854 :: Blind Loss : 7546.645661950424\n",
      "Iteration: 99\n",
      "Training :: Blind : 60.34761904761905\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7529.881206534341\n",
      "Iteration: 100\n",
      "Training :: Blind : 60.40793650793651\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7522.130819598203\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.884126984126986\n",
      "Validation  :: Blind : 53.38571428571428 :: Blind Loss : 9047.872550376354\n",
      "Iteration: 2\n",
      "Training :: Blind : 57.97460317460318\n",
      "Validation  :: Blind : 58.11428571428572 :: Blind Loss : 8157.4682127858605\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.460317460317455\n",
      "Validation  :: Blind : 58.98571428571429 :: Blind Loss : 7849.560252055917\n",
      "Iteration: 4\n",
      "Training :: Blind : 58.817460317460316\n",
      "Validation  :: Blind : 59.385714285714286 :: Blind Loss : 7754.16832456002\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.765079365079366\n",
      "Validation  :: Blind : 59.3 :: Blind Loss : 7744.725440833998\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.72857142857143\n",
      "Validation  :: Blind : 59.41428571428572 :: Blind Loss : 7746.912770103531\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.39206349206349\n",
      "Validation  :: Blind : 59.22857142857143 :: Blind Loss : 7760.593486989943\n",
      "Iteration: 8\n",
      "Training :: Blind : 59.144444444444446\n",
      "Validation  :: Blind : 59.74285714285714 :: Blind Loss : 7689.492560765392\n",
      "Iteration: 9\n",
      "Training :: Blind : 59.27936507936508\n",
      "Validation  :: Blind : 59.81428571428572 :: Blind Loss : 7672.30336768209\n",
      "Iteration: 10\n",
      "Training :: Blind : 59.477777777777774\n",
      "Validation  :: Blind : 59.885714285714286 :: Blind Loss : 7674.041754563285\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.644444444444446\n",
      "Validation  :: Blind : 60.042857142857144 :: Blind Loss : 7656.213522362486\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.7984126984127\n",
      "Validation  :: Blind : 60.21428571428571 :: Blind Loss : 7610.093291143837\n",
      "Iteration: 13\n",
      "Training :: Blind : 60.01904761904762\n",
      "Validation  :: Blind : 60.25714285714285 :: Blind Loss : 7579.293143472179\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.13333333333334\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7566.5138057909535\n",
      "Iteration: 15\n",
      "Training :: Blind : 59.98412698412699\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7567.611342055092\n",
      "Iteration: 16\n",
      "Training :: Blind : 60.1920634920635\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7528.746713689318\n",
      "Iteration: 17\n",
      "Training :: Blind : 60.16666666666667\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7510.029516716212\n",
      "Iteration: 18\n",
      "Training :: Blind : 60.24285714285714\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7511.3181606311555\n",
      "Iteration: 19\n",
      "Training :: Blind : 60.460317460317455\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7473.233694611959\n",
      "Iteration: 20\n",
      "Training :: Blind : 60.3920634920635\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7479.991764357507\n",
      "Iteration: 21\n",
      "Training :: Blind : 60.50952380952381\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7469.43527909662\n",
      "Iteration: 22\n",
      "Training :: Blind : 60.27619047619047\n",
      "Validation  :: Blind : 60.77142857142858 :: Blind Loss : 7493.109757112139\n",
      "Iteration: 23\n",
      "Training :: Blind : 60.46507936507937\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7463.554240356627\n",
      "Iteration: 24\n",
      "Training :: Blind : 60.439682539682536\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7469.509134825506\n",
      "Iteration: 25\n",
      "Training :: Blind : 60.319047619047616\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7477.734468202839\n",
      "Iteration: 26\n",
      "Training :: Blind : 60.338095238095235\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7478.640084974204\n",
      "Iteration: 27\n",
      "Training :: Blind : 60.371428571428574\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7482.219178315907\n",
      "Iteration: 28\n",
      "Training :: Blind : 60.29682539682539\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7484.292747487097\n",
      "Iteration: 29\n",
      "Training :: Blind : 60.50952380952381\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7471.86306929777\n",
      "Iteration: 30\n",
      "Training :: Blind : 60.5031746031746\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7462.767856572628\n",
      "Iteration: 31\n",
      "Training :: Blind : 60.48412698412699\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7472.912542480097\n",
      "Iteration: 32\n",
      "Training :: Blind : 60.368253968253974\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7476.171048327667\n",
      "Iteration: 33\n",
      "Training :: Blind : 60.31587301587301\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7486.958192466893\n",
      "Iteration: 34\n",
      "Training :: Blind : 60.42063492063492\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7480.250092043667\n",
      "Iteration: 35\n",
      "Training :: Blind : 60.56507936507936\n",
      "Validation  :: Blind : 60.74285714285714 :: Blind Loss : 7474.651342016441\n",
      "Iteration: 36\n",
      "Training :: Blind : 60.5031746031746\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7474.987775252288\n",
      "Iteration: 37\n",
      "Training :: Blind : 60.34285714285714\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7474.660659670526\n",
      "Iteration: 38\n",
      "Training :: Blind : 60.43174603174604\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7485.998094477143\n",
      "Iteration: 39\n",
      "Training :: Blind : 60.350793650793655\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7478.998286628821\n",
      "Iteration: 40\n",
      "Training :: Blind : 60.27460317460317\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7486.578659032165\n",
      "Iteration: 41\n",
      "Training :: Blind : 60.6015873015873\n",
      "Validation  :: Blind : 60.81428571428571 :: Blind Loss : 7463.740516551892\n",
      "Iteration: 42\n",
      "Training :: Blind : 60.46190476190476\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7473.643896634877\n",
      "Iteration: 43\n",
      "Training :: Blind : 60.37936507936508\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7477.096122365048\n",
      "Iteration: 44\n",
      "Training :: Blind : 60.43650793650793\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7469.801951642268\n",
      "Iteration: 45\n",
      "Training :: Blind : 60.28888888888889\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7482.888767204774\n",
      "Iteration: 46\n",
      "Training :: Blind : 60.30793650793651\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7476.567881736576\n",
      "Iteration: 47\n",
      "Training :: Blind : 60.523809523809526\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7472.957760695482\n",
      "Iteration: 48\n",
      "Training :: Blind : 60.32222222222222\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7479.344047356473\n",
      "Iteration: 49\n",
      "Training :: Blind : 60.393650793650785\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7469.488032528123\n",
      "Iteration: 50\n",
      "Training :: Blind : 60.55873015873016\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7460.8216085234335\n",
      "Iteration: 51\n",
      "Training :: Blind : 60.423809523809524\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7475.481881746055\n",
      "Iteration: 52\n",
      "Training :: Blind : 60.28095238095238\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7483.186628250191\n",
      "Iteration: 53\n",
      "Training :: Blind : 60.36666666666667\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7471.916320188655\n",
      "Iteration: 54\n",
      "Training :: Blind : 60.471428571428575\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7472.788105754475\n",
      "Iteration: 55\n",
      "Training :: Blind : 60.38730158730159\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7484.657503540325\n",
      "Iteration: 56\n",
      "Training :: Blind : 60.50476190476191\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7465.938872612786\n",
      "Iteration: 57\n",
      "Training :: Blind : 60.46507936507937\n",
      "Validation  :: Blind : 60.728571428571435 :: Blind Loss : 7479.497371262748\n",
      "Iteration: 58\n",
      "Training :: Blind : 60.46349206349206\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7471.610073072836\n",
      "Iteration: 59\n",
      "Training :: Blind : 60.423809523809524\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7473.998675100508\n",
      "Iteration: 60\n",
      "Training :: Blind : 60.34285714285714\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7468.098380739901\n",
      "Iteration: 61\n",
      "Training :: Blind : 60.26984126984127\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7487.987467124727\n",
      "Iteration: 62\n",
      "Training :: Blind : 60.34603174603175\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7472.5546407827715\n",
      "Iteration: 63\n",
      "Training :: Blind : 60.34126984126984\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7479.642041652853\n",
      "Iteration: 64\n",
      "Training :: Blind : 60.525396825396825\n",
      "Validation  :: Blind : 60.68571428571429 :: Blind Loss : 7469.355276815431\n",
      "Iteration: 65\n",
      "Training :: Blind : 60.41111111111112\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7471.176426949107\n",
      "Iteration: 66\n",
      "Training :: Blind : 60.546031746031744\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7476.735497638263\n",
      "Iteration: 67\n",
      "Training :: Blind : 60.303174603174604\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7486.874568367248\n",
      "Iteration: 68\n",
      "Training :: Blind : 60.53015873015873\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7464.867755835654\n",
      "Iteration: 69\n",
      "Training :: Blind : 60.23968253968254\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7489.672170915186\n",
      "Iteration: 70\n",
      "Training :: Blind : 60.63174603174603\n",
      "Validation  :: Blind : 60.92857142857143 :: Blind Loss : 7462.7649665301105\n",
      "Iteration: 71\n",
      "Training :: Blind : 60.24444444444445\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7483.680511912695\n",
      "Iteration: 72\n",
      "Training :: Blind : 60.39047619047619\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7472.973643991363\n",
      "Iteration: 73\n",
      "Training :: Blind : 60.52857142857143\n",
      "Validation  :: Blind : 60.9 :: Blind Loss : 7471.488442474547\n",
      "Iteration: 74\n",
      "Training :: Blind : 60.50952380952381\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7463.563510499944\n",
      "Iteration: 75\n",
      "Training :: Blind : 60.38253968253968\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7476.634593473176\n",
      "Iteration: 76\n",
      "Training :: Blind : 60.42857142857143\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7467.180840483895\n",
      "Iteration: 77\n",
      "Training :: Blind : 60.31269841269842\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7478.710128465993\n",
      "Iteration: 78\n",
      "Training :: Blind : 60.592063492063495\n",
      "Validation  :: Blind : 60.71428571428571 :: Blind Loss : 7459.008283759928\n",
      "Iteration: 79\n",
      "Training :: Blind : 60.3984126984127\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7480.273963834485\n",
      "Iteration: 80\n",
      "Training :: Blind : 60.590476190476195\n",
      "Validation  :: Blind : 60.75714285714285 :: Blind Loss : 7453.920425204272\n",
      "Iteration: 81\n",
      "Training :: Blind : 60.384126984126986\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7477.179872487182\n",
      "Iteration: 82\n",
      "Training :: Blind : 60.6031746031746\n",
      "Validation  :: Blind : 60.82857142857143 :: Blind Loss : 7462.956426024347\n",
      "Iteration: 83\n",
      "Training :: Blind : 60.34126984126984\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7472.56988971729\n",
      "Iteration: 84\n",
      "Training :: Blind : 60.29365079365079\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7480.571158743248\n",
      "Iteration: 85\n",
      "Training :: Blind : 60.249206349206354\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7486.52339922471\n",
      "Iteration: 86\n",
      "Training :: Blind : 60.6920634920635\n",
      "Validation  :: Blind : 60.81428571428571 :: Blind Loss : 7454.465308099167\n",
      "Iteration: 87\n",
      "Training :: Blind : 60.50476190476191\n",
      "Validation  :: Blind : 60.728571428571435 :: Blind Loss : 7467.917394119757\n",
      "Iteration: 88\n",
      "Training :: Blind : 60.50793650793651\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7467.639347662294\n",
      "Iteration: 89\n",
      "Training :: Blind : 60.579365079365076\n",
      "Validation  :: Blind : 60.728571428571435 :: Blind Loss : 7473.96341079257\n",
      "Iteration: 90\n",
      "Training :: Blind : 60.44920634920635\n",
      "Validation  :: Blind : 60.77142857142858 :: Blind Loss : 7472.871196190933\n",
      "Iteration: 91\n",
      "Training :: Blind : 60.30952380952381\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7484.93171187802\n",
      "Iteration: 92\n",
      "Training :: Blind : 60.368253968253974\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7482.534814876122\n",
      "Iteration: 93\n",
      "Training :: Blind : 60.285714285714285\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7487.8946465318\n",
      "Iteration: 94\n",
      "Training :: Blind : 60.55238095238096\n",
      "Validation  :: Blind : 60.957142857142856 :: Blind Loss : 7472.3570925166205\n",
      "Iteration: 95\n",
      "Training :: Blind : 60.544444444444444\n",
      "Validation  :: Blind : 60.71428571428571 :: Blind Loss : 7461.364838770323\n",
      "Iteration: 96\n",
      "Training :: Blind : 60.54761904761905\n",
      "Validation  :: Blind : 60.9 :: Blind Loss : 7468.318008671628\n",
      "Iteration: 97\n",
      "Training :: Blind : 60.368253968253974\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7482.73323588657\n",
      "Iteration: 98\n",
      "Training :: Blind : 60.54126984126984\n",
      "Validation  :: Blind : 60.77142857142858 :: Blind Loss : 7472.079094245446\n",
      "Iteration: 99\n",
      "Training :: Blind : 60.53968253968254\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7469.141312416912\n",
      "Iteration: 100\n",
      "Training :: Blind : 60.41269841269842\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7475.335309910804\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.93015873015873\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 9034.62881829177\n",
      "Iteration: 2\n",
      "Training :: Blind : 58.044444444444444\n",
      "Validation  :: Blind : 58.357142857142854 :: Blind Loss : 8143.100081461727\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.609523809523814\n",
      "Validation  :: Blind : 59.18571428571428 :: Blind Loss : 7831.283306944289\n",
      "Iteration: 4\n",
      "Training :: Blind : 58.61904761904761\n",
      "Validation  :: Blind : 59.17142857142858 :: Blind Loss : 7767.690756960694\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.592063492063495\n",
      "Validation  :: Blind : 59.18571428571428 :: Blind Loss : 7765.757895722002\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.988888888888894\n",
      "Validation  :: Blind : 59.542857142857144 :: Blind Loss : 7696.045871439092\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.869841269841274\n",
      "Validation  :: Blind : 59.72857142857143 :: Blind Loss : 7715.733626273899\n",
      "Iteration: 8\n",
      "Training :: Blind : 58.94761904761905\n",
      "Validation  :: Blind : 59.64285714285714 :: Blind Loss : 7716.738021039422\n",
      "Iteration: 9\n",
      "Training :: Blind : 59.2920634920635\n",
      "Validation  :: Blind : 59.72857142857143 :: Blind Loss : 7665.99834684703\n",
      "Iteration: 10\n",
      "Training :: Blind : 58.815873015873024\n",
      "Validation  :: Blind : 59.3 :: Blind Loss : 7726.671713460992\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.18571428571428\n",
      "Validation  :: Blind : 59.542857142857144 :: Blind Loss : 7670.488104292457\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.94444444444444\n",
      "Validation  :: Blind : 60.02857142857143 :: Blind Loss : 7625.078527586627\n",
      "Iteration: 13\n",
      "Training :: Blind : 60.10634920634921\n",
      "Validation  :: Blind : 60.25714285714285 :: Blind Loss : 7587.923823577679\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.092063492063495\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7677.619348071019\n",
      "Iteration: 15\n",
      "Training :: Blind : 60.579365079365076\n",
      "Validation  :: Blind : 60.9 :: Blind Loss : 7568.44863452879\n",
      "Iteration: 16\n",
      "Training :: Blind : 60.77142857142858\n",
      "Validation  :: Blind : 60.91428571428571 :: Blind Loss : 7522.1360303150395\n",
      "Iteration: 17\n",
      "Training :: Blind : 61.06666666666667\n",
      "Validation  :: Blind : 61.32857142857143 :: Blind Loss : 7471.132378942464\n",
      "Iteration: 18\n",
      "Training :: Blind : 61.238095238095234\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7417.23943828813\n",
      "Iteration: 19\n",
      "Training :: Blind : 61.11269841269841\n",
      "Validation  :: Blind : 61.32857142857143 :: Blind Loss : 7443.013683148725\n",
      "Iteration: 20\n",
      "Training :: Blind : 61.03492063492063\n",
      "Validation  :: Blind : 61.15714285714285 :: Blind Loss : 7539.646367705267\n",
      "Iteration: 21\n",
      "Training :: Blind : 61.13015873015873\n",
      "Validation  :: Blind : 60.84285714285714 :: Blind Loss : 7610.254285736559\n",
      "Iteration: 22\n",
      "Training :: Blind : 61.273015873015865\n",
      "Validation  :: Blind : 61.08571428571429 :: Blind Loss : 7569.178176148154\n",
      "Iteration: 23\n",
      "Training :: Blind : 61.30793650793651\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7555.118885120279\n",
      "Iteration: 24\n",
      "Training :: Blind : 61.39523809523809\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7534.393651606523\n",
      "Iteration: 25\n",
      "Training :: Blind : 61.55396825396825\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7525.614678192211\n",
      "Iteration: 26\n",
      "Training :: Blind : 61.56825396825397\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7528.21306824995\n",
      "Iteration: 27\n",
      "Training :: Blind : 61.46031746031746\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7533.299191937983\n",
      "Iteration: 28\n",
      "Training :: Blind : 61.45555555555555\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7520.64772695382\n",
      "Iteration: 29\n",
      "Training :: Blind : 61.523809523809526\n",
      "Validation  :: Blind : 61.65714285714286 :: Blind Loss : 7513.1773837547025\n",
      "Iteration: 30\n",
      "Training :: Blind : 61.357142857142854\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7531.685394822605\n",
      "Iteration: 31\n",
      "Training :: Blind : 61.468253968253975\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7516.160472782756\n",
      "Iteration: 32\n",
      "Training :: Blind : 61.525396825396825\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7519.795663806131\n",
      "Iteration: 33\n",
      "Training :: Blind : 61.542857142857144\n",
      "Validation  :: Blind : 61.642857142857146 :: Blind Loss : 7516.004901528318\n",
      "Iteration: 34\n",
      "Training :: Blind : 61.46507936507937\n",
      "Validation  :: Blind : 61.58571428571429 :: Blind Loss : 7521.522569638092\n",
      "Iteration: 35\n",
      "Training :: Blind : 61.49682539682539\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7517.553050926363\n",
      "Iteration: 36\n",
      "Training :: Blind : 61.45238095238095\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7535.804110961487\n",
      "Iteration: 37\n",
      "Training :: Blind : 61.476190476190474\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7530.692673338988\n",
      "Iteration: 38\n",
      "Training :: Blind : 61.45238095238095\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7531.094221573323\n",
      "Iteration: 39\n",
      "Training :: Blind : 61.42698412698413\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7532.227375782701\n",
      "Iteration: 40\n",
      "Training :: Blind : 61.477777777777774\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7524.7937046658535\n",
      "Iteration: 41\n",
      "Training :: Blind : 61.490476190476194\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7526.166998845452\n",
      "Iteration: 42\n",
      "Training :: Blind : 61.44920634920636\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7521.712453897755\n",
      "Iteration: 43\n",
      "Training :: Blind : 61.46349206349206\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7531.635590814023\n",
      "Iteration: 44\n",
      "Training :: Blind : 61.457142857142856\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7522.062534633254\n",
      "Iteration: 45\n",
      "Training :: Blind : 61.48253968253968\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7511.68258403829\n",
      "Iteration: 46\n",
      "Training :: Blind : 61.41587301587301\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7528.95396327579\n",
      "Iteration: 47\n",
      "Training :: Blind : 61.48571428571429\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7518.881225163517\n",
      "Iteration: 48\n",
      "Training :: Blind : 61.4015873015873\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7527.476730048579\n",
      "Iteration: 49\n",
      "Training :: Blind : 61.423809523809524\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7524.006938780312\n",
      "Iteration: 50\n",
      "Training :: Blind : 61.32222222222222\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7532.833048614837\n",
      "Iteration: 51\n",
      "Training :: Blind : 61.37619047619047\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7533.678334664458\n",
      "Iteration: 52\n",
      "Training :: Blind : 61.44285714285714\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7523.364314851913\n",
      "Iteration: 53\n",
      "Training :: Blind : 61.49841269841269\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7518.881817416221\n",
      "Iteration: 54\n",
      "Training :: Blind : 61.43492063492063\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7520.636859222079\n",
      "Iteration: 55\n",
      "Training :: Blind : 61.412698412698404\n",
      "Validation  :: Blind : 61.32857142857143 :: Blind Loss : 7530.825870025792\n",
      "Iteration: 56\n",
      "Training :: Blind : 61.40793650793651\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7528.919831698449\n",
      "Iteration: 57\n",
      "Training :: Blind : 61.37619047619047\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7537.954233447917\n",
      "Iteration: 58\n",
      "Training :: Blind : 61.522222222222226\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7524.029099041837\n",
      "Iteration: 59\n",
      "Training :: Blind : 61.37460317460317\n",
      "Validation  :: Blind : 61.32857142857143 :: Blind Loss : 7538.706486933817\n",
      "Iteration: 60\n",
      "Training :: Blind : 61.490476190476194\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7526.2332250197705\n",
      "Iteration: 61\n",
      "Training :: Blind : 61.39682539682539\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7530.037246667684\n",
      "Iteration: 62\n",
      "Training :: Blind : 61.512698412698406\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7520.5919369035\n",
      "Iteration: 63\n",
      "Training :: Blind : 61.4015873015873\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7530.4410792144245\n",
      "Iteration: 64\n",
      "Training :: Blind : 61.525396825396825\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7523.675326995741\n",
      "Iteration: 65\n",
      "Training :: Blind : 61.41904761904762\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7525.687411736038\n",
      "Iteration: 66\n",
      "Training :: Blind : 61.525396825396825\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7516.487599816504\n",
      "Iteration: 67\n",
      "Training :: Blind : 61.43015873015874\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7526.947322248535\n",
      "Iteration: 68\n",
      "Training :: Blind : 61.42063492063492\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7523.071876993932\n",
      "Iteration: 69\n",
      "Training :: Blind : 61.490476190476194\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7531.677138164804\n",
      "Iteration: 70\n",
      "Training :: Blind : 61.48412698412699\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7524.570246576368\n",
      "Iteration: 71\n",
      "Training :: Blind : 61.49682539682539\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7529.894789488273\n",
      "Iteration: 72\n",
      "Training :: Blind : 61.44126984126984\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7526.88088076202\n",
      "Iteration: 73\n",
      "Training :: Blind : 61.355555555555554\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7534.764691555239\n",
      "Iteration: 74\n",
      "Training :: Blind : 61.51746031746031\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7523.085164585185\n",
      "Iteration: 75\n",
      "Training :: Blind : 61.40634920634921\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7535.339870978174\n",
      "Iteration: 76\n",
      "Training :: Blind : 61.292063492063484\n",
      "Validation  :: Blind : 61.21428571428571 :: Blind Loss : 7541.89443546508\n",
      "Iteration: 77\n",
      "Training :: Blind : 61.350793650793655\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7521.963271369328\n",
      "Iteration: 78\n",
      "Training :: Blind : 61.471428571428575\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7523.123699906664\n",
      "Iteration: 79\n",
      "Training :: Blind : 61.50952380952381\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7520.78079302205\n",
      "Iteration: 80\n",
      "Training :: Blind : 61.45555555555555\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7518.136914919856\n",
      "Iteration: 81\n",
      "Training :: Blind : 61.39682539682539\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7529.082428793096\n",
      "Iteration: 82\n",
      "Training :: Blind : 61.384126984126986\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7530.83879593563\n",
      "Iteration: 83\n",
      "Training :: Blind : 61.55555555555555\n",
      "Validation  :: Blind : 61.542857142857144 :: Blind Loss : 7516.878217893779\n",
      "Iteration: 84\n",
      "Training :: Blind : 61.468253968253975\n",
      "Validation  :: Blind : 61.542857142857144 :: Blind Loss : 7522.828256714327\n",
      "Iteration: 85\n",
      "Training :: Blind : 61.542857142857144\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7514.603867522747\n",
      "Iteration: 86\n",
      "Training :: Blind : 61.490476190476194\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7511.839243937435\n",
      "Iteration: 87\n",
      "Training :: Blind : 61.41746031746032\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7527.399584071852\n",
      "Iteration: 88\n",
      "Training :: Blind : 61.44603174603175\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7521.206406631287\n",
      "Iteration: 89\n",
      "Training :: Blind : 61.45396825396825\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7521.221512939568\n",
      "Iteration: 90\n",
      "Training :: Blind : 61.39047619047619\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7526.317149276461\n",
      "Iteration: 91\n",
      "Training :: Blind : 61.355555555555554\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7531.153841429813\n",
      "Iteration: 92\n",
      "Training :: Blind : 61.48571428571429\n",
      "Validation  :: Blind : 61.32857142857143 :: Blind Loss : 7511.027728522611\n",
      "Iteration: 93\n",
      "Training :: Blind : 61.469841269841275\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7516.045161089088\n",
      "Iteration: 94\n",
      "Training :: Blind : 61.56825396825397\n",
      "Validation  :: Blind : 61.57142857142858 :: Blind Loss : 7517.140080027302\n",
      "Iteration: 95\n",
      "Training :: Blind : 61.423809523809524\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7521.772834772764\n",
      "Iteration: 96\n",
      "Training :: Blind : 61.44603174603175\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7525.776506621931\n",
      "Iteration: 97\n",
      "Training :: Blind : 61.44603174603175\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7525.053811348678\n",
      "Iteration: 98\n",
      "Training :: Blind : 61.53809523809524\n",
      "Validation  :: Blind : 61.614285714285714 :: Blind Loss : 7521.370337569378\n",
      "Iteration: 99\n",
      "Training :: Blind : 61.531746031746025\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7521.4229311555055\n",
      "Iteration: 100\n",
      "Training :: Blind : 61.46031746031746\n",
      "Validation  :: Blind : 61.614285714285714 :: Blind Loss : 7522.711739259421\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.97777777777778\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 9039.034024514873\n",
      "Iteration: 2\n",
      "Training :: Blind : 58.17936507936508\n",
      "Validation  :: Blind : 58.34285714285714 :: Blind Loss : 8138.620216066275\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.61904761904761\n",
      "Validation  :: Blind : 59.285714285714285 :: Blind Loss : 7823.273588224539\n",
      "Iteration: 4\n",
      "Training :: Blind : 58.960317460317455\n",
      "Validation  :: Blind : 59.61428571428572 :: Blind Loss : 7753.0902624775645\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.720634920634915\n",
      "Validation  :: Blind : 59.457142857142856 :: Blind Loss : 7756.8508996051905\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.369841269841274\n",
      "Validation  :: Blind : 59.01428571428572 :: Blind Loss : 7785.309673816755\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.72857142857143\n",
      "Validation  :: Blind : 59.3 :: Blind Loss : 7736.90124886424\n",
      "Iteration: 8\n",
      "Training :: Blind : 59.15714285714285\n",
      "Validation  :: Blind : 59.61428571428572 :: Blind Loss : 7696.538280531027\n",
      "Iteration: 9\n",
      "Training :: Blind : 59.24761904761905\n",
      "Validation  :: Blind : 59.77142857142857 :: Blind Loss : 7686.694966582581\n",
      "Iteration: 10\n",
      "Training :: Blind : 59.198412698412696\n",
      "Validation  :: Blind : 59.61428571428572 :: Blind Loss : 7683.687091282516\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.51587301587301\n",
      "Validation  :: Blind : 59.82857142857143 :: Blind Loss : 7652.657314050458\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.885714285714286\n",
      "Validation  :: Blind : 60.17142857142858 :: Blind Loss : 7609.19204308703\n",
      "Iteration: 13\n",
      "Training :: Blind : 60.1936507936508\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7565.559200549094\n",
      "Iteration: 14\n",
      "Training :: Blind : 59.973015873015875\n",
      "Validation  :: Blind : 60.25714285714285 :: Blind Loss : 7681.864244003348\n",
      "Iteration: 15\n",
      "Training :: Blind : 60.50952380952381\n",
      "Validation  :: Blind : 60.82857142857143 :: Blind Loss : 7567.44141715767\n",
      "Iteration: 16\n",
      "Training :: Blind : 60.77142857142858\n",
      "Validation  :: Blind : 61.042857142857144 :: Blind Loss : 7522.07383066373\n",
      "Iteration: 17\n",
      "Training :: Blind : 61.165079365079364\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7450.279431213243\n",
      "Iteration: 18\n",
      "Training :: Blind : 61.05555555555555\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7429.538381694023\n",
      "Iteration: 19\n",
      "Training :: Blind : 61.09523809523809\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7433.089042902491\n",
      "Iteration: 20\n",
      "Training :: Blind : 60.884126984126986\n",
      "Validation  :: Blind : 60.9 :: Blind Loss : 7511.319882983644\n",
      "Iteration: 21\n",
      "Training :: Blind : 60.923809523809524\n",
      "Validation  :: Blind : 60.871428571428574 :: Blind Loss : 7585.571889102597\n",
      "Iteration: 22\n",
      "Training :: Blind : 61.14761904761905\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7569.714797755775\n",
      "Iteration: 23\n",
      "Training :: Blind : 61.31111111111112\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7537.790542359822\n",
      "Iteration: 24\n",
      "Training :: Blind : 61.61904761904762\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7494.8363264981\n",
      "Iteration: 25\n",
      "Training :: Blind : 61.56349206349206\n",
      "Validation  :: Blind : 61.62857142857143 :: Blind Loss : 7501.192949827502\n",
      "Iteration: 26\n",
      "Training :: Blind : 61.614285714285714\n",
      "Validation  :: Blind : 61.671428571428564 :: Blind Loss : 7497.060714164418\n",
      "Iteration: 27\n",
      "Training :: Blind : 61.51428571428571\n",
      "Validation  :: Blind : 61.614285714285714 :: Blind Loss : 7515.856506800428\n",
      "Iteration: 28\n",
      "Training :: Blind : 61.46666666666667\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7527.521973156578\n",
      "Iteration: 29\n",
      "Training :: Blind : 61.422222222222224\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7536.23606461369\n",
      "Iteration: 30\n",
      "Training :: Blind : 61.50317460317461\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7533.044702843088\n",
      "Iteration: 31\n",
      "Training :: Blind : 61.49365079365079\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7535.164357529286\n",
      "Iteration: 32\n",
      "Training :: Blind : 61.43333333333333\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7532.618502003161\n",
      "Iteration: 33\n",
      "Training :: Blind : 61.38888888888889\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7548.689649898046\n",
      "Iteration: 34\n",
      "Training :: Blind : 61.40952380952381\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7537.851426050205\n",
      "Iteration: 35\n",
      "Training :: Blind : 61.44444444444444\n",
      "Validation  :: Blind : 61.542857142857144 :: Blind Loss : 7538.415294198198\n",
      "Iteration: 36\n",
      "Training :: Blind : 61.41746031746032\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7537.912310373228\n",
      "Iteration: 37\n",
      "Training :: Blind : 61.53492063492063\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7534.084597520802\n",
      "Iteration: 38\n",
      "Training :: Blind : 61.51746031746031\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7530.768515353245\n",
      "Iteration: 39\n",
      "Training :: Blind : 61.41428571428571\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7531.983482436501\n",
      "Iteration: 40\n",
      "Training :: Blind : 61.43174603174602\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7529.10448226272\n",
      "Iteration: 41\n",
      "Training :: Blind : 61.37936507936508\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7542.30190955777\n",
      "Iteration: 42\n",
      "Training :: Blind : 61.39047619047619\n",
      "Validation  :: Blind : 61.542857142857144 :: Blind Loss : 7537.134046895104\n",
      "Iteration: 43\n",
      "Training :: Blind : 61.41587301587301\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7541.252115012014\n",
      "Iteration: 44\n",
      "Training :: Blind : 61.44126984126984\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7544.193344267393\n",
      "Iteration: 45\n",
      "Training :: Blind : 61.46507936507937\n",
      "Validation  :: Blind : 61.58571428571429 :: Blind Loss : 7539.727170606563\n",
      "Iteration: 46\n",
      "Training :: Blind : 61.41428571428571\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7540.893190220295\n",
      "Iteration: 47\n",
      "Training :: Blind : 61.41746031746032\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7532.381334501351\n",
      "Iteration: 48\n",
      "Training :: Blind : 61.31111111111112\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7546.469599321254\n",
      "Iteration: 49\n",
      "Training :: Blind : 61.50952380952381\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7540.396302079014\n",
      "Iteration: 50\n",
      "Training :: Blind : 61.423809523809524\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7541.571942294694\n",
      "Iteration: 51\n",
      "Training :: Blind : 61.47301587301587\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7538.609251045404\n",
      "Iteration: 52\n",
      "Training :: Blind : 61.39047619047619\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7540.4437762529615\n",
      "Iteration: 53\n",
      "Training :: Blind : 61.41746031746032\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7543.735455008187\n",
      "Iteration: 54\n",
      "Training :: Blind : 61.44444444444444\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7534.1702226617745\n",
      "Iteration: 55\n",
      "Training :: Blind : 61.5\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7528.802728474786\n",
      "Iteration: 56\n",
      "Training :: Blind : 61.36666666666667\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7536.820017292167\n",
      "Iteration: 57\n",
      "Training :: Blind : 61.38730158730159\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7543.027509615856\n",
      "Iteration: 58\n",
      "Training :: Blind : 61.44126984126984\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7533.901150166408\n",
      "Iteration: 59\n",
      "Training :: Blind : 61.42698412698413\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7541.215318329525\n",
      "Iteration: 60\n",
      "Training :: Blind : 61.44761904761905\n",
      "Validation  :: Blind : 61.58571428571429 :: Blind Loss : 7535.5139377409905\n",
      "Iteration: 61\n",
      "Training :: Blind : 61.36507936507937\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7540.251223071248\n",
      "Iteration: 62\n",
      "Training :: Blind : 61.371428571428574\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7537.372505833734\n",
      "Iteration: 63\n",
      "Training :: Blind : 61.45555555555555\n",
      "Validation  :: Blind : 61.542857142857144 :: Blind Loss : 7542.7555578936945\n",
      "Iteration: 64\n",
      "Training :: Blind : 61.47460317460317\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7535.5828026173\n",
      "Iteration: 65\n",
      "Training :: Blind : 61.46031746031746\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7540.7293084144285\n",
      "Iteration: 66\n",
      "Training :: Blind : 61.468253968253975\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7533.816785227569\n",
      "Iteration: 67\n",
      "Training :: Blind : 61.48730158730159\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7530.507960651457\n",
      "Iteration: 68\n",
      "Training :: Blind : 61.49365079365079\n",
      "Validation  :: Blind : 61.62857142857143 :: Blind Loss : 7527.005524323857\n",
      "Iteration: 69\n",
      "Training :: Blind : 61.385714285714286\n",
      "Validation  :: Blind : 61.471428571428575 :: Blind Loss : 7539.315264083802\n",
      "Iteration: 70\n",
      "Training :: Blind : 61.50952380952381\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7534.649249501275\n",
      "Iteration: 71\n",
      "Training :: Blind : 61.44444444444444\n",
      "Validation  :: Blind : 61.542857142857144 :: Blind Loss : 7534.033848697253\n",
      "Iteration: 72\n",
      "Training :: Blind : 61.43333333333333\n",
      "Validation  :: Blind : 61.6 :: Blind Loss : 7536.722100864943\n",
      "Iteration: 73\n",
      "Training :: Blind : 61.50476190476191\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7533.694652347494\n",
      "Iteration: 74\n",
      "Training :: Blind : 61.5015873015873\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7529.39113702141\n",
      "Iteration: 75\n",
      "Training :: Blind : 61.44920634920636\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7537.228050217518\n",
      "Iteration: 76\n",
      "Training :: Blind : 61.48730158730159\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7533.262069807424\n",
      "Iteration: 77\n",
      "Training :: Blind : 61.450793650793656\n",
      "Validation  :: Blind : 61.614285714285714 :: Blind Loss : 7533.494653050402\n",
      "Iteration: 78\n",
      "Training :: Blind : 61.46349206349206\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7531.033385158418\n",
      "Iteration: 79\n",
      "Training :: Blind : 61.439682539682536\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7539.780478078901\n",
      "Iteration: 80\n",
      "Training :: Blind : 61.368253968253974\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7543.77053778307\n",
      "Iteration: 81\n",
      "Training :: Blind : 61.46507936507937\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7534.437704102568\n",
      "Iteration: 82\n",
      "Training :: Blind : 61.468253968253975\n",
      "Validation  :: Blind : 61.6 :: Blind Loss : 7534.944893169995\n",
      "Iteration: 83\n",
      "Training :: Blind : 61.39682539682539\n",
      "Validation  :: Blind : 61.44285714285714 :: Blind Loss : 7539.2058853458875\n",
      "Iteration: 84\n",
      "Training :: Blind : 61.37460317460317\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7542.8901966102785\n",
      "Iteration: 85\n",
      "Training :: Blind : 61.48095238095238\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7527.235166495664\n",
      "Iteration: 86\n",
      "Training :: Blind : 61.439682539682536\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7538.459161321145\n",
      "Iteration: 87\n",
      "Training :: Blind : 61.54761904761905\n",
      "Validation  :: Blind : 61.51428571428571 :: Blind Loss : 7525.69099716498\n",
      "Iteration: 88\n",
      "Training :: Blind : 61.43174603174602\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7541.989705469981\n",
      "Iteration: 89\n",
      "Training :: Blind : 61.43333333333333\n",
      "Validation  :: Blind : 61.55714285714286 :: Blind Loss : 7536.6109104807965\n",
      "Iteration: 90\n",
      "Training :: Blind : 61.48571428571429\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7532.366892893753\n",
      "Iteration: 91\n",
      "Training :: Blind : 61.43809523809524\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7541.131592324741\n",
      "Iteration: 92\n",
      "Training :: Blind : 61.458730158730155\n",
      "Validation  :: Blind : 61.4 :: Blind Loss : 7538.442976404597\n",
      "Iteration: 93\n",
      "Training :: Blind : 61.531746031746025\n",
      "Validation  :: Blind : 61.57142857142858 :: Blind Loss : 7517.427746288786\n",
      "Iteration: 94\n",
      "Training :: Blind : 61.38730158730159\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7538.362123566994\n",
      "Iteration: 95\n",
      "Training :: Blind : 61.373015873015866\n",
      "Validation  :: Blind : 61.31428571428571 :: Blind Loss : 7538.919289080612\n",
      "Iteration: 96\n",
      "Training :: Blind : 61.39365079365079\n",
      "Validation  :: Blind : 61.48571428571429 :: Blind Loss : 7538.922935157571\n",
      "Iteration: 97\n",
      "Training :: Blind : 61.469841269841275\n",
      "Validation  :: Blind : 61.57142857142858 :: Blind Loss : 7522.639429836626\n",
      "Iteration: 98\n",
      "Training :: Blind : 61.477777777777774\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7540.273121273842\n",
      "Iteration: 99\n",
      "Training :: Blind : 61.34126984126984\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7545.714966390188\n",
      "Iteration: 100\n",
      "Training :: Blind : 61.331746031746036\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7545.088303044728\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.9984126984127\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 9033.971507730592\n",
      "Iteration: 2\n",
      "Training :: Blind : 58.166666666666664\n",
      "Validation  :: Blind : 58.214285714285715 :: Blind Loss : 8145.441580745159\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.358730158730154\n",
      "Validation  :: Blind : 59.07142857142858 :: Blind Loss : 7845.4809523452495\n",
      "Iteration: 4\n",
      "Training :: Blind : 59.04126984126984\n",
      "Validation  :: Blind : 59.62857142857143 :: Blind Loss : 7740.002497411326\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.77619047619047\n",
      "Validation  :: Blind : 59.55714285714285 :: Blind Loss : 7746.092805082246\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.63015873015873\n",
      "Validation  :: Blind : 59.52857142857143 :: Blind Loss : 7752.065300294858\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.820634920634916\n",
      "Validation  :: Blind : 59.48571428571429 :: Blind Loss : 7726.246693462566\n",
      "Iteration: 8\n",
      "Training :: Blind : 58.93492063492064\n",
      "Validation  :: Blind : 59.52857142857143 :: Blind Loss : 7719.283904007658\n",
      "Iteration: 9\n",
      "Training :: Blind : 59.32539682539682\n",
      "Validation  :: Blind : 59.75714285714285 :: Blind Loss : 7674.170572141646\n",
      "Iteration: 10\n",
      "Training :: Blind : 58.94761904761905\n",
      "Validation  :: Blind : 59.3 :: Blind Loss : 7685.4118588484835\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.51904761904762\n",
      "Validation  :: Blind : 59.8 :: Blind Loss : 7648.693751258334\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.665079365079364\n",
      "Validation  :: Blind : 60.02857142857143 :: Blind Loss : 7634.561904558857\n",
      "Iteration: 13\n",
      "Training :: Blind : 60.05555555555555\n",
      "Validation  :: Blind : 60.199999999999996 :: Blind Loss : 7587.765814327176\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.096825396825395\n",
      "Validation  :: Blind : 60.32857142857143 :: Blind Loss : 7575.347958976951\n",
      "Iteration: 15\n",
      "Training :: Blind : 60.08571428571429\n",
      "Validation  :: Blind : 60.17142857142858 :: Blind Loss : 7569.251862693203\n",
      "Iteration: 16\n",
      "Training :: Blind : 59.96666666666667\n",
      "Validation  :: Blind : 60.21428571428571 :: Blind Loss : 7560.644123140699\n",
      "Iteration: 17\n",
      "Training :: Blind : 60.17936507936508\n",
      "Validation  :: Blind : 60.285714285714285 :: Blind Loss : 7532.451351506546\n",
      "Iteration: 18\n",
      "Training :: Blind : 59.887301587301586\n",
      "Validation  :: Blind : 60.08571428571429 :: Blind Loss : 7548.672586719582\n",
      "Iteration: 19\n",
      "Training :: Blind : 60.09523809523809\n",
      "Validation  :: Blind : 60.24285714285714 :: Blind Loss : 7509.950967046481\n",
      "Iteration: 20\n",
      "Training :: Blind : 60.16190476190476\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7506.0239224417255\n",
      "Iteration: 21\n",
      "Training :: Blind : 60.27936507936508\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7490.578146347474\n",
      "Iteration: 22\n",
      "Training :: Blind : 60.542857142857144\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7477.4613685215445\n",
      "Iteration: 23\n",
      "Training :: Blind : 60.22698412698413\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7487.988864170459\n",
      "Iteration: 24\n",
      "Training :: Blind : 60.22222222222222\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7507.555267784017\n",
      "Iteration: 25\n",
      "Training :: Blind : 60.44285714285714\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7484.492320838094\n",
      "Iteration: 26\n",
      "Training :: Blind : 60.4015873015873\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7478.172001418193\n",
      "Iteration: 27\n",
      "Training :: Blind : 60.46666666666667\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7479.385608284855\n",
      "Iteration: 28\n",
      "Training :: Blind : 60.50952380952381\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7473.519828031405\n",
      "Iteration: 29\n",
      "Training :: Blind : 60.51587301587301\n",
      "Validation  :: Blind : 60.71428571428571 :: Blind Loss : 7474.076810412733\n",
      "Iteration: 30\n",
      "Training :: Blind : 60.355555555555554\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7477.570644161724\n",
      "Iteration: 31\n",
      "Training :: Blind : 60.330158730158736\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7483.945360543954\n",
      "Iteration: 32\n",
      "Training :: Blind : 60.577777777777776\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7477.448526519174\n",
      "Iteration: 33\n",
      "Training :: Blind : 60.38095238095238\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7491.858236809\n",
      "Iteration: 34\n",
      "Training :: Blind : 60.46666666666667\n",
      "Validation  :: Blind : 60.74285714285714 :: Blind Loss : 7477.847504393943\n",
      "Iteration: 35\n",
      "Training :: Blind : 60.29682539682539\n",
      "Validation  :: Blind : 60.34285714285714 :: Blind Loss : 7484.806315744675\n",
      "Iteration: 36\n",
      "Training :: Blind : 60.304761904761904\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7488.739618497493\n",
      "Iteration: 37\n",
      "Training :: Blind : 60.43650793650793\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7478.469322367539\n",
      "Iteration: 38\n",
      "Training :: Blind : 60.44285714285714\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7489.649871866849\n",
      "Iteration: 39\n",
      "Training :: Blind : 60.357142857142854\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7483.803234734161\n",
      "Iteration: 40\n",
      "Training :: Blind : 60.50793650793651\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7470.1480576146705\n",
      "Iteration: 41\n",
      "Training :: Blind : 60.236507936507934\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7489.856019752942\n",
      "Iteration: 42\n",
      "Training :: Blind : 60.53174603174604\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7470.569125255677\n",
      "Iteration: 43\n",
      "Training :: Blind : 60.492063492063494\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7474.312607224481\n",
      "Iteration: 44\n",
      "Training :: Blind : 60.52063492063492\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7475.674324199235\n",
      "Iteration: 45\n",
      "Training :: Blind : 60.27460317460317\n",
      "Validation  :: Blind : 60.31428571428571 :: Blind Loss : 7495.04112179223\n",
      "Iteration: 46\n",
      "Training :: Blind : 60.42698412698413\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7473.618732394614\n",
      "Iteration: 47\n",
      "Training :: Blind : 60.506349206349206\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7481.134188085276\n",
      "Iteration: 48\n",
      "Training :: Blind : 60.28095238095238\n",
      "Validation  :: Blind : 60.48571428571429 :: Blind Loss : 7497.812794935501\n",
      "Iteration: 49\n",
      "Training :: Blind : 60.41269841269842\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7484.099212196927\n",
      "Iteration: 50\n",
      "Training :: Blind : 60.42063492063492\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7475.898769939371\n",
      "Iteration: 51\n",
      "Training :: Blind : 60.253968253968246\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7491.471385486511\n",
      "Iteration: 52\n",
      "Training :: Blind : 60.28095238095238\n",
      "Validation  :: Blind : 60.41428571428571 :: Blind Loss : 7489.6608253092345\n",
      "Iteration: 53\n",
      "Training :: Blind : 60.13968253968254\n",
      "Validation  :: Blind : 60.3 :: Blind Loss : 7502.332057896732\n",
      "Iteration: 54\n",
      "Training :: Blind : 60.51428571428571\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7470.942799030701\n",
      "Iteration: 55\n",
      "Training :: Blind : 60.15396825396825\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7511.920506970685\n",
      "Iteration: 56\n",
      "Training :: Blind : 60.44761904761905\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7479.647639260411\n",
      "Iteration: 57\n",
      "Training :: Blind : 60.32698412698413\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7495.139235066421\n",
      "Iteration: 58\n",
      "Training :: Blind : 60.542857142857144\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7471.226424430527\n",
      "Iteration: 59\n",
      "Training :: Blind : 60.43492063492063\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7477.488480532888\n",
      "Iteration: 60\n",
      "Training :: Blind : 60.51428571428571\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7472.971080205775\n",
      "Iteration: 61\n",
      "Training :: Blind : 60.38095238095238\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7478.806097825487\n",
      "Iteration: 62\n",
      "Training :: Blind : 60.48412698412699\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7479.069378002613\n",
      "Iteration: 63\n",
      "Training :: Blind : 60.34126984126984\n",
      "Validation  :: Blind : 60.371428571428574 :: Blind Loss : 7482.417375128547\n",
      "Iteration: 64\n",
      "Training :: Blind : 60.5\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7480.422621602349\n",
      "Iteration: 65\n",
      "Training :: Blind : 60.47460317460317\n",
      "Validation  :: Blind : 60.68571428571429 :: Blind Loss : 7478.502734632127\n",
      "Iteration: 66\n",
      "Training :: Blind : 60.460317460317455\n",
      "Validation  :: Blind : 60.471428571428575 :: Blind Loss : 7474.863772405257\n",
      "Iteration: 67\n",
      "Training :: Blind : 60.5015873015873\n",
      "Validation  :: Blind : 60.699999999999996 :: Blind Loss : 7467.299195691073\n",
      "Iteration: 68\n",
      "Training :: Blind : 60.592063492063495\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7474.015471503077\n",
      "Iteration: 69\n",
      "Training :: Blind : 60.368253968253974\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7491.820909231468\n",
      "Iteration: 70\n",
      "Training :: Blind : 60.41587301587301\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7477.298380439611\n",
      "Iteration: 71\n",
      "Training :: Blind : 60.38888888888889\n",
      "Validation  :: Blind : 60.55714285714286 :: Blind Loss : 7479.010729459434\n",
      "Iteration: 72\n",
      "Training :: Blind : 60.331746031746036\n",
      "Validation  :: Blind : 60.4 :: Blind Loss : 7490.384934108298\n",
      "Iteration: 73\n",
      "Training :: Blind : 60.34603174603175\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7489.134655012771\n",
      "Iteration: 74\n",
      "Training :: Blind : 60.5015873015873\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7475.709259743166\n",
      "Iteration: 75\n",
      "Training :: Blind : 60.45396825396825\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7474.760258970155\n",
      "Iteration: 76\n",
      "Training :: Blind : 60.31111111111112\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7483.825625863643\n",
      "Iteration: 77\n",
      "Training :: Blind : 60.576190476190476\n",
      "Validation  :: Blind : 60.6 :: Blind Loss : 7467.819382286127\n",
      "Iteration: 78\n",
      "Training :: Blind : 60.385714285714286\n",
      "Validation  :: Blind : 60.5 :: Blind Loss : 7486.650061486025\n",
      "Iteration: 79\n",
      "Training :: Blind : 60.56825396825397\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7469.188787044786\n",
      "Iteration: 80\n",
      "Training :: Blind : 60.403174603174605\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7478.368348273333\n",
      "Iteration: 81\n",
      "Training :: Blind : 60.52222222222222\n",
      "Validation  :: Blind : 60.58571428571429 :: Blind Loss : 7469.656213427935\n",
      "Iteration: 82\n",
      "Training :: Blind : 60.37301587301588\n",
      "Validation  :: Blind : 60.52857142857143 :: Blind Loss : 7489.508604153088\n",
      "Iteration: 83\n",
      "Training :: Blind : 60.51111111111111\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7471.740639562962\n",
      "Iteration: 84\n",
      "Training :: Blind : 60.41269841269842\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7480.782872561864\n",
      "Iteration: 85\n",
      "Training :: Blind : 60.38888888888889\n",
      "Validation  :: Blind : 60.57142857142858 :: Blind Loss : 7485.671689068966\n",
      "Iteration: 86\n",
      "Training :: Blind : 60.36190476190476\n",
      "Validation  :: Blind : 60.285714285714285 :: Blind Loss : 7471.123978641594\n",
      "Iteration: 87\n",
      "Training :: Blind : 60.43492063492063\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7475.6947104868395\n",
      "Iteration: 88\n",
      "Training :: Blind : 60.333333333333336\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7487.107671166675\n",
      "Iteration: 89\n",
      "Training :: Blind : 60.425396825396824\n",
      "Validation  :: Blind : 60.44285714285714 :: Blind Loss : 7479.2223329586595\n",
      "Iteration: 90\n",
      "Training :: Blind : 60.385714285714286\n",
      "Validation  :: Blind : 60.51428571428571 :: Blind Loss : 7484.377743109628\n",
      "Iteration: 91\n",
      "Training :: Blind : 60.56666666666667\n",
      "Validation  :: Blind : 60.385714285714286 :: Blind Loss : 7470.420962573269\n",
      "Iteration: 92\n",
      "Training :: Blind : 60.26031746031746\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7495.43004890476\n",
      "Iteration: 93\n",
      "Training :: Blind : 60.55555555555555\n",
      "Validation  :: Blind : 60.642857142857146 :: Blind Loss : 7477.474300056323\n",
      "Iteration: 94\n",
      "Training :: Blind : 60.404761904761905\n",
      "Validation  :: Blind : 60.61428571428571 :: Blind Loss : 7479.030887907213\n",
      "Iteration: 95\n",
      "Training :: Blind : 60.406349206349205\n",
      "Validation  :: Blind : 60.542857142857144 :: Blind Loss : 7479.275666918205\n",
      "Iteration: 96\n",
      "Training :: Blind : 60.609523809523814\n",
      "Validation  :: Blind : 60.62857142857143 :: Blind Loss : 7465.511769290066\n",
      "Iteration: 97\n",
      "Training :: Blind : 60.47301587301588\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7462.731646390865\n",
      "Iteration: 98\n",
      "Training :: Blind : 60.492063492063494\n",
      "Validation  :: Blind : 60.71428571428571 :: Blind Loss : 7478.763573268539\n",
      "Iteration: 99\n",
      "Training :: Blind : 60.46349206349206\n",
      "Validation  :: Blind : 60.67142857142858 :: Blind Loss : 7478.282373560602\n",
      "Iteration: 100\n",
      "Training :: Blind : 60.357142857142854\n",
      "Validation  :: Blind : 60.457142857142856 :: Blind Loss : 7471.370455878112\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 52.98571428571428\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 9025.102135733043\n",
      "Iteration: 2\n",
      "Training :: Blind : 58.12539682539683\n",
      "Validation  :: Blind : 58.4 :: Blind Loss : 8142.630078813778\n",
      "Iteration: 3\n",
      "Training :: Blind : 58.56031746031746\n",
      "Validation  :: Blind : 59.18571428571428 :: Blind Loss : 7831.58427211157\n",
      "Iteration: 4\n",
      "Training :: Blind : 58.82222222222222\n",
      "Validation  :: Blind : 59.357142857142854 :: Blind Loss : 7754.658234508883\n",
      "Iteration: 5\n",
      "Training :: Blind : 58.4936507936508\n",
      "Validation  :: Blind : 59.17142857142858 :: Blind Loss : 7777.486279363467\n",
      "Iteration: 6\n",
      "Training :: Blind : 58.8952380952381\n",
      "Validation  :: Blind : 59.64285714285714 :: Blind Loss : 7740.1335879596545\n",
      "Iteration: 7\n",
      "Training :: Blind : 58.73968253968253\n",
      "Validation  :: Blind : 59.42857142857143 :: Blind Loss : 7742.1563556262845\n",
      "Iteration: 8\n",
      "Training :: Blind : 59.112698412698414\n",
      "Validation  :: Blind : 59.5 :: Blind Loss : 7701.575842239116\n",
      "Iteration: 9\n",
      "Training :: Blind : 58.815873015873024\n",
      "Validation  :: Blind : 59.371428571428574 :: Blind Loss : 7698.725462377862\n",
      "Iteration: 10\n",
      "Training :: Blind : 59.13174603174603\n",
      "Validation  :: Blind : 59.42857142857143 :: Blind Loss : 7687.899868678802\n",
      "Iteration: 11\n",
      "Training :: Blind : 59.72380952380952\n",
      "Validation  :: Blind : 60.02857142857143 :: Blind Loss : 7646.064049240345\n",
      "Iteration: 12\n",
      "Training :: Blind : 59.871428571428574\n",
      "Validation  :: Blind : 60.08571428571429 :: Blind Loss : 7678.542669691679\n",
      "Iteration: 13\n",
      "Training :: Blind : 60.37936507936508\n",
      "Validation  :: Blind : 60.71428571428571 :: Blind Loss : 7579.532236771948\n",
      "Iteration: 14\n",
      "Training :: Blind : 60.66825396825397\n",
      "Validation  :: Blind : 61.21428571428571 :: Blind Loss : 7519.166533543506\n",
      "Iteration: 15\n",
      "Training :: Blind : 60.868253968253974\n",
      "Validation  :: Blind : 61.0 :: Blind Loss : 7455.52926221859\n",
      "Iteration: 16\n",
      "Training :: Blind : 61.03968253968254\n",
      "Validation  :: Blind : 61.12857142857143 :: Blind Loss : 7432.137436408351\n",
      "Iteration: 17\n",
      "Training :: Blind : 60.91428571428571\n",
      "Validation  :: Blind : 60.94285714285714 :: Blind Loss : 7484.57512531326\n",
      "Iteration: 18\n",
      "Training :: Blind : 60.86349206349206\n",
      "Validation  :: Blind : 60.65714285714285 :: Blind Loss : 7585.926283196618\n",
      "Iteration: 19\n",
      "Training :: Blind : 61.16984126984127\n",
      "Validation  :: Blind : 61.142857142857146 :: Blind Loss : 7565.621785830025\n",
      "Iteration: 20\n",
      "Training :: Blind : 61.24285714285714\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7544.974302337643\n",
      "Iteration: 21\n",
      "Training :: Blind : 61.469841269841275\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7503.898197915656\n",
      "Iteration: 22\n",
      "Training :: Blind : 61.50952380952381\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7503.445105740411\n",
      "Iteration: 23\n",
      "Training :: Blind : 61.43809523809524\n",
      "Validation  :: Blind : 61.357142857142854 :: Blind Loss : 7515.917307743723\n",
      "Iteration: 24\n",
      "Training :: Blind : 61.457142857142856\n",
      "Validation  :: Blind : 61.42857142857143 :: Blind Loss : 7513.099205693299\n",
      "Iteration: 25\n",
      "Training :: Blind : 61.577777777777776\n",
      "Validation  :: Blind : 61.457142857142856 :: Blind Loss : 7493.3354311518415\n",
      "Iteration: 26\n",
      "Training :: Blind : 61.45555555555555\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7509.60448261606\n",
      "Iteration: 27\n",
      "Training :: Blind : 61.49206349206349\n",
      "Validation  :: Blind : 61.5 :: Blind Loss : 7503.046560859191\n",
      "Iteration: 28\n",
      "Training :: Blind : 61.36507936507937\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7512.4508013471695\n",
      "Iteration: 29\n",
      "Training :: Blind : 61.22539682539683\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7523.998169696335\n",
      "Iteration: 30\n",
      "Training :: Blind : 61.319047619047616\n",
      "Validation  :: Blind : 61.52857142857143 :: Blind Loss : 7515.491185792584\n",
      "Iteration: 31\n",
      "Training :: Blind : 61.27936507936508\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7524.098912098139\n",
      "Iteration: 32\n",
      "Training :: Blind : 61.1920634920635\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7537.819458028349\n",
      "Iteration: 33\n",
      "Training :: Blind : 61.3\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7523.550375796256\n",
      "Iteration: 34\n",
      "Training :: Blind : 61.31111111111112\n",
      "Validation  :: Blind : 61.32857142857143 :: Blind Loss : 7526.1313760927715\n",
      "Iteration: 35\n",
      "Training :: Blind : 61.24285714285714\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7527.300362184226\n",
      "Iteration: 36\n",
      "Training :: Blind : 61.26190476190476\n",
      "Validation  :: Blind : 61.21428571428571 :: Blind Loss : 7526.322623244967\n",
      "Iteration: 37\n",
      "Training :: Blind : 61.20476190476191\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7528.572313509912\n",
      "Iteration: 38\n",
      "Training :: Blind : 61.28095238095238\n",
      "Validation  :: Blind : 61.15714285714285 :: Blind Loss : 7527.307668939486\n",
      "Iteration: 39\n",
      "Training :: Blind : 61.20634920634921\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7532.665248438784\n",
      "Iteration: 40\n",
      "Training :: Blind : 61.107936507936515\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7542.072362588596\n",
      "Iteration: 41\n",
      "Training :: Blind : 61.142857142857146\n",
      "Validation  :: Blind : 61.1 :: Blind Loss : 7540.509682651782\n",
      "Iteration: 42\n",
      "Training :: Blind : 61.08253968253968\n",
      "Validation  :: Blind : 61.08571428571429 :: Blind Loss : 7538.859915977297\n",
      "Iteration: 43\n",
      "Training :: Blind : 61.21269841269841\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7530.11557273582\n",
      "Iteration: 44\n",
      "Training :: Blind : 61.27936507936508\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7524.942895554483\n",
      "Iteration: 45\n",
      "Training :: Blind : 61.32222222222222\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7522.58407274028\n",
      "Iteration: 46\n",
      "Training :: Blind : 61.247619047619054\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7535.216565317333\n",
      "Iteration: 47\n",
      "Training :: Blind : 61.24603174603175\n",
      "Validation  :: Blind : 61.08571428571429 :: Blind Loss : 7530.990653374525\n",
      "Iteration: 48\n",
      "Training :: Blind : 61.266666666666666\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7526.950763252318\n",
      "Iteration: 49\n",
      "Training :: Blind : 61.20476190476191\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7535.209149558319\n",
      "Iteration: 50\n",
      "Training :: Blind : 61.22539682539683\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7518.269681327099\n",
      "Iteration: 51\n",
      "Training :: Blind : 61.217460317460315\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7534.1287744184065\n",
      "Iteration: 52\n",
      "Training :: Blind : 61.17460317460317\n",
      "Validation  :: Blind : 61.1 :: Blind Loss : 7532.692359833842\n",
      "Iteration: 53\n",
      "Training :: Blind : 61.25873015873016\n",
      "Validation  :: Blind : 61.142857142857146 :: Blind Loss : 7522.893345135552\n",
      "Iteration: 54\n",
      "Training :: Blind : 61.196825396825396\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7532.772090975486\n",
      "Iteration: 55\n",
      "Training :: Blind : 61.144444444444446\n",
      "Validation  :: Blind : 61.199999999999996 :: Blind Loss : 7538.875412397678\n",
      "Iteration: 56\n",
      "Training :: Blind : 61.265079365079366\n",
      "Validation  :: Blind : 61.15714285714285 :: Blind Loss : 7535.499720952537\n",
      "Iteration: 57\n",
      "Training :: Blind : 61.273015873015865\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7527.075584590221\n",
      "Iteration: 58\n",
      "Training :: Blind : 61.163492063492065\n",
      "Validation  :: Blind : 61.12857142857143 :: Blind Loss : 7541.952423788307\n",
      "Iteration: 59\n",
      "Training :: Blind : 61.12698412698413\n",
      "Validation  :: Blind : 61.0 :: Blind Loss : 7538.3467587619025\n",
      "Iteration: 60\n",
      "Training :: Blind : 61.28730158730159\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7517.736481654199\n",
      "Iteration: 61\n",
      "Training :: Blind : 61.18888888888889\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7533.794927433455\n",
      "Iteration: 62\n",
      "Training :: Blind : 61.25873015873016\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7529.476377615179\n",
      "Iteration: 63\n",
      "Training :: Blind : 61.24444444444445\n",
      "Validation  :: Blind : 61.25714285714285 :: Blind Loss : 7524.231852021089\n",
      "Iteration: 64\n",
      "Training :: Blind : 61.2015873015873\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7532.555140458595\n",
      "Iteration: 65\n",
      "Training :: Blind : 61.196825396825396\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7527.3139972903155\n",
      "Iteration: 66\n",
      "Training :: Blind : 61.28095238095238\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7519.245172317492\n",
      "Iteration: 67\n",
      "Training :: Blind : 61.17301587301587\n",
      "Validation  :: Blind : 61.385714285714286 :: Blind Loss : 7526.913867074406\n",
      "Iteration: 68\n",
      "Training :: Blind : 61.249206349206354\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7518.919579965388\n",
      "Iteration: 69\n",
      "Training :: Blind : 61.199999999999996\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7527.148034258185\n",
      "Iteration: 70\n",
      "Training :: Blind : 61.14126984126984\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7535.402764054379\n",
      "Iteration: 71\n",
      "Training :: Blind : 61.27142857142858\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7529.1954312274065\n",
      "Iteration: 72\n",
      "Training :: Blind : 61.215873015873015\n",
      "Validation  :: Blind : 61.41428571428571 :: Blind Loss : 7535.562979592735\n",
      "Iteration: 73\n",
      "Training :: Blind : 61.14761904761905\n",
      "Validation  :: Blind : 61.371428571428574 :: Blind Loss : 7531.276306133783\n",
      "Iteration: 74\n",
      "Training :: Blind : 61.16031746031746\n",
      "Validation  :: Blind : 61.25714285714285 :: Blind Loss : 7520.038723042813\n",
      "Iteration: 75\n",
      "Training :: Blind : 61.196825396825396\n",
      "Validation  :: Blind : 61.31428571428571 :: Blind Loss : 7526.990777406024\n",
      "Iteration: 76\n",
      "Training :: Blind : 61.18412698412698\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7528.629659353761\n",
      "Iteration: 77\n",
      "Training :: Blind : 61.11746031746031\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7533.808298097967\n",
      "Iteration: 78\n",
      "Training :: Blind : 61.17142857142858\n",
      "Validation  :: Blind : 61.15714285714285 :: Blind Loss : 7530.33464005531\n",
      "Iteration: 79\n",
      "Training :: Blind : 61.13650793650793\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7529.919295298682\n",
      "Iteration: 80\n",
      "Training :: Blind : 61.092063492063495\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7529.944412957971\n",
      "Iteration: 81\n",
      "Training :: Blind : 61.08730158730159\n",
      "Validation  :: Blind : 61.18571428571429 :: Blind Loss : 7526.971222144968\n",
      "Iteration: 82\n",
      "Training :: Blind : 61.2015873015873\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7531.029291465526\n",
      "Iteration: 83\n",
      "Training :: Blind : 61.13650793650793\n",
      "Validation  :: Blind : 61.285714285714285 :: Blind Loss : 7527.02059557619\n",
      "Iteration: 84\n",
      "Training :: Blind : 61.10634920634921\n",
      "Validation  :: Blind : 61.042857142857144 :: Blind Loss : 7530.522796424381\n",
      "Iteration: 85\n",
      "Training :: Blind : 61.17619047619047\n",
      "Validation  :: Blind : 61.31428571428571 :: Blind Loss : 7524.0664375556435\n",
      "Iteration: 86\n",
      "Training :: Blind : 61.12222222222222\n",
      "Validation  :: Blind : 61.24285714285714 :: Blind Loss : 7528.077405580818\n",
      "Iteration: 87\n",
      "Training :: Blind : 61.161904761904765\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7523.814197753754\n",
      "Iteration: 88\n",
      "Training :: Blind : 61.03809523809524\n",
      "Validation  :: Blind : 61.042857142857144 :: Blind Loss : 7537.357331938852\n",
      "Iteration: 89\n",
      "Training :: Blind : 61.16666666666667\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7529.578649961279\n",
      "Iteration: 90\n",
      "Training :: Blind : 61.115873015873014\n",
      "Validation  :: Blind : 61.31428571428571 :: Blind Loss : 7526.495536127305\n",
      "Iteration: 91\n",
      "Training :: Blind : 61.13015873015873\n",
      "Validation  :: Blind : 61.17142857142858 :: Blind Loss : 7533.294281187056\n",
      "Iteration: 92\n",
      "Training :: Blind : 61.107936507936515\n",
      "Validation  :: Blind : 61.34285714285714 :: Blind Loss : 7533.141197960105\n",
      "Iteration: 93\n",
      "Training :: Blind : 61.12539682539683\n",
      "Validation  :: Blind : 61.11428571428571 :: Blind Loss : 7531.674920176976\n",
      "Iteration: 94\n",
      "Training :: Blind : 61.18571428571429\n",
      "Validation  :: Blind : 61.25714285714285 :: Blind Loss : 7522.656318425731\n",
      "Iteration: 95\n",
      "Training :: Blind : 61.247619047619054\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7521.594078955395\n",
      "Iteration: 96\n",
      "Training :: Blind : 61.09523809523809\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7530.968759019878\n",
      "Iteration: 97\n",
      "Training :: Blind : 61.15873015873016\n",
      "Validation  :: Blind : 61.27142857142858 :: Blind Loss : 7524.123727170072\n",
      "Iteration: 98\n",
      "Training :: Blind : 61.11746031746031\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7525.420533292947\n",
      "Iteration: 99\n",
      "Training :: Blind : 61.15873015873016\n",
      "Validation  :: Blind : 61.3 :: Blind Loss : 7535.463978145426\n",
      "Iteration: 100\n",
      "Training :: Blind : 61.11428571428571\n",
      "Validation  :: Blind : 61.228571428571435 :: Blind Loss : 7528.921765939598\n"
     ]
    }
   ],
   "source": [
    "trainAccAsym = []\n",
    "valAccAsym = []\n",
    "trainLossAsym = []\n",
    "valLossAsym = []\n",
    "Asym = [100, 60, 50 ,30, 10, 5, 3, 1]\n",
    "for asym in Asym:\n",
    "    W1, b1, W2, b2, train_acc_quanttemp, val_acc_quanttemp, train_losstemp, val_losstemp, sum_weights = batch_grad_descent_with_var(x_train,y_train,batchsize,iter, lr, midLayerSize, mu, sigma, vDD, precision, step, discreteSteps, wRange, asym, onoff,seed = 47,print_op=1)\n",
    "    trainAccAsym.append(train_acc_quanttemp)\n",
    "    valAccAsym.append(val_acc_quanttemp)\n",
    "    trainLossAsym.append(train_losstemp)\n",
    "    valLossAsym.append(val_losstemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Dip in accuracy due to assymetry in the case of BP(12 bit arch, var = 0.001)')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "finAccforPlot = []\n",
    "for i in trainAccAsym:\n",
    "    finAccforPlot.append(i[-1])\n",
    "plt.plot(Asym, finAccforPlot, 'r.', label = \"Accuracies with assymetry\")\n",
    "plt.axhline(train_acc_quant[-1], label = \"Symmetrical circuit\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"Assymetry factor ($\\sigma_{asym} = \\frac{\\sigma}{factor} $)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Dip in accuracy due to assymetry in the case of BP(12 bit arch, var = 0.001)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.12698412698413\n",
      "Validation  :: Blind : 48.5 :: Blind Loss : 10521.294039489092\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.21269841269841\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 9577.887197802338\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.75714285714286\n",
      "Validation  :: Blind : 55.285714285714285 :: Blind Loss : 9192.230168858734\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.74761904761905\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8930.159920995946\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.582539682539675\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8728.061408209818\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.96825396825397\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8634.330740703728\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.18095238095238\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8576.600805842301\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.33809523809524\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8539.64358201489\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.322222222222216\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8558.169989871363\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.17142857142857\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8553.386593707066\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.17619047619048\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8574.670365458776\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.07142857142857\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8577.675326486722\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.044444444444444\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8590.005967485013\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.2031746031746\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8557.75634849114\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.3\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8534.00898184648\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.34761904761905\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8534.10121633385\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8543.649830454462\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.34285714285714\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8545.274491825394\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8532.658291836313\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.350793650793655\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8542.107302521115\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.24761904761905\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8551.112865473882\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.33650793650794\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8542.529278083599\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8543.332330776095\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8540.345897704434\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8545.638700951898\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.35555555555556\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8536.754986594668\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8536.233906846644\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8534.516004979247\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.353968253968254\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8543.205054973674\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8539.606779480026\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8537.095059783798\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8530.991952776974\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.30952380952381\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8536.014123422785\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8542.26530044862\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.29206349206349\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8535.216528327088\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8551.09409157427\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8530.989107884761\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8540.025555155526\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8538.105423591973\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.287301587301585\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8548.764512541877\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8548.002619368392\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8545.970029292519\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.36507936507936\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8544.285491097278\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8532.16326079215\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.35555555555556\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8538.638596270332\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.353968253968254\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8537.100833302986\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.350793650793655\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8538.214936344582\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8542.388419644503\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.37460317460318\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8537.310805029625\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8528.580079022919\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8537.595278018503\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8528.574996110152\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.26349206349206\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8534.797018143343\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8524.556723703012\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8541.567274283188\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8534.011683995659\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8537.874328924914\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8528.059605719123\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8533.981559895721\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8523.984865898188\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8528.109592876117\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8524.621395067768\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.25714285714286\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8546.597419919111\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8533.046896273536\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8539.37958221821\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.284126984126985\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8537.976682955405\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8537.400374753\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8535.417817952588\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.4015873015873\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8537.170993686956\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8527.957302024868\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.26031746031745\n",
      "Validation  :: Blind : 57.08571428571428 :: Blind Loss : 8545.701705443196\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8526.156250284588\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8532.006530321367\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.34285714285714\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8531.15471600444\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.32380952380952\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8530.86882042074\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8529.72960102416\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8531.600031601203\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8538.585128760555\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.4031746031746\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8531.304792759576\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8523.94522246757\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8538.125688362556\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.25555555555556\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8541.040855153358\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.322222222222216\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8532.698477700105\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.37460317460318\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8535.071676401829\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.33650793650794\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8530.135463460925\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8529.07889501825\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.38253968253968\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8533.26653110001\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8538.069015029405\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.34603174603174\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8530.72899279461\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8537.94406318411\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.304761904761904\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8532.536743706874\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8537.253064337045\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8542.233897543829\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.2936507936508\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8535.34743737358\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.34444444444444\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8531.327551107157\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8538.468856145166\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8524.097477189944\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8531.23793298322\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8539.333934510394\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8537.871671559567\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.12698412698413\n",
      "Validation  :: Blind : 48.5 :: Blind Loss : 10521.294039489092\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.21269841269841\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 9577.887197802338\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.75714285714286\n",
      "Validation  :: Blind : 55.285714285714285 :: Blind Loss : 9192.230168858734\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.74761904761905\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8930.159920995946\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.582539682539675\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8728.061408209818\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.96825396825397\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8634.330740703728\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.18095238095238\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8576.600805842301\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.33809523809524\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8539.64358201489\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.322222222222216\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8558.169989871363\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.17142857142857\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8553.386593707066\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.17619047619048\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8574.670365458776\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.07142857142857\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8577.675326486722\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.044444444444444\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8590.005967485013\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.2031746031746\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8557.75634849114\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.3\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8534.00898184648\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.34761904761905\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8534.10121633385\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8543.649830454462\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.34285714285714\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8545.274491825394\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8532.658291836313\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.350793650793655\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8542.107302521115\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.24761904761905\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8551.112865473882\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.33650793650794\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8542.529278083599\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8543.332330776095\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8540.345897704434\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8545.638700951898\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.35555555555556\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8536.754986594668\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8536.233906846644\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8534.516004979247\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.353968253968254\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8543.205054973674\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8539.606779480026\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8537.095059783798\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8530.991952776974\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.30952380952381\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8536.014123422785\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8542.26530044862\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.29206349206349\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8535.216528327088\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8551.09409157427\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8530.989107884761\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8540.025555155526\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8538.105423591973\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.287301587301585\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8548.764512541877\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8548.002619368392\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8545.970029292519\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.36507936507936\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8544.285491097278\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8532.16326079215\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.35555555555556\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8538.638596270332\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.353968253968254\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8537.100833302986\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.350793650793655\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8538.214936344582\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8542.388419644503\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.37460317460318\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8537.310805029625\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8528.580079022919\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8537.595278018503\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8528.574996110152\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.26349206349206\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8534.797018143343\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8524.556723703012\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8541.567274283188\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8534.011683995659\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8537.874328924914\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8528.059605719123\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8533.981559895721\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8523.984865898188\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8528.109592876117\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8524.621395067768\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.25714285714286\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8546.597419919111\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8533.046896273536\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8539.37958221821\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.284126984126985\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8537.976682955405\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8537.400374753\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8535.417817952588\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.4015873015873\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8537.170993686956\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8527.957302024868\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.26031746031745\n",
      "Validation  :: Blind : 57.08571428571428 :: Blind Loss : 8545.701705443196\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8526.156250284588\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8532.006530321367\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.34285714285714\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8531.15471600444\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.32380952380952\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8530.86882042074\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8529.72960102416\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8531.600031601203\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8538.585128760555\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.4031746031746\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8531.304792759576\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8523.94522246757\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8538.125688362556\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.25555555555556\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8541.040855153358\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.322222222222216\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8532.698477700105\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.37460317460318\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8535.071676401829\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.33650793650794\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8530.135463460925\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8529.07889501825\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.38253968253968\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8533.26653110001\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8538.069015029405\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.34603174603174\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8530.72899279461\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8537.94406318411\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.304761904761904\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8532.536743706874\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8537.253064337045\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8542.233897543829\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.2936507936508\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8535.34743737358\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.34444444444444\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8531.327551107157\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8538.468856145166\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8524.097477189944\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8531.23793298322\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8539.333934510394\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8537.871671559567\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.079365079365076\n",
      "Validation  :: Blind : 50.357142857142854 :: Blind Loss : 9394.547492752261\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.70793650793651\n",
      "Validation  :: Blind : 54.942857142857136 :: Blind Loss : 8704.17469085156\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.28253968253968\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8569.372098717588\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.679365079365084\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8540.660665586001\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.57619047619048\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8439.138017640169\n",
      "Iteration: 6\n",
      "Training :: Blind : 57.08571428571428\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8378.135130393817\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.18095238095238\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8337.333937881804\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.25873015873015\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8326.824305056456\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.315873015873024\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8325.776408873651\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.46507936507936\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8276.84173561398\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.27301587301588\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8305.61703425662\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.3\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8296.255946488542\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8292.132628161293\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.265079365079366\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8298.614015974814\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.284126984126985\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8299.358169238869\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.331746031746036\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8282.35128349582\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.57142857142858\n",
      "Validation  :: Blind : 57.92857142857143 :: Blind Loss : 8272.835607212874\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.34761904761905\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8293.171591728944\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.4015873015873\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8274.904943261257\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.48888888888889\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8266.477092348248\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.387301587301586\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8281.016823620179\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.50476190476191\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8264.071944964213\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.44761904761905\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8268.27702989817\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.46507936507936\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8270.297687854802\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8263.975359343032\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.407936507936505\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8281.03566128666\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.61428571428572\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8255.20387286873\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8266.533229907236\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.35555555555556\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8295.395959057216\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.593650793650795\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8268.837834281887\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8298.134185013918\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.525396825396825\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8268.540198335071\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.51269841269841\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8278.258935804599\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.426984126984124\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8280.532115996717\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.46825396825397\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8260.417503352519\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.55873015873016\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8274.026550137485\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8264.776965158497\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.417460317460325\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8277.25980489043\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8269.863610216638\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8275.294538221267\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.49999999999999\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8272.417499418374\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.50476190476191\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8265.69865886824\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.492063492063494\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8272.942961327819\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8281.745510282097\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.493650793650794\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8273.030329110075\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.52380952380952\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8257.76417912999\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.56031746031746\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8275.495367373478\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.479365079365074\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8264.685004150835\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.49999999999999\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8264.995194354397\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.580952380952375\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8250.78780433565\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.55555555555556\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8267.863189961201\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.56507936507936\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8263.285963986184\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8273.06407476877\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.45555555555556\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8269.616771856534\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.646031746031746\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8253.945398943455\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.546031746031744\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8256.006334864922\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.51269841269841\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8268.870664811682\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.63650793650794\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8266.170012524872\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.53333333333334\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8265.62456475862\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8274.354465525157\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.6015873015873\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8260.434727061323\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8275.386053205506\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.53968253968254\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8263.687597004955\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8270.652668477716\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.8015873015873\n",
      "Validation  :: Blind : 58.15714285714285 :: Blind Loss : 8231.813719177077\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8263.132201666343\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.657142857142865\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8250.785491846676\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.557142857142864\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8265.300072774224\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.63650793650794\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8257.48510379541\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.53809523809523\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8260.983408428167\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.58730158730159\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8269.912848140086\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8271.43455046889\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.45555555555556\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8291.87044139806\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.57777777777778\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8254.401197823458\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.60634920634921\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8267.346183008318\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8267.590599608622\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.580952380952375\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8262.068951378817\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8270.692007351074\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.598412698412695\n",
      "Validation  :: Blind : 57.92857142857143 :: Blind Loss : 8261.782632053695\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.58730158730159\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8262.286183302449\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.544444444444444\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8255.533368367865\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.54285714285714\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8265.641584446303\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.628571428571426\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8252.082901579204\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8264.486193279456\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8282.80025504572\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.6031746031746\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8275.023112870107\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.67301587301588\n",
      "Validation  :: Blind : 57.85714285714286 :: Blind Loss : 8237.458497534724\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.68571428571428\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8246.57858869555\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.5015873015873\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8262.509866647546\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8277.241389494913\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.53968253968254\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8253.452657630469\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.48888888888889\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8268.507816150104\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.62063492063491\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8256.651404592994\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.526984126984125\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8252.61717856185\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.44761904761905\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8281.969423455215\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.56666666666666\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8245.891363676572\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.53333333333334\n",
      "Validation  :: Blind : 57.85714285714286 :: Blind Loss : 8282.021205118459\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.526984126984125\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8259.106635170228\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8281.16733806058\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.628571428571426\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8256.97750029075\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 42.958730158730155\n",
      "Validation  :: Blind : 43.41428571428572 :: Blind Loss : 10636.159724446647\n",
      "Iteration: 2\n",
      "Training :: Blind : 48.682539682539684\n",
      "Validation  :: Blind : 48.47142857142857 :: Blind Loss : 9861.382363184874\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.83968253968254\n",
      "Validation  :: Blind : 51.48571428571429 :: Blind Loss : 9294.924227872978\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.68571428571428\n",
      "Validation  :: Blind : 51.78571428571429 :: Blind Loss : 9046.417731074745\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.20317460317461\n",
      "Validation  :: Blind : 53.31428571428572 :: Blind Loss : 8891.488020944571\n",
      "Iteration: 6\n",
      "Training :: Blind : 53.853968253968254\n",
      "Validation  :: Blind : 53.31428571428572 :: Blind Loss : 8785.676056981352\n",
      "Iteration: 7\n",
      "Training :: Blind : 53.88253968253969\n",
      "Validation  :: Blind : 53.47142857142857 :: Blind Loss : 9309.69187160864\n",
      "Iteration: 8\n",
      "Training :: Blind : 54.093650793650795\n",
      "Validation  :: Blind : 53.32857142857142 :: Blind Loss : 8480.100739372367\n",
      "Iteration: 9\n",
      "Training :: Blind : 54.09206349206349\n",
      "Validation  :: Blind : 53.5 :: Blind Loss : 8576.278259760227\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.34920634920635\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8587.05917096464\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.560317460317464\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8567.166873557755\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.63650793650794\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8594.846261124429\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.528571428571425\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8624.390458527201\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.47936507936508\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8600.144183460685\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.46984126984127\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8649.432601708251\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.425396825396824\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8467.149249897631\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.44920634920635\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8509.432679708456\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.53809523809524\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8525.754642283107\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.493650793650794\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8525.765365722826\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.426984126984124\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 8494.459871316434\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.492063492063494\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8527.492549753108\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8483.6589506355\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.51269841269841\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8495.090159737963\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.51587301587302\n",
      "Validation  :: Blind : 53.5 :: Blind Loss : 8447.824224204353\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.649206349206345\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8504.917607328465\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.509523809523806\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8509.66966286971\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.45079365079365\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8548.804018160336\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.36349206349206\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8504.886358571835\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.56666666666666\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8540.762369926817\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.50000000000001\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8525.124288626874\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.55079365079365\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8540.309842164006\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.45873015873016\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8496.087555816426\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.47777777777778\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8512.195620618653\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.387301587301586\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8529.789980157433\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.43015873015873\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8544.230146530543\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.55714285714286\n",
      "Validation  :: Blind : 54.08571428571428 :: Blind Loss : 8500.519290601373\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.474603174603175\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8523.783397483938\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.3984126984127\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8553.780942436755\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.44603174603174\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8518.43092256561\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.60793650793651\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8618.832877547342\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.560317460317464\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8560.416103785403\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.6031746031746\n",
      "Validation  :: Blind : 53.94285714285715 :: Blind Loss : 8601.271300936423\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.37936507936508\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8534.800840117121\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.282539682539685\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8499.378341826581\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.422222222222224\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8508.711232593634\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.539682539682545\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8520.983398404278\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.546031746031744\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8511.310453572974\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.422222222222224\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8549.398143091987\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.42380952380952\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8528.934134724434\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.48095238095239\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8517.604084307173\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 54.028571428571425 :: Blind Loss : 8513.007194579133\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.5984126984127\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8492.818059527108\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.547619047619044\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8458.269795974262\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.38412698412698\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8504.653332055239\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.48730158730159\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8558.153361361237\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.59206349206349\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8544.313364444122\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.39365079365079\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8496.441963298235\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.455555555555556\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8493.111018975265\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.53015873015873\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8550.786811071963\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.49047619047619\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8524.231632709047\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.4984126984127\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8491.871417309714\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.409523809523805\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8539.384137817728\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.528571428571425\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8527.797222783425\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.6031746031746\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8544.295630784605\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.45714285714286\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8539.197910326133\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.46666666666666\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8518.498209887694\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.42857142857142\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8531.394321800724\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.49047619047619\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8562.060008607796\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.336507936507935\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8536.423635353945\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.661904761904765\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8530.621453353357\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.507936507936506\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8501.793191471419\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.400000000000006\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8520.5400189441\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.388888888888886\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8483.535232146005\n",
      "Iteration: 74\n",
      "Training :: Blind : 54.573015873015876\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8507.374386940075\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.6031746031746\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8523.443521019211\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.48253968253969\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8569.585914081317\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.522222222222226\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8505.251767725797\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.547619047619044\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8543.76343108428\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.33174603174603\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8501.80204078533\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.48412698412698\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8558.948511500477\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.41746031746032\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8509.912936593017\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.38412698412698\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8488.894500453363\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.43650793650794\n",
      "Validation  :: Blind : 54.2 :: Blind Loss : 8529.150352134086\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.4984126984127\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8522.812867499644\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.507936507936506\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8525.701789734941\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.665079365079364\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8566.929695038867\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.355555555555554\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8489.777167760058\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.473015873015875\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8502.07309646303\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.51587301587302\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 8483.838544163016\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.34444444444444\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8513.053844061069\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.41111111111111\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8499.966168544792\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.45873015873016\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8511.458350513498\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.47619047619048\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8552.647438050153\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.47142857142857\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8511.076607268002\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.56666666666666\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8506.734706753537\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.52380952380952\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 8508.369873129206\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.53650793650794\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8527.7624986271\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.425396825396824\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8515.365936819991\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.576190476190476\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8575.936481296409\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.63809523809524\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8516.529653807276\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.358730158730154\n",
      "Validation  :: Blind : 48.6 :: Blind Loss : 10466.266107092211\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.334920634920636\n",
      "Validation  :: Blind : 52.028571428571425 :: Blind Loss : 9487.671488903781\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8976.837114652468\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.82698412698412\n",
      "Validation  :: Blind : 54.44285714285715 :: Blind Loss : 8866.058026265817\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.212698412698415\n",
      "Validation  :: Blind : 54.95714285714286 :: Blind Loss : 8786.927481988896\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.7047619047619\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8688.73001168701\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.77777777777778\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8662.24198367352\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.83809523809524\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8636.211305203291\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.93492063492064\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8629.879956773399\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.93492063492064\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8638.2093900332\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.84444444444444\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8657.278100369818\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.69206349206349\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8662.212554828413\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.92222222222222\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8681.445283678542\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.98253968253968\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8649.390389100405\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.97619047619048\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8664.828790451884\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.91269841269841\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8668.518043722983\n",
      "Iteration: 17\n",
      "Training :: Blind : 56.046031746031744\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8652.516284892808\n",
      "Iteration: 18\n",
      "Training :: Blind : 56.009523809523806\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8659.426276209413\n",
      "Iteration: 19\n",
      "Training :: Blind : 56.044444444444444\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8651.801339951566\n",
      "Iteration: 20\n",
      "Training :: Blind : 56.07777777777778\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8655.241591403505\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.961904761904755\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8666.288417478669\n",
      "Iteration: 22\n",
      "Training :: Blind : 56.05555555555556\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8649.822213654697\n",
      "Iteration: 23\n",
      "Training :: Blind : 56.05238095238095\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8659.414690654357\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.98888888888889\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8663.423716015259\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.08571428571428\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8647.515894138462\n",
      "Iteration: 26\n",
      "Training :: Blind : 56.044444444444444\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8652.983203085423\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.06031746031746\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8649.271268206377\n",
      "Iteration: 28\n",
      "Training :: Blind : 55.98888888888889\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8660.45804241807\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.05238095238095\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8650.412096576336\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.04126984126984\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8662.30359479444\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8650.958730717724\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.038095238095245\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8653.240697107212\n",
      "Iteration: 33\n",
      "Training :: Blind : 55.9952380952381\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8658.719439102897\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.046031746031744\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8653.503662179659\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.061904761904756\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8656.98441426331\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.0015873015873\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8650.562695610015\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.019047619047626\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8659.310661101299\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.03333333333333\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8658.021599696993\n",
      "Iteration: 39\n",
      "Training :: Blind : 56.046031746031744\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8655.706006833898\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.04920634920635\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8660.072376072978\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.06031746031746\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8652.455154358457\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.009523809523806\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8655.408517560632\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.10000000000001\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8641.937488685811\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.08571428571428\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8650.706987923719\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.13333333333333\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8642.414759857944\n",
      "Iteration: 46\n",
      "Training :: Blind : 55.98730158730159\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8647.838897311363\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.17619047619048\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8636.6372898073\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.10952380952381\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8643.86762557073\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.04126984126984\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8650.887670354532\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.147619047619045\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8631.306848656426\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.0968253968254\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8643.554966308433\n",
      "Iteration: 52\n",
      "Training :: Blind : 55.992063492063494\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8648.776510883552\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.082539682539675\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8643.61166554076\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.02063492063492\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8657.198808200805\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.11746031746032\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8634.380593881102\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.073015873015876\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8649.31320878063\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8643.662030845486\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.03015873015873\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8649.902979406666\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.093650793650795\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8638.014015490808\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.1031746031746\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8643.094191341333\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.039682539682545\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8640.057078422844\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.09047619047619\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8639.94355791508\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.12222222222222\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8640.430959951227\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.05238095238095\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8643.350159900248\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.13015873015873\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8638.40088440427\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.07142857142857\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8652.669596450623\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.112698412698414\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8642.605793926024\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.112698412698414\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8645.14825748953\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.15079365079365\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8636.871403433519\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.057142857142864\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8641.57125581433\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.01746031746032\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8649.620035968703\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.038095238095245\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8650.226191471585\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.139682539682546\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8633.253895647513\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.08888888888889\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8645.0342875471\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.14285714285714\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8630.069081162106\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.139682539682546\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8634.646632764581\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.0968253968254\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8646.852711161966\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.06825396825397\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8650.86889447581\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.06825396825397\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8642.251452761255\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.05238095238095\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8637.583497193136\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.09047619047619\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8641.811635844453\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8644.133797463259\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.04285714285714\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8648.210405102931\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.11587301587302\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8637.43649587092\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.17142857142857\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8624.868703942728\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.092063492063495\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8639.779128931437\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.080952380952375\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8648.709966501981\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.04126984126984\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8644.267223944804\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.03492063492064\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8653.447933115756\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8637.407884938893\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.046031746031744\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8645.149615474762\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.11746031746032\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8634.761594652482\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8651.398971968354\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.10793650793651\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8632.591648444712\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.114285714285714\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8644.292285169413\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.10793650793651\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8648.282322082086\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.044444444444444\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8648.045539367584\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.12698412698413\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8632.340364053056\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.12222222222222\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8648.587738801123\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.01111111111111\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8649.145211275203\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.16984126984127\n",
      "Validation  :: Blind : 48.5 :: Blind Loss : 10509.098796510683\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.112698412698414\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 9577.375006068763\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.769841269841265\n",
      "Validation  :: Blind : 55.15714285714286 :: Blind Loss : 9211.256692560177\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.63650793650794\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8930.421205717084\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.490476190476194\n",
      "Validation  :: Blind : 56.39999999999999 :: Blind Loss : 8735.260837724803\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.777777777777786\n",
      "Validation  :: Blind : 56.34285714285714 :: Blind Loss : 8657.434363482676\n",
      "Iteration: 7\n",
      "Training :: Blind : 56.94444444444444\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8586.487242153042\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.02222222222222\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8572.543023487082\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.13968253968253\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8567.074925891648\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.1015873015873\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8574.632702249737\n",
      "Iteration: 11\n",
      "Training :: Blind : 56.86349206349206\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8608.53199863372\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.025396825396825\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8601.852894154743\n",
      "Iteration: 13\n",
      "Training :: Blind : 56.86507936507936\n",
      "Validation  :: Blind : 56.599999999999994 :: Blind Loss : 8613.90565735772\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.01587301587302\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8577.049018763386\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.006349206349206\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8578.30341521107\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.15555555555556\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8575.204801024738\n",
      "Iteration: 17\n",
      "Training :: Blind : 56.97619047619048\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8574.309600305467\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.01428571428572\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8591.251974865547\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.057142857142864\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8577.9260564369\n",
      "Iteration: 20\n",
      "Training :: Blind : 56.9952380952381\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8578.92375893473\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.06825396825397\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8574.761485130155\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.02380952380952\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8592.48141344227\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8574.325234333899\n",
      "Iteration: 24\n",
      "Training :: Blind : 56.953968253968256\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8579.39069894203\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.08253968253968\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8577.19397625441\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8575.13402350283\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.9952380952381\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8579.416523558359\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.05079365079365\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8585.280183271025\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.13809523809525\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8569.859679522951\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.07619047619048\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8576.188577118479\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.03333333333334\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8578.161657332736\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.936507936507944\n",
      "Validation  :: Blind : 56.699999999999996 :: Blind Loss : 8585.091723670943\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.06666666666666\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8577.70164354489\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.03174603174603\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8581.557529175217\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.02063492063492\n",
      "Validation  :: Blind : 56.92857142857143 :: Blind Loss : 8584.808306970766\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.990476190476194\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8580.994615988395\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.0968253968254\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8587.713186266157\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.06349206349206\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8573.715865623843\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.08571428571428\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8580.0274885006\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.04285714285714\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8580.240303828145\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.073015873015876\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8582.23963851789\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.98253968253968\n",
      "Validation  :: Blind : 56.85714285714286 :: Blind Loss : 8569.481224471163\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.99841269841271\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8577.573582762372\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.061904761904756\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8581.95075157169\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.047619047619044\n",
      "Validation  :: Blind : 56.65714285714286 :: Blind Loss : 8574.230045459724\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.14285714285714\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8576.576595530381\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.195238095238096\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8570.672281417901\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.917460317460325\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8583.987961989871\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.11587301587302\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8576.415216067846\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.07142857142857\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8579.46692949185\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.923809523809524\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8575.001253108112\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.08888888888889\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8573.3022294236\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.16825396825397\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8580.984502892516\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.006349206349206\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8574.338700311002\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.026984126984125\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8578.506941725496\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.08571428571428\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8577.097134832964\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.979365079365074\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8587.264221755733\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.97777777777778\n",
      "Validation  :: Blind : 56.67142857142857 :: Blind Loss : 8588.264875361885\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.23492063492064\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8572.092075194998\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.02222222222222\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8584.06119889228\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.07777777777778\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8581.869768952023\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.047619047619044\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8574.13854131634\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.06507936507936\n",
      "Validation  :: Blind : 56.89999999999999 :: Blind Loss : 8577.423618148181\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.08412698412698\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8582.982454877398\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.044444444444444\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8580.297123189133\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.079365079365076\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8580.566175545102\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.06031746031746\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8583.59553481913\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.11904761904761\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8578.348937587562\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.05079365079365\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8581.337623378931\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.046031746031744\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8581.244399410676\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8577.749660488314\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.98253968253968\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8584.708417257354\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8583.293722501647\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8570.540720821999\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.03492063492064\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8581.581105385538\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.10793650793651\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8586.623644277435\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.006349206349206\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8580.436607137848\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.11587301587302\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8576.984826589593\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.95079365079365\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8587.315008409001\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.961904761904755\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8585.221110747781\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.06031746031746\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8584.363580211842\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.9968253968254\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8589.782122725397\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.0952380952381\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8589.525405674958\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.07777777777778\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8578.248240539835\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.080952380952375\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8569.289320572578\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.86507936507936\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8592.187666516318\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.14920634920635\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8580.05630482292\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.05396825396826\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8577.739701412176\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.12539682539683\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8575.08901848381\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.114285714285714\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8580.667735255716\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.04920634920635\n",
      "Validation  :: Blind : 56.85714285714286 :: Blind Loss : 8572.22517241681\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.06507936507936\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8579.71329015012\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.1031746031746\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8584.513600164577\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.938095238095244\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8586.822612195003\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.10793650793651\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8578.82556254498\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.093650793650795\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8580.947595539074\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.13809523809525\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8581.172712698592\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.95555555555556\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8591.561604468312\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.04285714285714\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8581.184751497014\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.917460317460325\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8578.462167993843\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.16984126984127\n",
      "Validation  :: Blind : 48.5 :: Blind Loss : 10509.098796510683\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.112698412698414\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 9577.375006068763\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.769841269841265\n",
      "Validation  :: Blind : 55.15714285714286 :: Blind Loss : 9211.256692560177\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.63650793650794\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8930.421205717084\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.490476190476194\n",
      "Validation  :: Blind : 56.39999999999999 :: Blind Loss : 8735.260837724803\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.777777777777786\n",
      "Validation  :: Blind : 56.34285714285714 :: Blind Loss : 8657.434363482676\n",
      "Iteration: 7\n",
      "Training :: Blind : 56.94444444444444\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8586.487242153042\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.02222222222222\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8572.543023487082\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.13968253968253\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8567.074925891648\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.1015873015873\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8574.632702249737\n",
      "Iteration: 11\n",
      "Training :: Blind : 56.86349206349206\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8608.53199863372\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.025396825396825\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8601.852894154743\n",
      "Iteration: 13\n",
      "Training :: Blind : 56.86507936507936\n",
      "Validation  :: Blind : 56.599999999999994 :: Blind Loss : 8613.90565735772\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.01587301587302\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8577.049018763386\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.006349206349206\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8578.30341521107\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.15555555555556\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8575.204801024738\n",
      "Iteration: 17\n",
      "Training :: Blind : 56.97619047619048\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8574.309600305467\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.01428571428572\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8591.251974865547\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.057142857142864\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8577.9260564369\n",
      "Iteration: 20\n",
      "Training :: Blind : 56.9952380952381\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8578.92375893473\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.06825396825397\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8574.761485130155\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.02380952380952\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8592.48141344227\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8574.325234333899\n",
      "Iteration: 24\n",
      "Training :: Blind : 56.953968253968256\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8579.39069894203\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.08253968253968\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8577.19397625441\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8575.13402350283\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.9952380952381\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8579.416523558359\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.05079365079365\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8585.280183271025\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.13809523809525\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8569.859679522951\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.07619047619048\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8576.188577118479\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.03333333333334\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8578.161657332736\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.936507936507944\n",
      "Validation  :: Blind : 56.699999999999996 :: Blind Loss : 8585.091723670943\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.06666666666666\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8577.70164354489\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.03174603174603\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8581.557529175217\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.02063492063492\n",
      "Validation  :: Blind : 56.92857142857143 :: Blind Loss : 8584.808306970766\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.990476190476194\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8580.994615988395\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.0968253968254\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8587.713186266157\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.06349206349206\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8573.715865623843\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.08571428571428\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8580.0274885006\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.04285714285714\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8580.240303828145\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.073015873015876\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8582.23963851789\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.98253968253968\n",
      "Validation  :: Blind : 56.85714285714286 :: Blind Loss : 8569.481224471163\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.99841269841271\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8577.573582762372\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.061904761904756\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8581.95075157169\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.047619047619044\n",
      "Validation  :: Blind : 56.65714285714286 :: Blind Loss : 8574.230045459724\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.14285714285714\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8576.576595530381\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.195238095238096\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8570.672281417901\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.917460317460325\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8583.987961989871\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.11587301587302\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8576.415216067846\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.07142857142857\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8579.46692949185\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.923809523809524\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8575.001253108112\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.08888888888889\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8573.3022294236\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.16825396825397\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8580.984502892516\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.006349206349206\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8574.338700311002\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.026984126984125\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8578.506941725496\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.08571428571428\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8577.097134832964\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.979365079365074\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8587.264221755733\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.97777777777778\n",
      "Validation  :: Blind : 56.67142857142857 :: Blind Loss : 8588.264875361885\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.23492063492064\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8572.092075194998\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.02222222222222\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8584.06119889228\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.07777777777778\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8581.869768952023\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.047619047619044\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8574.13854131634\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.06507936507936\n",
      "Validation  :: Blind : 56.89999999999999 :: Blind Loss : 8577.423618148181\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.08412698412698\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8582.982454877398\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.044444444444444\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8580.297123189133\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.079365079365076\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8580.566175545102\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.06031746031746\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8583.59553481913\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.11904761904761\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8578.348937587562\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.05079365079365\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8581.337623378931\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.046031746031744\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8581.244399410676\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8577.749660488314\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.98253968253968\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8584.708417257354\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8583.293722501647\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8570.540720821999\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.03492063492064\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8581.581105385538\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.10793650793651\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8586.623644277435\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.006349206349206\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8580.436607137848\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.11587301587302\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8576.984826589593\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.95079365079365\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8587.315008409001\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.961904761904755\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8585.221110747781\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.06031746031746\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8584.363580211842\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.9968253968254\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8589.782122725397\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.0952380952381\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8589.525405674958\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.07777777777778\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8578.248240539835\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.080952380952375\n",
      "Validation  :: Blind : 56.871428571428574 :: Blind Loss : 8569.289320572578\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.86507936507936\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8592.187666516318\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.14920634920635\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8580.05630482292\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.05396825396826\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8577.739701412176\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.12539682539683\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8575.08901848381\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.114285714285714\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8580.667735255716\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.04920634920635\n",
      "Validation  :: Blind : 56.85714285714286 :: Blind Loss : 8572.22517241681\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.06507936507936\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8579.71329015012\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.1031746031746\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8584.513600164577\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.938095238095244\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8586.822612195003\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.10793650793651\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8578.82556254498\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.093650793650795\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8580.947595539074\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.13809523809525\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8581.172712698592\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.95555555555556\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8591.561604468312\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.04285714285714\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8581.184751497014\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.917460317460325\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8578.462167993843\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.10476190476191\n",
      "Validation  :: Blind : 50.31428571428571 :: Blind Loss : 9381.460886710265\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.661904761904765\n",
      "Validation  :: Blind : 54.92857142857142 :: Blind Loss : 8724.01673601993\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.285714285714285\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8560.932654150065\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8501.844465596172\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.785714285714285\n",
      "Validation  :: Blind : 56.91428571428572 :: Blind Loss : 8423.40286686218\n",
      "Iteration: 6\n",
      "Training :: Blind : 57.08253968253968\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8344.593918905843\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.24444444444444\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8309.573287576153\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.27936507936507\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8311.838992899255\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8287.134650381538\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8279.662318582694\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8256.421338178581\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.12222222222222\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8298.81512846559\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8263.117720041882\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.35555555555556\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8259.944288816969\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.29206349206349\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8270.838989925636\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8274.59173782386\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8271.785454857767\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.53809523809523\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8253.94168053953\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8246.62860475009\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.92857142857143 :: Blind Loss : 8246.825318461673\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.54920634920635\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8232.521655777056\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.38253968253968\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8244.935977761681\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8251.751082532039\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.85714285714286 :: Blind Loss : 8239.65099924647\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8249.875999754353\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.53968253968254\n",
      "Validation  :: Blind : 57.92857142857143 :: Blind Loss : 8233.590995432187\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.23650793650794\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8254.528789095106\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.32380952380952\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8255.859435294857\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8232.085580005225\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.352380952380955\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8248.181249104358\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.331746031746036\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8248.26250922778\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.6047619047619\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8222.678760677114\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8229.305592100178\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.352380952380955\n",
      "Validation  :: Blind : 57.85714285714286 :: Blind Loss : 8239.871573178436\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.2936507936508\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8245.385242844686\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.47777777777778\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8235.312925886199\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8240.043884779532\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.51428571428572\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8243.201025756385\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.441269841269836\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8252.11559031887\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8245.053265967224\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.48412698412698\n",
      "Validation  :: Blind : 57.99999999999999 :: Blind Loss : 8237.19984791812\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.58888888888889\n",
      "Validation  :: Blind : 58.11428571428572 :: Blind Loss : 8223.44009758261\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.66031746031746\n",
      "Validation  :: Blind : 58.22857142857143 :: Blind Loss : 8214.160130967712\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.52063492063492\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8230.105299427087\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.48730158730159\n",
      "Validation  :: Blind : 57.85714285714286 :: Blind Loss : 8245.954171770836\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.38412698412698\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8252.140161128715\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.507936507936506\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8236.629457240706\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8242.764830087297\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.593650793650795\n",
      "Validation  :: Blind : 58.07142857142858 :: Blind Loss : 8234.084502843527\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.52857142857143\n",
      "Validation  :: Blind : 58.05714285714285 :: Blind Loss : 8237.806236248745\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.65873015873015\n",
      "Validation  :: Blind : 58.128571428571426 :: Blind Loss : 8233.762525193364\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.561904761904756\n",
      "Validation  :: Blind : 57.99999999999999 :: Blind Loss : 8231.626005150756\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.53174603174603\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8231.452163588998\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.593650793650795\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8239.551472403049\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.573015873015876\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8229.470022318555\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.58253968253968\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8242.688096795076\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.49999999999999\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8246.403795898937\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.57619047619048\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8239.106114658409\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.6015873015873\n",
      "Validation  :: Blind : 58.08571428571428 :: Blind Loss : 8229.269317586619\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.64285714285714\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8219.133121279598\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.557142857142864\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8235.730391641167\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.493650793650794\n",
      "Validation  :: Blind : 57.99999999999999 :: Blind Loss : 8236.163049446654\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.63809523809523\n",
      "Validation  :: Blind : 57.99999999999999 :: Blind Loss : 8233.260223123767\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.63174603174603\n",
      "Validation  :: Blind : 58.18571428571428 :: Blind Loss : 8227.175674317366\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.58571428571428\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8235.99075842396\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.62539682539683\n",
      "Validation  :: Blind : 58.05714285714285 :: Blind Loss : 8231.569029881122\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 58.05714285714285 :: Blind Loss : 8251.199393710544\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.63333333333334\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8234.555042128562\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8249.425260002696\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8238.395409293484\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.55238095238096\n",
      "Validation  :: Blind : 58.128571428571426 :: Blind Loss : 8230.336620121861\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.56507936507936\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8236.84886298947\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 58.07142857142858 :: Blind Loss : 8233.303216220178\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.612698412698414\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8232.432552981976\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.557142857142864\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8233.192625398919\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.574603174603176\n",
      "Validation  :: Blind : 58.04285714285714 :: Blind Loss : 8234.725775279225\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.63650793650794\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8236.887460738228\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.61746031746032\n",
      "Validation  :: Blind : 58.214285714285715 :: Blind Loss : 8229.86175584947\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.544444444444444\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8234.252935036435\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.5968253968254\n",
      "Validation  :: Blind : 58.128571428571426 :: Blind Loss : 8224.801537624639\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.544444444444444\n",
      "Validation  :: Blind : 58.05714285714285 :: Blind Loss : 8233.32800565113\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.61904761904761\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8242.52506160251\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.58412698412698\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8242.446332451029\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.55079365079365\n",
      "Validation  :: Blind : 58.05714285714285 :: Blind Loss : 8240.270204074657\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.6047619047619\n",
      "Validation  :: Blind : 58.11428571428572 :: Blind Loss : 8224.989686762732\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.64285714285714\n",
      "Validation  :: Blind : 58.08571428571428 :: Blind Loss : 8213.873731890919\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.644444444444446\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8228.699030949572\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.65396825396826\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8228.165809891383\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.54761904761905\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8232.59493004241\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.51746031746032\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8241.056479552828\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.628571428571426\n",
      "Validation  :: Blind : 58.15714285714285 :: Blind Loss : 8221.616304728657\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.60634920634921\n",
      "Validation  :: Blind : 58.199999999999996 :: Blind Loss : 8223.893917297926\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.52857142857143\n",
      "Validation  :: Blind : 58.07142857142858 :: Blind Loss : 8228.384970874125\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.417460317460325\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8251.623764051436\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.492063492063494\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8244.601687250548\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.61904761904761\n",
      "Validation  :: Blind : 58.22857142857143 :: Blind Loss : 8226.148150829526\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.76984126984127\n",
      "Validation  :: Blind : 58.22857142857143 :: Blind Loss : 8213.934744266717\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8242.874070065973\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.53492063492064\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8235.751515987362\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.46984126984127\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8252.523251315697\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 42.592063492063495\n",
      "Validation  :: Blind : 43.08571428571429 :: Blind Loss : 10671.386887713914\n",
      "Iteration: 2\n",
      "Training :: Blind : 48.35079365079365\n",
      "Validation  :: Blind : 48.18571428571429 :: Blind Loss : 9938.909157988057\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.53492063492063\n",
      "Validation  :: Blind : 51.128571428571426 :: Blind Loss : 9228.52917590999\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.87460317460317\n",
      "Validation  :: Blind : 51.87142857142857 :: Blind Loss : 9034.581578266138\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.20317460317461\n",
      "Validation  :: Blind : 53.44285714285715 :: Blind Loss : 8895.08058774094\n",
      "Iteration: 6\n",
      "Training :: Blind : 53.768253968253966\n",
      "Validation  :: Blind : 52.97142857142857 :: Blind Loss : 8724.193806005409\n",
      "Iteration: 7\n",
      "Training :: Blind : 53.72539682539682\n",
      "Validation  :: Blind : 53.042857142857144 :: Blind Loss : 8514.158368417326\n",
      "Iteration: 8\n",
      "Training :: Blind : 53.75555555555556\n",
      "Validation  :: Blind : 53.18571428571428 :: Blind Loss : 8575.951086739944\n",
      "Iteration: 9\n",
      "Training :: Blind : 53.82222222222223\n",
      "Validation  :: Blind : 53.28571428571428 :: Blind Loss : 8606.568370728386\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.29047619047619\n",
      "Validation  :: Blind : 53.55714285714286 :: Blind Loss : 8595.803632715235\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.21904761904762\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8561.946025983329\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.36349206349206\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8565.337389199114\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.33968253968254\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8550.095711212281\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.50000000000001\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8625.571964604918\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.407936507936505\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8761.14350508686\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.526984126984125\n",
      "Validation  :: Blind : 54.114285714285714 :: Blind Loss : 8662.357283195684\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.180952380952384\n",
      "Validation  :: Blind : 53.47142857142857 :: Blind Loss : 8475.584119733356\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.24920634920635\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8505.860567956403\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.08253968253969\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8480.051830210741\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.10793650793651\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8468.155505560424\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.07142857142857\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8480.569707431785\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.141269841269846\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 8445.401168956876\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.1\n",
      "Validation  :: Blind : 53.300000000000004 :: Blind Loss : 8445.164644197277\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.13650793650794\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8484.800079751927\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.195238095238096\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8455.123024483797\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.3063492063492\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8445.962453712593\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.31587301587302\n",
      "Validation  :: Blind : 53.48571428571428 :: Blind Loss : 8427.62829721622\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.317460317460316\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8490.277637727342\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.2063492063492\n",
      "Validation  :: Blind : 53.48571428571428 :: Blind Loss : 8449.898678554142\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.184126984126976\n",
      "Validation  :: Blind : 53.48571428571428 :: Blind Loss : 8448.396452282615\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.15555555555556\n",
      "Validation  :: Blind : 53.48571428571428 :: Blind Loss : 8451.096881725844\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.20317460317461\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8449.491889203358\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.05396825396826\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8472.584132124048\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.22380952380952\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8446.842058185148\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.15555555555556\n",
      "Validation  :: Blind : 53.47142857142857 :: Blind Loss : 8455.89050022281\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.285714285714285\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8464.36878205364\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.23015873015873\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8492.3885873691\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.12063492063493\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 8452.034030186072\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.23809523809524\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8426.83168424242\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.166666666666664\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8478.812438236124\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.304761904761904\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8459.807905904703\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.34285714285715\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8479.671411245801\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.19206349206349\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8484.945486192459\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.25396825396825\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8434.368641432553\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.142857142857146\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8508.13735474378\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.25714285714286\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8438.505290077674\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.25396825396825\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8473.744548403338\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.196825396825396\n",
      "Validation  :: Blind : 53.44285714285715 :: Blind Loss : 8466.372953271028\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.323809523809516\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8409.198673489987\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.280952380952385\n",
      "Validation  :: Blind : 53.41428571428571 :: Blind Loss : 8425.327951460727\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.196825396825396\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8471.275274336458\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.112698412698414\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8466.004592620568\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.16984126984127\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8469.653082667059\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.31269841269841\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8493.234295289378\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.5063492063492\n",
      "Validation  :: Blind : 53.457142857142856 :: Blind Loss : 8410.369904933767\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.15396825396825\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8465.483799418089\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.15238095238095\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 8486.10014145905\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.300000000000004\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8493.029018559553\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.166666666666664\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8496.062726914912\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.35873015873016\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8507.103088512355\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.266666666666666\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8521.206059152035\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.2079365079365\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8496.233088523373\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.24761904761905\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8453.850953901978\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.158730158730165\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8460.515380928742\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8552.039308035326\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.142857142857146\n",
      "Validation  :: Blind : 53.5 :: Blind Loss : 8491.235786074594\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.39365079365079\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8520.764629110807\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.250793650793646\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8483.977309528836\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.463492063492055\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8559.323820507694\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.31111111111111\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8498.42578741092\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.31269841269841\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8457.844994885376\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.32698412698412\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8511.244700582058\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.43492063492064\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8533.836351762426\n",
      "Iteration: 74\n",
      "Training :: Blind : 54.280952380952385\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8501.123262520585\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.285714285714285\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8489.84090115646\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.234920634920634\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8566.042381238338\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.280952380952385\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8567.396650280612\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.215873015873015\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8430.711758300426\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.3031746031746\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8429.42671511562\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.355555555555554\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8432.627671274266\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.36031746031746\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8525.494942323756\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.32222222222223\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8532.379957790134\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.35873015873016\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8469.603426552057\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.22380952380952\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8433.390228682247\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.24285714285715\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8532.276552385054\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.20952380952381\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8463.4015671278\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.24285714285715\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8480.73469897471\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.215873015873015\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8482.465632161176\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.139682539682546\n",
      "Validation  :: Blind : 53.38571428571428 :: Blind Loss : 8421.486646883812\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.234920634920634\n",
      "Validation  :: Blind : 53.42857142857142 :: Blind Loss : 8452.786721032677\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.214285714285715\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8468.218951308649\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.17619047619048\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8496.843507181798\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.18730158730158\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8462.530694836194\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.36031746031746\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8438.977855819041\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.196825396825396\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8486.780866063857\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.298412698412704\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8507.648496535016\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.214285714285715\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8416.352193414847\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.371428571428574\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8439.288302358244\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.33968253968254\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8506.471779044308\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.214285714285715\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8456.343821356506\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.23650793650794\n",
      "Validation  :: Blind : 48.3 :: Blind Loss : 10477.424582774373\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.31428571428572\n",
      "Validation  :: Blind : 51.857142857142854 :: Blind Loss : 9500.836007542672\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.39365079365079\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8983.564732828754\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.75396825396825\n",
      "Validation  :: Blind : 54.300000000000004 :: Blind Loss : 8868.444123448306\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.28412698412698\n",
      "Validation  :: Blind : 54.92857142857142 :: Blind Loss : 8767.330477229672\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.677777777777784\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8674.039698230725\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.63809523809524\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8691.108525293468\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.72539682539682\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8676.262372495388\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.71111111111111\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8659.848464010694\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.82857142857143\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8650.91691181534\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.75873015873016\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8646.43583863633\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.649206349206345\n",
      "Validation  :: Blind : 55.371428571428574 :: Blind Loss : 8659.24080347678\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.7\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8711.6744229294\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.8952380952381\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8665.435783018584\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.853968253968254\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8684.201609030119\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.96031746031747\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8671.391491712076\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.97936507936508\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8673.538165299966\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8687.85973175064\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.86031746031747\n",
      "Validation  :: Blind : 55.41428571428572 :: Blind Loss : 8681.687023529917\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8656.83615528725\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.46825396825397\n",
      "Validation  :: Blind : 55.128571428571426 :: Blind Loss : 8770.57490850478\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.65079365079365\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 8733.380807393969\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.601587301587294\n",
      "Validation  :: Blind : 55.18571428571428 :: Blind Loss : 8744.137574738226\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.82698412698412\n",
      "Validation  :: Blind : 55.27142857142857 :: Blind Loss : 8720.192822236892\n",
      "Iteration: 25\n",
      "Training :: Blind : 55.971428571428575\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8681.01425355412\n",
      "Iteration: 26\n",
      "Training :: Blind : 56.32857142857143\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8646.63583408557\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.222222222222214\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8647.035608700207\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.31904761904762\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8640.032773502797\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.16984126984127\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8650.597458269116\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8638.442143212815\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.18253968253968\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8623.423855299578\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.21111111111111\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8644.309528045716\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.16031746031746\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8639.455622988706\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.220634920634915\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8648.517421029264\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.1920634920635\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8640.783019949367\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.26031746031745\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8627.965198374051\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.21904761904762\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8633.460623353487\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.2968253968254\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8644.060654137731\n",
      "Iteration: 39\n",
      "Training :: Blind : 56.2\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8635.256134237176\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.25873015873017\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8638.208547472044\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.231746031746034\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8632.517094405248\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.25714285714286\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8630.872275282627\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.15396825396826\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8642.60128154611\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.13015873015873\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8644.415469002382\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.214285714285715\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8643.873095082408\n",
      "Iteration: 46\n",
      "Training :: Blind : 56.2031746031746\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8630.97105025338\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.14444444444444\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8644.28702500898\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.17142857142857\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8648.571406961264\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.13492063492064\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8647.68188416173\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.24444444444444\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8631.736350268433\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.22539682539682\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8630.741769877306\n",
      "Iteration: 52\n",
      "Training :: Blind : 56.20793650793651\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8640.937554921082\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.128571428571426\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8647.70422065676\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.18253968253968\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8635.983344543878\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.19047619047619\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8639.637304413154\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.2047619047619\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8636.659316944304\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.13809523809524\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8641.975491724024\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.24761904761905\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8638.56906624285\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.233333333333334\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8637.67674851598\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.13809523809524\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8643.51318320231\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.13809523809524\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8644.77973026273\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.2047619047619\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8639.870870628485\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.231746031746034\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8636.68718855798\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.2\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8643.487272414983\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.214285714285715\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8636.949419491266\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.20793650793651\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8644.382475583709\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.11746031746032\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8647.76088338217\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.2063492063492\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8644.79327824773\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.18095238095238\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8639.295597880566\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.13333333333333\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8650.690908339178\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.21904761904762\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8640.461089379116\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.287301587301585\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8624.85205280274\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.23492063492064\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8637.637439578402\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.21587301587302\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8632.380406308306\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.2031746031746\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8635.16327657036\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.28888888888889\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8633.261748146217\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.24285714285714\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8633.085695522432\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.23015873015873\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8648.008904418793\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.21111111111111\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8636.922433801548\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.222222222222214\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8635.872982972047\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.2031746031746\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8632.19998767025\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.20952380952381\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8649.505238311083\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.18888888888889\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8641.96328509733\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.201587301587296\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8642.209425236571\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.22539682539682\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8628.881791540145\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.1984126984127\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8642.113450034361\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.18095238095238\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8631.183342611745\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.179365079365084\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8646.521222170879\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.24603174603175\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8637.20411786262\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.29206349206349\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8628.903980464665\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.21587301587302\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8636.083795376671\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.22857142857143\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8643.242084927875\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.1936507936508\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8640.81680535174\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.22857142857143\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8637.038975093992\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.18571428571428\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8641.095462280395\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.1968253968254\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8641.58965840327\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.18412698412698\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8638.637622708586\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.18253968253968\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8639.055239681038\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.17460317460318\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8642.621106803417\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.212698412698415\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8640.946442605196\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.16666666666667\n",
      "Validation  :: Blind : 48.48571428571429 :: Blind Loss : 10515.06831692915\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.333333333333336\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 9579.673099414336\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.842857142857135\n",
      "Validation  :: Blind : 55.1 :: Blind Loss : 9173.223065455131\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.798412698412704\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8950.914218655522\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.592063492063495\n",
      "Validation  :: Blind : 56.599999999999994 :: Blind Loss : 8767.31007457554\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.960317460317455\n",
      "Validation  :: Blind : 56.557142857142864 :: Blind Loss : 8697.085062288585\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.08412698412698\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8617.378530546228\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8582.702306808558\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.17301587301588\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8584.0690434217\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8565.282873166456\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.1015873015873\n",
      "Validation  :: Blind : 56.85714285714286 :: Blind Loss : 8628.483740539961\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.18730158730159\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8606.983988513453\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.15238095238095\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8600.94264295165\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.22380952380952\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8593.981287872273\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.2063492063492\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8579.186300139947\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8537.216709642249\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8558.613977183613\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8546.217533665444\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8550.40787290305\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8554.752554578185\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8551.56897533506\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8552.410890676974\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8539.8133168983\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.45555555555556\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8550.076940881878\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.46507936507936\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8547.462219559326\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.50952380952381\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8536.672097554812\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.573015873015876\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8544.27440593639\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8549.466856965455\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8547.627385240361\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8551.581544940225\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8547.209195722035\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8538.484235521511\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.51587301587302\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8544.605933778937\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.55555555555556\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8543.55151086039\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8542.73080454097\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8546.235376238901\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.352380952380955\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8549.140878300055\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.492063492063494\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8538.860653388268\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8544.145280106903\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.55396825396826\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8537.095086015792\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8545.793880561416\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.474603174603175\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8552.568528997399\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.49841269841269\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8539.180558959657\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8551.988900643628\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.474603174603175\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8552.776693429389\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8548.71725052325\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8535.742045011975\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8546.856319138544\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.406349206349205\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8550.859778539547\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8542.653719472011\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8551.228480913145\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.525396825396825\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8541.865390978593\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8558.809109793106\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8550.719765455236\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8555.326344843208\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.53174603174603\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8550.144783304668\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8564.52343612305\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.37460317460318\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8547.208178330775\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.37301587301587\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8540.759747364446\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.34444444444444\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8546.738142798482\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.53492063492064\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8527.690831141583\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8541.409054489055\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8550.983969086854\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8548.676011647982\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8547.109942186806\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.53968253968254\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8544.266921305098\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.479365079365074\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8545.010429837026\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.42063492063492\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8556.602184336929\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.52857142857143\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8546.784137488497\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8556.195778072713\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.406349206349205\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8562.493067694038\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.51746031746032\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8545.191822121034\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8550.994827672694\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8558.41455880235\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8553.090595478696\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8551.5902785826\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.48412698412698\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8548.643180392337\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.507936507936506\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8550.361803601314\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.53333333333334\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8549.7751976183\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8551.95098701413\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8541.768102882148\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8541.198567494526\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.480952380952374\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8548.842285861741\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.480952380952374\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8554.28070200697\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8557.363929475627\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.41904761904762\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8561.748136327235\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8549.621082258176\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8543.31162675405\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8549.438667811108\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.406349206349205\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8545.947432209417\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.438095238095244\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8545.217852304271\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8556.713617552286\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8552.166573775368\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8545.74289609627\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8551.37572417506\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8550.684631948294\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.42857142857143\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8551.14055988446\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.39841269841269\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8552.597337504636\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.42857142857143\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8555.072440767919\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8546.142782183571\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.16666666666667\n",
      "Validation  :: Blind : 48.48571428571429 :: Blind Loss : 10515.06831692915\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.333333333333336\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 9579.673099414336\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.842857142857135\n",
      "Validation  :: Blind : 55.1 :: Blind Loss : 9173.223065455131\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.798412698412704\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8950.914218655522\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.592063492063495\n",
      "Validation  :: Blind : 56.599999999999994 :: Blind Loss : 8767.31007457554\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.960317460317455\n",
      "Validation  :: Blind : 56.557142857142864 :: Blind Loss : 8697.085062288585\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.08412698412698\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8617.378530546228\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8582.702306808558\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.17301587301588\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8584.0690434217\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8565.282873166456\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.1015873015873\n",
      "Validation  :: Blind : 56.85714285714286 :: Blind Loss : 8628.483740539961\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.18730158730159\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8606.983988513453\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.15238095238095\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8600.94264295165\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.22380952380952\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8593.981287872273\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.2063492063492\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8579.186300139947\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8537.216709642249\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8558.613977183613\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8546.217533665444\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8550.40787290305\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8554.752554578185\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8551.56897533506\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8552.410890676974\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8539.8133168983\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.45555555555556\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8550.076940881878\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.46507936507936\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8547.462219559326\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.50952380952381\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8536.672097554812\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.573015873015876\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8544.27440593639\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8549.466856965455\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8547.627385240361\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8551.581544940225\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8547.209195722035\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8538.484235521511\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.51587301587302\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8544.605933778937\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.55555555555556\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8543.55151086039\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8542.73080454097\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8546.235376238901\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.352380952380955\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8549.140878300055\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.492063492063494\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8538.860653388268\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8544.145280106903\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.55396825396826\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8537.095086015792\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8545.793880561416\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.474603174603175\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8552.568528997399\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.49841269841269\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8539.180558959657\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8551.988900643628\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.474603174603175\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8552.776693429389\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8548.71725052325\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8535.742045011975\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8546.856319138544\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.406349206349205\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8550.859778539547\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8542.653719472011\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8551.228480913145\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.525396825396825\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8541.865390978593\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8558.809109793106\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8550.719765455236\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8555.326344843208\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.53174603174603\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8550.144783304668\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8564.52343612305\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.37460317460318\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8547.208178330775\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.37301587301587\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8540.759747364446\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.34444444444444\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8546.738142798482\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.53492063492064\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8527.690831141583\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8541.409054489055\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8550.983969086854\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8548.676011647982\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8547.109942186806\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.53968253968254\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8544.266921305098\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.479365079365074\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8545.010429837026\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.42063492063492\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8556.602184336929\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.52857142857143\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8546.784137488497\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8556.195778072713\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.406349206349205\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8562.493067694038\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.51746031746032\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8545.191822121034\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8550.994827672694\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8558.41455880235\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8553.090595478696\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8551.5902785826\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.48412698412698\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8548.643180392337\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.507936507936506\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8550.361803601314\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.53333333333334\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8549.7751976183\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8551.95098701413\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8541.768102882148\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8541.198567494526\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.480952380952374\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8548.842285861741\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.480952380952374\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8554.28070200697\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8557.363929475627\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.41904761904762\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8561.748136327235\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8549.621082258176\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8543.31162675405\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8549.438667811108\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.406349206349205\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8545.947432209417\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.438095238095244\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8545.217852304271\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8556.713617552286\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8552.166573775368\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8545.74289609627\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8551.37572417506\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8550.684631948294\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.42857142857143\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8551.14055988446\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.39841269841269\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8552.597337504636\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.42857142857143\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8555.072440767919\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8546.142782183571\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.060317460317464\n",
      "Validation  :: Blind : 50.34285714285714 :: Blind Loss : 9390.665142189137\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.696825396825396\n",
      "Validation  :: Blind : 54.97142857142857 :: Blind Loss : 8714.261500948716\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.37301587301587\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8549.376474284069\n",
      "Iteration: 4\n",
      "Training :: Blind : 56.08888888888889\n",
      "Validation  :: Blind : 56.14285714285714 :: Blind Loss : 8488.640256221723\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.68253968253968\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8424.160345325907\n",
      "Iteration: 6\n",
      "Training :: Blind : 57.16825396825397\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8353.541951143143\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.1920634920635\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8330.78397658894\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.387301587301586\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8306.492617791295\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8299.18789153202\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.18888888888889\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8307.638969637192\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8281.937083719533\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.287301587301585\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8299.71114548892\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8282.44436356511\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8288.913533181807\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.34444444444444\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8288.316507934598\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8276.611318907006\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8290.494296541267\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8287.191089920005\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.64285714285714 :: Blind Loss : 8272.817231766376\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.5952380952381\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8260.079308448498\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.60634920634921\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8263.173716103154\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.592063492063495\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8243.166608531086\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.49841269841269\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8264.707239547504\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8278.50387363314\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.55555555555556\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8252.000240041414\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8286.049827783116\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8261.007917823652\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.56984126984127\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8258.216431486671\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.426984126984124\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8271.97261634715\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.74761904761905\n",
      "Validation  :: Blind : 58.04285714285714 :: Blind Loss : 8262.237103968942\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 58.05714285714285 :: Blind Loss : 8278.268759738185\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.6968253968254\n",
      "Validation  :: Blind : 58.14285714285714 :: Blind Loss : 8259.872750715374\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.56507936507936\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8278.806601929482\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.71587301587302\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8271.530149853676\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.711111111111116\n",
      "Validation  :: Blind : 58.11428571428572 :: Blind Loss : 8252.321129815504\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.590476190476195\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8262.217263450671\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.65396825396826\n",
      "Validation  :: Blind : 58.08571428571428 :: Blind Loss : 8263.206749239147\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.63968253968253\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8264.104575866795\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.77619047619048\n",
      "Validation  :: Blind : 58.199999999999996 :: Blind Loss : 8256.236449846525\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.75238095238095\n",
      "Validation  :: Blind : 58.199999999999996 :: Blind Loss : 8253.683893356647\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.65396825396826\n",
      "Validation  :: Blind : 58.17142857142857 :: Blind Loss : 8264.207268665848\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.719047619047615\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8252.761938388914\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.63492063492064\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8269.577801926265\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.74603174603175\n",
      "Validation  :: Blind : 58.32857142857143 :: Blind Loss : 8263.873926627864\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.6920634920635\n",
      "Validation  :: Blind : 58.17142857142857 :: Blind Loss : 8266.836883959335\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.78095238095238\n",
      "Validation  :: Blind : 58.128571428571426 :: Blind Loss : 8260.004359435949\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.77301587301588\n",
      "Validation  :: Blind : 58.22857142857143 :: Blind Loss : 8276.745298433467\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.66984126984127\n",
      "Validation  :: Blind : 58.07142857142858 :: Blind Loss : 8305.94442141424\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.441269841269836\n",
      "Validation  :: Blind : 57.85714285714286 :: Blind Loss : 8287.991530440777\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8280.486173275718\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.31428571428572\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8331.80750746068\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8336.127431980629\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8342.94967344288\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.992063492063494\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8367.013998396924\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.699999999999996\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8384.253548013225\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.36825396825397\n",
      "Validation  :: Blind : 56.39999999999999 :: Blind Loss : 8423.29410276701\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.76031746031745\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8389.40446343568\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.39047619047619\n",
      "Validation  :: Blind : 56.45714285714286 :: Blind Loss : 8426.018308264345\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.91587301587302\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8380.83104660179\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.88412698412698\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8374.020490088013\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.0984126984127\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8366.902900093786\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.06507936507936\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8373.754259841011\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.08571428571428\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8361.464720619597\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.8968253968254\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8392.981557498908\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.7936507936508\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8383.115205143862\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.00952380952381\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8374.153587017276\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.03492063492064\n",
      "Validation  :: Blind : 56.89999999999999 :: Blind Loss : 8377.184191736002\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.28095238095238\n",
      "Validation  :: Blind : 57.08571428571428 :: Blind Loss : 8354.013973921967\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.952380952380956\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8374.63717539913\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.91269841269842\n",
      "Validation  :: Blind : 57.028571428571425 :: Blind Loss : 8375.926362857199\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.92222222222222\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8392.663037371762\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.91428571428572 :: Blind Loss : 8380.555874274974\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.93333333333334\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8383.573257940512\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.84444444444444\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8401.75114932538\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.77301587301587\n",
      "Validation  :: Blind : 56.699999999999996 :: Blind Loss : 8408.093633450126\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.02222222222222\n",
      "Validation  :: Blind : 56.89999999999999 :: Blind Loss : 8397.34941639868\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.544444444444444\n",
      "Validation  :: Blind : 56.41428571428572 :: Blind Loss : 8418.470022406967\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.853968253968254\n",
      "Validation  :: Blind : 56.699999999999996 :: Blind Loss : 8406.588305366746\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.18888888888889\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8364.964597203654\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.75396825396825\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8413.659455169762\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.89206349206349\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8401.767674729437\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.7031746031746\n",
      "Validation  :: Blind : 56.75714285714286 :: Blind Loss : 8407.519917975445\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.099999999999994\n",
      "Validation  :: Blind : 56.92857142857143 :: Blind Loss : 8382.832818915178\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.60793650793651\n",
      "Validation  :: Blind : 56.48571428571428 :: Blind Loss : 8406.463271548591\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.74920634920635\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8416.658432534012\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.74603174603175\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8400.564849480092\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.701587301587296\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8405.600136436466\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.67460317460318\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8408.576667185005\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.939682539682536\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8401.92766908525\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.86507936507936\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8389.548882186009\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.885714285714286\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8405.058100121927\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.822222222222216\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8394.506439562507\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.665079365079364\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8405.579598344528\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.960317460317455\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8404.170303179184\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.93015873015873\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8388.591028666637\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.63650793650794\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8401.28466180718\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.079365079365076\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8361.400601647743\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.233333333333334\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8376.309103890017\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.666666666666664\n",
      "Validation  :: Blind : 56.65714285714286 :: Blind Loss : 8416.584372980518\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.006349206349206\n",
      "Validation  :: Blind : 56.92857142857143 :: Blind Loss : 8381.49038582789\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 42.81269841269841\n",
      "Validation  :: Blind : 43.28571428571429 :: Blind Loss : 10654.098258746144\n",
      "Iteration: 2\n",
      "Training :: Blind : 48.55079365079365\n",
      "Validation  :: Blind : 48.385714285714286 :: Blind Loss : 9956.25979763542\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.574603174603176\n",
      "Validation  :: Blind : 51.28571428571429 :: Blind Loss : 9219.831136693021\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.62380952380953\n",
      "Validation  :: Blind : 51.91428571428571 :: Blind Loss : 8955.788775886896\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.10793650793651\n",
      "Validation  :: Blind : 53.457142857142856 :: Blind Loss : 8862.231191971416\n",
      "Iteration: 6\n",
      "Training :: Blind : 53.72539682539682\n",
      "Validation  :: Blind : 53.22857142857143 :: Blind Loss : 8706.919109770224\n",
      "Iteration: 7\n",
      "Training :: Blind : 53.833333333333336\n",
      "Validation  :: Blind : 53.25714285714286 :: Blind Loss : 9293.187368527293\n",
      "Iteration: 8\n",
      "Training :: Blind : 53.596825396825395\n",
      "Validation  :: Blind : 53.17142857142857 :: Blind Loss : 8573.197086182552\n",
      "Iteration: 9\n",
      "Training :: Blind : 53.78412698412698\n",
      "Validation  :: Blind : 53.35714285714286 :: Blind Loss : 8554.685099734692\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.36031746031746\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8555.772784358862\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.12222222222223\n",
      "Validation  :: Blind : 53.38571428571428 :: Blind Loss : 8577.3122464978\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.32539682539682\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8712.984378293182\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.3079365079365\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8736.681384407482\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.34126984126985\n",
      "Validation  :: Blind : 53.47142857142857 :: Blind Loss : 8723.636855481149\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.26031746031747\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8521.872262963923\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.352380952380955\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 8556.630299328848\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.44285714285715\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8595.984543475835\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.36190476190477\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 8543.056235090626\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.34603174603174\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8573.588923161675\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.41746031746032\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8655.386181010852\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.4047619047619\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8569.178775840714\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.463492063492055\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8627.67438519979\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.44761904761904\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8658.287724462009\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.37301587301587\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8550.485533593965\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.473015873015875\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8593.861355462912\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.46825396825397\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8611.83925795392\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.407936507936505\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8582.616877844175\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.29365079365079\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8580.091194209066\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.268253968253966\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8644.931545081727\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.409523809523805\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8594.880629742289\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.371428571428574\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8664.791274869254\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.41587301587302\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8605.02464607757\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8701.271302096935\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.29047619047619\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8611.174361505904\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.298412698412704\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 8612.835758984946\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.24444444444444\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8684.244047488171\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.25555555555556\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8638.0132445009\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.47619047619048\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8560.140347157063\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.38571428571428\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8619.840656025237\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.53015873015873\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8709.081830068622\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.492063492063494\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8629.27113894632\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.334920634920636\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8628.514256368891\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.269841269841265\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8629.235268689452\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.38571428571428\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8560.097156372136\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.401587301587305\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8590.373864559651\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.44761904761904\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8576.990792489109\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.46984126984127\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8552.860972369914\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.31904761904762\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8570.72177519209\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.34761904761905\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8626.519785263794\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.49523809523809\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8649.220868685012\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.288888888888884\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8528.453957216843\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.5031746031746\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8649.316008496708\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.4031746031746\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8583.696417104387\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.39047619047619\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8677.454008198789\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.46507936507936\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8594.844487343842\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.33174603174603\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8549.772176977956\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.23968253968254\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8565.52859195042\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.36825396825397\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8631.120663099091\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.38095238095239\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8555.449033267683\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.39047619047619\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8563.316288136133\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8529.749931636678\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.34444444444444\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8640.528330108633\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.48095238095239\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8608.505723518565\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.36349206349206\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8608.032715627705\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.3952380952381\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8572.575134518618\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.36507936507936\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8601.123830275485\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.47936507936508\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8577.428474396424\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.43650793650794\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8597.86363040044\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.32857142857143\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8633.628417276937\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.43968253968254\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8567.378707335352\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.35079365079365\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8566.962923660292\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.44285714285715 :: Blind Loss : 8552.303252943475\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.32539682539682\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8587.205257748126\n",
      "Iteration: 74\n",
      "Training :: Blind : 54.266666666666666\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8646.419384720157\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.35714285714286\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8562.224038498014\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8599.471615344732\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.37936507936508\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8590.699583573354\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.47142857142857\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8632.404193983233\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.473015873015875\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 8564.54405700566\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.31269841269841\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8560.729697263549\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.32698412698412\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8574.935107065357\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.33968253968254\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8584.28353943777\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8558.712862517632\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.36984126984127\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8589.896330301512\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.336507936507935\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8538.950162092697\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.425396825396824\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 8596.625078004596\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.39206349206349\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8525.515144028494\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.57777777777778\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8580.400935079642\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8574.22606184892\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.37460317460317\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8584.31036008289\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.37777777777778\n",
      "Validation  :: Blind : 53.55714285714286 :: Blind Loss : 8528.874371008727\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.43968253968254\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8622.023988036319\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.333333333333336\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8563.862324211463\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.27619047619048\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8638.787653137035\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.39206349206349\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 8573.128128602846\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.453968253968256\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8544.551279888907\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.425396825396824\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8549.391745937077\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.42380952380952\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8614.202171866324\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.21904761904762\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8558.350087384879\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.38253968253969\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8673.042184779564\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.32222222222222\n",
      "Validation  :: Blind : 48.25714285714286 :: Blind Loss : 10475.0212044861\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.28571428571429\n",
      "Validation  :: Blind : 52.042857142857144 :: Blind Loss : 9504.094637902585\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.31111111111111\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 9005.721936162687\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.67301587301587\n",
      "Validation  :: Blind : 54.371428571428574 :: Blind Loss : 8880.623713507428\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.03492063492064\n",
      "Validation  :: Blind : 54.6 :: Blind Loss : 8808.040827095814\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.506349206349206\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8709.508921644092\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.56825396825397\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 8712.895583123769\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.822222222222216\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8680.829343821175\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.7936507936508\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8685.244551088064\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.973015873015875\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8623.55604724959\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.77460317460318\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8631.715294288877\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.561904761904756\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8660.459454745462\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.43015873015873\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 8690.919688307982\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.69047619047619\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8718.244737576444\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.85555555555556\n",
      "Validation  :: Blind : 55.300000000000004 :: Blind Loss : 8679.636537644868\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.955555555555556\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8673.303529299224\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.86349206349206\n",
      "Validation  :: Blind : 55.214285714285715 :: Blind Loss : 8685.469125115225\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.86666666666667\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8687.180217739206\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.89206349206349\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8681.063929659049\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.53015873015873\n",
      "Validation  :: Blind : 55.14285714285714 :: Blind Loss : 8779.9431105763\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.58095238095239\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8773.202757408933\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.8031746031746\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8719.226755347247\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.79206349206349\n",
      "Validation  :: Blind : 55.15714285714286 :: Blind Loss : 8723.053427154038\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.84761904761905\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8714.47696160366\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.00000000000001\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8684.455883221843\n",
      "Iteration: 26\n",
      "Training :: Blind : 56.15396825396826\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8645.325202790347\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.25079365079365\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8633.904715841021\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.25555555555556\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8628.141061181472\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.24603174603175\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8616.582927020929\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.23492063492064\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8613.068763883473\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.34444444444444\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8613.134474038343\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.23492063492064\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8627.176577366667\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.285714285714285\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8618.50836759236\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.279365079365085\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8624.312816879286\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.285714285714285\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8620.565083038462\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.32857142857143\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8609.737002664839\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.25079365079365\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8621.660625543784\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.23015873015873\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8629.071970154466\n",
      "Iteration: 39\n",
      "Training :: Blind : 56.28095238095238\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8625.35051016562\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.22380952380952\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8634.765506314314\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.29047619047619\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8624.213828399748\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.29206349206349\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8623.601433267926\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.24920634920635\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8620.94460576414\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.26507936507936\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8618.43641503015\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.28095238095238\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8618.5068859031\n",
      "Iteration: 46\n",
      "Training :: Blind : 56.25396825396825\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8623.24323148245\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.26031746031745\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8628.09505225481\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.3031746031746\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8627.735783974256\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.3015873015873\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8624.16428288374\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.26349206349206\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8625.651114657448\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.26984126984127\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8624.198126404812\n",
      "Iteration: 52\n",
      "Training :: Blind : 56.398412698412706\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8622.53273071229\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.25714285714286\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8623.576940383358\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.28888888888889\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8625.067255581334\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.32539682539682\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8623.839996902136\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.201587301587296\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8627.826172808793\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.26507936507936\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8620.353298851705\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.23968253968255\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8631.122555874521\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.32380952380952\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8628.280638225979\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.25396825396825\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8631.98539924775\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.25079365079365\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8629.702947018879\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.419047619047625\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8617.509443790139\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.28412698412698\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8630.761392588727\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.31587301587302\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8624.917510151015\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.24920634920635\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8625.714943820682\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.2063492063492\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8631.435998748422\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.387301587301586\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8613.156477801356\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.277777777777786\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8623.570473674125\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.32539682539682\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8622.392769959084\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.29047619047619\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8627.047073534632\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.334920634920636\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8618.880677823774\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.43650793650794\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8614.954666144178\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.304761904761904\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8624.341411517316\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.25396825396825\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8636.553053691467\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.26507936507936\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8626.41163633299\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.31428571428572\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8624.634212922354\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.25238095238095\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8625.074995660565\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.24603174603175\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8626.629096652032\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.27301587301587\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8625.941665764934\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.23809523809524\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8635.434047475934\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.28412698412698\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8622.843470997243\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.35555555555556\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8622.261913557708\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.222222222222214\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8618.074613014105\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.22857142857143\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8631.644170734242\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.334920634920636\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8624.278780216657\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.37936507936509\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8617.852598583411\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.26507936507936\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8618.804980350022\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.39206349206349\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8619.932786098216\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.26984126984127\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8625.760336987907\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.34603174603174\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8623.941876643039\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.33015873015873\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8613.05329728646\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.31111111111111\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8621.88949465126\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.26984126984127\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8623.075137869426\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.27619047619048\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8627.469922833232\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.30952380952381\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8624.895632793392\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.31904761904762\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8621.142521090418\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.3015873015873\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8622.194357807819\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.341269841269835\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8625.724049786546\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.24920634920635\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8628.60445397681\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.322222222222216\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8618.168526066827\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.333333333333336\n",
      "Validation  :: Blind : 48.614285714285714 :: Blind Loss : 10510.304257952685\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.25396825396825\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 9573.35268220549\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.57142857142857\n",
      "Validation  :: Blind : 54.6 :: Blind Loss : 9230.723594728563\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.582539682539675\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8962.35877573611\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.24126984126983\n",
      "Validation  :: Blind : 56.35714285714286 :: Blind Loss : 8782.264568131572\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.73015873015873\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8670.557684617137\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.01111111111111\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8605.042262548348\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.2031746031746\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8560.011316401415\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8516.580314462792\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.14920634920635\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8544.659127788456\n",
      "Iteration: 11\n",
      "Training :: Blind : 56.99841269841271\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8593.308098481733\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.11746031746032\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8583.282906927247\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.038095238095245\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8574.360289361775\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8558.349026791882\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.214285714285715\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8560.723970814433\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.25714285714286\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8539.304643534455\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.18571428571428\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8543.996877923179\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.23650793650794\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8537.96865447586\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8540.525955592371\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.24920634920635\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8538.183895966198\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.18253968253968\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8546.963163633573\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.16190476190476\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8553.525981945713\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.18888888888889\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8542.046815376656\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.18730158730159\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8551.087958554686\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.25714285714286\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8536.36514193966\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.165079365079364\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8543.384800813154\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.028571428571425 :: Blind Loss : 8553.393273107766\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8550.275180322204\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.08571428571428 :: Blind Loss : 8541.203080450876\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.219047619047615\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8550.173790344947\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.17619047619048\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8541.158752367366\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.15079365079365\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8544.71269671704\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.16190476190476\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8546.712091910624\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.195238095238096\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8541.355230409628\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.20952380952381\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8542.188590715828\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8545.488486955885\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.06825396825397\n",
      "Validation  :: Blind : 56.91428571428572 :: Blind Loss : 8546.00549717419\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.05396825396826\n",
      "Validation  :: Blind : 57.028571428571425 :: Blind Loss : 8548.892446136606\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.1936507936508\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8548.199519459373\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.24285714285714\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8537.046919339413\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.166666666666664\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8544.323606611202\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.1968253968254\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8547.541319100415\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.146031746031746\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8541.279846702888\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.268253968253966\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8548.808506465044\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.17301587301588\n",
      "Validation  :: Blind : 57.08571428571428 :: Blind Loss : 8548.018390455309\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.14285714285714\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8548.842126763666\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8552.457046997028\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8536.686910261895\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.30952380952381\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8525.02351300931\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8524.395905336485\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8522.517324678256\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8524.84482446142\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8530.899584729968\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8529.740813796783\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8530.696091224043\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8525.042716979193\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.4015873015873\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8527.903585128943\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8535.779550526833\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8516.66396066639\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8525.627770206727\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8517.386446702325\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8515.451503305627\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8529.276748562806\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8520.813423696032\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8524.73255013048\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8523.419806609276\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8526.25946130783\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8525.181253294682\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8527.517744601377\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.441269841269836\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8509.596403548092\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8525.140326778439\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8523.282051772992\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.417460317460325\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8518.674446259189\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8532.49687588681\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.34761904761905\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8524.412427310595\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8533.953782693254\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8525.392484043743\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.4\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8519.482343505924\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8522.657282736089\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.36190476190476\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8526.44479038407\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.417460317460325\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8527.122086052077\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.34285714285714\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8518.58790973895\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8533.575815435794\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8530.164082962743\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8519.383965854562\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.3968253968254\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8524.497678608539\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.388888888888886\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8525.243199650296\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.27936507936507\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8535.62390368865\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8529.02912917381\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8523.922423365731\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8536.574002184107\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8515.9062780372\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8530.436176925097\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8523.922776844389\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.42063492063492\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8515.236436977177\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8529.359488670852\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8523.529733611369\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8526.960930169724\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.31428571428572\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8533.876740203668\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8532.534209290658\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.333333333333336\n",
      "Validation  :: Blind : 48.614285714285714 :: Blind Loss : 10510.304257952685\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.25396825396825\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 9573.35268220549\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.57142857142857\n",
      "Validation  :: Blind : 54.6 :: Blind Loss : 9230.723594728563\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.582539682539675\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8962.35877573611\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.24126984126983\n",
      "Validation  :: Blind : 56.35714285714286 :: Blind Loss : 8782.264568131572\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.73015873015873\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8670.557684617137\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.01111111111111\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8605.042262548348\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.2031746031746\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8560.011316401415\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8516.580314462792\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.14920634920635\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8544.659127788456\n",
      "Iteration: 11\n",
      "Training :: Blind : 56.99841269841271\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8593.308098481733\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.11746031746032\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8583.282906927247\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.038095238095245\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8574.360289361775\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.12380952380952\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8558.349026791882\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.214285714285715\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8560.723970814433\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.25714285714286\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8539.304643534455\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.18571428571428\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8543.996877923179\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.23650793650794\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8537.96865447586\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 56.942857142857136 :: Blind Loss : 8540.525955592371\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.24920634920635\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8538.183895966198\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.18253968253968\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8546.963163633573\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.16190476190476\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8553.525981945713\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.18888888888889\n",
      "Validation  :: Blind : 57.099999999999994 :: Blind Loss : 8542.046815376656\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.18730158730159\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8551.087958554686\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.25714285714286\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8536.36514193966\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.165079365079364\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8543.384800813154\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.028571428571425 :: Blind Loss : 8553.393273107766\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8550.275180322204\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.08571428571428 :: Blind Loss : 8541.203080450876\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.219047619047615\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8550.173790344947\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.17619047619048\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8541.158752367366\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.15079365079365\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8544.71269671704\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.16190476190476\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8546.712091910624\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.195238095238096\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8541.355230409628\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.20952380952381\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8542.188590715828\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8545.488486955885\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.06825396825397\n",
      "Validation  :: Blind : 56.91428571428572 :: Blind Loss : 8546.00549717419\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.05396825396826\n",
      "Validation  :: Blind : 57.028571428571425 :: Blind Loss : 8548.892446136606\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.1936507936508\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8548.199519459373\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.24285714285714\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8537.046919339413\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.166666666666664\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8544.323606611202\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.1968253968254\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8547.541319100415\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.146031746031746\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8541.279846702888\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.268253968253966\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8548.808506465044\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.17301587301588\n",
      "Validation  :: Blind : 57.08571428571428 :: Blind Loss : 8548.018390455309\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.14285714285714\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8548.842126763666\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 56.971428571428575 :: Blind Loss : 8552.457046997028\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.16984126984127\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8536.686910261895\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.30952380952381\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8525.02351300931\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8524.395905336485\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8522.517324678256\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8524.84482446142\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8530.899584729968\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8529.740813796783\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8530.696091224043\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8525.042716979193\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.4015873015873\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8527.903585128943\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8535.779550526833\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8516.66396066639\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8525.627770206727\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8517.386446702325\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8515.451503305627\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8529.276748562806\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8520.813423696032\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8524.73255013048\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8523.419806609276\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8526.25946130783\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8525.181253294682\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8527.517744601377\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.441269841269836\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8509.596403548092\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8525.140326778439\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8523.282051772992\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.417460317460325\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8518.674446259189\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8532.49687588681\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.34761904761905\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8524.412427310595\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8533.953782693254\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8525.392484043743\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.4\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8519.482343505924\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8522.657282736089\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.36190476190476\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8526.44479038407\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.417460317460325\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8527.122086052077\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.34285714285714\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8518.58790973895\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8533.575815435794\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8530.164082962743\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8519.383965854562\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.3968253968254\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8524.497678608539\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.388888888888886\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8525.243199650296\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.27936507936507\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8535.62390368865\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8529.02912917381\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8523.922423365731\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8536.574002184107\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8515.9062780372\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8530.436176925097\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8523.922776844389\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.42063492063492\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8515.236436977177\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8529.359488670852\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8523.529733611369\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8526.960930169724\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.31428571428572\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8533.876740203668\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8532.534209290658\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.05396825396825\n",
      "Validation  :: Blind : 50.271428571428565 :: Blind Loss : 9381.151265768232\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.6984126984127\n",
      "Validation  :: Blind : 55.08571428571428 :: Blind Loss : 8700.57789971073\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.400000000000006\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8554.900922326515\n",
      "Iteration: 4\n",
      "Training :: Blind : 56.08730158730159\n",
      "Validation  :: Blind : 56.27142857142857 :: Blind Loss : 8496.19993002909\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.71746031746032\n",
      "Validation  :: Blind : 56.92857142857143 :: Blind Loss : 8431.610251967017\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.91428571428572\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8388.844734251272\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.17936507936508\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8329.720427517046\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.099999999999994\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8343.711012072043\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8302.209561015774\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8278.55337686666\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.322222222222216\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8293.26770444263\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8282.450978355431\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.21587301587302\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8297.796226326656\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.471428571428575\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8268.250683032158\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8282.445223058869\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.425396825396824\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8272.946750909232\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.27619047619048\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8293.768161542775\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8270.686350245944\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8272.005870867786\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.4031746031746\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8269.30539310372\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8264.488015936116\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.61904761904761\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8254.97559269469\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.479365079365074\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8265.31110021336\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8279.279499203996\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.507936507936506\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8268.700416002972\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.574603174603176\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8266.608482388425\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.63492063492064\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8257.043253080657\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.612698412698414\n",
      "Validation  :: Blind : 58.01428571428572 :: Blind Loss : 8263.688965308143\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.592063492063495\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8251.087693530748\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.66984126984127\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8249.954253287651\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.52857142857143\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8255.494207247315\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.388888888888886\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8279.642815807265\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8282.217191561807\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.53809523809523\n",
      "Validation  :: Blind : 57.99999999999999 :: Blind Loss : 8283.15895973936\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.493650793650794\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8259.870271815631\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.579365079365076\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8255.817671027376\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.58730158730159\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8263.288982651866\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.51746031746032\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8268.637678034542\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8270.999289107953\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8285.805716584982\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.66984126984127\n",
      "Validation  :: Blind : 57.99999999999999 :: Blind Loss : 8243.166719473984\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.480952380952374\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8265.394583080892\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.360317460317454\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8293.63585311339\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.53809523809523\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8251.324653779355\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.50952380952381\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8266.603962921567\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8273.305789079042\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8269.854900971844\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.58253968253968\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8260.067708196208\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.58730158730159\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8258.043886757474\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8287.497600236833\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.53174603174603\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8267.220378262391\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.48412698412698\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8272.71553085661\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.47777777777778\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8277.604826785602\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.546031746031744\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8266.065005962468\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.54126984126984\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8266.118670632566\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.56825396825397\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8266.68673613567\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.65873015873015\n",
      "Validation  :: Blind : 57.92857142857143 :: Blind Loss : 8255.359768265638\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8268.735566687017\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8277.030566116415\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8267.625653624942\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.64126984126984\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8251.757934188487\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.5968253968254\n",
      "Validation  :: Blind : 57.98571428571429 :: Blind Loss : 8260.559825757155\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.628571428571426\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8257.44855872014\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.6015873015873\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8258.059575772942\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.64285714285714\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8250.714337219717\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.646031746031746\n",
      "Validation  :: Blind : 57.92857142857143 :: Blind Loss : 8244.518076835793\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.61746031746032\n",
      "Validation  :: Blind : 57.971428571428575 :: Blind Loss : 8259.23797868417\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.611111111111114\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8260.249992073146\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.5968253968254\n",
      "Validation  :: Blind : 57.92857142857143 :: Blind Loss : 8260.049850860247\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.4968253968254\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8283.592707475706\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.44761904761905\n",
      "Validation  :: Blind : 57.85714285714286 :: Blind Loss : 8276.802126763047\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.730158730158735\n",
      "Validation  :: Blind : 58.14285714285714 :: Blind Loss : 8241.204761739793\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.67777777777777\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8251.513971441662\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.54761904761905\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8262.811817090329\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.61428571428572\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8251.928880771018\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.546031746031744\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8272.18446425585\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8267.28181412832\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.561904761904756\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8263.320237875509\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.580952380952375\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8253.87965474163\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.50476190476191\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8268.411307595805\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.526984126984125\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8264.390028461701\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.611111111111114\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8260.177492739766\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8282.185375523855\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.4968253968254\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8262.322762954705\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.63492063492064\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8266.82105816218\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.48730158730159\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8269.043347176932\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.628571428571426\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8262.30482008083\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.53968253968254\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8259.55888813581\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.65396825396826\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8254.938953963567\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.53809523809523\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8269.960615800519\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.730158730158735\n",
      "Validation  :: Blind : 58.07142857142858 :: Blind Loss : 8257.041410919112\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.731746031746034\n",
      "Validation  :: Blind : 58.24285714285714 :: Blind Loss : 8266.324688220666\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.5968253968254\n",
      "Validation  :: Blind : 57.95714285714286 :: Blind Loss : 8277.907139044286\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.75873015873015\n",
      "Validation  :: Blind : 58.128571428571426 :: Blind Loss : 8267.80748581683\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.75238095238095\n",
      "Validation  :: Blind : 58.128571428571426 :: Blind Loss : 8261.02451705921\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.784126984126985\n",
      "Validation  :: Blind : 58.07142857142858 :: Blind Loss : 8257.848582278208\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.57619047619048\n",
      "Validation  :: Blind : 58.08571428571428 :: Blind Loss : 8266.509176807598\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.82539682539682\n",
      "Validation  :: Blind : 58.3 :: Blind Loss : 8243.637315800253\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.77460317460318\n",
      "Validation  :: Blind : 58.199999999999996 :: Blind Loss : 8249.10649921419\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.86190476190476\n",
      "Validation  :: Blind : 58.25714285714285 :: Blind Loss : 8236.368407907383\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 42.922222222222224\n",
      "Validation  :: Blind : 43.27142857142857 :: Blind Loss : 10638.008610115749\n",
      "Iteration: 2\n",
      "Training :: Blind : 48.598412698412695\n",
      "Validation  :: Blind : 48.42857142857142 :: Blind Loss : 9905.050272629825\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.549206349206344\n",
      "Validation  :: Blind : 51.114285714285714 :: Blind Loss : 9254.925228378133\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.83809523809524\n",
      "Validation  :: Blind : 52.042857142857144 :: Blind Loss : 9031.3199265246\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.19206349206349\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8874.081841072863\n",
      "Iteration: 6\n",
      "Training :: Blind : 53.887301587301586\n",
      "Validation  :: Blind : 52.92857142857142 :: Blind Loss : 8654.281478575054\n",
      "Iteration: 7\n",
      "Training :: Blind : 53.9984126984127\n",
      "Validation  :: Blind : 53.48571428571428 :: Blind Loss : 9300.472672751097\n",
      "Iteration: 8\n",
      "Training :: Blind : 53.85873015873016\n",
      "Validation  :: Blind : 53.15714285714286 :: Blind Loss : 8526.460080876783\n",
      "Iteration: 9\n",
      "Training :: Blind : 54.22698412698412\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8515.83369505363\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.62222222222223\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8554.049365823998\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.547619047619044\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8576.72725743779\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.51587301587302\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8567.722769530592\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.47619047619048\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8566.004572106976\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.58412698412698\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8595.663290563603\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.549206349206344\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8579.056639242583\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.55714285714286\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8505.148954299311\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.5063492063492\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8497.7949477699\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.576190476190476\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8484.652146397739\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.420634920634924\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8454.683844943975\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.41587301587302\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 8473.464945454894\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.43492063492064\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8419.463228780054\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.31428571428572\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 8451.749034335156\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.453968253968256\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8507.142526843882\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.33809523809524\n",
      "Validation  :: Blind : 53.457142857142856 :: Blind Loss : 8462.793738523656\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.36507936507936\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 8471.329461791807\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.50000000000001\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8489.861891069202\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.44126984126984\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8437.817045060448\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.34285714285715\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8471.008157605793\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.34920634920635\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8471.158384947852\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.5031746031746\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8477.954417082266\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.54444444444444\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8458.921502604655\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.46031746031746\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8473.109426829698\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.387301587301586\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8449.466495127394\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.48412698412698\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8453.456819692783\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.32698412698412\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8436.263356553378\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.46666666666666\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8469.495362685619\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.422222222222224\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8428.35457549314\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.422222222222224\n",
      "Validation  :: Blind : 53.37142857142857 :: Blind Loss : 8427.98041486442\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.63333333333333\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8497.824464763402\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.5031746031746\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8444.848499546548\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.46666666666666\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8510.249284339\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.41111111111111\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8484.65060086202\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.49523809523809\n",
      "Validation  :: Blind : 53.44285714285715 :: Blind Loss : 8421.92190249241\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.520634920634926\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8475.118952589535\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.44920634920635\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8468.542409491574\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.55396825396826\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8444.092970152597\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.401587301587305\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8468.744235668053\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.39365079365079\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8439.073381283819\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.41111111111111\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8483.364090559719\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.56190476190477\n",
      "Validation  :: Blind : 53.528571428571425 :: Blind Loss : 8407.203082840366\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.33809523809524\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8471.101643809365\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.55396825396826\n",
      "Validation  :: Blind : 54.028571428571425 :: Blind Loss : 8507.737099354505\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.43174603174603\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8477.970378589558\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.31111111111111\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8479.860192127195\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.453968253968256\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8484.727245891328\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.41269841269841\n",
      "Validation  :: Blind : 53.94285714285715 :: Blind Loss : 8510.153728971309\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.37777777777778\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8508.957571228566\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.45873015873016\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8533.619537130064\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.407936507936505\n",
      "Validation  :: Blind : 53.48571428571428 :: Blind Loss : 8417.694570871765\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.388888888888886\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8479.699711133246\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.51746031746032\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8465.523378796915\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.43492063492064\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8471.842180053505\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.558730158730164\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8507.76512647631\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.38253968253969\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8455.266167527727\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.46031746031746\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8465.430259459663\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.406349206349205\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8510.47262839149\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.3968253968254\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8498.815483220276\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.48095238095239\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8440.603536384027\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.45873015873016\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8486.682928742579\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.51428571428571\n",
      "Validation  :: Blind : 53.47142857142857 :: Blind Loss : 8413.91567033435\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.62222222222223\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8426.156612509814\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.34126984126985\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8466.274306427022\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.42857142857142\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 8451.974171840578\n",
      "Iteration: 74\n",
      "Training :: Blind : 54.38095238095239\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8450.647339125473\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.45873015873016\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8451.561691532908\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.48253968253969\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8486.84555080545\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.36190476190477\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8485.691815621552\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.46507936507936\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8482.63843481586\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.526984126984125\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8528.804679585852\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.355555555555554\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8502.301552074536\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.522222222222226\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8453.756245349305\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.58571428571428\n",
      "Validation  :: Blind : 54.028571428571425 :: Blind Loss : 8484.16063971663\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.549206349206344\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8412.36109242429\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.46031746031746\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8474.880319863812\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.261904761904766\n",
      "Validation  :: Blind : 53.5 :: Blind Loss : 8452.674027328942\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.38412698412698\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8434.906205228246\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.387301587301586\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8464.3390770033\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.336507936507935\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8494.677902023093\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.46507936507936\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8455.278776363706\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.4984126984127\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8510.98119998047\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.3968253968254\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8463.015196513765\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.355555555555554\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8481.008345521794\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.455555555555556\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8458.692738559583\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.44761904761904\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8445.261165098655\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.371428571428574\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8484.864182128378\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.5968253968254\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8472.480463248256\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.36349206349206\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8476.152234454545\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.45714285714286\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8458.315343369493\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.46984126984127\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8445.598351324088\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.41111111111111\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8485.698057692085\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.304761904761904\n",
      "Validation  :: Blind : 48.34285714285714 :: Blind Loss : 10475.714810329779\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.23809523809524\n",
      "Validation  :: Blind : 51.75714285714286 :: Blind Loss : 9508.706731775845\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.3031746031746\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 9003.734669348327\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.7063492063492\n",
      "Validation  :: Blind : 54.38571428571428 :: Blind Loss : 8881.751672770031\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.23015873015873\n",
      "Validation  :: Blind : 54.74285714285714 :: Blind Loss : 8797.898627978553\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.41746031746032\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 8713.774511911897\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.57936507936508\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8697.21067705983\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.70793650793651\n",
      "Validation  :: Blind : 55.41428571428572 :: Blind Loss : 8680.203560976679\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.79206349206349\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8656.0654924314\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.77460317460318\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8632.594761128727\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.65396825396826\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8644.992626786732\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.65555555555556\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8643.2670237443\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.47619047619048\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8661.988566907236\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.563492063492056\n",
      "Validation  :: Blind : 55.285714285714285 :: Blind Loss : 8719.560452746078\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.86825396825397\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8677.282448694335\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.95873015873016\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8665.37560669775\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.834920634920636\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8674.964046084835\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.38253968253968\n",
      "Validation  :: Blind : 54.95714285714286 :: Blind Loss : 8780.849162344603\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.63174603174603\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8762.352418878127\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.714285714285715\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8724.798967120678\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.509523809523806\n",
      "Validation  :: Blind : 55.14285714285714 :: Blind Loss : 8749.541008166125\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.75873015873016\n",
      "Validation  :: Blind : 55.285714285714285 :: Blind Loss : 8711.984766687714\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.86666666666667\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8685.259298645737\n",
      "Iteration: 24\n",
      "Training :: Blind : 56.17301587301588\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8628.05590112477\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.21587301587302\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8623.957157999921\n",
      "Iteration: 26\n",
      "Training :: Blind : 56.29047619047619\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8623.169723327861\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.25873015873017\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8616.401099039878\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.287301587301585\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8612.830274332096\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.26190476190476\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8616.730739561306\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.146031746031746\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8630.102915929554\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.287301587301585\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8619.203478710422\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.12380952380952\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8623.413118759076\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.222222222222214\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8625.884975302382\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.23650793650794\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8626.799158559566\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.12063492063491\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8630.178750644056\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.26031746031745\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8617.561423660378\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.22857142857143\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8622.64844981593\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.17460317460318\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8618.281294366061\n",
      "Iteration: 39\n",
      "Training :: Blind : 56.1984126984127\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8622.938510916187\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.17619047619048\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8623.977491570808\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.23015873015873\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8626.771816750503\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.0968253968254\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8640.065388381214\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.25714285714286\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8617.877208384913\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.16190476190476\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8614.297223639816\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.28412698412698\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8624.421663792527\n",
      "Iteration: 46\n",
      "Training :: Blind : 56.2952380952381\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8608.709357622638\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.25555555555556\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8613.304526426324\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.17619047619048\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8630.066242345987\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.3\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8614.099399779847\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.15396825396826\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8629.264943796341\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.18730158730158\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8623.765132807337\n",
      "Iteration: 52\n",
      "Training :: Blind : 56.18095238095238\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8624.151810456158\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.20952380952381\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8617.298089254753\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.24444444444444\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8617.779074711176\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.24603174603175\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8624.434822310177\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.18571428571428\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8619.819742197094\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.23492063492064\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8615.623324710465\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.26190476190476\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8615.021874236641\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.23968253968255\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8622.342524818843\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.21111111111111\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8620.284684431665\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.092063492063495\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8635.910044288867\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.2\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8619.088357827502\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.23015873015873\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8624.192728389524\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.24126984126983\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8618.58219006895\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.15238095238095\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8635.879139810682\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.165079365079364\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8632.082656114071\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.2031746031746\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8619.352546228754\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.28253968253968\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8619.377521145005\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.322222222222216\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8620.60746566507\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.279365079365085\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8613.900278811943\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.18253968253968\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8625.548434099363\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.11746031746032\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8630.288813421936\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.31111111111111\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8610.476806370909\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8633.695041998326\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.1968253968254\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8620.91631072469\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.21746031746032\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8625.005247192361\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.25555555555556\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8620.940679873049\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.23968253968255\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8624.296779238026\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.268253968253966\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8612.091366574357\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.26349206349206\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8619.10144147648\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.3015873015873\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8616.605000749854\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.31428571428572\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8612.043933651112\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.25714285714286\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8626.140957391395\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.266666666666666\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8616.089918387926\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.22857142857143\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8616.190877048317\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.231746031746034\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8622.16989703456\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.31111111111111\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8613.090368621553\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.3\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8632.404670416832\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.21111111111111\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8621.608861457764\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.2063492063492\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8619.609166157261\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.285714285714285\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8615.146417597694\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.17301587301588\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8629.773172004025\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.24444444444444\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8615.864002636028\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.26984126984127\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8615.110439137956\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.22539682539682\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8622.814892752693\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.2047619047619\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8625.246027388523\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.21904761904762\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8620.297704089837\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.21904761904762\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8614.654283130138\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.22857142857143\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8618.337014065834\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.279365079365085\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8617.697290263099\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.20476190476191\n",
      "Validation  :: Blind : 48.41428571428572 :: Blind Loss : 10523.606233521885\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.37619047619048\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 9577.521887446477\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.71904761904762\n",
      "Validation  :: Blind : 55.07142857142857 :: Blind Loss : 9176.535386334022\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.93650793650794\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8952.151176109306\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.57777777777778\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8762.820255097105\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.960317460317455\n",
      "Validation  :: Blind : 56.599999999999994 :: Blind Loss : 8685.402847696603\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.17142857142857\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8613.602575171746\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.201587301587296\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8571.173033347763\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8574.998893961652\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.166666666666664\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8569.963243335937\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.10952380952381\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8586.988213460289\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.13650793650794\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8609.05964819252\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.17619047619048\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8594.938475819605\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.16190476190476\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8590.774937301125\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8573.606861602217\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8573.223696579622\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8569.529290231167\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.48888888888889\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8534.388003672984\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8544.71344097206\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8541.09043081995\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.58253968253968\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8541.053041141698\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8540.93035184926\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8544.490202588877\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.46507936507936\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8540.460159376988\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.52222222222222\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8532.575060852838\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.436507936507944\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8548.86142895693\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8545.859598107543\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.52222222222222\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8540.142032498163\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.425396825396824\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8548.563205248358\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8550.490966076413\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8544.07850314777\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8550.11230186185\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8543.117846015495\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8554.365903004427\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8547.244482352547\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8550.87580627562\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.471428571428575\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8551.831899833109\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8551.277752773818\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8546.101711675232\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.479365079365074\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8541.803492422176\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.48888888888889\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8543.675654878085\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.50952380952381\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8538.604591592511\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.45714285714286\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8543.557951528132\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8543.623517808639\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.46825396825397\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8538.382505797674\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8546.639924815518\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.43174603174603\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8553.73241650624\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8540.333972852477\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.52380952380952\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8540.133403753414\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8534.807743197067\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8541.285809089954\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8538.050407386323\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8544.498458648395\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8544.715672865452\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.58253968253968\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8532.095597412957\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8543.912550964344\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.45555555555556\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8539.089786929242\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.47777777777778\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8552.771073889096\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8535.79050845775\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.50952380952381\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8535.094778443268\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8542.04187903295\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8547.855681020512\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8547.784287360746\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.5015873015873\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8536.299237295269\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8545.442042640507\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8543.562732653887\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8544.81885590159\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8541.394623810656\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.385714285714286\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8556.213641617356\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.4968253968254\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8541.271547107597\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8552.720217555632\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.51746031746032\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8542.381515481477\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.5031746031746\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8546.099122671782\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8540.363558960986\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8536.772507828566\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8549.141309643026\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8545.250893350201\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8552.13742440883\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8545.069054516549\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.50476190476191\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8536.44455898549\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.4952380952381\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8543.975860171246\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8538.7238773732\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8550.04846369306\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8547.931168790325\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.52063492063492\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8538.733761795427\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.55873015873016\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8541.255089815068\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.43174603174603\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8552.661568289433\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.480952380952374\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8546.350018165394\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.4\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8535.895150367302\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.45714285714286\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8536.370197884324\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.47619047619048\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8545.018105770707\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8541.122159264509\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.4968253968254\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8541.174645043979\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8533.665402255026\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.3968253968254\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8552.217876223425\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8547.939947337069\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8548.249886855885\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8553.417636477754\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.54285714285714\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8542.521874786216\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.52380952380952\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8538.545293768919\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.20476190476191\n",
      "Validation  :: Blind : 48.41428571428572 :: Blind Loss : 10523.606233521885\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.37619047619048\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 9577.521887446477\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.71904761904762\n",
      "Validation  :: Blind : 55.07142857142857 :: Blind Loss : 9176.535386334022\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.93650793650794\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8952.151176109306\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.57777777777778\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8762.820255097105\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.960317460317455\n",
      "Validation  :: Blind : 56.599999999999994 :: Blind Loss : 8685.402847696603\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.17142857142857\n",
      "Validation  :: Blind : 56.98571428571428 :: Blind Loss : 8613.602575171746\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.201587301587296\n",
      "Validation  :: Blind : 57.114285714285714 :: Blind Loss : 8571.173033347763\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8574.998893961652\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.166666666666664\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8569.963243335937\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.10952380952381\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8586.988213460289\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.13650793650794\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8609.05964819252\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.17619047619048\n",
      "Validation  :: Blind : 57.04285714285714 :: Blind Loss : 8594.938475819605\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.16190476190476\n",
      "Validation  :: Blind : 57.07142857142857 :: Blind Loss : 8590.774937301125\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8573.606861602217\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8573.223696579622\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8569.529290231167\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.48888888888889\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8534.388003672984\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8544.71344097206\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8541.09043081995\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.58253968253968\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8541.053041141698\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8540.93035184926\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8544.490202588877\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.46507936507936\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8540.460159376988\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.52222222222222\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8532.575060852838\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.436507936507944\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8548.86142895693\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8545.859598107543\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.52222222222222\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8540.142032498163\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.425396825396824\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8548.563205248358\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8550.490966076413\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8544.07850314777\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8550.11230186185\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8543.117846015495\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8554.365903004427\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8547.244482352547\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8550.87580627562\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.471428571428575\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8551.831899833109\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8551.277752773818\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8546.101711675232\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.479365079365074\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8541.803492422176\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.48888888888889\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8543.675654878085\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.50952380952381\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8538.604591592511\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.45714285714286\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8543.557951528132\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8543.623517808639\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.46825396825397\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8538.382505797674\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8546.639924815518\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.43174603174603\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8553.73241650624\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8540.333972852477\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.52380952380952\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8540.133403753414\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8534.807743197067\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8541.285809089954\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.53015873015873\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8538.050407386323\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8544.498458648395\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8544.715672865452\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.58253968253968\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8532.095597412957\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8543.912550964344\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.45555555555556\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8539.089786929242\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.47777777777778\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8552.771073889096\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8535.79050845775\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.50952380952381\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8535.094778443268\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8542.04187903295\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8547.855681020512\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8547.784287360746\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.5015873015873\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8536.299237295269\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.42222222222222\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8545.442042640507\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8543.562732653887\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8544.81885590159\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8541.394623810656\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.385714285714286\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8556.213641617356\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.4968253968254\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8541.271547107597\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.423809523809524\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8552.720217555632\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.51746031746032\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8542.381515481477\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.5031746031746\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8546.099122671782\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.44285714285714\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8540.363558960986\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8536.772507828566\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8549.141309643026\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8545.250893350201\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8552.13742440883\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.44920634920635\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8545.069054516549\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.50476190476191\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8536.44455898549\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.4952380952381\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8543.975860171246\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8538.7238773732\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.46190476190476\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8550.04846369306\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8547.931168790325\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.52063492063492\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8538.733761795427\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.55873015873016\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8541.255089815068\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.43174603174603\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8552.661568289433\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.480952380952374\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8546.350018165394\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.4\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8535.895150367302\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.45714285714286\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8536.370197884324\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.47619047619048\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8545.018105770707\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8541.122159264509\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.4968253968254\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8541.174645043979\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8533.665402255026\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.3968253968254\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8552.217876223425\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8547.939947337069\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8548.249886855885\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.43015873015873\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8553.417636477754\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.54285714285714\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8542.521874786216\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.52380952380952\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8538.545293768919\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.196825396825396\n",
      "Validation  :: Blind : 50.47142857142857 :: Blind Loss : 9379.705425629638\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.39365079365079\n",
      "Validation  :: Blind : 54.97142857142857 :: Blind Loss : 8725.51478568056\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.27460317460318\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8569.151187077892\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.93968253968254\n",
      "Validation  :: Blind : 56.08571428571428 :: Blind Loss : 8504.715133797392\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.74285714285714\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8426.49252736489\n",
      "Iteration: 6\n",
      "Training :: Blind : 57.10793650793651\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8368.19154926431\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.10634920634921\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8338.696710943179\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8315.642738536939\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.25396825396826\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8321.352318116958\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8299.19060145005\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.43333333333334\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8285.057869043934\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8303.280153095013\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.2063492063492\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8300.37273979824\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.331746031746036\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8295.020616348242\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.544444444444444\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8266.465346926474\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8277.364128165867\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8293.553206235605\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8296.092568735628\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.4\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8281.107997481904\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8281.679136021372\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.3968253968254\n",
      "Validation  :: Blind : 57.64285714285714 :: Blind Loss : 8274.832244094878\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.5952380952381\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8267.886950822087\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8297.20415827227\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.471428571428575\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8286.066569037088\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.474603174603175\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8265.60690873341\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.4952380952381\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8279.935744747861\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8293.057134942253\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.48571428571429\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8271.072467534977\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8271.091250597823\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8283.44332173699\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.44761904761905\n",
      "Validation  :: Blind : 57.885714285714286 :: Blind Loss : 8284.327630424576\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.53333333333334\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8271.161244455796\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.265079365079366\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8315.51223730182\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.425396825396824\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8284.178613746079\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.36507936507936\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8289.251358452577\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8292.254406020169\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.439682539682536\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8284.514912000675\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.49841269841269\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8265.152392053202\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8274.837644220024\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8287.651510912718\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8257.536520734056\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.507936507936506\n",
      "Validation  :: Blind : 57.81428571428572 :: Blind Loss : 8275.80887564339\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8276.188212313287\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8284.698700685665\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8297.744598681957\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.561904761904756\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8260.549242154504\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.52063492063492\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8267.063330983408\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.592063492063495\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8265.728109396423\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.64285714285714 :: Blind Loss : 8291.680647927522\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.34603174603174\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8289.056235421122\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.64285714285714 :: Blind Loss : 8305.335805921692\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8276.526872877519\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8295.974987294816\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8291.180255139307\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8274.698312604669\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8264.759690623965\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.25873015873015\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8302.649538815936\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.50476190476191\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8262.920192152083\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8279.719730302499\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.507936507936506\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8267.807473809842\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.388888888888886\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8285.611474910082\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.46984126984127\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8270.805966284403\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.436507936507944\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8297.196566753657\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.388888888888886\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8279.825203136188\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.51746031746032\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8274.730689090586\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.56984126984127\n",
      "Validation  :: Blind : 57.871428571428574 :: Blind Loss : 8267.677305424888\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8271.63036491455\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.41111111111111\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8283.602173793919\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.38253968253968\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8279.017708090181\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.64285714285714 :: Blind Loss : 8278.90243032539\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8270.520492568688\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.51269841269841\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8270.633739084675\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.426984126984124\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8270.6457284856\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.53492063492064\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8280.839760709223\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8283.778570148264\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8279.485991513619\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.38253968253968\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8286.160213521298\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.5031746031746\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8269.028023709852\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.52222222222222\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8276.970923803521\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.544444444444444\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8269.194013235145\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.4968253968254\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8267.189546548074\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.544444444444444\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8261.882847601395\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8287.618707032252\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.51269841269841\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8284.995572948228\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8281.622255661086\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.37301587301587\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8272.219018416366\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.425396825396824\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8282.736124568157\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.40952380952381\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8287.034295484762\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.44761904761905\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8284.980690231005\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8258.08537190048\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.5015873015873\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8277.660729935655\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8289.73212609302\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.506349206349206\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8255.49632834337\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8278.331763324524\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8281.510114829056\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.331746031746036\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8277.06340904107\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8280.284985708495\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8273.389612222534\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8292.009505790236\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8279.990122915638\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 42.77777777777778\n",
      "Validation  :: Blind : 43.15714285714286 :: Blind Loss : 10660.114072481567\n",
      "Iteration: 2\n",
      "Training :: Blind : 48.661904761904765\n",
      "Validation  :: Blind : 48.614285714285714 :: Blind Loss : 9922.2734507213\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.86349206349207\n",
      "Validation  :: Blind : 51.41428571428571 :: Blind Loss : 9270.630278939383\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.56507936507937\n",
      "Validation  :: Blind : 51.957142857142856 :: Blind Loss : 9016.143874235102\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.15079365079365\n",
      "Validation  :: Blind : 53.47142857142857 :: Blind Loss : 8869.143795436634\n",
      "Iteration: 6\n",
      "Training :: Blind : 53.57301587301587\n",
      "Validation  :: Blind : 52.84285714285715 :: Blind Loss : 8555.526814329358\n",
      "Iteration: 7\n",
      "Training :: Blind : 53.782539682539685\n",
      "Validation  :: Blind : 53.38571428571428 :: Blind Loss : 9309.55619512484\n",
      "Iteration: 8\n",
      "Training :: Blind : 53.953968253968256\n",
      "Validation  :: Blind : 53.18571428571428 :: Blind Loss : 8489.813255917263\n",
      "Iteration: 9\n",
      "Training :: Blind : 54.060317460317464\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 8486.094185115962\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.58095238095238\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8602.251164749876\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.56666666666666\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8518.901239480008\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.46666666666666\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8615.90589783874\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.58888888888889\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8599.951130625672\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.426984126984124\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8559.91415974265\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.34603174603174\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8546.81081841361\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.45238095238095\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8495.775314490631\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.160317460317465\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8435.208762286606\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.25555555555556\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8438.23180071808\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.37936507936508\n",
      "Validation  :: Blind : 53.5 :: Blind Loss : 8435.296321728852\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.666666666666664\n",
      "Validation  :: Blind : 54.35714285714286 :: Blind Loss : 9088.107151953722\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.409523809523805\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8456.561094355588\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.53809523809524\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8404.820232364174\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.301587301587304\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8441.473755643608\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.31904761904762\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8427.910316857055\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.406349206349205\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8491.29931197748\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.41428571428572\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8424.479595872672\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.5063492063492\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8424.420007627878\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.4031746031746\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8426.558171616838\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.352380952380955\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8447.334912855302\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.769841269841265\n",
      "Validation  :: Blind : 54.42857142857142 :: Blind Loss : 9117.665985030257\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.44603174603174\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8413.125053950054\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.6047619047619\n",
      "Validation  :: Blind : 54.2 :: Blind Loss : 9103.06571965182\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8491.951975431468\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8458.19021922862\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.41111111111111\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8438.995806559935\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.34444444444444\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8474.355541701309\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.49047619047619\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 8462.604014137896\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.3952380952381\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8515.723300487254\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.34126984126985\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8452.771319583397\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.36984126984127\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8509.98134136189\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.42380952380952\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8478.447805748387\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.32222222222223\n",
      "Validation  :: Blind : 54.17142857142857 :: Blind Loss : 8457.315029410218\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.3063492063492\n",
      "Validation  :: Blind : 54.18571428571428 :: Blind Loss : 8488.18712879861\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.401587301587305\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8452.117533996052\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.3063492063492\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8536.664613870144\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.352380952380955\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8467.393789657153\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.250793650793646\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 8475.15836301482\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.35714285714286\n",
      "Validation  :: Blind : 54.17142857142857 :: Blind Loss : 8490.345268629502\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.425396825396824\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8440.120934040275\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.38412698412698\n",
      "Validation  :: Blind : 54.22857142857143 :: Blind Loss : 8483.77833683601\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.5031746031746\n",
      "Validation  :: Blind : 54.07142857142857 :: Blind Loss : 8536.10208500495\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.24444444444444\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 8498.68087442693\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.493650793650794\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 8466.425500762667\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.3968253968254\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8519.61831006037\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.507936507936506\n",
      "Validation  :: Blind : 54.22857142857143 :: Blind Loss : 8474.74401629627\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.419047619047625\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 8470.53090496418\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.3952380952381\n",
      "Validation  :: Blind : 54.08571428571428 :: Blind Loss : 8496.088074246152\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.51587301587302\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 8487.643602748634\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.474603174603175\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8494.126826379706\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.43015873015873\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8513.967154799615\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.4047619047619\n",
      "Validation  :: Blind : 54.07142857142857 :: Blind Loss : 8503.562918075446\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.45714285714286\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8465.211516719513\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.46984126984127\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 8486.51341809356\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.48253968253969\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8474.286774222937\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.29365079365079\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8476.572312663893\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.300000000000004\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8501.10327621311\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.34761904761905\n",
      "Validation  :: Blind : 54.24285714285715 :: Blind Loss : 8483.239693108768\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.4968253968254\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8443.116622163627\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.55714285714286\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8479.007912179186\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.539682539682545\n",
      "Validation  :: Blind : 54.214285714285715 :: Blind Loss : 8512.32936076908\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.48571428571428\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8523.019344269354\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 54.17142857142857 :: Blind Loss : 8492.697300767239\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.44920634920635\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 8480.560698994374\n",
      "Iteration: 74\n",
      "Training :: Blind : 54.57777777777778\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8560.29234379276\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8440.927036438366\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.407936507936505\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8483.462738967013\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.36666666666666\n",
      "Validation  :: Blind : 54.114285714285714 :: Blind Loss : 8519.018662458167\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.36349206349206\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8479.627570067803\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.3031746031746\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8462.476142698502\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.44603174603174\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8453.814068521646\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.407936507936505\n",
      "Validation  :: Blind : 53.94285714285715 :: Blind Loss : 8514.769238824127\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.27301587301587\n",
      "Validation  :: Blind : 54.028571428571425 :: Blind Loss : 8482.16362125729\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.3063492063492\n",
      "Validation  :: Blind : 54.114285714285714 :: Blind Loss : 8484.270762958204\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.400000000000006\n",
      "Validation  :: Blind : 54.22857142857143 :: Blind Loss : 8480.515113477446\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.438095238095244\n",
      "Validation  :: Blind : 54.028571428571425 :: Blind Loss : 8468.055199027178\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.35714285714286\n",
      "Validation  :: Blind : 54.25714285714286 :: Blind Loss : 8468.821768880287\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.46507936507936\n",
      "Validation  :: Blind : 54.08571428571428 :: Blind Loss : 8464.283864398465\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.2968253968254\n",
      "Validation  :: Blind : 54.114285714285714 :: Blind Loss : 8511.01403504033\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.36825396825397\n",
      "Validation  :: Blind : 54.24285714285715 :: Blind Loss : 8483.724751063757\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.3968253968254\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8430.987535008793\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.3984126984127\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8468.819129354197\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.50000000000001\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8507.14216431946\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.58095238095238\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8526.15272309239\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.34285714285715\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8451.933544291001\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.58253968253969\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 8561.833331378963\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.353968253968254\n",
      "Validation  :: Blind : 54.18571428571428 :: Blind Loss : 8472.903213763904\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.36190476190477\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8450.261696441114\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.50158730158731\n",
      "Validation  :: Blind : 54.18571428571428 :: Blind Loss : 8473.545128509508\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.31269841269841\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8503.118787224106\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.50000000000001\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 8457.77876004166\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.285714285714285\n",
      "Validation  :: Blind : 48.3 :: Blind Loss : 10478.31038373639\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.32063492063492\n",
      "Validation  :: Blind : 51.91428571428571 :: Blind Loss : 9504.01270676712\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.266666666666666\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8992.788840827592\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.666666666666664\n",
      "Validation  :: Blind : 54.45714285714286 :: Blind Loss : 8870.143371144943\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.147619047619045\n",
      "Validation  :: Blind : 54.68571428571428 :: Blind Loss : 8771.706326957425\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.67619047619048\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8660.877908772296\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.72539682539682\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8636.38676204553\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.75079365079365\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8634.640658453185\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.65714285714286\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8631.443369961975\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.56507936507936\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8638.802287621496\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.507936507936506\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8649.062275803506\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.546031746031744\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8640.285777098698\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.58412698412698\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8626.325335960759\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.547619047619044\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8648.5583161358\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.62539682539682\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8691.579541912495\n",
      "Iteration: 16\n",
      "Training :: Blind : 56.177777777777784\n",
      "Validation  :: Blind : 56.15714285714286 :: Blind Loss : 8632.980130186525\n",
      "Iteration: 17\n",
      "Training :: Blind : 56.16349206349206\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8618.111085742923\n",
      "Iteration: 18\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8624.737719994884\n",
      "Iteration: 19\n",
      "Training :: Blind : 56.179365079365084\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8623.782263893027\n",
      "Iteration: 20\n",
      "Training :: Blind : 56.13174603174603\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8617.922915608815\n",
      "Iteration: 21\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8619.157270385796\n",
      "Iteration: 22\n",
      "Training :: Blind : 56.17142857142857\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8622.135115765706\n",
      "Iteration: 23\n",
      "Training :: Blind : 56.114285714285714\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8637.553756832329\n",
      "Iteration: 24\n",
      "Training :: Blind : 56.14444444444444\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8622.686778569572\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.1031746031746\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8629.768372672865\n",
      "Iteration: 26\n",
      "Training :: Blind : 56.18412698412698\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8614.14066599033\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.17619047619048\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8611.912127185427\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.13492063492064\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8613.68308411479\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.11587301587302\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8619.791315323946\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.11587301587302\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8623.439872181252\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.201587301587296\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8608.12883996604\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.21587301587302\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8626.045958045272\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.27460317460318\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8603.066347611388\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.18730158730158\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8614.896062992422\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.24603174603175\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8621.34209617167\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.18412698412698\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8615.755921380709\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.2\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8609.132510826614\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.14126984126984\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8619.086882508318\n",
      "Iteration: 39\n",
      "Training :: Blind : 56.23650793650794\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8608.348216698148\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.17460317460318\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8617.782420484651\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.21111111111111\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8610.505292426082\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.16984126984127\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8622.474426740315\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.17142857142857\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8612.215694887145\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.166666666666664\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8615.004938751936\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.20793650793651\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8617.66550568158\n",
      "Iteration: 46\n",
      "Training :: Blind : 56.2\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8607.731663964296\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.11904761904762\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8621.843308512962\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.16031746031746\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8625.961878941898\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.25396825396825\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8612.733021548414\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.12698412698413\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8623.91801444328\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.14444444444444\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8628.65317234486\n",
      "Iteration: 52\n",
      "Training :: Blind : 56.15079365079365\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8622.517792563107\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.17142857142857\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8612.528702346124\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.25238095238095\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8610.767479951599\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.101587301587294\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8625.918272337687\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.12698412698413\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8613.856706129449\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.165079365079364\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8612.981715867398\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.21587301587302\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8609.338754975855\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.20952380952381\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8619.910327610753\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.1047619047619\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8624.003254927227\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.17142857142857\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8619.339168453214\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.16031746031746\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8616.357253551123\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.16349206349206\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8622.105266464692\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8622.192616093449\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.23492063492064\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8618.624595727259\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.195238095238096\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8613.629094117787\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.25238095238095\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8612.43516119889\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.21111111111111\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8616.144528707031\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.13809523809524\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8621.175158815182\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.08888888888889\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8623.725192477248\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.13015873015873\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8628.095706288641\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.1920634920635\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8612.894384922085\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.13492063492064\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8619.442155177552\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.18412698412698\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8614.078318785236\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.27460317460318\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8606.726871380943\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.24126984126983\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8618.84866658292\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.128571428571426\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8629.034288483082\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.19047619047619\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8623.85314490815\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.111111111111114\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8631.561587052442\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.201587301587296\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8608.520383627429\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8618.680371000719\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.147619047619045\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8618.334642316931\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.24920634920635\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8612.278656939368\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.14444444444444\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8618.112555353062\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.2936507936508\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8608.419566582434\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.18888888888889\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8617.174692575383\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.10952380952381\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8619.13968197298\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.12063492063491\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8617.92880391383\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.18412698412698\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8618.791279349743\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.16190476190476\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8622.061665809455\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.21746031746032\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8615.34789990295\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.101587301587294\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8626.187673301738\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.146031746031746\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8616.665431624491\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.082539682539675\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8628.320507113705\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.20793650793651\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8617.010575966167\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.21587301587302\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8611.368670622522\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.06031746031746\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8626.26109248523\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.18730158730158\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8616.115430869384\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.13333333333333\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8626.697788260912\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.12222222222222\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8627.536411623201\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.33015873015873\n",
      "Validation  :: Blind : 48.5 :: Blind Loss : 10527.763685625945\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.266666666666666\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 9581.503061223939\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.58253968253969\n",
      "Validation  :: Blind : 54.54285714285714 :: Blind Loss : 9271.98656589969\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.43174603174603\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 9017.117870952705\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.26031746031745\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8836.075672490628\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.61904761904763\n",
      "Validation  :: Blind : 56.528571428571425 :: Blind Loss : 8699.656670045424\n",
      "Iteration: 7\n",
      "Training :: Blind : 56.831746031746036\n",
      "Validation  :: Blind : 56.54285714285714 :: Blind Loss : 8635.589883967099\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.17142857142857\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8557.81684715717\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8517.876824671735\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.1984126984127\n",
      "Validation  :: Blind : 56.89999999999999 :: Blind Loss : 8538.906496488555\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8537.519043837416\n",
      "Iteration: 12\n",
      "Training :: Blind : 56.942857142857136\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8580.36709002144\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.061904761904756\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8572.92293885996\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.13650793650794\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8550.955499595782\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8517.17798813302\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8533.948586760856\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8528.705735449767\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8534.048067623118\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8529.442556590544\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.27301587301588\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8523.453334022848\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.28253968253968\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8515.155123368291\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8518.198578126081\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.352380952380955\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8510.327119587051\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8515.516062829302\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8506.196491664961\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8518.324790155251\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8514.743204619092\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8517.224291959617\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.231746031746034\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8512.971739022163\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8523.21213814926\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8522.73545820892\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.233333333333334\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8522.45115265107\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8510.929810684145\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8512.618213431018\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8507.957722672392\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.26349206349206\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8522.152712167524\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.284126984126985\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8526.731511017964\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.24761904761905\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8515.43220083121\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8516.908191518674\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8520.700280131889\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8511.165041926626\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8519.793902503156\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.30952380952381\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8526.10316140945\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8503.805573640293\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.24285714285714\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8519.956210137276\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8518.201662950996\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8505.749760125618\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.2936507936508\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8520.49274654423\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8514.357395893767\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.266666666666666\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8514.112614642803\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.24761904761905\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8522.131115537253\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.22222222222222\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8525.098602268172\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8512.442086426403\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.22380952380952\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8522.16912123982\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8525.798758953912\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.22222222222222\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8517.37031299833\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.287301587301585\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8513.520790659228\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.1984126984127\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8525.662222257546\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8500.396908908182\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.32857142857143\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8513.883465994415\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8522.27583881585\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8523.022973983336\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8504.780033358216\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.25079365079365\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8513.16617231505\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8525.59811800073\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8522.412344275132\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8512.233702083528\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.23492063492064\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8522.500985887104\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8514.28489325021\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.319047619047616\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8511.302814961595\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8519.00743242626\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8519.163518162019\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8517.420976519992\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.220634920634915\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8523.437971230891\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8514.891207248937\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8519.471603535922\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8511.623363754807\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.231746031746034\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8517.30708874005\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.265079365079366\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8522.429366391814\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8505.508842113246\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.266666666666666\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8515.125260410421\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8512.97632971298\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.24126984126984\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8513.132327673418\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8513.323252789734\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8502.955121817615\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8520.211962970803\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.24603174603175\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8515.012186231068\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8511.865223876654\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8523.988711296523\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.24920634920635\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8515.585080834868\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8515.081185965277\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8506.449127978904\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.27142857142857\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8510.73396125684\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.304761904761904\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8512.442552552366\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.26031746031745\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8519.750528079352\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8503.802742411975\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8513.064864707569\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8512.302204235348\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.24285714285714\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8518.223685220884\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.25238095238095\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8529.580117663503\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.33015873015873\n",
      "Validation  :: Blind : 48.5 :: Blind Loss : 10527.763685625945\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.266666666666666\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 9581.503061223939\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.58253968253969\n",
      "Validation  :: Blind : 54.54285714285714 :: Blind Loss : 9271.98656589969\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.43174603174603\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 9017.117870952705\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.26031746031745\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8836.075672490628\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.61904761904763\n",
      "Validation  :: Blind : 56.528571428571425 :: Blind Loss : 8699.656670045424\n",
      "Iteration: 7\n",
      "Training :: Blind : 56.831746031746036\n",
      "Validation  :: Blind : 56.54285714285714 :: Blind Loss : 8635.589883967099\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.17142857142857\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8557.81684715717\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8517.876824671735\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.1984126984127\n",
      "Validation  :: Blind : 56.89999999999999 :: Blind Loss : 8538.906496488555\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.112698412698414\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8537.519043837416\n",
      "Iteration: 12\n",
      "Training :: Blind : 56.942857142857136\n",
      "Validation  :: Blind : 56.8 :: Blind Loss : 8580.36709002144\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.061904761904756\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8572.92293885996\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.13650793650794\n",
      "Validation  :: Blind : 57.057142857142864 :: Blind Loss : 8550.955499595782\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8517.17798813302\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8533.948586760856\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8528.705735449767\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8534.048067623118\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8529.442556590544\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.27301587301588\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8523.453334022848\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.28253968253968\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8515.155123368291\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8518.198578126081\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.352380952380955\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8510.327119587051\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8515.516062829302\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8506.196491664961\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8518.324790155251\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8514.743204619092\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8517.224291959617\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.231746031746034\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8512.971739022163\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8523.21213814926\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8522.73545820892\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.233333333333334\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8522.45115265107\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8510.929810684145\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8512.618213431018\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8507.957722672392\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.26349206349206\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8522.152712167524\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.284126984126985\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8526.731511017964\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.24761904761905\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8515.43220083121\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.3015873015873\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8516.908191518674\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.23968253968253\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8520.700280131889\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.32698412698413\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8511.165041926626\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.30793650793651\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8519.793902503156\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.30952380952381\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8526.10316140945\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8503.805573640293\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.24285714285714\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8519.956210137276\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8518.201662950996\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8505.749760125618\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.2936507936508\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8520.49274654423\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8514.357395893767\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.266666666666666\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8514.112614642803\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.24761904761905\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8522.131115537253\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.22222222222222\n",
      "Validation  :: Blind : 57.214285714285715 :: Blind Loss : 8525.098602268172\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8512.442086426403\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.22380952380952\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8522.16912123982\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8525.798758953912\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.22222222222222\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8517.37031299833\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.287301587301585\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8513.520790659228\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.1984126984127\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8525.662222257546\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8500.396908908182\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.32857142857143\n",
      "Validation  :: Blind : 57.22857142857143 :: Blind Loss : 8513.883465994415\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8522.27583881585\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8523.022973983336\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8504.780033358216\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.25079365079365\n",
      "Validation  :: Blind : 57.17142857142857 :: Blind Loss : 8513.16617231505\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.29841269841269\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8525.59811800073\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8522.412344275132\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8512.233702083528\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.23492063492064\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8522.500985887104\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8514.28489325021\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.319047619047616\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8511.302814961595\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8519.00743242626\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8519.163518162019\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8517.420976519992\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.220634920634915\n",
      "Validation  :: Blind : 57.3 :: Blind Loss : 8523.437971230891\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8514.891207248937\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.285714285714285\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8519.471603535922\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8511.623363754807\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.231746031746034\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8517.30708874005\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.265079365079366\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8522.429366391814\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.36666666666667\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8505.508842113246\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.266666666666666\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8515.125260410421\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8512.97632971298\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.24126984126984\n",
      "Validation  :: Blind : 57.199999999999996 :: Blind Loss : 8513.132327673418\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8513.323252789734\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8502.955121817615\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.341269841269835\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8520.211962970803\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.24603174603175\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8515.012186231068\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8511.865223876654\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8523.988711296523\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.24920634920635\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8515.585080834868\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.32857142857143 :: Blind Loss : 8515.081185965277\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8506.449127978904\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.27142857142857\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8510.73396125684\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.304761904761904\n",
      "Validation  :: Blind : 57.18571428571428 :: Blind Loss : 8512.442552552366\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.26031746031745\n",
      "Validation  :: Blind : 57.371428571428574 :: Blind Loss : 8519.750528079352\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8503.802742411975\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 57.27142857142857 :: Blind Loss : 8513.064864707569\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.31746031746032\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8512.302204235348\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.24285714285714\n",
      "Validation  :: Blind : 57.24285714285714 :: Blind Loss : 8518.223685220884\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.25238095238095\n",
      "Validation  :: Blind : 57.25714285714286 :: Blind Loss : 8529.580117663503\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.11904761904762\n",
      "Validation  :: Blind : 50.357142857142854 :: Blind Loss : 9386.257749383762\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.573015873015876\n",
      "Validation  :: Blind : 54.785714285714285 :: Blind Loss : 8731.503419809163\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.25555555555556\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8553.827256950184\n",
      "Iteration: 4\n",
      "Training :: Blind : 56.01587301587302\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8491.631819942568\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.611111111111114\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8432.656749212014\n",
      "Iteration: 6\n",
      "Training :: Blind : 57.01904761904761\n",
      "Validation  :: Blind : 57.128571428571426 :: Blind Loss : 8383.875691230285\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.17460317460318\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8341.918006221787\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.13015873015873\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8337.095693255382\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.371428571428574\n",
      "Validation  :: Blind : 57.91428571428572 :: Blind Loss : 8283.974866906445\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8276.490479901493\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8280.519498323856\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.63650793650794\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8243.792634617463\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.14126984126984\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8303.952451874397\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.37777777777778\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8263.948534488218\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.26190476190476\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8283.573046369416\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.18412698412698\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8296.013908736135\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8281.386422023403\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.22857142857143\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8289.759336321542\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.199999999999996\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8291.886137241989\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.36349206349206\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8271.039982853728\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.25238095238095\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8293.972370930562\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.15079365079365\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8302.365307529337\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8284.826229999948\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.23650793650794\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8284.9257402235\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8289.813090253861\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8284.090262199446\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.46984126984127\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8264.30938753414\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8285.78731614016\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.441269841269836\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8274.702217225033\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.425396825396824\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8284.351399846182\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8284.238079505576\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.26190476190476\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8295.265147259714\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.43174603174603\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8271.15555150167\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8270.444813639628\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8277.735528584737\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.54285714285714\n",
      "Validation  :: Blind : 57.9 :: Blind Loss : 8265.27094022156\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.45714285714286\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8265.374850035647\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8272.886466815136\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.407936507936505\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8276.45028283596\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8281.139910401818\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8281.95851159752\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.4\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8290.486360289238\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.37619047619048\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8283.029220561451\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.35714285714286\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8292.817347965742\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8289.970809672981\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.22222222222222\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8306.638928526314\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8286.429665177711\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.319047619047616\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8279.459634913635\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.42063492063492\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8281.36220750798\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.436507936507944\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8280.182944730881\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.3063492063492\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8285.293596174055\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.51904761904761\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8271.560628994292\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8286.892843096295\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.3968253968254\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8278.224786911698\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.460317460317455\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8266.020437241043\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.45555555555556\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8272.100910637157\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.35555555555556\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8278.203254468008\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.4952380952381\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8264.942039425849\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8284.72839197106\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.53174603174603\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8278.10583104099\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.304761904761904\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8280.77604221484\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8272.610947823152\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.36507936507936\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8281.421015647054\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.453968253968256\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8268.77599473175\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.36190476190476\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8281.411119895547\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.452380952380956\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8267.42888100131\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.39841269841269\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8287.290828698493\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.51587301587302\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8257.919419680757\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.331746031746036\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8285.107898499391\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.471428571428575\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8272.459052210852\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.425396825396824\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8271.748288582145\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8280.714934307289\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.322222222222216\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8287.164135637482\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.438095238095244\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8273.615680417457\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.44444444444444\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8269.527641864366\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.304761904761904\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8287.96956271873\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.339682539682535\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8282.775131233828\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.37301587301587\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8276.917840887527\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.41269841269842\n",
      "Validation  :: Blind : 57.64285714285714 :: Blind Loss : 8266.62018050003\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.525396825396825\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8269.273823518648\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.331746031746036\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8288.77761675023\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.41904761904762\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8272.091949274552\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8294.308838462319\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.51111111111111\n",
      "Validation  :: Blind : 57.84285714285714 :: Blind Loss : 8268.041698628733\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8290.250732483984\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.490476190476194\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8266.652532523276\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.407936507936505\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8279.79472208066\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8279.46460440709\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.24126984126984\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8290.341643806532\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.358730158730154\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8284.595632619123\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.438095238095244\n",
      "Validation  :: Blind : 57.82857142857143 :: Blind Loss : 8272.052754716718\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.55555555555556\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8260.57765771799\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.43174603174603\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8271.7052645761\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.46349206349206\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8287.891587191796\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.41587301587302\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8275.024561934075\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.56349206349206\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8259.234092134058\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.24920634920635\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8301.856980731824\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.77142857142857 :: Blind Loss : 8285.304602330309\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.75714285714286 :: Blind Loss : 8274.697181093614\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.2952380952381\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8284.400592001515\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 42.833333333333336\n",
      "Validation  :: Blind : 43.31428571428572 :: Blind Loss : 10650.11887830763\n",
      "Iteration: 2\n",
      "Training :: Blind : 48.44603174603175\n",
      "Validation  :: Blind : 48.34285714285714 :: Blind Loss : 9961.152314669245\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.61111111111111\n",
      "Validation  :: Blind : 51.32857142857142 :: Blind Loss : 9301.485871081442\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.59206349206349\n",
      "Validation  :: Blind : 51.78571428571429 :: Blind Loss : 9035.430818926987\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.139682539682546\n",
      "Validation  :: Blind : 53.38571428571428 :: Blind Loss : 8896.888021301505\n",
      "Iteration: 6\n",
      "Training :: Blind : 53.5\n",
      "Validation  :: Blind : 52.98571428571428 :: Blind Loss : 8571.11166684117\n",
      "Iteration: 7\n",
      "Training :: Blind : 54.01904761904762\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 9400.451130747071\n",
      "Iteration: 8\n",
      "Training :: Blind : 53.81587301587302\n",
      "Validation  :: Blind : 53.142857142857146 :: Blind Loss : 8539.507288940222\n",
      "Iteration: 9\n",
      "Training :: Blind : 54.009523809523806\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 8526.376426212233\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.612698412698414\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8602.743720243936\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.7063492063492\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 8635.730789327805\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.804761904761904\n",
      "Validation  :: Blind : 54.17142857142857 :: Blind Loss : 8726.638496425174\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.541269841269845\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8631.017171308104\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.58095238095238\n",
      "Validation  :: Blind : 54.18571428571428 :: Blind Loss : 8665.915926268788\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.43333333333334\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8557.2865405749\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.36984126984127\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8522.134223205176\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.45238095238095\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8451.064580226392\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.661904761904765\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8513.996980355412\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.560317460317464\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8528.045228072668\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.547619047619044\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8536.972807325012\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.6063492063492\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8558.772862350925\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.628571428571426\n",
      "Validation  :: Blind : 54.08571428571428 :: Blind Loss : 8536.295249194063\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.74761904761905\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8584.942548749343\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.60793650793651\n",
      "Validation  :: Blind : 54.07142857142857 :: Blind Loss : 8614.298474721109\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.712698412698415\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8542.906566218437\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.576190476190476\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8588.98381082744\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.63809523809524\n",
      "Validation  :: Blind : 54.114285714285714 :: Blind Loss : 8557.627843278637\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.539682539682545\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8606.00540812432\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.628571428571426\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8491.901304384377\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.69047619047619\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8531.642608747718\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.55714285714286\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8560.41835637682\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.614285714285714\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8493.501048430488\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.62698412698413\n",
      "Validation  :: Blind : 54.07142857142857 :: Blind Loss : 8587.619704894307\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.53650793650794\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8540.096982954301\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.74603174603174\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8529.95460691756\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.666666666666664\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8545.767309092198\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.68571428571428\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8627.242077048468\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.60952380952381\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8613.953131098171\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.65555555555556\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8557.920875117568\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.573015873015876\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8512.364461035762\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.50158730158731\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8503.183709206482\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.541269841269845\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8571.928030066862\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.52380952380952\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8585.283602160896\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.715873015873015\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8517.926580882879\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.52380952380952\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8543.892559559426\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.560317460317464\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8512.201185112326\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.64444444444444\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8513.137995605883\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.65079365079365\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8603.606448540148\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.72222222222223\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 8595.861025787512\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.58095238095238\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8496.09690359838\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.50158730158731\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8531.102507519561\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.684126984126976\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8570.787531731316\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.63650793650794\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8575.583422076712\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.67142857142857\n",
      "Validation  :: Blind : 53.57142857142857 :: Blind Loss : 8435.248200566115\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.7063492063492\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8581.19770200242\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.61746031746032\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8525.355266807765\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.62222222222223\n",
      "Validation  :: Blind : 54.07142857142857 :: Blind Loss : 8534.20874418455\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.61587301587302\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8552.855336469484\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.768253968253966\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8566.660787637096\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.541269841269845\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8526.014695958675\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.60158730158731\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8482.129863674465\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.61746031746032\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8545.347119622038\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.65555555555556\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 8502.190773695664\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.493650793650794\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8538.979281231917\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.68253968253968\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8586.45638536036\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.68253968253968\n",
      "Validation  :: Blind : 54.07142857142857 :: Blind Loss : 8569.375956168737\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.56666666666666\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8507.906620103295\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.614285714285714\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8556.630222673542\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.696825396825396\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8553.946053511136\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.71111111111111\n",
      "Validation  :: Blind : 54.300000000000004 :: Blind Loss : 8561.881191999026\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.83015873015873\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8549.75974202422\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.62698412698413\n",
      "Validation  :: Blind : 54.2 :: Blind Loss : 8611.417828355126\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.64444444444444\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8523.949851571582\n",
      "Iteration: 74\n",
      "Training :: Blind : 54.56666666666666\n",
      "Validation  :: Blind : 54.028571428571425 :: Blind Loss : 8537.830327036463\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.61746031746032\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8551.707342962063\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.58888888888889\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8521.891976242028\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.574603174603176\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8559.997947408861\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.61904761904762\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8502.182874284468\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.65079365079365\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8475.594079559494\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.63333333333333\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8527.371780555615\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.6031746031746\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8548.71116367011\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.595238095238095\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8533.000499629086\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.49523809523809\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8538.164513174097\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.641269841269846\n",
      "Validation  :: Blind : 54.0 :: Blind Loss : 8548.467602681034\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.6031746031746\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8523.119475571872\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.56190476190477\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8567.795419433169\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.6063492063492\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8577.59148593884\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.612698412698414\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8591.902048584698\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.63333333333333\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8530.233652764968\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.74126984126985\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 8503.715185697449\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.630158730158726\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8524.685106932568\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.57142857142857\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8528.702816408244\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.661904761904765\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8549.028235609352\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.55714285714286\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 8495.552477341913\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.46507936507936\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8525.045801781736\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.576190476190476\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8593.94149668901\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.661904761904765\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8580.790179074529\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.74920634920635\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8543.786826238733\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.800000000000004\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8520.919753121867\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.7\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 8506.431620000709\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.161904761904765\n",
      "Validation  :: Blind : 48.24285714285714 :: Blind Loss : 10482.458255516589\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.34920634920635\n",
      "Validation  :: Blind : 51.87142857142857 :: Blind Loss : 9483.719137357657\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.25396825396825\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8994.904224973317\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.77301587301587\n",
      "Validation  :: Blind : 54.57142857142857 :: Blind Loss : 8860.414802497333\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.9984126984127\n",
      "Validation  :: Blind : 54.68571428571428 :: Blind Loss : 8788.922529869771\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.51428571428572\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8683.732715751288\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.66984126984127\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8636.161915234492\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.84920634920635\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8625.257994352454\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.833333333333336\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8616.003065921393\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.84444444444444\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8611.262921755555\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.582539682539675\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8644.466270642624\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.601587301587294\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8697.861483794011\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.83809523809524\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8665.71735496273\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.89047619047619\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8665.3971034334\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.72380952380952\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8686.371277520673\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.2968253968254\n",
      "Validation  :: Blind : 55.27142857142857 :: Blind Loss : 8780.57323588843\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.800000000000004\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8709.617928801717\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.46666666666666\n",
      "Validation  :: Blind : 55.371428571428574 :: Blind Loss : 8760.161568690868\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.86507936507936\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8703.865570666358\n",
      "Iteration: 20\n",
      "Training :: Blind : 56.061904761904756\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8672.91094182507\n",
      "Iteration: 21\n",
      "Training :: Blind : 56.21746031746032\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8647.943219341778\n",
      "Iteration: 22\n",
      "Training :: Blind : 56.29047619047619\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8635.51189818513\n",
      "Iteration: 23\n",
      "Training :: Blind : 56.36507936507936\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8609.741977448128\n",
      "Iteration: 24\n",
      "Training :: Blind : 56.3\n",
      "Validation  :: Blind : 56.10000000000001 :: Blind Loss : 8608.080020499037\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.333333333333336\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8610.692446300327\n",
      "Iteration: 26\n",
      "Training :: Blind : 56.25714285714286\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8614.0087581884\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.34761904761905\n",
      "Validation  :: Blind : 56.10000000000001 :: Blind Loss : 8601.044820135692\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.28095238095238\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8624.075510052371\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.35079365079365\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8614.172548598479\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.33968253968254\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8617.651855090338\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.25079365079365\n",
      "Validation  :: Blind : 56.08571428571428 :: Blind Loss : 8627.195322415937\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.28253968253968\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8628.045884821788\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.25873015873017\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8626.67767919934\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.331746031746036\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8625.829266950932\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.24444444444444\n",
      "Validation  :: Blind : 55.942857142857136 :: Blind Loss : 8634.436517523376\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.31746031746032\n",
      "Validation  :: Blind : 56.128571428571426 :: Blind Loss : 8624.00473916865\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.333333333333336\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8624.37635726716\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.26507936507936\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8632.480728244995\n",
      "Iteration: 39\n",
      "Training :: Blind : 56.277777777777786\n",
      "Validation  :: Blind : 56.128571428571426 :: Blind Loss : 8628.570279343816\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.25396825396825\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8626.319810467103\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.26984126984127\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8627.835427338254\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.29047619047619\n",
      "Validation  :: Blind : 56.08571428571428 :: Blind Loss : 8621.297960883412\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.28253968253968\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8628.688044284485\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.28888888888889\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8628.878872690133\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.36190476190476\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8622.810406626504\n",
      "Iteration: 46\n",
      "Training :: Blind : 56.3079365079365\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8619.822487903532\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.2063492063492\n",
      "Validation  :: Blind : 55.885714285714286 :: Blind Loss : 8636.950554319803\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.2952380952381\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8624.81208376821\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.28253968253968\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8624.035489958558\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.29047619047619\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8621.872377084506\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.37936507936509\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8623.955667896125\n",
      "Iteration: 52\n",
      "Training :: Blind : 56.29047619047619\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8622.874579814714\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.24285714285714\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8629.715062761603\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.31587301587302\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8625.51561004315\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.298412698412704\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8627.801367727483\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.34920634920635\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8619.91144967272\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.287301587301585\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8621.377637467624\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.28888888888889\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8629.448841038098\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.304761904761904\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8631.335761411085\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.28253968253968\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8632.568381667626\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.285714285714285\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8629.541575614876\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.24444444444444\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8632.771504543149\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.322222222222216\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8618.03428979742\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.287301587301585\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8624.693362900864\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.34285714285714\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8615.511531902555\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.27460317460318\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8634.874559458583\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.304761904761904\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8633.149805211\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.34920634920635\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8623.640193958436\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.32857142857143\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8629.136565177067\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.3079365079365\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8620.705245881854\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.231746031746034\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8629.605769767433\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.29206349206349\n",
      "Validation  :: Blind : 56.10000000000001 :: Blind Loss : 8619.269369972848\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.1968253968254\n",
      "Validation  :: Blind : 56.10000000000001 :: Blind Loss : 8636.283412871657\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.31428571428572\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8621.003264082792\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.304761904761904\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8625.54050752045\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.266666666666666\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8626.284519850116\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.24761904761905\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8630.111672403546\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.353968253968254\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8628.189123562774\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.24444444444444\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8623.26114670964\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.27301587301587\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8630.980599551214\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.33015873015873\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8620.319379225552\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.3063492063492\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8631.20392025229\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.35714285714286\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8615.137084600257\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.25079365079365\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8628.018078047106\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.333333333333336\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8620.873694858199\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.18095238095238\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8632.920422478961\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.3063492063492\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8627.710360281731\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.2952380952381\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8623.380605693019\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.2063492063492\n",
      "Validation  :: Blind : 56.128571428571426 :: Blind Loss : 8632.6158853317\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.298412698412704\n",
      "Validation  :: Blind : 55.92857142857143 :: Blind Loss : 8627.148221316838\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.31746031746032\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8625.490802155422\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.25555555555556\n",
      "Validation  :: Blind : 56.00000000000001 :: Blind Loss : 8629.56521456665\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.304761904761904\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8626.51850013973\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.29206349206349\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8626.449518887115\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.31111111111111\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8633.32961346734\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.28888888888889\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8630.012753132756\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.22857142857143\n",
      "Validation  :: Blind : 56.07142857142857 :: Blind Loss : 8628.037332305974\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.26507936507936\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8628.259222663673\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.3015873015873\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8618.838587321145\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.27301587301587\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8631.020314614772\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.75555555555556\n",
      "Validation  :: Blind : 48.8 :: Blind Loss : 10488.802992518053\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.3984126984127\n",
      "Validation  :: Blind : 53.44285714285715 :: Blind Loss : 9587.336374130911\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.44920634920635\n",
      "Validation  :: Blind : 53.94285714285715 :: Blind Loss : 9324.908103346397\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.32857142857143\n",
      "Validation  :: Blind : 54.74285714285714 :: Blind Loss : 9175.10431113159\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.461904761904755\n",
      "Validation  :: Blind : 54.92857142857142 :: Blind Loss : 9099.710082184452\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.39047619047619\n",
      "Validation  :: Blind : 55.07142857142857 :: Blind Loss : 9076.501309980027\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.630158730158726\n",
      "Validation  :: Blind : 55.25714285714286 :: Blind Loss : 9041.375817361986\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.804761904761904\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8974.581784911908\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.84920634920635\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8927.272536685748\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.82539682539682\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8945.716048600229\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.60952380952381\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 9011.023109390891\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.73809523809524\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8998.305213932266\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.68253968253968\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8992.545714370353\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.72380952380952\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8998.575377745121\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8964.801339749978\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.83015873015873\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8951.650511185027\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.82380952380952\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8950.883420773713\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8951.507310523331\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.82698412698412\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8952.798711065165\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.841269841269835\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8965.121589111826\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.94603174603174\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8951.194581807267\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.9031746031746\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8958.771730736551\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.833333333333336\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8959.73495584964\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.98888888888889\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8939.51565662061\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.006349206349206\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8955.013126077061\n",
      "Iteration: 26\n",
      "Training :: Blind : 55.98253968253968\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8938.473832692627\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.02222222222222\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8945.59215227309\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.039682539682545\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8951.554858363499\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.01428571428572\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8950.256699727426\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.03650793650794\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8944.154162069273\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.019047619047626\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8935.875984065111\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8948.031501060192\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.02380952380952\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8945.020655222783\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.00000000000001\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8948.528014025047\n",
      "Iteration: 35\n",
      "Training :: Blind : 55.96349206349206\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8951.362127609951\n",
      "Iteration: 36\n",
      "Training :: Blind : 55.952380952380956\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8942.2853454486\n",
      "Iteration: 37\n",
      "Training :: Blind : 55.917460317460325\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8940.194188977723\n",
      "Iteration: 38\n",
      "Training :: Blind : 55.92857142857143\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8950.005650531213\n",
      "Iteration: 39\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8954.615864460688\n",
      "Iteration: 40\n",
      "Training :: Blind : 55.93174603174603\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8939.860502229476\n",
      "Iteration: 41\n",
      "Training :: Blind : 55.925396825396824\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8949.79130450456\n",
      "Iteration: 42\n",
      "Training :: Blind : 55.96825396825397\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8957.824690796793\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.01269841269841\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8942.787211038147\n",
      "Iteration: 44\n",
      "Training :: Blind : 55.953968253968256\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8940.905940673918\n",
      "Iteration: 45\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8944.941328239658\n",
      "Iteration: 46\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8941.64954495918\n",
      "Iteration: 47\n",
      "Training :: Blind : 55.953968253968256\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8937.067509996974\n",
      "Iteration: 48\n",
      "Training :: Blind : 55.833333333333336\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8946.569850270662\n",
      "Iteration: 49\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8946.630654059853\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.02063492063492\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8948.18467810841\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.074603174603176\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8946.733272474914\n",
      "Iteration: 52\n",
      "Training :: Blind : 55.95873015873016\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8944.772745299546\n",
      "Iteration: 53\n",
      "Training :: Blind : 55.99047619047619\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8938.026124496508\n",
      "Iteration: 54\n",
      "Training :: Blind : 55.93174603174603\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8950.606582835959\n",
      "Iteration: 55\n",
      "Training :: Blind : 55.898412698412706\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8953.022011804136\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.026984126984125\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8934.002971216012\n",
      "Iteration: 57\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8951.740825060813\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.03015873015873\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8946.093486472371\n",
      "Iteration: 59\n",
      "Training :: Blind : 55.974603174603175\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8949.199536821794\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.058730158730164\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8938.440767026352\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.026984126984125\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8942.610466072145\n",
      "Iteration: 62\n",
      "Training :: Blind : 55.93333333333334\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8936.447190527702\n",
      "Iteration: 63\n",
      "Training :: Blind : 55.96031746031747\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8940.411958523395\n",
      "Iteration: 64\n",
      "Training :: Blind : 55.93968253968254\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8945.22129161286\n",
      "Iteration: 65\n",
      "Training :: Blind : 55.97619047619048\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8950.445010314317\n",
      "Iteration: 66\n",
      "Training :: Blind : 55.993650793650794\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8952.38556665898\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.05396825396826\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8939.835262805762\n",
      "Iteration: 68\n",
      "Training :: Blind : 55.98253968253968\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8941.632178143947\n",
      "Iteration: 69\n",
      "Training :: Blind : 55.93968253968254\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8950.597764511971\n",
      "Iteration: 70\n",
      "Training :: Blind : 55.95873015873016\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8939.269194666476\n",
      "Iteration: 71\n",
      "Training :: Blind : 55.92380952380952\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8951.607512645702\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.02063492063492\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8948.416501689317\n",
      "Iteration: 73\n",
      "Training :: Blind : 55.974603174603175\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8945.0185803266\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.019047619047626\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8939.715500581877\n",
      "Iteration: 75\n",
      "Training :: Blind : 55.942857142857136\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8938.05691458603\n",
      "Iteration: 76\n",
      "Training :: Blind : 55.919047619047625\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8946.933442337227\n",
      "Iteration: 77\n",
      "Training :: Blind : 55.97619047619048\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8958.243332360713\n",
      "Iteration: 78\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8947.351479493418\n",
      "Iteration: 79\n",
      "Training :: Blind : 55.9968253968254\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8942.99629080669\n",
      "Iteration: 80\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8955.254451238376\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.0047619047619\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8950.795974959025\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.03333333333333\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8943.296233671697\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.01269841269841\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8947.695907090176\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.01746031746032\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8940.002840503712\n",
      "Iteration: 85\n",
      "Training :: Blind : 55.97619047619048\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8945.719450941393\n",
      "Iteration: 86\n",
      "Training :: Blind : 55.9952380952381\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8950.246003156812\n",
      "Iteration: 87\n",
      "Training :: Blind : 55.98571428571428\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8940.86774305852\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.02380952380952\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8946.035432559707\n",
      "Iteration: 89\n",
      "Training :: Blind : 55.993650793650794\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8949.874775839946\n",
      "Iteration: 90\n",
      "Training :: Blind : 55.96349206349206\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8949.45715953083\n",
      "Iteration: 91\n",
      "Training :: Blind : 55.953968253968256\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8951.59177424477\n",
      "Iteration: 92\n",
      "Training :: Blind : 55.84444444444444\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8947.712341115042\n",
      "Iteration: 93\n",
      "Training :: Blind : 55.942857142857136\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8952.40999871345\n",
      "Iteration: 94\n",
      "Training :: Blind : 55.98412698412698\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8949.844720289171\n",
      "Iteration: 95\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8941.988495043186\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.0015873015873\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8944.042102601255\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.01269841269841\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8953.415146990414\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.038095238095245\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8945.526091889673\n",
      "Iteration: 99\n",
      "Training :: Blind : 55.85714285714286\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8951.876102612781\n",
      "Iteration: 100\n",
      "Training :: Blind : 55.95714285714286\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8945.219134905528\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.75555555555556\n",
      "Validation  :: Blind : 48.8 :: Blind Loss : 10488.802992518053\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.3984126984127\n",
      "Validation  :: Blind : 53.44285714285715 :: Blind Loss : 9587.336374130911\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.44920634920635\n",
      "Validation  :: Blind : 53.94285714285715 :: Blind Loss : 9324.908103346397\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.32857142857143\n",
      "Validation  :: Blind : 54.74285714285714 :: Blind Loss : 9175.10431113159\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.461904761904755\n",
      "Validation  :: Blind : 54.92857142857142 :: Blind Loss : 9099.710082184452\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.39047619047619\n",
      "Validation  :: Blind : 55.07142857142857 :: Blind Loss : 9076.501309980027\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.630158730158726\n",
      "Validation  :: Blind : 55.25714285714286 :: Blind Loss : 9041.375817361986\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.804761904761904\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8974.581784911908\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.84920634920635\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8927.272536685748\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.82539682539682\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8945.716048600229\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.60952380952381\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 9011.023109390891\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.73809523809524\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8998.305213932266\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.68253968253968\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8992.545714370353\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.72380952380952\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8998.575377745121\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8964.801339749978\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.83015873015873\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8951.650511185027\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.82380952380952\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8950.883420773713\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8951.507310523331\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.82698412698412\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8952.798711065165\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.841269841269835\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8965.121589111826\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.94603174603174\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8951.194581807267\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.9031746031746\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8958.771730736551\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.833333333333336\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8959.73495584964\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.98888888888889\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8939.51565662061\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.006349206349206\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8955.013126077061\n",
      "Iteration: 26\n",
      "Training :: Blind : 55.98253968253968\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8938.473832692627\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.02222222222222\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8945.59215227309\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.039682539682545\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8951.554858363499\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.01428571428572\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8950.256699727426\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.03650793650794\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8944.154162069273\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.019047619047626\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8935.875984065111\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8948.031501060192\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.02380952380952\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8945.020655222783\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.00000000000001\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8948.528014025047\n",
      "Iteration: 35\n",
      "Training :: Blind : 55.96349206349206\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8951.362127609951\n",
      "Iteration: 36\n",
      "Training :: Blind : 55.952380952380956\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8942.2853454486\n",
      "Iteration: 37\n",
      "Training :: Blind : 55.917460317460325\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8940.194188977723\n",
      "Iteration: 38\n",
      "Training :: Blind : 55.92857142857143\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8950.005650531213\n",
      "Iteration: 39\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8954.615864460688\n",
      "Iteration: 40\n",
      "Training :: Blind : 55.93174603174603\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8939.860502229476\n",
      "Iteration: 41\n",
      "Training :: Blind : 55.925396825396824\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8949.79130450456\n",
      "Iteration: 42\n",
      "Training :: Blind : 55.96825396825397\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8957.824690796793\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.01269841269841\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8942.787211038147\n",
      "Iteration: 44\n",
      "Training :: Blind : 55.953968253968256\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8940.905940673918\n",
      "Iteration: 45\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8944.941328239658\n",
      "Iteration: 46\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8941.64954495918\n",
      "Iteration: 47\n",
      "Training :: Blind : 55.953968253968256\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8937.067509996974\n",
      "Iteration: 48\n",
      "Training :: Blind : 55.833333333333336\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8946.569850270662\n",
      "Iteration: 49\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8946.630654059853\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.02063492063492\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8948.18467810841\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.074603174603176\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8946.733272474914\n",
      "Iteration: 52\n",
      "Training :: Blind : 55.95873015873016\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8944.772745299546\n",
      "Iteration: 53\n",
      "Training :: Blind : 55.99047619047619\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8938.026124496508\n",
      "Iteration: 54\n",
      "Training :: Blind : 55.93174603174603\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8950.606582835959\n",
      "Iteration: 55\n",
      "Training :: Blind : 55.898412698412706\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8953.022011804136\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.026984126984125\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8934.002971216012\n",
      "Iteration: 57\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8951.740825060813\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.03015873015873\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8946.093486472371\n",
      "Iteration: 59\n",
      "Training :: Blind : 55.974603174603175\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8949.199536821794\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.058730158730164\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8938.440767026352\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.026984126984125\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8942.610466072145\n",
      "Iteration: 62\n",
      "Training :: Blind : 55.93333333333334\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8936.447190527702\n",
      "Iteration: 63\n",
      "Training :: Blind : 55.96031746031747\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8940.411958523395\n",
      "Iteration: 64\n",
      "Training :: Blind : 55.93968253968254\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8945.22129161286\n",
      "Iteration: 65\n",
      "Training :: Blind : 55.97619047619048\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8950.445010314317\n",
      "Iteration: 66\n",
      "Training :: Blind : 55.993650793650794\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8952.38556665898\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.05396825396826\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8939.835262805762\n",
      "Iteration: 68\n",
      "Training :: Blind : 55.98253968253968\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8941.632178143947\n",
      "Iteration: 69\n",
      "Training :: Blind : 55.93968253968254\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8950.597764511971\n",
      "Iteration: 70\n",
      "Training :: Blind : 55.95873015873016\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8939.269194666476\n",
      "Iteration: 71\n",
      "Training :: Blind : 55.92380952380952\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8951.607512645702\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.02063492063492\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8948.416501689317\n",
      "Iteration: 73\n",
      "Training :: Blind : 55.974603174603175\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8945.0185803266\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.019047619047626\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8939.715500581877\n",
      "Iteration: 75\n",
      "Training :: Blind : 55.942857142857136\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8938.05691458603\n",
      "Iteration: 76\n",
      "Training :: Blind : 55.919047619047625\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8946.933442337227\n",
      "Iteration: 77\n",
      "Training :: Blind : 55.97619047619048\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8958.243332360713\n",
      "Iteration: 78\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8947.351479493418\n",
      "Iteration: 79\n",
      "Training :: Blind : 55.9968253968254\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8942.99629080669\n",
      "Iteration: 80\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8955.254451238376\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.0047619047619\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8950.795974959025\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.03333333333333\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8943.296233671697\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.01269841269841\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8947.695907090176\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.01746031746032\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8940.002840503712\n",
      "Iteration: 85\n",
      "Training :: Blind : 55.97619047619048\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8945.719450941393\n",
      "Iteration: 86\n",
      "Training :: Blind : 55.9952380952381\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8950.246003156812\n",
      "Iteration: 87\n",
      "Training :: Blind : 55.98571428571428\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8940.86774305852\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.02380952380952\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8946.035432559707\n",
      "Iteration: 89\n",
      "Training :: Blind : 55.993650793650794\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8949.874775839946\n",
      "Iteration: 90\n",
      "Training :: Blind : 55.96349206349206\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8949.45715953083\n",
      "Iteration: 91\n",
      "Training :: Blind : 55.953968253968256\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8951.59177424477\n",
      "Iteration: 92\n",
      "Training :: Blind : 55.84444444444444\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8947.712341115042\n",
      "Iteration: 93\n",
      "Training :: Blind : 55.942857142857136\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8952.40999871345\n",
      "Iteration: 94\n",
      "Training :: Blind : 55.98412698412698\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8949.844720289171\n",
      "Iteration: 95\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.871428571428574 :: Blind Loss : 8941.988495043186\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.0015873015873\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8944.042102601255\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.01269841269841\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8953.415146990414\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.038095238095245\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8945.526091889673\n",
      "Iteration: 99\n",
      "Training :: Blind : 55.85714285714286\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8951.876102612781\n",
      "Iteration: 100\n",
      "Training :: Blind : 55.95714285714286\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8945.219134905528\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.17777777777778\n",
      "Validation  :: Blind : 50.28571428571429 :: Blind Loss : 9384.276870667163\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.53650793650794\n",
      "Validation  :: Blind : 54.85714285714286 :: Blind Loss : 8732.425788187951\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.3952380952381\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8542.801137920516\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8512.916574271569\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.77936507936507\n",
      "Validation  :: Blind : 56.99999999999999 :: Blind Loss : 8419.813439456184\n",
      "Iteration: 6\n",
      "Training :: Blind : 56.89206349206349\n",
      "Validation  :: Blind : 57.14285714285714 :: Blind Loss : 8393.704308883154\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.201587301587296\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8341.299108010026\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.304761904761904\n",
      "Validation  :: Blind : 57.785714285714285 :: Blind Loss : 8311.315671352186\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.473015873015875\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8289.581076942628\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.46507936507936\n",
      "Validation  :: Blind : 58.02857142857143 :: Blind Loss : 8270.288950705488\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8277.449154812024\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.8 :: Blind Loss : 8293.432798005251\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.46984126984127\n",
      "Validation  :: Blind : 57.94285714285714 :: Blind Loss : 8253.85581184214\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.79841269841269\n",
      "Validation  :: Blind : 58.22857142857143 :: Blind Loss : 8253.427898404698\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.78253968253968\n",
      "Validation  :: Blind : 58.22857142857143 :: Blind Loss : 8249.811069787302\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.78888888888889\n",
      "Validation  :: Blind : 58.17142857142857 :: Blind Loss : 8239.926323669093\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.77142857142857\n",
      "Validation  :: Blind : 58.22857142857143 :: Blind Loss : 8252.05197334967\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.67936507936508\n",
      "Validation  :: Blind : 58.11428571428572 :: Blind Loss : 8267.997004320536\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.646031746031746\n",
      "Validation  :: Blind : 58.099999999999994 :: Blind Loss : 8265.821877916263\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.77142857142857\n",
      "Validation  :: Blind : 58.199999999999996 :: Blind Loss : 8263.244928217815\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.869841269841274\n",
      "Validation  :: Blind : 58.31428571428572 :: Blind Loss : 8244.386171804947\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.88412698412698\n",
      "Validation  :: Blind : 58.471428571428575 :: Blind Loss : 8268.273541632832\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.75714285714286\n",
      "Validation  :: Blind : 58.15714285714285 :: Blind Loss : 8278.01998740227\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.46666666666667\n",
      "Validation  :: Blind : 57.74285714285714 :: Blind Loss : 8333.424593473552\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.45079365079365\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8302.604148119415\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.29206349206349\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8314.560847276533\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.24444444444444\n",
      "Validation  :: Blind : 57.285714285714285 :: Blind Loss : 8349.116008672958\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.822222222222216\n",
      "Validation  :: Blind : 56.699999999999996 :: Blind Loss : 8396.724935456748\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.41269841269841\n",
      "Validation  :: Blind : 56.371428571428574 :: Blind Loss : 8431.397663198117\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.419047619047625\n",
      "Validation  :: Blind : 56.49999999999999 :: Blind Loss : 8413.18572961322\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.768253968253966\n",
      "Validation  :: Blind : 56.699999999999996 :: Blind Loss : 8394.021720865152\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.592063492063495\n",
      "Validation  :: Blind : 56.45714285714286 :: Blind Loss : 8401.691697841814\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.885714285714286\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8387.015820368535\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.63968253968253\n",
      "Validation  :: Blind : 56.51428571428572 :: Blind Loss : 8392.09052315327\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.860317460317454\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8406.532146666665\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.52063492063492\n",
      "Validation  :: Blind : 56.51428571428572 :: Blind Loss : 8415.320978871607\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.81904761904762\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8390.464290321877\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.65555555555556\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8403.748179379843\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.093650793650795\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8375.596042942923\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.85714285714286\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8390.96066385755\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.599999999999994\n",
      "Validation  :: Blind : 56.48571428571428 :: Blind Loss : 8399.448900083902\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.798412698412704\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8402.572666079926\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.8952380952381\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8390.886780162376\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.599999999999994\n",
      "Validation  :: Blind : 56.599999999999994 :: Blind Loss : 8411.219969719668\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.777777777777786\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8389.374354718386\n",
      "Iteration: 46\n",
      "Training :: Blind : 56.557142857142864\n",
      "Validation  :: Blind : 56.54285714285714 :: Blind Loss : 8422.897305797916\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.971428571428575\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8377.60710758798\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.507936507936506\n",
      "Validation  :: Blind : 56.442857142857136 :: Blind Loss : 8424.689169632906\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.67142857142857\n",
      "Validation  :: Blind : 56.528571428571425 :: Blind Loss : 8395.467784028737\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.907936507936505\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8377.568619017922\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.03650793650794\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8381.99607654448\n",
      "Iteration: 52\n",
      "Training :: Blind : 56.84444444444444\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8388.937215134702\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.56984126984127\n",
      "Validation  :: Blind : 56.35714285714286 :: Blind Loss : 8411.858629014387\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.519047619047626\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8421.632793418788\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.56825396825397\n",
      "Validation  :: Blind : 56.54285714285714 :: Blind Loss : 8418.597162176577\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.6936507936508\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8411.186480017208\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.822222222222216\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8385.82996410695\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.733333333333334\n",
      "Validation  :: Blind : 56.57142857142857 :: Blind Loss : 8402.527771771402\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.90952380952381\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8387.072401652546\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.906349206349205\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8387.056532489645\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.614285714285714\n",
      "Validation  :: Blind : 56.48571428571428 :: Blind Loss : 8399.469459269116\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.86507936507936\n",
      "Validation  :: Blind : 56.81428571428572 :: Blind Loss : 8381.971610254019\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.46825396825397\n",
      "Validation  :: Blind : 56.54285714285714 :: Blind Loss : 8409.6679298614\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.712698412698415\n",
      "Validation  :: Blind : 56.557142857142864 :: Blind Loss : 8400.14361963929\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.853968253968254\n",
      "Validation  :: Blind : 56.785714285714285 :: Blind Loss : 8384.994171815017\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.65079365079365\n",
      "Validation  :: Blind : 56.528571428571425 :: Blind Loss : 8406.130560913178\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.839682539682535\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8385.149524475633\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.490476190476194\n",
      "Validation  :: Blind : 56.39999999999999 :: Blind Loss : 8430.141077335922\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.57619047619048\n",
      "Validation  :: Blind : 56.51428571428572 :: Blind Loss : 8402.828818971582\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.712698412698415\n",
      "Validation  :: Blind : 56.58571428571428 :: Blind Loss : 8401.674254042766\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.30952380952381\n",
      "Validation  :: Blind : 56.35714285714286 :: Blind Loss : 8426.112354073399\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.8952380952381\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8378.575302276693\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.95714285714286\n",
      "Validation  :: Blind : 57.01428571428572 :: Blind Loss : 8392.394694755476\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.712698412698415\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8411.118696236792\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.6920634920635\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8397.322071608178\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.917460317460325\n",
      "Validation  :: Blind : 56.58571428571428 :: Blind Loss : 8384.241871504837\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.990476190476194\n",
      "Validation  :: Blind : 56.95714285714286 :: Blind Loss : 8389.486235276228\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.60793650793651\n",
      "Validation  :: Blind : 56.58571428571428 :: Blind Loss : 8413.993845312698\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.601587301587294\n",
      "Validation  :: Blind : 56.64285714285714 :: Blind Loss : 8411.950476830865\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.419047619047625\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8417.549325106122\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.7031746031746\n",
      "Validation  :: Blind : 56.54285714285714 :: Blind Loss : 8403.884092747867\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.91587301587302\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8386.16554836531\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.82857142857143\n",
      "Validation  :: Blind : 56.77142857142857 :: Blind Loss : 8396.040503382434\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.582539682539675\n",
      "Validation  :: Blind : 56.54285714285714 :: Blind Loss : 8418.47241671808\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.007936507936506\n",
      "Validation  :: Blind : 56.885714285714286 :: Blind Loss : 8374.771381212315\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.66190476190476\n",
      "Validation  :: Blind : 56.628571428571426 :: Blind Loss : 8406.617757274615\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.904761904761905\n",
      "Validation  :: Blind : 56.84285714285714 :: Blind Loss : 8391.734907053346\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.81269841269842\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8384.798168560894\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.65396825396826\n",
      "Validation  :: Blind : 56.57142857142857 :: Blind Loss : 8396.305046589361\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.77301587301587\n",
      "Validation  :: Blind : 56.58571428571428 :: Blind Loss : 8401.440978361592\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.88412698412698\n",
      "Validation  :: Blind : 56.82857142857143 :: Blind Loss : 8384.52046098794\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.8015873015873\n",
      "Validation  :: Blind : 56.65714285714286 :: Blind Loss : 8387.786587705223\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.57777777777778\n",
      "Validation  :: Blind : 56.614285714285714 :: Blind Loss : 8402.476535166814\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.64285714285714\n",
      "Validation  :: Blind : 56.57142857142857 :: Blind Loss : 8401.559878935403\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.668253968253964\n",
      "Validation  :: Blind : 56.65714285714286 :: Blind Loss : 8410.182715248457\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.426984126984124\n",
      "Validation  :: Blind : 56.3 :: Blind Loss : 8411.860940940265\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.6920634920635\n",
      "Validation  :: Blind : 56.68571428571428 :: Blind Loss : 8401.778791385666\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.677777777777784\n",
      "Validation  :: Blind : 56.72857142857143 :: Blind Loss : 8407.207607207256\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.77619047619048\n",
      "Validation  :: Blind : 56.74285714285714 :: Blind Loss : 8390.227715942046\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.7936507936508\n",
      "Validation  :: Blind : 56.714285714285715 :: Blind Loss : 8394.112174749484\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 42.80634920634921\n",
      "Validation  :: Blind : 43.385714285714286 :: Blind Loss : 10691.594783640197\n",
      "Iteration: 2\n",
      "Training :: Blind : 48.74603174603175\n",
      "Validation  :: Blind : 48.84285714285714 :: Blind Loss : 9938.494768403158\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.74126984126984\n",
      "Validation  :: Blind : 51.08571428571429 :: Blind Loss : 9334.337033938715\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.62380952380953\n",
      "Validation  :: Blind : 51.92857142857142 :: Blind Loss : 9126.07505056277\n",
      "Iteration: 5\n",
      "Training :: Blind : 53.94285714285715\n",
      "Validation  :: Blind : 53.32857142857142 :: Blind Loss : 8868.965261969932\n",
      "Iteration: 6\n",
      "Training :: Blind : 53.75238095238095\n",
      "Validation  :: Blind : 52.92857142857142 :: Blind Loss : 8585.660336996705\n",
      "Iteration: 7\n",
      "Training :: Blind : 54.11746031746032\n",
      "Validation  :: Blind : 53.400000000000006 :: Blind Loss : 9510.801096629213\n",
      "Iteration: 8\n",
      "Training :: Blind : 54.19206349206349\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 9263.548649863718\n",
      "Iteration: 9\n",
      "Training :: Blind : 54.18888888888888\n",
      "Validation  :: Blind : 53.65714285714286 :: Blind Loss : 8510.500780491358\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.541269841269845\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8487.437253752762\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.64444444444444\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8608.873040069982\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.58888888888889\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8659.362488642653\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.86349206349206\n",
      "Validation  :: Blind : 54.35714285714286 :: Blind Loss : 8680.739342954193\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.79047619047619\n",
      "Validation  :: Blind : 54.01428571428571 :: Blind Loss : 8569.13182912764\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.407936507936505\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 8541.455525166959\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.43015873015873\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8523.058295578856\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.298412698412704\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8534.665793351804\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.17777777777778\n",
      "Validation  :: Blind : 53.35714285714286 :: Blind Loss : 8518.319195112292\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.304761904761904\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8512.819799313744\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.21746031746032\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8501.449223332682\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.43015873015873\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8519.167924575228\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.35714285714286\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8531.587715919119\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.27619047619048\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8535.046419708984\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.25555555555556\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8537.326730194838\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.455555555555556\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8512.861583117745\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.6936507936508\n",
      "Validation  :: Blind : 54.285714285714285 :: Blind Loss : 9154.139058055553\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.142857142857146\n",
      "Validation  :: Blind : 53.55714285714286 :: Blind Loss : 8504.391391702677\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.114285714285714\n",
      "Validation  :: Blind : 53.37142857142857 :: Blind Loss : 8496.186253591013\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.409523809523805\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8569.984610758958\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.353968253968254\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8481.178057168549\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.215873015873015\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8518.410091175185\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.38253968253969\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8547.98907011867\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.060317460317464\n",
      "Validation  :: Blind : 53.25714285714286 :: Blind Loss : 8482.005762351231\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.60952380952381\n",
      "Validation  :: Blind : 54.214285714285715 :: Blind Loss : 9175.051229642824\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.31269841269841\n",
      "Validation  :: Blind : 53.457142857142856 :: Blind Loss : 8489.06387391809\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.595238095238095\n",
      "Validation  :: Blind : 53.957142857142856 :: Blind Loss : 9106.593926253427\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.130158730158726\n",
      "Validation  :: Blind : 53.67142857142857 :: Blind Loss : 8540.464837747002\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.353968253968254\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8541.286063237496\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.628571428571426\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 9156.23635917611\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.65555555555556\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 9171.485934218923\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.547619047619044\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 9191.465402889069\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.54444444444444\n",
      "Validation  :: Blind : 54.214285714285715 :: Blind Loss : 9144.29671731087\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.55079365079365\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 9141.468264562323\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.558730158730164\n",
      "Validation  :: Blind : 54.25714285714286 :: Blind Loss : 9162.938188768374\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.13333333333333\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8537.784612008323\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.53015873015873\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 9162.130363353745\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.44444444444444\n",
      "Validation  :: Blind : 53.97142857142857 :: Blind Loss : 9179.43081308536\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.53174603174603\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 9201.833111912263\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.23968253968254\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8533.213735341647\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.34603174603174\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8523.633742790158\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.279365079365085\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8549.993928428063\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.2063492063492\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8537.514615509135\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8529.192910365316\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.24126984126985\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8552.402585046068\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.32857142857143\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8538.928340287577\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.37936507936508\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8553.181628418635\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.30952380952381\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8558.869780424888\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.317460317460316\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8540.562347098992\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.31428571428572\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8557.556433187765\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.49047619047619\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8565.822152008663\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.38253968253969\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8572.989703727702\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.32539682539682\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8559.155845769954\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.31428571428572\n",
      "Validation  :: Blind : 53.74285714285715 :: Blind Loss : 8562.789142542402\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.24444444444444\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8575.947337200532\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.2952380952381\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8549.97072988119\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.44920634920635\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8606.082976655058\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.3079365079365\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8590.451458319234\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.25396825396825\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8556.37796822005\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.22063492063492\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8549.275490197455\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.36825396825397\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8591.679894529301\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.31904761904762\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8578.875051649484\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.27619047619048\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8552.999156462734\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.37936507936508\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8592.67403317456\n",
      "Iteration: 74\n",
      "Training :: Blind : 54.36666666666666\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8518.34630529966\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.37460317460317\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8558.71322441152\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.29206349206349\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8556.70261213775\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.39206349206349\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8522.760725764168\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.474603174603175\n",
      "Validation  :: Blind : 53.84285714285715 :: Blind Loss : 8621.967085989267\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.35873015873016\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8559.316741681956\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.41269841269841\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8525.482442455579\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.317460317460316\n",
      "Validation  :: Blind : 53.55714285714286 :: Blind Loss : 8484.93779693168\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.563492063492056\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 8604.253279814066\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.31111111111111\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8562.528815266025\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.492063492063494\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8595.206657046027\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.37619047619048\n",
      "Validation  :: Blind : 53.85714285714286 :: Blind Loss : 8580.201749684704\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.43015873015873\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8578.168067691757\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.44285714285715\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8541.57110340141\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.520634920634926\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8555.709721467709\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.44761904761904\n",
      "Validation  :: Blind : 53.91428571428571 :: Blind Loss : 8524.167293428934\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.38253968253969\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8570.978395104488\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.32063492063492\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8510.470532733809\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.29365079365079\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8605.723744668569\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.334920634920636\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8540.926157880236\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.16507936507936\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8533.172916044352\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.26507936507936\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8556.319967728969\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.24285714285715\n",
      "Validation  :: Blind : 53.77142857142857 :: Blind Loss : 8561.690506640594\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.37301587301587\n",
      "Validation  :: Blind : 53.98571428571428 :: Blind Loss : 8521.249953253271\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.38412698412698\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8552.130005614357\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.34920634920635\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8529.233938456804\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.32222222222223\n",
      "Validation  :: Blind : 53.72857142857143 :: Blind Loss : 8575.485482870607\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.10793650793651\n",
      "Validation  :: Blind : 48.142857142857146 :: Blind Loss : 10494.644534947256\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.06349206349207\n",
      "Validation  :: Blind : 51.72857142857142 :: Blind Loss : 9536.512969149459\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.20317460317461\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 9015.536980438479\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.6031746031746\n",
      "Validation  :: Blind : 54.27142857142857 :: Blind Loss : 8876.948578488646\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.093650793650795\n",
      "Validation  :: Blind : 54.842857142857135 :: Blind Loss : 8781.035589462661\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.31587301587302\n",
      "Validation  :: Blind : 55.114285714285714 :: Blind Loss : 8709.179450825593\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.560317460317464\n",
      "Validation  :: Blind : 55.27142857142857 :: Blind Loss : 8709.55415953583\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.55079365079365\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8716.962315718218\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.46507936507936\n",
      "Validation  :: Blind : 54.98571428571428 :: Blind Loss : 8730.435977209694\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.54285714285714\n",
      "Validation  :: Blind : 55.114285714285714 :: Blind Loss : 8713.503782978356\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.29047619047619\n",
      "Validation  :: Blind : 54.77142857142857 :: Blind Loss : 8748.129640740717\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.139682539682546\n",
      "Validation  :: Blind : 54.800000000000004 :: Blind Loss : 8766.676238775093\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.195238095238096\n",
      "Validation  :: Blind : 54.88571428571428 :: Blind Loss : 8756.920387122795\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.17619047619048\n",
      "Validation  :: Blind : 54.75714285714286 :: Blind Loss : 8768.21902437434\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.24920634920635\n",
      "Validation  :: Blind : 54.95714285714286 :: Blind Loss : 8747.008681432155\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.8968253968254\n",
      "Validation  :: Blind : 54.57142857142857 :: Blind Loss : 8861.40241979204\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.060317460317464\n",
      "Validation  :: Blind : 54.528571428571425 :: Blind Loss : 8823.170570033966\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.24603174603174\n",
      "Validation  :: Blind : 54.871428571428574 :: Blind Loss : 8795.234208948403\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.52380952380952\n",
      "Validation  :: Blind : 55.1 :: Blind Loss : 8745.064981520878\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.6984126984127\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8714.024867779483\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.85079365079365\n",
      "Validation  :: Blind : 55.214285714285715 :: Blind Loss : 8718.548557408609\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.63650793650794\n",
      "Validation  :: Blind : 55.15714285714286 :: Blind Loss : 8728.002846312665\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.63174603174603\n",
      "Validation  :: Blind : 55.17142857142857 :: Blind Loss : 8751.175449503533\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.98571428571428\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8731.352008559224\n",
      "Iteration: 25\n",
      "Training :: Blind : 56.01111111111111\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8719.242312391747\n",
      "Iteration: 26\n",
      "Training :: Blind : 56.0968253968254\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8719.012927515038\n",
      "Iteration: 27\n",
      "Training :: Blind : 56.1031746031746\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8722.346360990463\n",
      "Iteration: 28\n",
      "Training :: Blind : 56.03650793650794\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8728.304394770284\n",
      "Iteration: 29\n",
      "Training :: Blind : 56.058730158730164\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8723.2970420663\n",
      "Iteration: 30\n",
      "Training :: Blind : 56.06666666666666\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8722.158289162264\n",
      "Iteration: 31\n",
      "Training :: Blind : 56.02380952380952\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8725.053537750377\n",
      "Iteration: 32\n",
      "Training :: Blind : 56.03174603174603\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8729.57310649805\n",
      "Iteration: 33\n",
      "Training :: Blind : 56.128571428571426\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8710.850893588497\n",
      "Iteration: 34\n",
      "Training :: Blind : 56.114285714285714\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8713.984608939876\n",
      "Iteration: 35\n",
      "Training :: Blind : 56.11746031746032\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8708.683067224618\n",
      "Iteration: 36\n",
      "Training :: Blind : 56.028571428571425\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8721.684165905692\n",
      "Iteration: 37\n",
      "Training :: Blind : 56.15396825396826\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8711.040846207037\n",
      "Iteration: 38\n",
      "Training :: Blind : 56.06507936507936\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8719.278878053585\n",
      "Iteration: 39\n",
      "Training :: Blind : 56.17460317460318\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8703.722494651196\n",
      "Iteration: 40\n",
      "Training :: Blind : 56.09047619047619\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8718.90299098875\n",
      "Iteration: 41\n",
      "Training :: Blind : 56.14444444444444\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8717.138766386288\n",
      "Iteration: 42\n",
      "Training :: Blind : 56.08412698412698\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8720.252382684579\n",
      "Iteration: 43\n",
      "Training :: Blind : 56.04920634920635\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8718.358799532925\n",
      "Iteration: 44\n",
      "Training :: Blind : 56.06349206349206\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8721.095697057564\n",
      "Iteration: 45\n",
      "Training :: Blind : 56.114285714285714\n",
      "Validation  :: Blind : 55.82857142857143 :: Blind Loss : 8716.20839702908\n",
      "Iteration: 46\n",
      "Training :: Blind : 56.15555555555556\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8715.317194069823\n",
      "Iteration: 47\n",
      "Training :: Blind : 56.080952380952375\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8717.423105330827\n",
      "Iteration: 48\n",
      "Training :: Blind : 56.166666666666664\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8709.087157773063\n",
      "Iteration: 49\n",
      "Training :: Blind : 56.12063492063491\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8717.572877785946\n",
      "Iteration: 50\n",
      "Training :: Blind : 56.179365079365084\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8710.957546621761\n",
      "Iteration: 51\n",
      "Training :: Blind : 56.139682539682546\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8710.715733533856\n",
      "Iteration: 52\n",
      "Training :: Blind : 56.11587301587302\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8714.603576135647\n",
      "Iteration: 53\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8721.029938534304\n",
      "Iteration: 54\n",
      "Training :: Blind : 56.05555555555556\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8723.139672434625\n",
      "Iteration: 55\n",
      "Training :: Blind : 56.06825396825397\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8716.031410263797\n",
      "Iteration: 56\n",
      "Training :: Blind : 56.12380952380952\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8715.784730302075\n",
      "Iteration: 57\n",
      "Training :: Blind : 56.08571428571428\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8725.541039961483\n",
      "Iteration: 58\n",
      "Training :: Blind : 56.046031746031744\n",
      "Validation  :: Blind : 55.85714285714286 :: Blind Loss : 8720.66753771306\n",
      "Iteration: 59\n",
      "Training :: Blind : 56.10634920634921\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8713.558992151018\n",
      "Iteration: 60\n",
      "Training :: Blind : 56.12539682539682\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8721.701074580926\n",
      "Iteration: 61\n",
      "Training :: Blind : 56.1047619047619\n",
      "Validation  :: Blind : 55.84285714285714 :: Blind Loss : 8710.05772546284\n",
      "Iteration: 62\n",
      "Training :: Blind : 56.10952380952381\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8721.834291483337\n",
      "Iteration: 63\n",
      "Training :: Blind : 56.007936507936506\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8730.311580254463\n",
      "Iteration: 64\n",
      "Training :: Blind : 56.14285714285714\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8710.727126031736\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.114285714285714\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8720.204142476072\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.06825396825397\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8720.212159181916\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.10634920634921\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8718.840056261146\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.10793650793651\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8719.366500529773\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.0984126984127\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8712.95826657019\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.11746031746032\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8713.365800005537\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.082539682539675\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8719.844205768939\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.0984126984127\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8722.895620837127\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.08571428571428\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8717.708239650747\n",
      "Iteration: 74\n",
      "Training :: Blind : 55.98888888888889\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8732.816874819719\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.01111111111111\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8726.987769426676\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.10000000000001\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8723.881345041525\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.165079365079364\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8704.636708684164\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.17301587301588\n",
      "Validation  :: Blind : 55.785714285714285 :: Blind Loss : 8709.12518093104\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.06825396825397\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8720.926958155369\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.128571428571426\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8715.832812765993\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.06984126984127\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8718.433563802011\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.15079365079365\n",
      "Validation  :: Blind : 55.900000000000006 :: Blind Loss : 8704.639063929346\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.05079365079365\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8723.659387250897\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.101587301587294\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8715.892257890026\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.13809523809524\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8715.621417296063\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.07777777777778\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8723.390625442502\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.13650793650794\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8707.387834252826\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.13015873015873\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8718.543356329617\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.12380952380952\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8709.75899572572\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.147619047619045\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8708.767840628594\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.074603174603176\n",
      "Validation  :: Blind : 55.800000000000004 :: Blind Loss : 8722.725283337015\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.11904761904762\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8715.147074215072\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.07777777777778\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8718.058550988459\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.05079365079365\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8731.61138283797\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.128571428571426\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8713.54086302225\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.13809523809524\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8715.01243456784\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.12222222222222\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8704.318563342607\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.13492063492064\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8705.60060921123\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.13492063492064\n",
      "Validation  :: Blind : 55.72857142857143 :: Blind Loss : 8720.469972363051\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.101587301587294\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8715.701800064178\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 49.2015873015873\n",
      "Validation  :: Blind : 49.214285714285715 :: Blind Loss : 10409.608861083248\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.48888888888889\n",
      "Validation  :: Blind : 53.24285714285715 :: Blind Loss : 9610.342832501155\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.4984126984127\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 9332.14047360192\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.061904761904756\n",
      "Validation  :: Blind : 54.32857142857143 :: Blind Loss : 9208.341379257388\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.233333333333334\n",
      "Validation  :: Blind : 54.77142857142857 :: Blind Loss : 9142.270081589762\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.298412698412704\n",
      "Validation  :: Blind : 54.68571428571428 :: Blind Loss : 9107.181060753732\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.4984126984127\n",
      "Validation  :: Blind : 55.214285714285715 :: Blind Loss : 9056.83124430177\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.75079365079365\n",
      "Validation  :: Blind : 55.214285714285715 :: Blind Loss : 9007.498400873137\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.822222222222216\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8961.480655295007\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.887301587301586\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8957.90256570428\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.62222222222222\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 9024.422583322801\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.601587301587294\n",
      "Validation  :: Blind : 55.17142857142857 :: Blind Loss : 9024.083656810944\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.63809523809524\n",
      "Validation  :: Blind : 55.07142857142857 :: Blind Loss : 9010.718812950674\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.63809523809524\n",
      "Validation  :: Blind : 55.14285714285714 :: Blind Loss : 9010.928177142487\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.76031746031747\n",
      "Validation  :: Blind : 55.2 :: Blind Loss : 8974.52633387538\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.800000000000004\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8975.92268551622\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.804761904761904\n",
      "Validation  :: Blind : 55.371428571428574 :: Blind Loss : 8978.219406193595\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.78253968253968\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8980.14792718285\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.766666666666666\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8985.370645110823\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8972.525600721545\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.7952380952381\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8983.480094571001\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.81111111111111\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8975.933215200936\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.81428571428572\n",
      "Validation  :: Blind : 55.27142857142857 :: Blind Loss : 8968.461640252497\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.81904761904762\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8975.108326695074\n",
      "Iteration: 25\n",
      "Training :: Blind : 55.79206349206349\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8973.864446349813\n",
      "Iteration: 26\n",
      "Training :: Blind : 55.81269841269842\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8972.334046521622\n",
      "Iteration: 27\n",
      "Training :: Blind : 55.798412698412704\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8983.862100462884\n",
      "Iteration: 28\n",
      "Training :: Blind : 55.80952380952381\n",
      "Validation  :: Blind : 55.1 :: Blind Loss : 8967.505365415556\n",
      "Iteration: 29\n",
      "Training :: Blind : 55.77142857142857\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8975.137994665565\n",
      "Iteration: 30\n",
      "Training :: Blind : 55.78095238095238\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8982.445065956976\n",
      "Iteration: 31\n",
      "Training :: Blind : 55.766666666666666\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8979.035378237671\n",
      "Iteration: 32\n",
      "Training :: Blind : 55.78253968253968\n",
      "Validation  :: Blind : 55.285714285714285 :: Blind Loss : 8972.235803664178\n",
      "Iteration: 33\n",
      "Training :: Blind : 55.779365079365085\n",
      "Validation  :: Blind : 55.25714285714286 :: Blind Loss : 8977.041278696517\n",
      "Iteration: 34\n",
      "Training :: Blind : 55.80952380952381\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 8981.873828502654\n",
      "Iteration: 35\n",
      "Training :: Blind : 55.8952380952381\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8978.260351455512\n",
      "Iteration: 36\n",
      "Training :: Blind : 55.77301587301587\n",
      "Validation  :: Blind : 55.34285714285714 :: Blind Loss : 8977.8366565187\n",
      "Iteration: 37\n",
      "Training :: Blind : 55.77619047619048\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8976.240102644831\n",
      "Iteration: 38\n",
      "Training :: Blind : 55.87301587301587\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8983.350939422317\n",
      "Iteration: 39\n",
      "Training :: Blind : 55.86031746031747\n",
      "Validation  :: Blind : 55.300000000000004 :: Blind Loss : 8977.257849320595\n",
      "Iteration: 40\n",
      "Training :: Blind : 55.77142857142857\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 8998.35183237159\n",
      "Iteration: 41\n",
      "Training :: Blind : 55.7968253968254\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 8981.094915402618\n",
      "Iteration: 42\n",
      "Training :: Blind : 55.85555555555556\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8981.500123549547\n",
      "Iteration: 43\n",
      "Training :: Blind : 55.885714285714286\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8985.19603022396\n",
      "Iteration: 44\n",
      "Training :: Blind : 55.81111111111111\n",
      "Validation  :: Blind : 55.25714285714286 :: Blind Loss : 8986.560670604284\n",
      "Iteration: 45\n",
      "Training :: Blind : 55.85079365079365\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8973.216796983965\n",
      "Iteration: 46\n",
      "Training :: Blind : 55.7936507936508\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8980.991833823264\n",
      "Iteration: 47\n",
      "Training :: Blind : 55.80952380952381\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8974.424273151139\n",
      "Iteration: 48\n",
      "Training :: Blind : 55.95714285714286\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8960.336723022063\n",
      "Iteration: 49\n",
      "Training :: Blind : 55.900000000000006\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 8962.429849214626\n",
      "Iteration: 50\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8964.630457086401\n",
      "Iteration: 51\n",
      "Training :: Blind : 55.9015873015873\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8958.894505702108\n",
      "Iteration: 52\n",
      "Training :: Blind : 55.91111111111111\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8965.481949182853\n",
      "Iteration: 53\n",
      "Training :: Blind : 55.87936507936509\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8955.098108448761\n",
      "Iteration: 54\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.41428571428572 :: Blind Loss : 8949.121189870972\n",
      "Iteration: 55\n",
      "Training :: Blind : 55.89206349206349\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8960.816404252262\n",
      "Iteration: 56\n",
      "Training :: Blind : 55.925396825396824\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8960.731790614605\n",
      "Iteration: 57\n",
      "Training :: Blind : 55.888888888888886\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8954.165406401618\n",
      "Iteration: 58\n",
      "Training :: Blind : 55.91111111111111\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8963.120285477811\n",
      "Iteration: 59\n",
      "Training :: Blind : 55.96349206349206\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8948.588026729902\n",
      "Iteration: 60\n",
      "Training :: Blind : 55.92222222222222\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8961.878965518368\n",
      "Iteration: 61\n",
      "Training :: Blind : 55.7936507936508\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8976.35333566765\n",
      "Iteration: 62\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8961.043616872985\n",
      "Iteration: 63\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.300000000000004 :: Blind Loss : 8961.937193408625\n",
      "Iteration: 64\n",
      "Training :: Blind : 55.885714285714286\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8960.876809564288\n",
      "Iteration: 65\n",
      "Training :: Blind : 55.852380952380955\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8961.994202350827\n",
      "Iteration: 66\n",
      "Training :: Blind : 55.919047619047625\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8955.376687402593\n",
      "Iteration: 67\n",
      "Training :: Blind : 55.84603174603174\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8966.8088090544\n",
      "Iteration: 68\n",
      "Training :: Blind : 55.9015873015873\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8960.483710446992\n",
      "Iteration: 69\n",
      "Training :: Blind : 55.853968253968254\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8950.244394263993\n",
      "Iteration: 70\n",
      "Training :: Blind : 55.9047619047619\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8959.320125797982\n",
      "Iteration: 71\n",
      "Training :: Blind : 55.95714285714286\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8965.910576770731\n",
      "Iteration: 72\n",
      "Training :: Blind : 55.917460317460325\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8956.351027645087\n",
      "Iteration: 73\n",
      "Training :: Blind : 55.82857142857143\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8966.701158483085\n",
      "Iteration: 74\n",
      "Training :: Blind : 55.83650793650794\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8975.26858899105\n",
      "Iteration: 75\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8968.579190179804\n",
      "Iteration: 76\n",
      "Training :: Blind : 55.81587301587302\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8966.1179713061\n",
      "Iteration: 77\n",
      "Training :: Blind : 55.82539682539682\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8964.54844838845\n",
      "Iteration: 78\n",
      "Training :: Blind : 55.871428571428574\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8973.256508274748\n",
      "Iteration: 79\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8953.75617327097\n",
      "Iteration: 80\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8953.608186507881\n",
      "Iteration: 81\n",
      "Training :: Blind : 55.852380952380955\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8957.883327851658\n",
      "Iteration: 82\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8965.095087277245\n",
      "Iteration: 83\n",
      "Training :: Blind : 55.871428571428574\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8966.010331914676\n",
      "Iteration: 84\n",
      "Training :: Blind : 55.852380952380955\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8961.736598202224\n",
      "Iteration: 85\n",
      "Training :: Blind : 55.9031746031746\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8958.087003814426\n",
      "Iteration: 86\n",
      "Training :: Blind : 55.942857142857136\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8959.026182013566\n",
      "Iteration: 87\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8963.535911505976\n",
      "Iteration: 88\n",
      "Training :: Blind : 55.89047619047619\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8965.358373487647\n",
      "Iteration: 89\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8968.292255871305\n",
      "Iteration: 90\n",
      "Training :: Blind : 55.84444444444444\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8965.010574084949\n",
      "Iteration: 91\n",
      "Training :: Blind : 55.79047619047619\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8970.26821160587\n",
      "Iteration: 92\n",
      "Training :: Blind : 55.9047619047619\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8953.73606667581\n",
      "Iteration: 93\n",
      "Training :: Blind : 55.94603174603174\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8953.50183716967\n",
      "Iteration: 94\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8965.588879924719\n",
      "Iteration: 95\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8957.022489492232\n",
      "Iteration: 96\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8953.541443928472\n",
      "Iteration: 97\n",
      "Training :: Blind : 55.84920634920635\n",
      "Validation  :: Blind : 55.27142857142857 :: Blind Loss : 8970.28283316765\n",
      "Iteration: 98\n",
      "Training :: Blind : 55.92857142857143\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8954.688339917451\n",
      "Iteration: 99\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.34285714285714 :: Blind Loss : 8960.69211973718\n",
      "Iteration: 100\n",
      "Training :: Blind : 55.834920634920636\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8965.217007807107\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 49.2015873015873\n",
      "Validation  :: Blind : 49.214285714285715 :: Blind Loss : 10409.608861083248\n",
      "Iteration: 2\n",
      "Training :: Blind : 53.48888888888889\n",
      "Validation  :: Blind : 53.24285714285715 :: Blind Loss : 9610.342832501155\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.4984126984127\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 9332.14047360192\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.061904761904756\n",
      "Validation  :: Blind : 54.32857142857143 :: Blind Loss : 9208.341379257388\n",
      "Iteration: 5\n",
      "Training :: Blind : 55.233333333333334\n",
      "Validation  :: Blind : 54.77142857142857 :: Blind Loss : 9142.270081589762\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.298412698412704\n",
      "Validation  :: Blind : 54.68571428571428 :: Blind Loss : 9107.181060753732\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.4984126984127\n",
      "Validation  :: Blind : 55.214285714285715 :: Blind Loss : 9056.83124430177\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.75079365079365\n",
      "Validation  :: Blind : 55.214285714285715 :: Blind Loss : 9007.498400873137\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.822222222222216\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8961.480655295007\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.887301587301586\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8957.90256570428\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.62222222222222\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 9024.422583322801\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.601587301587294\n",
      "Validation  :: Blind : 55.17142857142857 :: Blind Loss : 9024.083656810944\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.63809523809524\n",
      "Validation  :: Blind : 55.07142857142857 :: Blind Loss : 9010.718812950674\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.63809523809524\n",
      "Validation  :: Blind : 55.14285714285714 :: Blind Loss : 9010.928177142487\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.76031746031747\n",
      "Validation  :: Blind : 55.2 :: Blind Loss : 8974.52633387538\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.800000000000004\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8975.92268551622\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.804761904761904\n",
      "Validation  :: Blind : 55.371428571428574 :: Blind Loss : 8978.219406193595\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.78253968253968\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8980.14792718285\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.766666666666666\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8985.370645110823\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8972.525600721545\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.7952380952381\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8983.480094571001\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.81111111111111\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8975.933215200936\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.81428571428572\n",
      "Validation  :: Blind : 55.27142857142857 :: Blind Loss : 8968.461640252497\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.81904761904762\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8975.108326695074\n",
      "Iteration: 25\n",
      "Training :: Blind : 55.79206349206349\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8973.864446349813\n",
      "Iteration: 26\n",
      "Training :: Blind : 55.81269841269842\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8972.334046521622\n",
      "Iteration: 27\n",
      "Training :: Blind : 55.798412698412704\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8983.862100462884\n",
      "Iteration: 28\n",
      "Training :: Blind : 55.80952380952381\n",
      "Validation  :: Blind : 55.1 :: Blind Loss : 8967.505365415556\n",
      "Iteration: 29\n",
      "Training :: Blind : 55.77142857142857\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8975.137994665565\n",
      "Iteration: 30\n",
      "Training :: Blind : 55.78095238095238\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8982.445065956976\n",
      "Iteration: 31\n",
      "Training :: Blind : 55.766666666666666\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8979.035378237671\n",
      "Iteration: 32\n",
      "Training :: Blind : 55.78253968253968\n",
      "Validation  :: Blind : 55.285714285714285 :: Blind Loss : 8972.235803664178\n",
      "Iteration: 33\n",
      "Training :: Blind : 55.779365079365085\n",
      "Validation  :: Blind : 55.25714285714286 :: Blind Loss : 8977.041278696517\n",
      "Iteration: 34\n",
      "Training :: Blind : 55.80952380952381\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 8981.873828502654\n",
      "Iteration: 35\n",
      "Training :: Blind : 55.8952380952381\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8978.260351455512\n",
      "Iteration: 36\n",
      "Training :: Blind : 55.77301587301587\n",
      "Validation  :: Blind : 55.34285714285714 :: Blind Loss : 8977.8366565187\n",
      "Iteration: 37\n",
      "Training :: Blind : 55.77619047619048\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8976.240102644831\n",
      "Iteration: 38\n",
      "Training :: Blind : 55.87301587301587\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8983.350939422317\n",
      "Iteration: 39\n",
      "Training :: Blind : 55.86031746031747\n",
      "Validation  :: Blind : 55.300000000000004 :: Blind Loss : 8977.257849320595\n",
      "Iteration: 40\n",
      "Training :: Blind : 55.77142857142857\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 8998.35183237159\n",
      "Iteration: 41\n",
      "Training :: Blind : 55.7968253968254\n",
      "Validation  :: Blind : 55.22857142857143 :: Blind Loss : 8981.094915402618\n",
      "Iteration: 42\n",
      "Training :: Blind : 55.85555555555556\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8981.500123549547\n",
      "Iteration: 43\n",
      "Training :: Blind : 55.885714285714286\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8985.19603022396\n",
      "Iteration: 44\n",
      "Training :: Blind : 55.81111111111111\n",
      "Validation  :: Blind : 55.25714285714286 :: Blind Loss : 8986.560670604284\n",
      "Iteration: 45\n",
      "Training :: Blind : 55.85079365079365\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8973.216796983965\n",
      "Iteration: 46\n",
      "Training :: Blind : 55.7936507936508\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8980.991833823264\n",
      "Iteration: 47\n",
      "Training :: Blind : 55.80952380952381\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8974.424273151139\n",
      "Iteration: 48\n",
      "Training :: Blind : 55.95714285714286\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8960.336723022063\n",
      "Iteration: 49\n",
      "Training :: Blind : 55.900000000000006\n",
      "Validation  :: Blind : 55.35714285714286 :: Blind Loss : 8962.429849214626\n",
      "Iteration: 50\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8964.630457086401\n",
      "Iteration: 51\n",
      "Training :: Blind : 55.9015873015873\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8958.894505702108\n",
      "Iteration: 52\n",
      "Training :: Blind : 55.91111111111111\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8965.481949182853\n",
      "Iteration: 53\n",
      "Training :: Blind : 55.87936507936509\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8955.098108448761\n",
      "Iteration: 54\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.41428571428572 :: Blind Loss : 8949.121189870972\n",
      "Iteration: 55\n",
      "Training :: Blind : 55.89206349206349\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8960.816404252262\n",
      "Iteration: 56\n",
      "Training :: Blind : 55.925396825396824\n",
      "Validation  :: Blind : 55.75714285714286 :: Blind Loss : 8960.731790614605\n",
      "Iteration: 57\n",
      "Training :: Blind : 55.888888888888886\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8954.165406401618\n",
      "Iteration: 58\n",
      "Training :: Blind : 55.91111111111111\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8963.120285477811\n",
      "Iteration: 59\n",
      "Training :: Blind : 55.96349206349206\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8948.588026729902\n",
      "Iteration: 60\n",
      "Training :: Blind : 55.92222222222222\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8961.878965518368\n",
      "Iteration: 61\n",
      "Training :: Blind : 55.7936507936508\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8976.35333566765\n",
      "Iteration: 62\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8961.043616872985\n",
      "Iteration: 63\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.300000000000004 :: Blind Loss : 8961.937193408625\n",
      "Iteration: 64\n",
      "Training :: Blind : 55.885714285714286\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8960.876809564288\n",
      "Iteration: 65\n",
      "Training :: Blind : 55.852380952380955\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8961.994202350827\n",
      "Iteration: 66\n",
      "Training :: Blind : 55.919047619047625\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8955.376687402593\n",
      "Iteration: 67\n",
      "Training :: Blind : 55.84603174603174\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8966.8088090544\n",
      "Iteration: 68\n",
      "Training :: Blind : 55.9015873015873\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8960.483710446992\n",
      "Iteration: 69\n",
      "Training :: Blind : 55.853968253968254\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8950.244394263993\n",
      "Iteration: 70\n",
      "Training :: Blind : 55.9047619047619\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8959.320125797982\n",
      "Iteration: 71\n",
      "Training :: Blind : 55.95714285714286\n",
      "Validation  :: Blind : 55.77142857142857 :: Blind Loss : 8965.910576770731\n",
      "Iteration: 72\n",
      "Training :: Blind : 55.917460317460325\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8956.351027645087\n",
      "Iteration: 73\n",
      "Training :: Blind : 55.82857142857143\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8966.701158483085\n",
      "Iteration: 74\n",
      "Training :: Blind : 55.83650793650794\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8975.26858899105\n",
      "Iteration: 75\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8968.579190179804\n",
      "Iteration: 76\n",
      "Training :: Blind : 55.81587301587302\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8966.1179713061\n",
      "Iteration: 77\n",
      "Training :: Blind : 55.82539682539682\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8964.54844838845\n",
      "Iteration: 78\n",
      "Training :: Blind : 55.871428571428574\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8973.256508274748\n",
      "Iteration: 79\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8953.75617327097\n",
      "Iteration: 80\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8953.608186507881\n",
      "Iteration: 81\n",
      "Training :: Blind : 55.852380952380955\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8957.883327851658\n",
      "Iteration: 82\n",
      "Training :: Blind : 55.906349206349205\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8965.095087277245\n",
      "Iteration: 83\n",
      "Training :: Blind : 55.871428571428574\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8966.010331914676\n",
      "Iteration: 84\n",
      "Training :: Blind : 55.852380952380955\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8961.736598202224\n",
      "Iteration: 85\n",
      "Training :: Blind : 55.9031746031746\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8958.087003814426\n",
      "Iteration: 86\n",
      "Training :: Blind : 55.942857142857136\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8959.026182013566\n",
      "Iteration: 87\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8963.535911505976\n",
      "Iteration: 88\n",
      "Training :: Blind : 55.89047619047619\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8965.358373487647\n",
      "Iteration: 89\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.81428571428572 :: Blind Loss : 8968.292255871305\n",
      "Iteration: 90\n",
      "Training :: Blind : 55.84444444444444\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8965.010574084949\n",
      "Iteration: 91\n",
      "Training :: Blind : 55.79047619047619\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8970.26821160587\n",
      "Iteration: 92\n",
      "Training :: Blind : 55.9047619047619\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8953.73606667581\n",
      "Iteration: 93\n",
      "Training :: Blind : 55.94603174603174\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8953.50183716967\n",
      "Iteration: 94\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.442857142857136 :: Blind Loss : 8965.588879924719\n",
      "Iteration: 95\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8957.022489492232\n",
      "Iteration: 96\n",
      "Training :: Blind : 55.90952380952381\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8953.541443928472\n",
      "Iteration: 97\n",
      "Training :: Blind : 55.84920634920635\n",
      "Validation  :: Blind : 55.27142857142857 :: Blind Loss : 8970.28283316765\n",
      "Iteration: 98\n",
      "Training :: Blind : 55.92857142857143\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8954.688339917451\n",
      "Iteration: 99\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.34285714285714 :: Blind Loss : 8960.69211973718\n",
      "Iteration: 100\n",
      "Training :: Blind : 55.834920634920636\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8965.217007807107\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 50.15238095238095\n",
      "Validation  :: Blind : 50.44285714285714 :: Blind Loss : 9385.470245611645\n",
      "Iteration: 2\n",
      "Training :: Blind : 54.317460317460316\n",
      "Validation  :: Blind : 54.47142857142857 :: Blind Loss : 8742.421468165336\n",
      "Iteration: 3\n",
      "Training :: Blind : 55.23650793650794\n",
      "Validation  :: Blind : 55.31428571428572 :: Blind Loss : 8569.78984626928\n",
      "Iteration: 4\n",
      "Training :: Blind : 55.52380952380952\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8558.237836432534\n",
      "Iteration: 5\n",
      "Training :: Blind : 56.07142857142857\n",
      "Validation  :: Blind : 56.24285714285714 :: Blind Loss : 8484.189107171063\n",
      "Iteration: 6\n",
      "Training :: Blind : 57.04126984126984\n",
      "Validation  :: Blind : 57.15714285714286 :: Blind Loss : 8374.323970356647\n",
      "Iteration: 7\n",
      "Training :: Blind : 57.14126984126984\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8333.91853365478\n",
      "Iteration: 8\n",
      "Training :: Blind : 57.25396825396826\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8327.523560938713\n",
      "Iteration: 9\n",
      "Training :: Blind : 57.458730158730155\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8287.349521713126\n",
      "Iteration: 10\n",
      "Training :: Blind : 57.28888888888889\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8287.13615116507\n",
      "Iteration: 11\n",
      "Training :: Blind : 57.52063492063492\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8265.330821518153\n",
      "Iteration: 12\n",
      "Training :: Blind : 57.13333333333333\n",
      "Validation  :: Blind : 57.34285714285714 :: Blind Loss : 8298.190884389314\n",
      "Iteration: 13\n",
      "Training :: Blind : 57.1984126984127\n",
      "Validation  :: Blind : 57.31428571428572 :: Blind Loss : 8290.122626267563\n",
      "Iteration: 14\n",
      "Training :: Blind : 57.10634920634921\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8304.941087787487\n",
      "Iteration: 15\n",
      "Training :: Blind : 57.27460317460318\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8281.950497087193\n",
      "Iteration: 16\n",
      "Training :: Blind : 57.24603174603175\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8293.828541384817\n",
      "Iteration: 17\n",
      "Training :: Blind : 57.39047619047619\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8277.195484634889\n",
      "Iteration: 18\n",
      "Training :: Blind : 57.41904761904762\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8271.248741202879\n",
      "Iteration: 19\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8272.866853527808\n",
      "Iteration: 20\n",
      "Training :: Blind : 57.42857142857143\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8278.766460611885\n",
      "Iteration: 21\n",
      "Training :: Blind : 57.4031746031746\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8279.887332137716\n",
      "Iteration: 22\n",
      "Training :: Blind : 57.41904761904762\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8282.234008551515\n",
      "Iteration: 23\n",
      "Training :: Blind : 57.3952380952381\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8283.845706399894\n",
      "Iteration: 24\n",
      "Training :: Blind : 57.407936507936505\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8283.514997196016\n",
      "Iteration: 25\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8276.289546365962\n",
      "Iteration: 26\n",
      "Training :: Blind : 57.387301587301586\n",
      "Validation  :: Blind : 57.49999999999999 :: Blind Loss : 8269.492690661296\n",
      "Iteration: 27\n",
      "Training :: Blind : 57.296825396825405\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8294.278753722527\n",
      "Iteration: 28\n",
      "Training :: Blind : 57.219047619047615\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8302.651295249789\n",
      "Iteration: 29\n",
      "Training :: Blind : 57.34603174603174\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8286.23108300589\n",
      "Iteration: 30\n",
      "Training :: Blind : 57.199999999999996\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8283.577059359632\n",
      "Iteration: 31\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8283.287731616841\n",
      "Iteration: 32\n",
      "Training :: Blind : 57.219047619047615\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8294.879471451883\n",
      "Iteration: 33\n",
      "Training :: Blind : 57.42857142857143\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8266.43437380611\n",
      "Iteration: 34\n",
      "Training :: Blind : 57.26984126984127\n",
      "Validation  :: Blind : 57.45714285714286 :: Blind Loss : 8289.5684375366\n",
      "Iteration: 35\n",
      "Training :: Blind : 57.36825396825397\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8273.120718723374\n",
      "Iteration: 36\n",
      "Training :: Blind : 57.27142857142857\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8284.384767122014\n",
      "Iteration: 37\n",
      "Training :: Blind : 57.16031746031746\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8309.67271847155\n",
      "Iteration: 38\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8297.802472116418\n",
      "Iteration: 39\n",
      "Training :: Blind : 57.25873015873015\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8292.951910856125\n",
      "Iteration: 40\n",
      "Training :: Blind : 57.32380952380952\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8281.157119291836\n",
      "Iteration: 41\n",
      "Training :: Blind : 57.4015873015873\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8275.666125891725\n",
      "Iteration: 42\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8279.248195016338\n",
      "Iteration: 43\n",
      "Training :: Blind : 57.22857142857143\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8298.308345776924\n",
      "Iteration: 44\n",
      "Training :: Blind : 57.34761904761905\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8274.901483244623\n",
      "Iteration: 45\n",
      "Training :: Blind : 57.26031746031745\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8286.174212240187\n",
      "Iteration: 46\n",
      "Training :: Blind : 57.28253968253968\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8285.739629933163\n",
      "Iteration: 47\n",
      "Training :: Blind : 57.37301587301587\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8277.0795096437\n",
      "Iteration: 48\n",
      "Training :: Blind : 57.320634920634916\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8281.734192419179\n",
      "Iteration: 49\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8269.54421318707\n",
      "Iteration: 50\n",
      "Training :: Blind : 57.34603174603174\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8275.50856127548\n",
      "Iteration: 51\n",
      "Training :: Blind : 57.2031746031746\n",
      "Validation  :: Blind : 57.385714285714286 :: Blind Loss : 8298.877296860795\n",
      "Iteration: 52\n",
      "Training :: Blind : 57.18253968253968\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8308.00116101919\n",
      "Iteration: 53\n",
      "Training :: Blind : 57.34603174603174\n",
      "Validation  :: Blind : 57.599999999999994 :: Blind Loss : 8279.377848091362\n",
      "Iteration: 54\n",
      "Training :: Blind : 57.34920634920635\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8268.286974831932\n",
      "Iteration: 55\n",
      "Training :: Blind : 57.42063492063492\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8277.858297285702\n",
      "Iteration: 56\n",
      "Training :: Blind : 57.2936507936508\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8284.91392798641\n",
      "Iteration: 57\n",
      "Training :: Blind : 57.3968253968254\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8279.52863738573\n",
      "Iteration: 58\n",
      "Training :: Blind : 57.277777777777786\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8289.410957409564\n",
      "Iteration: 59\n",
      "Training :: Blind : 57.31428571428572\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8287.269385077652\n",
      "Iteration: 60\n",
      "Training :: Blind : 57.3936507936508\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8271.769215220032\n",
      "Iteration: 61\n",
      "Training :: Blind : 57.315873015873024\n",
      "Validation  :: Blind : 57.41428571428572 :: Blind Loss : 8273.784868760133\n",
      "Iteration: 62\n",
      "Training :: Blind : 57.212698412698415\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8297.687711672837\n",
      "Iteration: 63\n",
      "Training :: Blind : 57.315873015873024\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8282.416930403406\n",
      "Iteration: 64\n",
      "Training :: Blind : 57.24603174603175\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8290.918474315913\n",
      "Iteration: 65\n",
      "Training :: Blind : 57.29047619047619\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8278.52822579716\n",
      "Iteration: 66\n",
      "Training :: Blind : 57.24761904761905\n",
      "Validation  :: Blind : 57.471428571428575 :: Blind Loss : 8293.60420527194\n",
      "Iteration: 67\n",
      "Training :: Blind : 57.31269841269842\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8286.818559662806\n",
      "Iteration: 68\n",
      "Training :: Blind : 57.211111111111116\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8289.229602279134\n",
      "Iteration: 69\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8282.200610044838\n",
      "Iteration: 70\n",
      "Training :: Blind : 57.28253968253968\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8287.215955741905\n",
      "Iteration: 71\n",
      "Training :: Blind : 57.2031746031746\n",
      "Validation  :: Blind : 57.42857142857143 :: Blind Loss : 8298.135337495687\n",
      "Iteration: 72\n",
      "Training :: Blind : 57.333333333333336\n",
      "Validation  :: Blind : 57.51428571428572 :: Blind Loss : 8280.03885170542\n",
      "Iteration: 73\n",
      "Training :: Blind : 57.38253968253968\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8270.888961531002\n",
      "Iteration: 74\n",
      "Training :: Blind : 57.334920634920636\n",
      "Validation  :: Blind : 57.57142857142858 :: Blind Loss : 8282.299693422505\n",
      "Iteration: 75\n",
      "Training :: Blind : 57.166666666666664\n",
      "Validation  :: Blind : 57.4 :: Blind Loss : 8315.195673551101\n",
      "Iteration: 76\n",
      "Training :: Blind : 57.231746031746034\n",
      "Validation  :: Blind : 57.44285714285714 :: Blind Loss : 8302.101777012549\n",
      "Iteration: 77\n",
      "Training :: Blind : 57.46984126984127\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8268.42542607785\n",
      "Iteration: 78\n",
      "Training :: Blind : 57.38095238095238\n",
      "Validation  :: Blind : 57.67142857142857 :: Blind Loss : 8273.328455558632\n",
      "Iteration: 79\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8267.357671267098\n",
      "Iteration: 80\n",
      "Training :: Blind : 57.41428571428572\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8281.536699045508\n",
      "Iteration: 81\n",
      "Training :: Blind : 57.3031746031746\n",
      "Validation  :: Blind : 57.68571428571428 :: Blind Loss : 8280.095945683024\n",
      "Iteration: 82\n",
      "Training :: Blind : 57.369841269841274\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8275.037788373578\n",
      "Iteration: 83\n",
      "Training :: Blind : 57.39841269841269\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8279.112571261943\n",
      "Iteration: 84\n",
      "Training :: Blind : 57.33015873015873\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8273.696277698216\n",
      "Iteration: 85\n",
      "Training :: Blind : 57.44603174603174\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8266.248340464237\n",
      "Iteration: 86\n",
      "Training :: Blind : 57.436507936507944\n",
      "Validation  :: Blind : 57.58571428571428 :: Blind Loss : 8266.699965020787\n",
      "Iteration: 87\n",
      "Training :: Blind : 57.39047619047619\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8279.601164926291\n",
      "Iteration: 88\n",
      "Training :: Blind : 57.352380952380955\n",
      "Validation  :: Blind : 57.714285714285715 :: Blind Loss : 8289.934500135134\n",
      "Iteration: 89\n",
      "Training :: Blind : 57.39206349206349\n",
      "Validation  :: Blind : 57.54285714285714 :: Blind Loss : 8278.276924309248\n",
      "Iteration: 90\n",
      "Training :: Blind : 57.39841269841269\n",
      "Validation  :: Blind : 57.628571428571426 :: Blind Loss : 8268.202267660967\n",
      "Iteration: 91\n",
      "Training :: Blind : 57.48253968253968\n",
      "Validation  :: Blind : 57.72857142857143 :: Blind Loss : 8267.805313739764\n",
      "Iteration: 92\n",
      "Training :: Blind : 57.266666666666666\n",
      "Validation  :: Blind : 57.52857142857143 :: Blind Loss : 8278.359244573097\n",
      "Iteration: 93\n",
      "Training :: Blind : 57.37460317460318\n",
      "Validation  :: Blind : 57.657142857142865 :: Blind Loss : 8274.23924798529\n",
      "Iteration: 94\n",
      "Training :: Blind : 57.31111111111111\n",
      "Validation  :: Blind : 57.48571428571429 :: Blind Loss : 8285.973498348027\n",
      "Iteration: 95\n",
      "Training :: Blind : 57.43492063492064\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8270.532745070166\n",
      "Iteration: 96\n",
      "Training :: Blind : 57.38253968253968\n",
      "Validation  :: Blind : 57.699999999999996 :: Blind Loss : 8281.498257636544\n",
      "Iteration: 97\n",
      "Training :: Blind : 57.387301587301586\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8274.09387110777\n",
      "Iteration: 98\n",
      "Training :: Blind : 57.1920634920635\n",
      "Validation  :: Blind : 57.35714285714286 :: Blind Loss : 8297.637749271169\n",
      "Iteration: 99\n",
      "Training :: Blind : 57.32539682539682\n",
      "Validation  :: Blind : 57.557142857142864 :: Blind Loss : 8281.947902432465\n",
      "Iteration: 100\n",
      "Training :: Blind : 57.37936507936507\n",
      "Validation  :: Blind : 57.61428571428572 :: Blind Loss : 8282.763428476119\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 43.24126984126984\n",
      "Validation  :: Blind : 43.471428571428575 :: Blind Loss : 10650.892622311527\n",
      "Iteration: 2\n",
      "Training :: Blind : 49.098412698412695\n",
      "Validation  :: Blind : 49.028571428571425 :: Blind Loss : 9891.819183355303\n",
      "Iteration: 3\n",
      "Training :: Blind : 51.7095238095238\n",
      "Validation  :: Blind : 51.41428571428571 :: Blind Loss : 9321.124676701995\n",
      "Iteration: 4\n",
      "Training :: Blind : 52.39365079365079\n",
      "Validation  :: Blind : 51.68571428571429 :: Blind Loss : 9132.096469105247\n",
      "Iteration: 5\n",
      "Training :: Blind : 53.3063492063492\n",
      "Validation  :: Blind : 52.78571428571428 :: Blind Loss : 8911.410663464078\n",
      "Iteration: 6\n",
      "Training :: Blind : 52.87142857142857\n",
      "Validation  :: Blind : 52.485714285714295 :: Blind Loss : 8662.262620983023\n",
      "Iteration: 7\n",
      "Training :: Blind : 52.6984126984127\n",
      "Validation  :: Blind : 52.35714285714286 :: Blind Loss : 8652.073252401595\n",
      "Iteration: 8\n",
      "Training :: Blind : 53.973015873015875\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 9444.053874267298\n",
      "Iteration: 9\n",
      "Training :: Blind : 54.23174603174603\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 9333.212268177593\n",
      "Iteration: 10\n",
      "Training :: Blind : 54.43015873015873\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8626.391481905423\n",
      "Iteration: 11\n",
      "Training :: Blind : 54.49047619047619\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8658.664448086518\n",
      "Iteration: 12\n",
      "Training :: Blind : 54.71746031746032\n",
      "Validation  :: Blind : 54.08571428571428 :: Blind Loss : 8609.279742661529\n",
      "Iteration: 13\n",
      "Training :: Blind : 54.766666666666666\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 8680.692063136094\n",
      "Iteration: 14\n",
      "Training :: Blind : 54.684126984126976\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8550.014964916696\n",
      "Iteration: 15\n",
      "Training :: Blind : 54.6063492063492\n",
      "Validation  :: Blind : 53.88571428571428 :: Blind Loss : 8634.944499829322\n",
      "Iteration: 16\n",
      "Training :: Blind : 54.5984126984127\n",
      "Validation  :: Blind : 53.92857142857142 :: Blind Loss : 8499.989548474226\n",
      "Iteration: 17\n",
      "Training :: Blind : 54.38253968253969\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8547.25503114477\n",
      "Iteration: 18\n",
      "Training :: Blind : 54.287301587301585\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8507.476364874059\n",
      "Iteration: 19\n",
      "Training :: Blind : 54.45873015873016\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8460.782309465554\n",
      "Iteration: 20\n",
      "Training :: Blind : 54.269841269841265\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8518.02817117222\n",
      "Iteration: 21\n",
      "Training :: Blind : 54.234920634920634\n",
      "Validation  :: Blind : 53.68571428571428 :: Blind Loss : 8478.663137656995\n",
      "Iteration: 22\n",
      "Training :: Blind : 54.696825396825396\n",
      "Validation  :: Blind : 54.05714285714286 :: Blind Loss : 9166.398556794\n",
      "Iteration: 23\n",
      "Training :: Blind : 54.61111111111111\n",
      "Validation  :: Blind : 54.27142857142857 :: Blind Loss : 9186.04154204944\n",
      "Iteration: 24\n",
      "Training :: Blind : 54.695238095238096\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8425.227356067542\n",
      "Iteration: 25\n",
      "Training :: Blind : 54.78412698412698\n",
      "Validation  :: Blind : 54.18571428571428 :: Blind Loss : 9150.893393019123\n",
      "Iteration: 26\n",
      "Training :: Blind : 54.768253968253966\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 9163.001916168318\n",
      "Iteration: 27\n",
      "Training :: Blind : 54.73015873015873\n",
      "Validation  :: Blind : 54.22857142857143 :: Blind Loss : 9115.481119132712\n",
      "Iteration: 28\n",
      "Training :: Blind : 54.780952380952385\n",
      "Validation  :: Blind : 54.48571428571428 :: Blind Loss : 9144.195442514083\n",
      "Iteration: 29\n",
      "Training :: Blind : 54.788888888888884\n",
      "Validation  :: Blind : 54.042857142857144 :: Blind Loss : 8446.483047847114\n",
      "Iteration: 30\n",
      "Training :: Blind : 54.73968253968254\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 9189.080177690394\n",
      "Iteration: 31\n",
      "Training :: Blind : 54.83968253968254\n",
      "Validation  :: Blind : 54.24285714285715 :: Blind Loss : 9151.859788226931\n",
      "Iteration: 32\n",
      "Training :: Blind : 54.834920634920636\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 9195.812500381127\n",
      "Iteration: 33\n",
      "Training :: Blind : 54.250793650793646\n",
      "Validation  :: Blind : 53.642857142857146 :: Blind Loss : 8478.419051678662\n",
      "Iteration: 34\n",
      "Training :: Blind : 54.823809523809516\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 9197.019719949276\n",
      "Iteration: 35\n",
      "Training :: Blind : 54.35079365079365\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 8447.711223254386\n",
      "Iteration: 36\n",
      "Training :: Blind : 54.71746031746032\n",
      "Validation  :: Blind : 54.42857142857142 :: Blind Loss : 9163.974216524362\n",
      "Iteration: 37\n",
      "Training :: Blind : 54.787301587301585\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 9173.385580960026\n",
      "Iteration: 38\n",
      "Training :: Blind : 54.646031746031746\n",
      "Validation  :: Blind : 54.17142857142857 :: Blind Loss : 9115.767017266528\n",
      "Iteration: 39\n",
      "Training :: Blind : 54.355555555555554\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 8445.195031343505\n",
      "Iteration: 40\n",
      "Training :: Blind : 54.27301587301587\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8483.642339950868\n",
      "Iteration: 41\n",
      "Training :: Blind : 54.81269841269841\n",
      "Validation  :: Blind : 54.300000000000004 :: Blind Loss : 9172.333245324235\n",
      "Iteration: 42\n",
      "Training :: Blind : 54.61111111111111\n",
      "Validation  :: Blind : 54.31428571428572 :: Blind Loss : 9151.982296508482\n",
      "Iteration: 43\n",
      "Training :: Blind : 54.65238095238095\n",
      "Validation  :: Blind : 54.300000000000004 :: Blind Loss : 9093.515905640234\n",
      "Iteration: 44\n",
      "Training :: Blind : 54.33174603174603\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8458.916069656056\n",
      "Iteration: 45\n",
      "Training :: Blind : 54.70158730158731\n",
      "Validation  :: Blind : 54.2 :: Blind Loss : 9205.66935624307\n",
      "Iteration: 46\n",
      "Training :: Blind : 54.65396825396825\n",
      "Validation  :: Blind : 54.34285714285715 :: Blind Loss : 9179.953115813076\n",
      "Iteration: 47\n",
      "Training :: Blind : 54.67460317460318\n",
      "Validation  :: Blind : 54.114285714285714 :: Blind Loss : 9196.48032553076\n",
      "Iteration: 48\n",
      "Training :: Blind : 54.761904761904766\n",
      "Validation  :: Blind : 54.08571428571428 :: Blind Loss : 9132.842659587684\n",
      "Iteration: 49\n",
      "Training :: Blind : 54.70952380952381\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 9129.068198272264\n",
      "Iteration: 50\n",
      "Training :: Blind : 54.420634920634924\n",
      "Validation  :: Blind : 53.82857142857142 :: Blind Loss : 8450.270574495022\n",
      "Iteration: 51\n",
      "Training :: Blind : 54.68571428571428\n",
      "Validation  :: Blind : 54.17142857142857 :: Blind Loss : 9137.89396572798\n",
      "Iteration: 52\n",
      "Training :: Blind : 54.3952380952381\n",
      "Validation  :: Blind : 53.81428571428572 :: Blind Loss : 8450.096857026238\n",
      "Iteration: 53\n",
      "Training :: Blind : 54.72380952380952\n",
      "Validation  :: Blind : 54.15714285714286 :: Blind Loss : 9164.056411867568\n",
      "Iteration: 54\n",
      "Training :: Blind : 54.509523809523806\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8432.135130195438\n",
      "Iteration: 55\n",
      "Training :: Blind : 54.74603174603174\n",
      "Validation  :: Blind : 54.1 :: Blind Loss : 9161.04916325107\n",
      "Iteration: 56\n",
      "Training :: Blind : 54.355555555555554\n",
      "Validation  :: Blind : 53.714285714285715 :: Blind Loss : 8453.99571982622\n",
      "Iteration: 57\n",
      "Training :: Blind : 54.74285714285714\n",
      "Validation  :: Blind : 54.285714285714285 :: Blind Loss : 9218.049766138112\n",
      "Iteration: 58\n",
      "Training :: Blind : 54.48730158730159\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8454.942628899897\n",
      "Iteration: 59\n",
      "Training :: Blind : 54.800000000000004\n",
      "Validation  :: Blind : 54.34285714285715 :: Blind Loss : 9167.715501479348\n",
      "Iteration: 60\n",
      "Training :: Blind : 54.766666666666666\n",
      "Validation  :: Blind : 54.2 :: Blind Loss : 9149.083758242361\n",
      "Iteration: 61\n",
      "Training :: Blind : 54.75238095238095\n",
      "Validation  :: Blind : 54.028571428571425 :: Blind Loss : 8444.177230878484\n",
      "Iteration: 62\n",
      "Training :: Blind : 54.11111111111111\n",
      "Validation  :: Blind : 53.41428571428571 :: Blind Loss : 8479.318778702553\n",
      "Iteration: 63\n",
      "Training :: Blind : 54.304761904761904\n",
      "Validation  :: Blind : 53.5 :: Blind Loss : 8457.600123430577\n",
      "Iteration: 64\n",
      "Training :: Blind : 54.16507936507936\n",
      "Validation  :: Blind : 53.457142857142856 :: Blind Loss : 8469.261499071301\n",
      "Iteration: 65\n",
      "Training :: Blind : 54.67619047619048\n",
      "Validation  :: Blind : 54.142857142857146 :: Blind Loss : 9115.3682940508\n",
      "Iteration: 66\n",
      "Training :: Blind : 54.279365079365085\n",
      "Validation  :: Blind : 53.400000000000006 :: Blind Loss : 8468.15307831861\n",
      "Iteration: 67\n",
      "Training :: Blind : 54.41269841269841\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8453.12211480927\n",
      "Iteration: 68\n",
      "Training :: Blind : 54.768253968253966\n",
      "Validation  :: Blind : 54.24285714285715 :: Blind Loss : 9175.791665238083\n",
      "Iteration: 69\n",
      "Training :: Blind : 54.6936507936508\n",
      "Validation  :: Blind : 54.18571428571428 :: Blind Loss : 9162.743450144477\n",
      "Iteration: 70\n",
      "Training :: Blind : 54.72380952380952\n",
      "Validation  :: Blind : 54.214285714285715 :: Blind Loss : 9121.051638692425\n",
      "Iteration: 71\n",
      "Training :: Blind : 54.17460317460318\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8483.464299951534\n",
      "Iteration: 72\n",
      "Training :: Blind : 54.58412698412698\n",
      "Validation  :: Blind : 53.800000000000004 :: Blind Loss : 8452.224419661143\n",
      "Iteration: 73\n",
      "Training :: Blind : 54.233333333333334\n",
      "Validation  :: Blind : 53.58571428571428 :: Blind Loss : 8481.148321420342\n",
      "Iteration: 74\n",
      "Training :: Blind : 55.028571428571425\n",
      "Validation  :: Blind : 54.45714285714286 :: Blind Loss : 8454.684742074334\n",
      "Iteration: 75\n",
      "Training :: Blind : 54.474603174603175\n",
      "Validation  :: Blind : 53.7 :: Blind Loss : 8440.313552395342\n",
      "Iteration: 76\n",
      "Training :: Blind : 54.81587301587302\n",
      "Validation  :: Blind : 54.32857142857143 :: Blind Loss : 9177.989585952917\n",
      "Iteration: 77\n",
      "Training :: Blind : 54.7936507936508\n",
      "Validation  :: Blind : 54.285714285714285 :: Blind Loss : 9190.34709262601\n",
      "Iteration: 78\n",
      "Training :: Blind : 54.72222222222223\n",
      "Validation  :: Blind : 54.25714285714286 :: Blind Loss : 9130.276626140185\n",
      "Iteration: 79\n",
      "Training :: Blind : 54.27142857142857\n",
      "Validation  :: Blind : 53.44285714285715 :: Blind Loss : 8469.42024641997\n",
      "Iteration: 80\n",
      "Training :: Blind : 54.72063492063492\n",
      "Validation  :: Blind : 54.31428571428572 :: Blind Loss : 9086.30108235074\n",
      "Iteration: 81\n",
      "Training :: Blind : 54.48571428571428\n",
      "Validation  :: Blind : 53.87142857142857 :: Blind Loss : 8442.82951120163\n",
      "Iteration: 82\n",
      "Training :: Blind : 54.41269841269841\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8448.616111019568\n",
      "Iteration: 83\n",
      "Training :: Blind : 54.33809523809524\n",
      "Validation  :: Blind : 53.400000000000006 :: Blind Loss : 8456.943689427415\n",
      "Iteration: 84\n",
      "Training :: Blind : 54.44761904761904\n",
      "Validation  :: Blind : 53.78571428571428 :: Blind Loss : 8454.614729576864\n",
      "Iteration: 85\n",
      "Training :: Blind : 54.680952380952384\n",
      "Validation  :: Blind : 54.128571428571426 :: Blind Loss : 9177.275281451362\n",
      "Iteration: 86\n",
      "Training :: Blind : 54.65079365079365\n",
      "Validation  :: Blind : 53.900000000000006 :: Blind Loss : 8429.296061129486\n",
      "Iteration: 87\n",
      "Training :: Blind : 54.7047619047619\n",
      "Validation  :: Blind : 54.18571428571428 :: Blind Loss : 9175.403151432765\n",
      "Iteration: 88\n",
      "Training :: Blind : 54.696825396825396\n",
      "Validation  :: Blind : 54.25714285714286 :: Blind Loss : 9155.83340900047\n",
      "Iteration: 89\n",
      "Training :: Blind : 54.77619047619048\n",
      "Validation  :: Blind : 54.2 :: Blind Loss : 9177.751218849735\n",
      "Iteration: 90\n",
      "Training :: Blind : 54.74126984126985\n",
      "Validation  :: Blind : 54.24285714285715 :: Blind Loss : 9149.537254405286\n",
      "Iteration: 91\n",
      "Training :: Blind : 54.38571428571428\n",
      "Validation  :: Blind : 53.628571428571426 :: Blind Loss : 8457.009545156041\n",
      "Iteration: 92\n",
      "Training :: Blind : 54.72857142857143\n",
      "Validation  :: Blind : 54.27142857142857 :: Blind Loss : 9153.293368707771\n",
      "Iteration: 93\n",
      "Training :: Blind : 54.658730158730165\n",
      "Validation  :: Blind : 54.24285714285715 :: Blind Loss : 9117.083626237747\n",
      "Iteration: 94\n",
      "Training :: Blind : 54.49523809523809\n",
      "Validation  :: Blind : 53.6 :: Blind Loss : 8451.786839774766\n",
      "Iteration: 95\n",
      "Training :: Blind : 54.65238095238095\n",
      "Validation  :: Blind : 53.614285714285714 :: Blind Loss : 8455.76540075764\n",
      "Iteration: 96\n",
      "Training :: Blind : 54.353968253968254\n",
      "Validation  :: Blind : 53.51428571428571 :: Blind Loss : 8461.79835321998\n",
      "Iteration: 97\n",
      "Training :: Blind : 54.66984126984127\n",
      "Validation  :: Blind : 54.27142857142857 :: Blind Loss : 9167.562564701539\n",
      "Iteration: 98\n",
      "Training :: Blind : 54.149206349206345\n",
      "Validation  :: Blind : 53.300000000000004 :: Blind Loss : 8471.183492829063\n",
      "Iteration: 99\n",
      "Training :: Blind : 54.761904761904766\n",
      "Validation  :: Blind : 54.45714285714286 :: Blind Loss : 9173.518945013706\n",
      "Iteration: 100\n",
      "Training :: Blind : 54.1984126984127\n",
      "Validation  :: Blind : 53.542857142857144 :: Blind Loss : 8475.516961058805\n",
      "Params Initialised\n",
      "Iteration: 1\n",
      "Training :: Blind : 48.15238095238095\n",
      "Validation  :: Blind : 48.57142857142857 :: Blind Loss : 10381.99297805538\n",
      "Iteration: 2\n",
      "Training :: Blind : 52.41746031746032\n",
      "Validation  :: Blind : 52.114285714285714 :: Blind Loss : 9445.776461827303\n",
      "Iteration: 3\n",
      "Training :: Blind : 54.1936507936508\n",
      "Validation  :: Blind : 53.75714285714286 :: Blind Loss : 8964.249575548896\n",
      "Iteration: 4\n",
      "Training :: Blind : 54.56666666666666\n",
      "Validation  :: Blind : 54.371428571428574 :: Blind Loss : 8860.955661975295\n",
      "Iteration: 5\n",
      "Training :: Blind : 54.887301587301586\n",
      "Validation  :: Blind : 54.871428571428574 :: Blind Loss : 8770.750217898654\n",
      "Iteration: 6\n",
      "Training :: Blind : 55.41587301587302\n",
      "Validation  :: Blind : 55.371428571428574 :: Blind Loss : 8680.086128764076\n",
      "Iteration: 7\n",
      "Training :: Blind : 55.42857142857143\n",
      "Validation  :: Blind : 55.300000000000004 :: Blind Loss : 8677.085596702296\n",
      "Iteration: 8\n",
      "Training :: Blind : 55.62222222222222\n",
      "Validation  :: Blind : 55.371428571428574 :: Blind Loss : 8643.73872521414\n",
      "Iteration: 9\n",
      "Training :: Blind : 55.76507936507936\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8632.328504613617\n",
      "Iteration: 10\n",
      "Training :: Blind : 55.62380952380952\n",
      "Validation  :: Blind : 55.32857142857143 :: Blind Loss : 8624.184670088896\n",
      "Iteration: 11\n",
      "Training :: Blind : 55.558730158730164\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8629.335252907456\n",
      "Iteration: 12\n",
      "Training :: Blind : 55.47777777777778\n",
      "Validation  :: Blind : 55.34285714285714 :: Blind Loss : 8628.848748641818\n",
      "Iteration: 13\n",
      "Training :: Blind : 55.35714285714286\n",
      "Validation  :: Blind : 55.385714285714286 :: Blind Loss : 8649.815121962207\n",
      "Iteration: 14\n",
      "Training :: Blind : 55.558730158730164\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8670.64937396604\n",
      "Iteration: 15\n",
      "Training :: Blind : 55.85079365079365\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8632.059687615016\n",
      "Iteration: 16\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.57142857142857 :: Blind Loss : 8623.559180418975\n",
      "Iteration: 17\n",
      "Training :: Blind : 55.86984126984127\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8621.133089031526\n",
      "Iteration: 18\n",
      "Training :: Blind : 55.938095238095244\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8606.474796608916\n",
      "Iteration: 19\n",
      "Training :: Blind : 55.93650793650794\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8616.592677980781\n",
      "Iteration: 20\n",
      "Training :: Blind : 55.88412698412698\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8622.761352628633\n",
      "Iteration: 21\n",
      "Training :: Blind : 55.900000000000006\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8612.40082684192\n",
      "Iteration: 22\n",
      "Training :: Blind : 55.92380952380952\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8609.782697043709\n",
      "Iteration: 23\n",
      "Training :: Blind : 55.900000000000006\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8614.048588690093\n",
      "Iteration: 24\n",
      "Training :: Blind : 55.96031746031747\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8605.810511290509\n",
      "Iteration: 25\n",
      "Training :: Blind : 55.95079365079365\n",
      "Validation  :: Blind : 55.628571428571426 :: Blind Loss : 8614.902021234057\n",
      "Iteration: 26\n",
      "Training :: Blind : 55.87619047619048\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8617.827816402181\n",
      "Iteration: 27\n",
      "Training :: Blind : 55.84285714285714\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8615.18890796433\n",
      "Iteration: 28\n",
      "Training :: Blind : 55.96666666666666\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8603.104175126533\n",
      "Iteration: 29\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.7 :: Blind Loss : 8616.017221447664\n",
      "Iteration: 30\n",
      "Training :: Blind : 55.900000000000006\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8610.596827290366\n",
      "Iteration: 31\n",
      "Training :: Blind : 55.852380952380955\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8621.568374456758\n",
      "Iteration: 32\n",
      "Training :: Blind : 55.92857142857143\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8607.34410643963\n",
      "Iteration: 33\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8616.214676565152\n",
      "Iteration: 34\n",
      "Training :: Blind : 55.86507936507936\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8617.64955705768\n",
      "Iteration: 35\n",
      "Training :: Blind : 55.87936507936509\n",
      "Validation  :: Blind : 55.400000000000006 :: Blind Loss : 8619.147870928726\n",
      "Iteration: 36\n",
      "Training :: Blind : 55.84285714285714\n",
      "Validation  :: Blind : 55.471428571428575 :: Blind Loss : 8623.92958236955\n",
      "Iteration: 37\n",
      "Training :: Blind : 55.86984126984127\n",
      "Validation  :: Blind : 55.50000000000001 :: Blind Loss : 8626.863359096244\n",
      "Iteration: 38\n",
      "Training :: Blind : 55.92063492063492\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8616.014387398422\n",
      "Iteration: 39\n",
      "Training :: Blind : 55.82063492063492\n",
      "Validation  :: Blind : 55.45714285714286 :: Blind Loss : 8628.4996803615\n",
      "Iteration: 40\n",
      "Training :: Blind : 55.885714285714286\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8615.466716491494\n",
      "Iteration: 41\n",
      "Training :: Blind : 55.84761904761905\n",
      "Validation  :: Blind : 55.54285714285714 :: Blind Loss : 8627.501107790185\n",
      "Iteration: 42\n",
      "Training :: Blind : 55.84920634920635\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8621.482002314344\n",
      "Iteration: 43\n",
      "Training :: Blind : 55.93015873015873\n",
      "Validation  :: Blind : 55.714285714285715 :: Blind Loss : 8617.57596140862\n",
      "Iteration: 44\n",
      "Training :: Blind : 55.94603174603174\n",
      "Validation  :: Blind : 55.58571428571428 :: Blind Loss : 8621.056483630537\n",
      "Iteration: 45\n",
      "Training :: Blind : 55.87619047619048\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8621.64268143965\n",
      "Iteration: 46\n",
      "Training :: Blind : 55.95873015873016\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8614.638439287663\n",
      "Iteration: 47\n",
      "Training :: Blind : 55.87777777777778\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8616.497306952971\n",
      "Iteration: 48\n",
      "Training :: Blind : 55.822222222222216\n",
      "Validation  :: Blind : 55.48571428571428 :: Blind Loss : 8624.913316584509\n",
      "Iteration: 49\n",
      "Training :: Blind : 55.953968253968256\n",
      "Validation  :: Blind : 55.67142857142857 :: Blind Loss : 8616.084152716396\n",
      "Iteration: 50\n",
      "Training :: Blind : 55.907936507936505\n",
      "Validation  :: Blind : 55.557142857142864 :: Blind Loss : 8624.587651171745\n",
      "Iteration: 51\n",
      "Training :: Blind : 55.94920634920635\n",
      "Validation  :: Blind : 55.65714285714286 :: Blind Loss : 8609.732291130182\n",
      "Iteration: 52\n",
      "Training :: Blind : 55.91111111111111\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8616.02339838598\n",
      "Iteration: 53\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.64285714285714 :: Blind Loss : 8621.655790900531\n",
      "Iteration: 54\n",
      "Training :: Blind : 55.82539682539682\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8626.510257533519\n",
      "Iteration: 55\n",
      "Training :: Blind : 55.89365079365079\n",
      "Validation  :: Blind : 55.614285714285714 :: Blind Loss : 8625.92698598581\n",
      "Iteration: 56\n",
      "Training :: Blind : 55.8031746031746\n",
      "Validation  :: Blind : 55.51428571428572 :: Blind Loss : 8627.127962293584\n",
      "Iteration: 57\n",
      "Training :: Blind : 55.88412698412698\n",
      "Validation  :: Blind : 55.68571428571428 :: Blind Loss : 8617.285677249421\n",
      "Iteration: 58\n",
      "Training :: Blind : 55.871428571428574\n",
      "Validation  :: Blind : 55.60000000000001 :: Blind Loss : 8623.403950130898\n",
      "Iteration: 59\n",
      "Training :: Blind : 55.9031746031746\n",
      "Validation  :: Blind : 55.528571428571425 :: Blind Loss : 8618.72936755211\n",
      "Iteration: 60\n",
      "Training :: Blind : 55.546031746031744\n",
      "Validation  :: Blind : 55.42857142857143 :: Blind Loss : 8707.373977133117\n",
      "Iteration: 61\n",
      "Training :: Blind : 55.44761904761904\n",
      "Validation  :: Blind : 55.24285714285714 :: Blind Loss : 8742.928710098831\n",
      "Iteration: 62\n",
      "Training :: Blind : 55.67301587301587\n",
      "Validation  :: Blind : 55.25714285714286 :: Blind Loss : 8685.284335108776\n",
      "Iteration: 63\n",
      "Training :: Blind : 55.51428571428572\n",
      "Validation  :: Blind : 54.97142857142857 :: Blind Loss : 8719.0212040796\n",
      "Iteration: 64\n",
      "Training :: Blind : 55.82857142857143\n",
      "Validation  :: Blind : 55.371428571428574 :: Blind Loss : 8676.842528491945\n",
      "Iteration: 65\n",
      "Training :: Blind : 56.046031746031744\n",
      "Validation  :: Blind : 55.74285714285714 :: Blind Loss : 8627.728056550997\n",
      "Iteration: 66\n",
      "Training :: Blind : 56.30952380952381\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8594.797868373664\n",
      "Iteration: 67\n",
      "Training :: Blind : 56.33015873015873\n",
      "Validation  :: Blind : 55.95714285714286 :: Blind Loss : 8589.31529714944\n",
      "Iteration: 68\n",
      "Training :: Blind : 56.331746031746036\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8580.096446865484\n",
      "Iteration: 69\n",
      "Training :: Blind : 56.473015873015875\n",
      "Validation  :: Blind : 55.971428571428575 :: Blind Loss : 8561.24771440023\n",
      "Iteration: 70\n",
      "Training :: Blind : 56.360317460317454\n",
      "Validation  :: Blind : 55.91428571428572 :: Blind Loss : 8566.98574389368\n",
      "Iteration: 71\n",
      "Training :: Blind : 56.31904761904762\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8572.484397786788\n",
      "Iteration: 72\n",
      "Training :: Blind : 56.44920634920635\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8555.216662092727\n",
      "Iteration: 73\n",
      "Training :: Blind : 56.441269841269836\n",
      "Validation  :: Blind : 56.128571428571426 :: Blind Loss : 8544.866557215046\n",
      "Iteration: 74\n",
      "Training :: Blind : 56.49999999999999\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8544.71196490819\n",
      "Iteration: 75\n",
      "Training :: Blind : 56.42222222222222\n",
      "Validation  :: Blind : 56.08571428571428 :: Blind Loss : 8557.701405203828\n",
      "Iteration: 76\n",
      "Training :: Blind : 56.455555555555556\n",
      "Validation  :: Blind : 56.028571428571425 :: Blind Loss : 8553.743879190319\n",
      "Iteration: 77\n",
      "Training :: Blind : 56.39206349206349\n",
      "Validation  :: Blind : 56.08571428571428 :: Blind Loss : 8555.657707800729\n",
      "Iteration: 78\n",
      "Training :: Blind : 56.46507936507936\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8541.625894580857\n",
      "Iteration: 79\n",
      "Training :: Blind : 56.480952380952374\n",
      "Validation  :: Blind : 56.14285714285714 :: Blind Loss : 8552.256416023141\n",
      "Iteration: 80\n",
      "Training :: Blind : 56.455555555555556\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8554.75813066891\n",
      "Iteration: 81\n",
      "Training :: Blind : 56.53015873015873\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8546.546595451375\n",
      "Iteration: 82\n",
      "Training :: Blind : 56.385714285714286\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8558.559045434706\n",
      "Iteration: 83\n",
      "Training :: Blind : 56.4015873015873\n",
      "Validation  :: Blind : 55.98571428571428 :: Blind Loss : 8556.18567730375\n",
      "Iteration: 84\n",
      "Training :: Blind : 56.41587301587302\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8559.210882843752\n",
      "Iteration: 85\n",
      "Training :: Blind : 56.43015873015873\n",
      "Validation  :: Blind : 56.27142857142857 :: Blind Loss : 8558.648699121819\n",
      "Iteration: 86\n",
      "Training :: Blind : 56.45079365079365\n",
      "Validation  :: Blind : 56.08571428571428 :: Blind Loss : 8553.846908870659\n",
      "Iteration: 87\n",
      "Training :: Blind : 56.404761904761905\n",
      "Validation  :: Blind : 56.057142857142864 :: Blind Loss : 8552.555910226716\n",
      "Iteration: 88\n",
      "Training :: Blind : 56.46507936507936\n",
      "Validation  :: Blind : 56.14285714285714 :: Blind Loss : 8553.066700288444\n",
      "Iteration: 89\n",
      "Training :: Blind : 56.4968253968254\n",
      "Validation  :: Blind : 56.18571428571428 :: Blind Loss : 8540.451144794479\n",
      "Iteration: 90\n",
      "Training :: Blind : 56.38253968253968\n",
      "Validation  :: Blind : 56.10000000000001 :: Blind Loss : 8559.54417188477\n",
      "Iteration: 91\n",
      "Training :: Blind : 56.42857142857143\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8550.982364480711\n",
      "Iteration: 92\n",
      "Training :: Blind : 56.471428571428575\n",
      "Validation  :: Blind : 56.10000000000001 :: Blind Loss : 8553.655169559792\n",
      "Iteration: 93\n",
      "Training :: Blind : 56.47619047619048\n",
      "Validation  :: Blind : 56.08571428571428 :: Blind Loss : 8547.377072672689\n",
      "Iteration: 94\n",
      "Training :: Blind : 56.455555555555556\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8560.794410364902\n",
      "Iteration: 95\n",
      "Training :: Blind : 56.36666666666667\n",
      "Validation  :: Blind : 56.04285714285714 :: Blind Loss : 8563.498416935734\n",
      "Iteration: 96\n",
      "Training :: Blind : 56.43015873015873\n",
      "Validation  :: Blind : 56.01428571428572 :: Blind Loss : 8554.147014322267\n",
      "Iteration: 97\n",
      "Training :: Blind : 56.41587301587302\n",
      "Validation  :: Blind : 56.128571428571426 :: Blind Loss : 8556.126943917829\n",
      "Iteration: 98\n",
      "Training :: Blind : 56.45079365079365\n",
      "Validation  :: Blind : 56.10000000000001 :: Blind Loss : 8549.549301480049\n",
      "Iteration: 99\n",
      "Training :: Blind : 56.43174603174603\n",
      "Validation  :: Blind : 56.114285714285714 :: Blind Loss : 8554.571308268252\n",
      "Iteration: 100\n",
      "Training :: Blind : 56.45714285714286\n",
      "Validation  :: Blind : 56.17142857142857 :: Blind Loss : 8547.89782383763\n"
     ]
    }
   ],
   "source": [
    "trainAccAsymStatTotal = []\n",
    "valAccAsymStatTotal = []\n",
    "trainLossAsymStatTotal = []\n",
    "valLossAsymStatTotal= []\n",
    "Seeding = np.random.randint(10, 100, 5)\n",
    "Asym = [100, 60, 50 ,30, 10, 5, 3, 1]\n",
    "for asym in Asym:\n",
    "    trainAccAsymStat = []\n",
    "    valAccAsymStat = []\n",
    "    trainLossAsymStat = []\n",
    "    valLossAsymStat = []\n",
    "    for seed in Seeding:\n",
    "        W1, b1, W2, b2, train_acc_quanttemp, val_acc_quanttemp, train_losstemp, val_losstemp, sum_weights = batch_grad_descent_with_var(x_train,y_train,batchsize,iter, lr, midLayerSize, mu, sigma, vDD, precision, step, discreteSteps, wRange, asym, onoff, seed = seed ,print_op=1)\n",
    "        trainAccAsymStat.append(train_acc_quanttemp)\n",
    "        valAccAsymStat.append(val_acc_quanttemp)\n",
    "        trainLossAsymStat.append(train_losstemp)\n",
    "        valLossAsymStat.append(val_losstemp)\n",
    "    trainAccAsymStatTotal.append(trainAccAsymStat)\n",
    "    valAccAsymStatTotal.append(valAccAsymStat)\n",
    "    trainLossAsymStatTotal.append(trainLossAsymStat)\n",
    "    valLossAsymStatTotal.append(valLossAsymStat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x227c22ce740>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 0 \n",
    "for i in trainLossAsymStat:\n",
    "    plt.plot(i, alpha=0.4, label = f\"Statistical run {ind}\")\n",
    "    ind +=1\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 0 \n",
    "for j in trainAccAsymStatTotal:\n",
    "    finAccforPlot = []\n",
    "    for i in j:\n",
    "        finAccforPlot.append(i[-1])\n",
    "    plt.plot([Asym[ind]]*5, finAccforPlot)\n",
    "    ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
