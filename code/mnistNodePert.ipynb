{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  print(\"BP \\n \", dZ3)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z1\")\n",
    "    #print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "def batchGDNP(X,Y,iter, lr, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "      #print(W1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsToTrain = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertList = [1,0.1, 0.01, 0.001, 0.0001]\n",
    "trainAccPertList = []\n",
    "valAccPertList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 69.841269841269834\n",
      "Iteration: 1\n",
      "Train accuracy: 70.33809523809524\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 1 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 2\n",
      "Train accuracy: 79.0079365079365\n",
      "Val accuracy: 78.08571428571427\n",
      "Iter 2 -> sub iter 99 : 82.69841269841271\n",
      "Iteration: 3\n",
      "Train accuracy: 82.1079365079365\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 3 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 4\n",
      "Train accuracy: 84.05238095238096\n",
      "Val accuracy: 83.52857142857142\n",
      "Iter 4 -> sub iter 99 : 85.23809523809524\n",
      "Iteration: 5\n",
      "Train accuracy: 85.35396825396825\n",
      "Val accuracy: 84.88571428571429\n",
      "Iter 5 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 6\n",
      "Train accuracy: 86.41904761904762\n",
      "Val accuracy: 85.97142857142858\n",
      "Iter 6 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 7\n",
      "Train accuracy: 87.17777777777778\n",
      "Val accuracy: 86.62857142857143\n",
      "Iter 7 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 8\n",
      "Train accuracy: 87.86825396825397\n",
      "Val accuracy: 87.41428571428571\n",
      "Iter 8 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 9\n",
      "Train accuracy: 88.39047619047619\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 9 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 10\n",
      "Train accuracy: 88.84761904761905\n",
      "Val accuracy: 88.25714285714285\n",
      "Iter 10 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 11\n",
      "Train accuracy: 89.2095238095238\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 11 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 12\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 12 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 13\n",
      "Train accuracy: 89.89047619047619\n",
      "Val accuracy: 89.4\n",
      "Iter 13 -> sub iter 99 : 90.01111111111111\n",
      "Iteration: 14\n",
      "Train accuracy: 90.15555555555555\n",
      "Val accuracy: 89.64285714285715\n",
      "Iter 14 -> sub iter 99 : 90.08730158730158\n",
      "Iteration: 15\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.84285714285714\n",
      "Iter 15 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 16\n",
      "Train accuracy: 90.62857142857142\n",
      "Val accuracy: 90.17142857142856\n",
      "Iter 16 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 17\n",
      "Train accuracy: 90.86349206349207\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 17 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 18\n",
      "Train accuracy: 91.09047619047618\n",
      "Val accuracy: 90.64285714285715\n",
      "Iter 18 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 19\n",
      "Train accuracy: 91.25079365079365\n",
      "Val accuracy: 90.8\n",
      "Iter 19 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 20\n",
      "Train accuracy: 91.42857142857143\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 20 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 21\n",
      "Train accuracy: 91.58730158730158\n",
      "Val accuracy: 91.01428571428572\n",
      "Iter 21 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 22\n",
      "Train accuracy: 91.73174603174603\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 22 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 23\n",
      "Train accuracy: 91.87777777777778\n",
      "Val accuracy: 91.42857142857143\n",
      "Iter 23 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 24\n",
      "Train accuracy: 91.99841269841271\n",
      "Val accuracy: 91.41428571428571\n",
      "Iter 24 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 25\n",
      "Train accuracy: 92.0984126984127\n",
      "Val accuracy: 91.54285714285714\n",
      "Iter 25 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 26\n",
      "Train accuracy: 92.2031746031746\n",
      "Val accuracy: 91.60000000000001\n",
      "Iter 26 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 27\n",
      "Train accuracy: 92.3079365079365\n",
      "Val accuracy: 91.75714285714285\n",
      "Iter 27 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 28\n",
      "Train accuracy: 92.41428571428571\n",
      "Val accuracy: 91.88571428571429\n",
      "Iter 28 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 29\n",
      "Train accuracy: 92.5063492063492\n",
      "Val accuracy: 92.01428571428572\n",
      "Iter 29 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 30\n",
      "Train accuracy: 92.57619047619048\n",
      "Val accuracy: 92.02857142857142\n",
      "Iter 30 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 31\n",
      "Train accuracy: 92.67301587301587\n",
      "Val accuracy: 92.12857142857143\n",
      "Iter 31 -> sub iter 99 : 91.90476190476191\n",
      "Iteration: 32\n",
      "Train accuracy: 92.74761904761904\n",
      "Val accuracy: 92.14285714285714\n",
      "Iter 32 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 33\n",
      "Train accuracy: 92.83809523809524\n",
      "Val accuracy: 92.21428571428572\n",
      "Iter 33 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 34\n",
      "Train accuracy: 92.93174603174603\n",
      "Val accuracy: 92.34285714285714\n",
      "Iter 34 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 35\n",
      "Train accuracy: 92.9968253968254\n",
      "Val accuracy: 92.4\n",
      "Iter 35 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 36\n",
      "Train accuracy: 93.06349206349206\n",
      "Val accuracy: 92.5\n",
      "Iter 36 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 37\n",
      "Train accuracy: 93.12222222222222\n",
      "Val accuracy: 92.51428571428572\n",
      "Iter 37 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 38\n",
      "Train accuracy: 93.16349206349206\n",
      "Val accuracy: 92.58571428571429\n",
      "Iter 38 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 39\n",
      "Train accuracy: 93.2047619047619\n",
      "Val accuracy: 92.62857142857143\n",
      "Iter 39 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 40\n",
      "Train accuracy: 93.25873015873016\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 40 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 41\n",
      "Train accuracy: 93.32222222222222\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 41 -> sub iter 99 : 92.69841269841273\n",
      "Iteration: 42\n",
      "Train accuracy: 93.38253968253967\n",
      "Val accuracy: 92.77142857142857\n",
      "Iter 42 -> sub iter 99 : 93.01587301587301\n",
      "Iteration: 43\n",
      "Train accuracy: 93.42857142857143\n",
      "Val accuracy: 92.85714285714286\n",
      "Iter 43 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 44\n",
      "Train accuracy: 93.4888888888889\n",
      "Val accuracy: 92.95714285714286\n",
      "Iter 44 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 45\n",
      "Train accuracy: 93.55079365079365\n",
      "Val accuracy: 93.05714285714286\n",
      "Iter 45 -> sub iter 99 : 93.49206349206356\n",
      "Iteration: 46\n",
      "Train accuracy: 93.6015873015873\n",
      "Val accuracy: 93.12857142857143\n",
      "Iter 46 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 47\n",
      "Train accuracy: 93.66507936507936\n",
      "Val accuracy: 93.18571428571428\n",
      "Iter 47 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 48\n",
      "Train accuracy: 93.72222222222221\n",
      "Val accuracy: 93.22857142857143\n",
      "Iter 48 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 49\n",
      "Train accuracy: 93.76984126984127\n",
      "Val accuracy: 93.27142857142857\n",
      "Iter 49 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 50\n",
      "Train accuracy: 93.83968253968254\n",
      "Val accuracy: 93.31428571428572\n",
      "Iter 50 -> sub iter 99 : 94.12698412698413\n",
      "Iteration: 51\n",
      "Train accuracy: 93.88571428571429\n",
      "Val accuracy: 93.32857142857142\n",
      "Iter 51 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 93.92380952380952\n",
      "Val accuracy: 93.34285714285714\n",
      "Iter 52 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 53\n",
      "Train accuracy: 93.97777777777779\n",
      "Val accuracy: 93.38571428571429\n",
      "Iter 53 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 54\n",
      "Train accuracy: 94.01428571428572\n",
      "Val accuracy: 93.41428571428571\n",
      "Iter 54 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 55\n",
      "Train accuracy: 94.07142857142857\n",
      "Val accuracy: 93.4\n",
      "Iter 55 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 56\n",
      "Train accuracy: 94.12063492063491\n",
      "Val accuracy: 93.45714285714286\n",
      "Iter 56 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 57\n",
      "Train accuracy: 94.13650793650794\n",
      "Val accuracy: 93.47142857142858\n",
      "Iter 57 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 58\n",
      "Train accuracy: 94.16349206349206\n",
      "Val accuracy: 93.54285714285714\n",
      "Iter 58 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 59\n",
      "Train accuracy: 94.22222222222221\n",
      "Val accuracy: 93.58571428571429\n",
      "Iter 59 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 60\n",
      "Train accuracy: 94.24285714285713\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 60 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 61\n",
      "Train accuracy: 94.28412698412698\n",
      "Val accuracy: 93.57142857142857\n",
      "Iter 61 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 62\n",
      "Train accuracy: 94.33174603174604\n",
      "Val accuracy: 93.61428571428571\n",
      "Iter 62 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 63\n",
      "Train accuracy: 94.4031746031746\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 63 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 64\n",
      "Train accuracy: 94.43492063492064\n",
      "Val accuracy: 93.62857142857143\n",
      "Iter 64 -> sub iter 99 : 94.60317460317463\n",
      "Iteration: 65\n",
      "Train accuracy: 94.46190476190476\n",
      "Val accuracy: 93.64285714285714\n",
      "Iter 65 -> sub iter 99 : 94.60317460317467\n",
      "Iteration: 66\n",
      "Train accuracy: 94.4888888888889\n",
      "Val accuracy: 93.68571428571428\n",
      "Iter 66 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 94.51587301587303\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 67 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 68\n",
      "Train accuracy: 94.54444444444444\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 68 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 69\n",
      "Train accuracy: 94.57301587301588\n",
      "Val accuracy: 93.8\n",
      "Iter 69 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 70\n",
      "Train accuracy: 94.61587301587302\n",
      "Val accuracy: 93.8\n",
      "Iter 70 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 71\n",
      "Train accuracy: 94.63492063492063\n",
      "Val accuracy: 93.84285714285714\n",
      "Iter 71 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 72\n",
      "Train accuracy: 94.65396825396826\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 72 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 73\n",
      "Train accuracy: 94.67301587301587\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 73 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 74\n",
      "Train accuracy: 94.71269841269842\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 74 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 75\n",
      "Train accuracy: 94.74126984126984\n",
      "Val accuracy: 93.91428571428571\n",
      "Iter 75 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 76\n",
      "Train accuracy: 94.77777777777779\n",
      "Val accuracy: 93.92857142857143\n",
      "Iter 76 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 77\n",
      "Train accuracy: 94.80952380952381\n",
      "Val accuracy: 93.95714285714286\n",
      "Iter 77 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 78\n",
      "Train accuracy: 94.82698412698413\n",
      "Val accuracy: 93.98571428571428\n",
      "Iter 78 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 79\n",
      "Train accuracy: 94.83968253968254\n",
      "Val accuracy: 94.01428571428572\n",
      "Iter 79 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 80\n",
      "Train accuracy: 94.86825396825397\n",
      "Val accuracy: 94.04285714285714\n",
      "Iter 80 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 81\n",
      "Train accuracy: 94.89999999999999\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 81 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 82\n",
      "Train accuracy: 94.92063492063491\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 82 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 83\n",
      "Train accuracy: 94.94920634920635\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 83 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 84\n",
      "Train accuracy: 94.98253968253968\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 84 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 85\n",
      "Train accuracy: 95.0031746031746\n",
      "Val accuracy: 94.1\n",
      "Iter 85 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 86\n",
      "Train accuracy: 95.03174603174604\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 86 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 87\n",
      "Train accuracy: 95.06190476190476\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 87 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 88\n",
      "Train accuracy: 95.09206349206349\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 88 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 89\n",
      "Train accuracy: 95.11904761904762\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 89 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 90\n",
      "Train accuracy: 95.13174603174603\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 90 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 91\n",
      "Train accuracy: 95.18095238095238\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 91 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 92\n",
      "Train accuracy: 95.21111111111111\n",
      "Val accuracy: 94.15714285714286\n",
      "Iter 92 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 93\n",
      "Train accuracy: 95.24603174603175\n",
      "Val accuracy: 94.18571428571428\n",
      "Iter 93 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 94\n",
      "Train accuracy: 95.25396825396825\n",
      "Val accuracy: 94.21428571428572\n",
      "Iter 94 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 95\n",
      "Train accuracy: 95.3\n",
      "Val accuracy: 94.28571428571428\n",
      "Iter 95 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 96\n",
      "Train accuracy: 95.31904761904761\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 96 -> sub iter 99 : 95.39682539682549\n",
      "Iteration: 97\n",
      "Train accuracy: 95.33015873015873\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 97 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 98\n",
      "Train accuracy: 95.34444444444445\n",
      "Val accuracy: 94.22857142857143\n",
      "Iter 98 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 99\n",
      "Train accuracy: 95.36666666666666\n",
      "Val accuracy: 94.3\n",
      "Iter 99 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 100\n",
      "Train accuracy: 95.3920634920635\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 100 -> sub iter 99 : 95.71428571428572\n",
      "Iteration: 101\n",
      "Train accuracy: 95.41428571428571\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 101 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 102\n",
      "Train accuracy: 95.42222222222222\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 102 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 103\n",
      "Train accuracy: 95.43333333333334\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 103 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 104\n",
      "Train accuracy: 95.46507936507936\n",
      "Val accuracy: 94.38571428571429\n",
      "Iter 104 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 105\n",
      "Train accuracy: 95.47936507936508\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 105 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 106\n",
      "Train accuracy: 95.5079365079365\n",
      "Val accuracy: 94.39999999999999\n",
      "Iter 106 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 107\n",
      "Train accuracy: 95.51587301587303\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 107 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 108\n",
      "Train accuracy: 95.52857142857142\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 108 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 109\n",
      "Train accuracy: 95.54126984126984\n",
      "Val accuracy: 94.42857142857143\n",
      "Iter 109 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 110\n",
      "Train accuracy: 95.56031746031746\n",
      "Val accuracy: 94.47142857142858\n",
      "Iter 110 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 111\n",
      "Train accuracy: 95.55873015873016\n",
      "Val accuracy: 94.48571428571428\n",
      "Iter 111 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 112\n",
      "Train accuracy: 95.57460317460318\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 112 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 113\n",
      "Train accuracy: 95.6047619047619\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 113 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 114\n",
      "Train accuracy: 95.62380952380953\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 114 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 115\n",
      "Train accuracy: 95.63968253968254\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 115 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 116\n",
      "Train accuracy: 95.65079365079366\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 116 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 117\n",
      "Train accuracy: 95.67142857142858\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 117 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 118\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 118 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 119\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 119 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 120\n",
      "Train accuracy: 95.71111111111111\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 120 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 121\n",
      "Train accuracy: 95.72063492063492\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 121 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 122\n",
      "Train accuracy: 95.72539682539683\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 122 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 123\n",
      "Train accuracy: 95.73174603174604\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 123 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 124\n",
      "Train accuracy: 95.74920634920635\n",
      "Val accuracy: 94.6\n",
      "Iter 124 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 125\n",
      "Train accuracy: 95.74603174603175\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 125 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 126\n",
      "Train accuracy: 95.75396825396825\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 126 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 127\n",
      "Train accuracy: 95.75873015873016\n",
      "Val accuracy: 94.6\n",
      "Iter 127 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 128\n",
      "Train accuracy: 95.78571428571429\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 128 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 129\n",
      "Train accuracy: 95.80634920634921\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 129 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 130\n",
      "Train accuracy: 95.82380952380952\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 130 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 131\n",
      "Train accuracy: 95.83650793650794\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 131 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 132\n",
      "Train accuracy: 95.85555555555555\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 132 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 133\n",
      "Train accuracy: 95.86031746031746\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 133 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 134\n",
      "Train accuracy: 95.87142857142858\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 134 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 135\n",
      "Train accuracy: 95.87301587301587\n",
      "Val accuracy: 94.65714285714286\n",
      "Iter 135 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 136\n",
      "Train accuracy: 95.8873015873016\n",
      "Val accuracy: 94.67142857142858\n",
      "Iter 136 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 137\n",
      "Train accuracy: 95.91428571428573\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 137 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 138\n",
      "Train accuracy: 95.92222222222222\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 138 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 139\n",
      "Train accuracy: 95.94126984126984\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 139 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 140\n",
      "Train accuracy: 95.95714285714286\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 140 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 141\n",
      "Train accuracy: 95.97777777777777\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 141 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 142\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.72857142857143\n",
      "Iter 142 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 143\n",
      "Train accuracy: 96.02857142857142\n",
      "Val accuracy: 94.77142857142857\n",
      "Iter 143 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 144\n",
      "Train accuracy: 96.04285714285714\n",
      "Val accuracy: 94.8\n",
      "Iter 144 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 145\n",
      "Train accuracy: 96.06507936507937\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 145 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 146\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 146 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 147\n",
      "Train accuracy: 96.0936507936508\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 147 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 148\n",
      "Train accuracy: 96.10634920634921\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 148 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 149\n",
      "Train accuracy: 96.13174603174603\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 149 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 150\n",
      "Train accuracy: 96.15079365079366\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 150 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 151\n",
      "Train accuracy: 96.16666666666667\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 151 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 152\n",
      "Train accuracy: 96.18253968253968\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 152 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 153\n",
      "Train accuracy: 96.18571428571428\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 153 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 154\n",
      "Train accuracy: 96.2\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 154 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 155\n",
      "Train accuracy: 96.20793650793651\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 155 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 156\n",
      "Train accuracy: 96.21428571428572\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 156 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 157\n",
      "Train accuracy: 96.22539682539683\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 157 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 158\n",
      "Train accuracy: 96.22857142857143\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 158 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 159\n",
      "Train accuracy: 96.24126984126984\n",
      "Val accuracy: 94.91428571428571\n",
      "Iter 159 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 160\n",
      "Train accuracy: 96.24444444444444\n",
      "Val accuracy: 94.94285714285714\n",
      "Iter 160 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 161\n",
      "Train accuracy: 96.25396825396825\n",
      "Val accuracy: 94.97142857142858\n",
      "Iter 161 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 162\n",
      "Train accuracy: 96.27142857142857\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 162 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 163\n",
      "Train accuracy: 96.27777777777777\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 163 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 164\n",
      "Train accuracy: 96.3047619047619\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 164 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 165\n",
      "Train accuracy: 96.32063492063492\n",
      "Val accuracy: 95.0\n",
      "Iter 165 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 166\n",
      "Train accuracy: 96.33809523809524\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 166 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 167\n",
      "Train accuracy: 96.34126984126983\n",
      "Val accuracy: 95.04285714285714\n",
      "Iter 167 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 168\n",
      "Train accuracy: 96.34761904761905\n",
      "Val accuracy: 95.08571428571429\n",
      "Iter 168 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 169\n",
      "Train accuracy: 96.35873015873015\n",
      "Val accuracy: 95.12857142857143\n",
      "Iter 169 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 170\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 170 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 171\n",
      "Train accuracy: 96.3873015873016\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 171 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 172\n",
      "Train accuracy: 96.3984126984127\n",
      "Val accuracy: 95.15714285714286\n",
      "Iter 172 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 173\n",
      "Train accuracy: 96.41428571428573\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 173 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 174\n",
      "Train accuracy: 96.42857142857143\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 174 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 175\n",
      "Train accuracy: 96.44285714285714\n",
      "Val accuracy: 95.21428571428572\n",
      "Iter 175 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 176\n",
      "Train accuracy: 96.45396825396826\n",
      "Val accuracy: 95.22857142857143\n",
      "Iter 176 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 177\n",
      "Train accuracy: 96.46031746031746\n",
      "Val accuracy: 95.27142857142857\n",
      "Iter 177 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 178\n",
      "Train accuracy: 96.46666666666667\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 178 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 179\n",
      "Train accuracy: 96.46825396825398\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 179 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 180\n",
      "Train accuracy: 96.48095238095237\n",
      "Val accuracy: 95.3\n",
      "Iter 180 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 181\n",
      "Train accuracy: 96.49682539682539\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 181 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 182\n",
      "Train accuracy: 96.50634920634921\n",
      "Val accuracy: 95.3\n",
      "Iter 182 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 183\n",
      "Train accuracy: 96.52222222222223\n",
      "Val accuracy: 95.3\n",
      "Iter 183 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 184\n",
      "Train accuracy: 96.53015873015873\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 184 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 185\n",
      "Train accuracy: 96.54444444444444\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 185 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 186\n",
      "Train accuracy: 96.54761904761905\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 186 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 187\n",
      "Train accuracy: 96.56349206349206\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 187 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 188\n",
      "Train accuracy: 96.56031746031746\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 188 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 189\n",
      "Train accuracy: 96.56825396825397\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 189 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 190\n",
      "Train accuracy: 96.58571428571429\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 190 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 191\n",
      "Train accuracy: 96.5952380952381\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 191 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 192\n",
      "Train accuracy: 96.60317460317461\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 192 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 193\n",
      "Train accuracy: 96.61111111111111\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 193 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 194\n",
      "Train accuracy: 96.62380952380953\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 194 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 195\n",
      "Train accuracy: 96.62857142857143\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 195 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 196\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 196 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 197\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 197 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 198\n",
      "Train accuracy: 96.63492063492065\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 198 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 199\n",
      "Train accuracy: 96.63809523809523\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 199 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 200\n",
      "Train accuracy: 96.65238095238095\n",
      "Val accuracy: 95.35714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 23.333333333333332\n",
      "Iteration: 1\n",
      "Train accuracy: 21.192063492063493\n",
      "Val accuracy: 21.185714285714287\n",
      "Iter 1 -> sub iter 99 : 32.380952380952387\n",
      "Iteration: 2\n",
      "Train accuracy: 30.338095238095235\n",
      "Val accuracy: 30.185714285714287\n",
      "Iter 2 -> sub iter 99 : 39.365079365079374\n",
      "Iteration: 3\n",
      "Train accuracy: 36.630158730158726\n",
      "Val accuracy: 35.885714285714286\n",
      "Iter 3 -> sub iter 99 : 43.333333333333336\n",
      "Iteration: 4\n",
      "Train accuracy: 40.7015873015873\n",
      "Val accuracy: 39.957142857142856\n",
      "Iter 4 -> sub iter 99 : 45.873015873015874\n",
      "Iteration: 5\n",
      "Train accuracy: 43.32222222222222\n",
      "Val accuracy: 43.042857142857144\n",
      "Iter 5 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 6\n",
      "Train accuracy: 45.303174603174604\n",
      "Val accuracy: 44.94285714285714\n",
      "Iter 6 -> sub iter 99 : 49.365079365079376\n",
      "Iteration: 7\n",
      "Train accuracy: 48.33174603174603\n",
      "Val accuracy: 47.3\n",
      "Iter 7 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 8\n",
      "Train accuracy: 53.77301587301587\n",
      "Val accuracy: 52.67142857142857\n",
      "Iter 8 -> sub iter 99 : 58.888888888888896\n",
      "Iteration: 9\n",
      "Train accuracy: 57.51904761904761\n",
      "Val accuracy: 56.471428571428575\n",
      "Iter 9 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 10\n",
      "Train accuracy: 60.73968253968254\n",
      "Val accuracy: 59.68571428571428\n",
      "Iter 10 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 11\n",
      "Train accuracy: 63.834920634920636\n",
      "Val accuracy: 62.97142857142857\n",
      "Iter 11 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 12\n",
      "Train accuracy: 66.26349206349207\n",
      "Val accuracy: 65.65714285714286\n",
      "Iter 12 -> sub iter 99 : 66.349206349206345\n",
      "Iteration: 13\n",
      "Train accuracy: 68.01904761904763\n",
      "Val accuracy: 67.37142857142857\n",
      "Iter 13 -> sub iter 99 : 67.619047619047625\n",
      "Iteration: 14\n",
      "Train accuracy: 69.34920634920636\n",
      "Val accuracy: 68.45714285714286\n",
      "Iter 14 -> sub iter 99 : 68.57142857142857\n",
      "Iteration: 15\n",
      "Train accuracy: 70.35714285714286\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 15 -> sub iter 99 : 69.68253968253968\n",
      "Iteration: 16\n",
      "Train accuracy: 71.15873015873015\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 16 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 17\n",
      "Train accuracy: 71.80952380952381\n",
      "Val accuracy: 70.6\n",
      "Iter 17 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 18\n",
      "Train accuracy: 72.47777777777777\n",
      "Val accuracy: 71.31428571428572\n",
      "Iter 18 -> sub iter 99 : 72.22222222222221\n",
      "Iteration: 19\n",
      "Train accuracy: 73.23015873015873\n",
      "Val accuracy: 72.17142857142858\n",
      "Iter 19 -> sub iter 99 : 73.49206349206354\n",
      "Iteration: 20\n",
      "Train accuracy: 74.12222222222222\n",
      "Val accuracy: 73.08571428571429\n",
      "Iter 20 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 21\n",
      "Train accuracy: 75.04603174603174\n",
      "Val accuracy: 73.75714285714285\n",
      "Iter 21 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 22\n",
      "Train accuracy: 75.86190476190477\n",
      "Val accuracy: 74.5142857142857\n",
      "Iter 22 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 23\n",
      "Train accuracy: 76.63809523809523\n",
      "Val accuracy: 74.9857142857143\n",
      "Iter 23 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 24\n",
      "Train accuracy: 77.25555555555556\n",
      "Val accuracy: 75.74285714285715\n",
      "Iter 24 -> sub iter 99 : 77.30158730158733\n",
      "Iteration: 25\n",
      "Train accuracy: 77.86031746031746\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 25 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 26\n",
      "Train accuracy: 78.41269841269842\n",
      "Val accuracy: 76.91428571428571\n",
      "Iter 26 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 27\n",
      "Train accuracy: 78.94285714285715\n",
      "Val accuracy: 77.58571428571429\n",
      "Iter 27 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 28\n",
      "Train accuracy: 79.43650793650794\n",
      "Val accuracy: 78.01428571428572\n",
      "Iter 28 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 29\n",
      "Train accuracy: 79.9\n",
      "Val accuracy: 78.65714285714286\n",
      "Iter 29 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 80.32857142857142\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 30 -> sub iter 99 : 80.07460317460318\n",
      "Iteration: 31\n",
      "Train accuracy: 80.68095238095238\n",
      "Val accuracy: 79.31428571428572\n",
      "Iter 31 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 32\n",
      "Train accuracy: 81.01428571428572\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 32 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 33\n",
      "Train accuracy: 81.36984126984127\n",
      "Val accuracy: 79.94285714285714\n",
      "Iter 33 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 34\n",
      "Train accuracy: 81.67619047619048\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 34 -> sub iter 99 : 81.90476190476196\n",
      "Iteration: 35\n",
      "Train accuracy: 81.95396825396826\n",
      "Val accuracy: 80.4\n",
      "Iter 35 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 36\n",
      "Train accuracy: 82.28095238095237\n",
      "Val accuracy: 80.7\n",
      "Iter 36 -> sub iter 99 : 81.90476190476198\n",
      "Iteration: 37\n",
      "Train accuracy: 82.53809523809524\n",
      "Val accuracy: 81.10000000000001\n",
      "Iter 37 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 38\n",
      "Train accuracy: 82.73492063492064\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 38 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 39\n",
      "Train accuracy: 82.9968253968254\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 39 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 40\n",
      "Train accuracy: 83.2\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 40 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 41\n",
      "Train accuracy: 83.46507936507936\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 41 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 42\n",
      "Train accuracy: 83.66349206349206\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 42 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 43\n",
      "Train accuracy: 83.87142857142858\n",
      "Val accuracy: 82.47142857142858\n",
      "Iter 43 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 44\n",
      "Train accuracy: 84.04761904761905\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 44 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 45\n",
      "Train accuracy: 84.21111111111111\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 45 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 46\n",
      "Train accuracy: 84.38571428571429\n",
      "Val accuracy: 83.05714285714285\n",
      "Iter 46 -> sub iter 99 : 85.07936507936508\n",
      "Iteration: 47\n",
      "Train accuracy: 84.57301587301588\n",
      "Val accuracy: 83.12857142857143\n",
      "Iter 47 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 48\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 83.34285714285714\n",
      "Iter 48 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 49\n",
      "Train accuracy: 84.91746031746031\n",
      "Val accuracy: 83.5\n",
      "Iter 49 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 50\n",
      "Train accuracy: 85.06984126984128\n",
      "Val accuracy: 83.61428571428571\n",
      "Iter 50 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 51\n",
      "Train accuracy: 85.22222222222223\n",
      "Val accuracy: 83.78571428571429\n",
      "Iter 51 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 52\n",
      "Train accuracy: 85.39999999999999\n",
      "Val accuracy: 83.84285714285714\n",
      "Iter 52 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 53\n",
      "Train accuracy: 85.54603174603174\n",
      "Val accuracy: 83.92857142857143\n",
      "Iter 53 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 54\n",
      "Train accuracy: 85.66984126984127\n",
      "Val accuracy: 84.12857142857143\n",
      "Iter 54 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 55\n",
      "Train accuracy: 85.8\n",
      "Val accuracy: 84.2\n",
      "Iter 55 -> sub iter 99 : 86.03174603174604\n",
      "Iteration: 56\n",
      "Train accuracy: 85.91587301587302\n",
      "Val accuracy: 84.31428571428572\n",
      "Iter 56 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 57\n",
      "Train accuracy: 85.99365079365079\n",
      "Val accuracy: 84.41428571428573\n",
      "Iter 57 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 58\n",
      "Train accuracy: 86.1\n",
      "Val accuracy: 84.52857142857142\n",
      "Iter 58 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 59\n",
      "Train accuracy: 86.23015873015873\n",
      "Val accuracy: 84.61428571428571\n",
      "Iter 59 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 60\n",
      "Train accuracy: 86.34444444444445\n",
      "Val accuracy: 84.68571428571428\n",
      "Iter 60 -> sub iter 99 : 86.50793650793652\n",
      "Iteration: 61\n",
      "Train accuracy: 86.42857142857143\n",
      "Val accuracy: 84.72857142857143\n",
      "Iter 61 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 62\n",
      "Train accuracy: 86.53968253968254\n",
      "Val accuracy: 84.85714285714285\n",
      "Iter 62 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 63\n",
      "Train accuracy: 86.65555555555555\n",
      "Val accuracy: 85.02857142857142\n",
      "Iter 63 -> sub iter 99 : 86.98412698412699\n",
      "Iteration: 64\n",
      "Train accuracy: 86.76984126984128\n",
      "Val accuracy: 85.08571428571429\n",
      "Iter 64 -> sub iter 99 : 87.14285714285714\n",
      "Iteration: 65\n",
      "Train accuracy: 86.89206349206349\n",
      "Val accuracy: 85.21428571428571\n",
      "Iter 65 -> sub iter 99 : 87.30158730158735\n",
      "Iteration: 66\n",
      "Train accuracy: 86.9920634920635\n",
      "Val accuracy: 85.3\n",
      "Iter 66 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 67\n",
      "Train accuracy: 87.06349206349206\n",
      "Val accuracy: 85.45714285714286\n",
      "Iter 67 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 68\n",
      "Train accuracy: 87.14603174603175\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 68 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 69\n",
      "Train accuracy: 87.1984126984127\n",
      "Val accuracy: 85.58571428571429\n",
      "Iter 69 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 70\n",
      "Train accuracy: 87.27936507936508\n",
      "Val accuracy: 85.8\n",
      "Iter 70 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 71\n",
      "Train accuracy: 87.34603174603176\n",
      "Val accuracy: 85.91428571428571\n",
      "Iter 71 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 72\n",
      "Train accuracy: 87.44761904761906\n",
      "Val accuracy: 86.02857142857144\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 87.55238095238094\n",
      "Val accuracy: 86.11428571428571\n",
      "Iter 73 -> sub iter 99 : 87.77777777777777\n",
      "Iteration: 74\n",
      "Train accuracy: 87.61904761904762\n",
      "Val accuracy: 86.18571428571428\n",
      "Iter 74 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 75\n",
      "Train accuracy: 87.69206349206348\n",
      "Val accuracy: 86.25714285714285\n",
      "Iter 75 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 76\n",
      "Train accuracy: 87.7968253968254\n",
      "Val accuracy: 86.34285714285714\n",
      "Iter 76 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 77\n",
      "Train accuracy: 87.87460317460317\n",
      "Val accuracy: 86.37142857142858\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.96666666666667\n",
      "Val accuracy: 86.5142857142857\n",
      "Iter 78 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 79\n",
      "Train accuracy: 88.00634920634921\n",
      "Val accuracy: 86.61428571428571\n",
      "Iter 79 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 80\n",
      "Train accuracy: 88.07301587301588\n",
      "Val accuracy: 86.71428571428571\n",
      "Iter 80 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 81\n",
      "Train accuracy: 88.16190476190476\n",
      "Val accuracy: 86.81428571428572\n",
      "Iter 81 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 82\n",
      "Train accuracy: 88.22539682539683\n",
      "Val accuracy: 86.88571428571429\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.27301587301586\n",
      "Val accuracy: 87.02857142857144\n",
      "Iter 83 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 84\n",
      "Train accuracy: 88.33333333333333\n",
      "Val accuracy: 87.08571428571429\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.38571428571429\n",
      "Val accuracy: 87.1\n",
      "Iter 85 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 86\n",
      "Train accuracy: 88.43174603174603\n",
      "Val accuracy: 87.14285714285714\n",
      "Iter 86 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 87\n",
      "Train accuracy: 88.47301587301587\n",
      "Val accuracy: 87.25714285714285\n",
      "Iter 87 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 88\n",
      "Train accuracy: 88.50634920634921\n",
      "Val accuracy: 87.37142857142857\n",
      "Iter 88 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 89\n",
      "Train accuracy: 88.55079365079365\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 89 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 90\n",
      "Train accuracy: 88.60317460317461\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 90 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 91\n",
      "Train accuracy: 88.66349206349207\n",
      "Val accuracy: 87.42857142857143\n",
      "Iter 91 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 92\n",
      "Train accuracy: 88.70476190476191\n",
      "Val accuracy: 87.44285714285715\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 88.75714285714285\n",
      "Val accuracy: 87.5142857142857\n",
      "Iter 93 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 94\n",
      "Train accuracy: 88.81269841269841\n",
      "Val accuracy: 87.55714285714285\n",
      "Iter 94 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 95\n",
      "Train accuracy: 88.87142857142857\n",
      "Val accuracy: 87.6\n",
      "Iter 95 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 96\n",
      "Train accuracy: 88.93015873015872\n",
      "Val accuracy: 87.64285714285714\n",
      "Iter 96 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 97\n",
      "Train accuracy: 89.0047619047619\n",
      "Val accuracy: 87.71428571428571\n",
      "Iter 97 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 98\n",
      "Train accuracy: 89.04761904761904\n",
      "Val accuracy: 87.78571428571429\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 87.81428571428572\n",
      "Iter 99 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 100\n",
      "Train accuracy: 89.13650793650794\n",
      "Val accuracy: 87.84285714285714\n",
      "Iter 100 -> sub iter 99 : 90.02222222222223\n",
      "Iteration: 101\n",
      "Train accuracy: 89.17142857142856\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 101 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 102\n",
      "Train accuracy: 89.23968253968255\n",
      "Val accuracy: 87.97142857142856\n",
      "Iter 102 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 103\n",
      "Train accuracy: 89.28888888888889\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 103 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 104\n",
      "Train accuracy: 89.33174603174604\n",
      "Val accuracy: 88.12857142857143\n",
      "Iter 104 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 105\n",
      "Train accuracy: 89.36666666666667\n",
      "Val accuracy: 88.2\n",
      "Iter 105 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 106\n",
      "Train accuracy: 89.4015873015873\n",
      "Val accuracy: 88.27142857142857\n",
      "Iter 106 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 107\n",
      "Train accuracy: 89.43968253968254\n",
      "Val accuracy: 88.34285714285714\n",
      "Iter 107 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 108\n",
      "Train accuracy: 89.46666666666667\n",
      "Val accuracy: 88.35714285714286\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 89.5047619047619\n",
      "Val accuracy: 88.38571428571429\n",
      "Iter 109 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 110\n",
      "Train accuracy: 89.55396825396825\n",
      "Val accuracy: 88.44285714285715\n",
      "Iter 110 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 111\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 88.54285714285714\n",
      "Iter 111 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 112\n",
      "Train accuracy: 89.64444444444445\n",
      "Val accuracy: 88.6\n",
      "Iter 112 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 113\n",
      "Train accuracy: 89.67936507936508\n",
      "Val accuracy: 88.62857142857142\n",
      "Iter 113 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 114\n",
      "Train accuracy: 89.72698412698412\n",
      "Val accuracy: 88.67142857142856\n",
      "Iter 114 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 115\n",
      "Train accuracy: 89.78095238095239\n",
      "Val accuracy: 88.68571428571428\n",
      "Iter 115 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 116\n",
      "Train accuracy: 89.8079365079365\n",
      "Val accuracy: 88.77142857142857\n",
      "Iter 116 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 117\n",
      "Train accuracy: 89.83968253968254\n",
      "Val accuracy: 88.81428571428572\n",
      "Iter 117 -> sub iter 99 : 90.79365079365088\n",
      "Iteration: 118\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 88.87142857142857\n",
      "Iter 118 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 119\n",
      "Train accuracy: 89.91428571428571\n",
      "Val accuracy: 88.88571428571429\n",
      "Iter 119 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 120\n",
      "Train accuracy: 89.95714285714286\n",
      "Val accuracy: 88.91428571428571\n",
      "Iter 120 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 121\n",
      "Train accuracy: 89.97777777777777\n",
      "Val accuracy: 88.97142857142856\n",
      "Iter 121 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 122\n",
      "Train accuracy: 90.0079365079365\n",
      "Val accuracy: 89.07142857142857\n",
      "Iter 122 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 123\n",
      "Train accuracy: 90.04126984126984\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 123 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 124\n",
      "Train accuracy: 90.07619047619048\n",
      "Val accuracy: 89.22857142857143\n",
      "Iter 124 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 125\n",
      "Train accuracy: 90.11587301587302\n",
      "Val accuracy: 89.24285714285715\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 90.14761904761905\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 90.16507936507936\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 90.1968253968254\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 90.23492063492064\n",
      "Val accuracy: 89.31428571428572\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 90.26190476190476\n",
      "Val accuracy: 89.37142857142857\n",
      "Iter 130 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 131\n",
      "Train accuracy: 90.27936507936508\n",
      "Val accuracy: 89.34285714285714\n",
      "Iter 131 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 132\n",
      "Train accuracy: 90.32063492063493\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 132 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 133\n",
      "Train accuracy: 90.34920634920634\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 133 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 134\n",
      "Train accuracy: 90.36825396825397\n",
      "Val accuracy: 89.45714285714286\n",
      "Iter 134 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 135\n",
      "Train accuracy: 90.4\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 135 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 136\n",
      "Train accuracy: 90.42857142857143\n",
      "Val accuracy: 89.60000000000001\n",
      "Iter 136 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 137\n",
      "Train accuracy: 90.44761904761904\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 137 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 138\n",
      "Train accuracy: 90.46825396825396\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 138 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 139\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 139 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 140\n",
      "Train accuracy: 90.4952380952381\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 140 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 141\n",
      "Train accuracy: 90.51269841269841\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 141 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 142\n",
      "Train accuracy: 90.52222222222223\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 142 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 143\n",
      "Train accuracy: 90.54603174603174\n",
      "Val accuracy: 89.6857142857143\n",
      "Iter 143 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 144\n",
      "Train accuracy: 90.57777777777778\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 144 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 145\n",
      "Train accuracy: 90.6079365079365\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 145 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 146\n",
      "Train accuracy: 90.62222222222222\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 146 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 147\n",
      "Train accuracy: 90.63015873015873\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 147 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 148\n",
      "Train accuracy: 90.65396825396826\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 148 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 149\n",
      "Train accuracy: 90.67936507936508\n",
      "Val accuracy: 89.8\n",
      "Iter 149 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 150\n",
      "Train accuracy: 90.6920634920635\n",
      "Val accuracy: 89.81428571428572\n",
      "Training for 0.1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 13.650793650793653\n",
      "Iteration: 1\n",
      "Train accuracy: 13.998412698412698\n",
      "Val accuracy: 14.32857142857143\n",
      "Iter 1 -> sub iter 99 : 19.365079365079367\n",
      "Iteration: 2\n",
      "Train accuracy: 20.185714285714283\n",
      "Val accuracy: 20.314285714285717\n",
      "Iter 2 -> sub iter 99 : 22.380952380952383\n",
      "Iteration: 3\n",
      "Train accuracy: 23.184126984126983\n",
      "Val accuracy: 23.32857142857143\n",
      "Iter 3 -> sub iter 99 : 25.555555555555554\n",
      "Iteration: 4\n",
      "Train accuracy: 26.05396825396825\n",
      "Val accuracy: 26.285714285714285\n",
      "Iter 4 -> sub iter 99 : 29.682539682539684\n",
      "Iteration: 5\n",
      "Train accuracy: 29.965079365079365\n",
      "Val accuracy: 30.542857142857144\n",
      "Iter 5 -> sub iter 99 : 39.841269841269845\n",
      "Iteration: 6\n",
      "Train accuracy: 39.268253968253966\n",
      "Val accuracy: 39.628571428571426\n",
      "Iter 6 -> sub iter 99 : 45.714285714285715\n",
      "Iteration: 7\n",
      "Train accuracy: 44.371428571428574\n",
      "Val accuracy: 44.800000000000004\n",
      "Iter 7 -> sub iter 99 : 48.888888888888886\n",
      "Iteration: 8\n",
      "Train accuracy: 47.52222222222222\n",
      "Val accuracy: 47.92857142857142\n",
      "Iter 8 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 9\n",
      "Train accuracy: 49.804761904761904\n",
      "Val accuracy: 50.2\n",
      "Iter 9 -> sub iter 99 : 53.174603174603185\n",
      "Iteration: 10\n",
      "Train accuracy: 51.74126984126984\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 10 -> sub iter 99 : 54.444444444444444\n",
      "Iteration: 11\n",
      "Train accuracy: 53.179365079365084\n",
      "Val accuracy: 53.214285714285715\n",
      "Iter 11 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 12\n",
      "Train accuracy: 54.36984126984127\n",
      "Val accuracy: 54.38571428571428\n",
      "Iter 12 -> sub iter 99 : 57.301587301587396\n",
      "Iteration: 13\n",
      "Train accuracy: 55.74761904761905\n",
      "Val accuracy: 55.51428571428572\n",
      "Iter 13 -> sub iter 99 : 60.793650793650794\n",
      "Iteration: 14\n",
      "Train accuracy: 58.53174603174603\n",
      "Val accuracy: 58.08571428571428\n",
      "Iter 14 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 15\n",
      "Train accuracy: 61.68730158730159\n",
      "Val accuracy: 61.01428571428571\n",
      "Iter 15 -> sub iter 99 : 66.507936507936544\n",
      "Iteration: 16\n",
      "Train accuracy: 63.53809523809524\n",
      "Val accuracy: 63.15714285714286\n",
      "Iter 16 -> sub iter 99 : 67.619047619047626\n",
      "Iteration: 17\n",
      "Train accuracy: 64.57619047619048\n",
      "Val accuracy: 64.31428571428572\n",
      "Iter 17 -> sub iter 99 : 68.730158730158735\n",
      "Iteration: 18\n",
      "Train accuracy: 65.37777777777778\n",
      "Val accuracy: 65.11428571428571\n",
      "Iter 18 -> sub iter 99 : 68.888888888888895\n",
      "Iteration: 19\n",
      "Train accuracy: 66.02698412698412\n",
      "Val accuracy: 65.75714285714285\n",
      "Iter 19 -> sub iter 99 : 69.523809523809526\n",
      "Iteration: 20\n",
      "Train accuracy: 66.6126984126984\n",
      "Val accuracy: 66.4\n",
      "Iter 20 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 21\n",
      "Train accuracy: 67.1952380952381\n",
      "Val accuracy: 66.81428571428572\n",
      "Iter 21 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 22\n",
      "Train accuracy: 68.56507936507936\n",
      "Val accuracy: 68.37142857142857\n",
      "Iter 22 -> sub iter 99 : 73.33333333333333\n",
      "Iteration: 23\n",
      "Train accuracy: 70.21904761904761\n",
      "Val accuracy: 70.35714285714286\n",
      "Iter 23 -> sub iter 99 : 73.80952380952381\n",
      "Iteration: 24\n",
      "Train accuracy: 71.43174603174603\n",
      "Val accuracy: 71.8\n",
      "Iter 24 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 25\n",
      "Train accuracy: 72.41587301587302\n",
      "Val accuracy: 73.2\n",
      "Iter 25 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 26\n",
      "Train accuracy: 73.25714285714285\n",
      "Val accuracy: 73.87142857142858\n",
      "Iter 26 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 27\n",
      "Train accuracy: 73.98095238095237\n",
      "Val accuracy: 74.58571428571429\n",
      "Iter 27 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 28\n",
      "Train accuracy: 74.55555555555556\n",
      "Val accuracy: 75.17142857142856\n",
      "Iter 28 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 29\n",
      "Train accuracy: 75.05873015873016\n",
      "Val accuracy: 75.72857142857143\n",
      "Iter 29 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 30\n",
      "Train accuracy: 75.51904761904763\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 30 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 31\n",
      "Train accuracy: 75.84444444444445\n",
      "Val accuracy: 76.5142857142857\n",
      "Iter 31 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 32\n",
      "Train accuracy: 76.22698412698414\n",
      "Val accuracy: 76.84285714285714\n",
      "Iter 32 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 33\n",
      "Train accuracy: 76.47460317460317\n",
      "Val accuracy: 77.14285714285715\n",
      "Iter 33 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 34\n",
      "Train accuracy: 76.77777777777777\n",
      "Val accuracy: 77.4\n",
      "Iter 34 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 35\n",
      "Train accuracy: 77.0015873015873\n",
      "Val accuracy: 77.65714285714286\n",
      "Iter 35 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 36\n",
      "Train accuracy: 77.21904761904761\n",
      "Val accuracy: 77.81428571428572\n",
      "Iter 36 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 37\n",
      "Train accuracy: 77.44920634920635\n",
      "Val accuracy: 78.02857142857142\n",
      "Iter 37 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 38\n",
      "Train accuracy: 77.64603174603174\n",
      "Val accuracy: 78.18571428571428\n",
      "Iter 38 -> sub iter 99 : 79.52380952380952\n",
      "Iteration: 39\n",
      "Train accuracy: 77.83333333333333\n",
      "Val accuracy: 78.3\n",
      "Iter 39 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 40\n",
      "Train accuracy: 78.01904761904763\n",
      "Val accuracy: 78.54285714285714\n",
      "Iter 40 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 41\n",
      "Train accuracy: 78.1984126984127\n",
      "Val accuracy: 78.74285714285715\n",
      "Iter 41 -> sub iter 99 : 80.00634920634929\n",
      "Iteration: 42\n",
      "Train accuracy: 78.37936507936509\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 42 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 43\n",
      "Train accuracy: 78.54603174603174\n",
      "Val accuracy: 78.98571428571428\n",
      "Iter 43 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 44\n",
      "Train accuracy: 78.71904761904761\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 44 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 45\n",
      "Train accuracy: 78.84761904761905\n",
      "Val accuracy: 79.27142857142857\n",
      "Iter 45 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 46\n",
      "Train accuracy: 78.98730158730159\n",
      "Val accuracy: 79.47142857142858\n",
      "Iter 46 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 47\n",
      "Train accuracy: 79.11587301587302\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 47 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 48\n",
      "Train accuracy: 79.23492063492064\n",
      "Val accuracy: 79.74285714285713\n",
      "Iter 48 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 49\n",
      "Train accuracy: 79.34126984126985\n",
      "Val accuracy: 79.87142857142857\n",
      "Iter 49 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 50\n",
      "Train accuracy: 79.43015873015872\n",
      "Val accuracy: 79.88571428571429\n",
      "Iter 50 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 51\n",
      "Train accuracy: 79.53492063492064\n",
      "Val accuracy: 80.01428571428572\n",
      "Iter 51 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 52\n",
      "Train accuracy: 79.62857142857143\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 52 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 53\n",
      "Train accuracy: 79.74285714285713\n",
      "Val accuracy: 80.27142857142857\n",
      "Iter 53 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 54\n",
      "Train accuracy: 79.84126984126985\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 54 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 55\n",
      "Train accuracy: 79.94920634920635\n",
      "Val accuracy: 80.37142857142857\n",
      "Iter 55 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 80.04603174603174\n",
      "Val accuracy: 80.44285714285714\n",
      "Iter 56 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 57\n",
      "Train accuracy: 80.12698412698413\n",
      "Val accuracy: 80.52857142857142\n",
      "Iter 57 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 58\n",
      "Train accuracy: 80.22857142857143\n",
      "Val accuracy: 80.57142857142857\n",
      "Iter 58 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 59\n",
      "Train accuracy: 80.32380952380952\n",
      "Val accuracy: 80.64285714285714\n",
      "Iter 59 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 60\n",
      "Train accuracy: 80.4047619047619\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 60 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 61\n",
      "Train accuracy: 80.51428571428572\n",
      "Val accuracy: 80.85714285714286\n",
      "Iter 61 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 62\n",
      "Train accuracy: 80.61111111111111\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 62 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 63\n",
      "Train accuracy: 80.72857142857143\n",
      "Val accuracy: 80.94285714285714\n",
      "Iter 63 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 64\n",
      "Train accuracy: 80.81428571428572\n",
      "Val accuracy: 81.01428571428572\n",
      "Iter 64 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 65\n",
      "Train accuracy: 80.95079365079366\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 65 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 66\n",
      "Train accuracy: 81.2047619047619\n",
      "Val accuracy: 81.21428571428572\n",
      "Iter 66 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 81.55238095238096\n",
      "Val accuracy: 81.55714285714286\n",
      "Iter 67 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 68\n",
      "Train accuracy: 82.17142857142858\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 68 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 69\n",
      "Train accuracy: 82.97301587301588\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 69 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 70\n",
      "Train accuracy: 83.9015873015873\n",
      "Val accuracy: 83.6\n",
      "Iter 70 -> sub iter 99 : 87.30158730158731\n",
      "Iteration: 71\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 84.48571428571428\n",
      "Iter 71 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 72\n",
      "Train accuracy: 85.50952380952381\n",
      "Val accuracy: 85.28571428571429\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 86.03650793650793\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 73 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 74\n",
      "Train accuracy: 86.52063492063492\n",
      "Val accuracy: 85.94285714285715\n",
      "Iter 74 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 75\n",
      "Train accuracy: 86.90952380952382\n",
      "Val accuracy: 86.52857142857144\n",
      "Iter 75 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 76\n",
      "Train accuracy: 87.25714285714285\n",
      "Val accuracy: 86.92857142857143\n",
      "Iter 76 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 77\n",
      "Train accuracy: 87.59523809523809\n",
      "Val accuracy: 87.22857142857143\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.83492063492064\n",
      "Val accuracy: 87.4\n",
      "Iter 78 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 79\n",
      "Train accuracy: 88.07460317460317\n",
      "Val accuracy: 87.62857142857143\n",
      "Iter 79 -> sub iter 99 : 88.41269841269849\n",
      "Iteration: 80\n",
      "Train accuracy: 88.23968253968253\n",
      "Val accuracy: 87.8\n",
      "Iter 80 -> sub iter 99 : 88.41269841269842\n",
      "Iteration: 81\n",
      "Train accuracy: 88.4063492063492\n",
      "Val accuracy: 87.94285714285715\n",
      "Iter 81 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 82\n",
      "Train accuracy: 88.58095238095238\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.73174603174603\n",
      "Val accuracy: 88.15714285714286\n",
      "Iter 83 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 84\n",
      "Train accuracy: 88.86349206349206\n",
      "Val accuracy: 88.3\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.97142857142856\n",
      "Val accuracy: 88.37142857142857\n",
      "Iter 85 -> sub iter 99 : 88.41269841269845\n",
      "Iteration: 86\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 88.45714285714286\n",
      "Iter 86 -> sub iter 99 : 88.41269841269844\n",
      "Iteration: 87\n",
      "Train accuracy: 89.19365079365079\n",
      "Val accuracy: 88.52857142857142\n",
      "Iter 87 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 88\n",
      "Train accuracy: 89.29206349206349\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 88 -> sub iter 99 : 88.41269841269848\n",
      "Iteration: 89\n",
      "Train accuracy: 89.40476190476191\n",
      "Val accuracy: 88.8\n",
      "Iter 89 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 90\n",
      "Train accuracy: 89.51269841269841\n",
      "Val accuracy: 88.9857142857143\n",
      "Iter 90 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 91\n",
      "Train accuracy: 89.63333333333333\n",
      "Val accuracy: 89.1\n",
      "Iter 91 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 92\n",
      "Train accuracy: 89.71746031746032\n",
      "Val accuracy: 89.17142857142856\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 89.80317460317461\n",
      "Val accuracy: 89.2\n",
      "Iter 93 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 94\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 94 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 95\n",
      "Train accuracy: 89.94920634920635\n",
      "Val accuracy: 89.35714285714286\n",
      "Iter 95 -> sub iter 99 : 89.52380952380953\n",
      "Iteration: 96\n",
      "Train accuracy: 90.01904761904763\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 96 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 97\n",
      "Train accuracy: 90.09206349206349\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 97 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 98\n",
      "Train accuracy: 90.14126984126985\n",
      "Val accuracy: 89.47142857142858\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 90.2095238095238\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 99 -> sub iter 99 : 90.02857142857143\n",
      "Iteration: 100\n",
      "Train accuracy: 90.2936507936508\n",
      "Val accuracy: 89.58571428571429\n",
      "Iter 100 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 101\n",
      "Train accuracy: 90.36031746031746\n",
      "Val accuracy: 89.67142857142856\n",
      "Iter 101 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 102\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.75714285714285\n",
      "Iter 102 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 103\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.81428571428572\n",
      "Iter 103 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 104\n",
      "Train accuracy: 90.54920634920634\n",
      "Val accuracy: 89.87142857142857\n",
      "Iter 104 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 105\n",
      "Train accuracy: 90.60952380952381\n",
      "Val accuracy: 89.9\n",
      "Iter 105 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 106\n",
      "Train accuracy: 90.65873015873017\n",
      "Val accuracy: 89.97142857142858\n",
      "Iter 106 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 107\n",
      "Train accuracy: 90.7031746031746\n",
      "Val accuracy: 90.04285714285714\n",
      "Iter 107 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 108\n",
      "Train accuracy: 90.73174603174603\n",
      "Val accuracy: 90.08571428571429\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 90.78412698412698\n",
      "Val accuracy: 90.12857142857142\n",
      "Iter 109 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 110\n",
      "Train accuracy: 90.84603174603174\n",
      "Val accuracy: 90.15714285714286\n",
      "Iter 110 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 111\n",
      "Train accuracy: 90.89365079365079\n",
      "Val accuracy: 90.2\n",
      "Iter 111 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 112\n",
      "Train accuracy: 90.94920634920635\n",
      "Val accuracy: 90.24285714285715\n",
      "Iter 112 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 113\n",
      "Train accuracy: 91.0079365079365\n",
      "Val accuracy: 90.25714285714285\n",
      "Iter 113 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 114\n",
      "Train accuracy: 91.06349206349206\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 114 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 115\n",
      "Train accuracy: 91.10000000000001\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 115 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 116\n",
      "Train accuracy: 91.14603174603174\n",
      "Val accuracy: 90.31428571428572\n",
      "Iter 116 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 117\n",
      "Train accuracy: 91.1952380952381\n",
      "Val accuracy: 90.34285714285714\n",
      "Iter 117 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 118\n",
      "Train accuracy: 91.23968253968255\n",
      "Val accuracy: 90.4\n",
      "Iter 118 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 119\n",
      "Train accuracy: 91.28571428571428\n",
      "Val accuracy: 90.4\n",
      "Iter 119 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 120\n",
      "Train accuracy: 91.33015873015873\n",
      "Val accuracy: 90.42857142857143\n",
      "Iter 120 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 121\n",
      "Train accuracy: 91.35555555555555\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 121 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 122\n",
      "Train accuracy: 91.41269841269842\n",
      "Val accuracy: 90.52857142857142\n",
      "Iter 122 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 123\n",
      "Train accuracy: 91.43968253968254\n",
      "Val accuracy: 90.58571428571427\n",
      "Iter 123 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 124\n",
      "Train accuracy: 91.48412698412697\n",
      "Val accuracy: 90.60000000000001\n",
      "Iter 124 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 125\n",
      "Train accuracy: 91.53333333333333\n",
      "Val accuracy: 90.62857142857142\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 91.57142857142857\n",
      "Val accuracy: 90.65714285714286\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 91.60634920634921\n",
      "Val accuracy: 90.68571428571428\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 91.63650793650794\n",
      "Val accuracy: 90.71428571428571\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 91.66349206349206\n",
      "Val accuracy: 90.74285714285715\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 91.70158730158731\n",
      "Val accuracy: 90.8\n",
      "Iter 130 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 131\n",
      "Train accuracy: 91.72063492063492\n",
      "Val accuracy: 90.82857142857142\n",
      "Iter 131 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 132\n",
      "Train accuracy: 91.74285714285715\n",
      "Val accuracy: 90.84285714285714\n",
      "Iter 132 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 133\n",
      "Train accuracy: 91.76507936507936\n",
      "Val accuracy: 90.87142857142857\n",
      "Iter 133 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 134\n",
      "Train accuracy: 91.8015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 134 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 135\n",
      "Train accuracy: 91.83015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 135 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 136\n",
      "Train accuracy: 91.86666666666666\n",
      "Val accuracy: 90.9\n",
      "Iter 136 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 137\n",
      "Train accuracy: 91.91746031746032\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 137 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 138\n",
      "Train accuracy: 91.94761904761904\n",
      "Val accuracy: 90.97142857142858\n",
      "Iter 138 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 139\n",
      "Train accuracy: 91.98888888888888\n",
      "Val accuracy: 91.05714285714286\n",
      "Iter 139 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 140\n",
      "Train accuracy: 92.02698412698412\n",
      "Val accuracy: 91.11428571428571\n",
      "Iter 140 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 141\n",
      "Train accuracy: 92.05238095238096\n",
      "Val accuracy: 91.12857142857142\n",
      "Iter 141 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 142\n",
      "Train accuracy: 92.08571428571429\n",
      "Val accuracy: 91.15714285714286\n",
      "Iter 142 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 143\n",
      "Train accuracy: 92.1079365079365\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 143 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 144\n",
      "Train accuracy: 92.13809523809525\n",
      "Val accuracy: 91.21428571428571\n",
      "Iter 144 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 145\n",
      "Train accuracy: 92.17142857142858\n",
      "Val accuracy: 91.24285714285715\n",
      "Iter 145 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 146\n",
      "Train accuracy: 92.1968253968254\n",
      "Val accuracy: 91.25714285714285\n",
      "Iter 146 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 147\n",
      "Train accuracy: 92.23174603174603\n",
      "Val accuracy: 91.27142857142857\n",
      "Iter 147 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 148\n",
      "Train accuracy: 92.25873015873016\n",
      "Val accuracy: 91.3\n",
      "Iter 148 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 149\n",
      "Train accuracy: 92.28253968253968\n",
      "Val accuracy: 91.3\n",
      "Iter 149 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 150\n",
      "Train accuracy: 92.3015873015873\n",
      "Val accuracy: 91.3\n",
      "Training for 0.01\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 24.285714285714285\n",
      "Iteration: 1\n",
      "Train accuracy: 21.285714285714285\n",
      "Val accuracy: 20.42857142857143\n",
      "Iter 1 -> sub iter 99 : 26.666666666666668\n",
      "Iteration: 2\n",
      "Train accuracy: 24.23968253968254\n",
      "Val accuracy: 23.414285714285715\n",
      "Iter 2 -> sub iter 99 : 29.523809523809526\n",
      "Iteration: 3\n",
      "Train accuracy: 26.51904761904762\n",
      "Val accuracy: 25.785714285714285\n",
      "Iter 3 -> sub iter 99 : 34.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 30.831746031746032\n",
      "Val accuracy: 30.242857142857144\n",
      "Iter 4 -> sub iter 99 : 37.301587301587304\n",
      "Iteration: 5\n",
      "Train accuracy: 33.77460317460317\n",
      "Val accuracy: 33.15714285714286\n",
      "Iter 5 -> sub iter 99 : 39.047619047619054\n",
      "Iteration: 6\n",
      "Train accuracy: 35.75238095238095\n",
      "Val accuracy: 35.25714285714286\n",
      "Iter 6 -> sub iter 99 : 39.523809523809526\n",
      "Iteration: 7\n",
      "Train accuracy: 37.7\n",
      "Val accuracy: 37.25714285714285\n",
      "Iter 7 -> sub iter 99 : 42.698412698412696\n",
      "Iteration: 8\n",
      "Train accuracy: 41.15555555555556\n",
      "Val accuracy: 40.34285714285714\n",
      "Iter 8 -> sub iter 99 : 46.190476190476195\n",
      "Iteration: 9\n",
      "Train accuracy: 44.84444444444444\n",
      "Val accuracy: 43.67142857142857\n",
      "Iter 9 -> sub iter 99 : 51.904761904761916\n",
      "Iteration: 10\n",
      "Train accuracy: 52.49206349206349\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 10 -> sub iter 99 : 55.238095238095246\n",
      "Iteration: 11\n",
      "Train accuracy: 56.10793650793651\n",
      "Val accuracy: 55.442857142857136\n",
      "Iter 11 -> sub iter 99 : 58.412698412698425\n",
      "Iteration: 12\n",
      "Train accuracy: 58.439682539682536\n",
      "Val accuracy: 57.72857142857143\n",
      "Iter 12 -> sub iter 99 : 60.317460317460316\n",
      "Iteration: 13\n",
      "Train accuracy: 60.098412698412695\n",
      "Val accuracy: 59.61428571428572\n",
      "Iter 13 -> sub iter 99 : 62.698412698412696\n",
      "Iteration: 14\n",
      "Train accuracy: 61.66031746031746\n",
      "Val accuracy: 61.42857142857143\n",
      "Iter 14 -> sub iter 99 : 65.238095238095245\n",
      "Iteration: 15\n",
      "Train accuracy: 63.32698412698413\n",
      "Val accuracy: 62.857142857142854\n",
      "Iter 15 -> sub iter 99 : 66.190476190476196\n",
      "Iteration: 16\n",
      "Train accuracy: 64.60317460317461\n",
      "Val accuracy: 64.14285714285714\n",
      "Iter 16 -> sub iter 99 : 66.507936507936545\n",
      "Iteration: 17\n",
      "Train accuracy: 65.4952380952381\n",
      "Val accuracy: 65.17142857142856\n",
      "Iter 17 -> sub iter 99 : 67.777777777777796\n",
      "Iteration: 18\n",
      "Train accuracy: 66.21904761904761\n",
      "Val accuracy: 65.9\n",
      "Iter 18 -> sub iter 99 : 68.730158730158736\n",
      "Iteration: 19\n",
      "Train accuracy: 66.91111111111111\n",
      "Val accuracy: 66.60000000000001\n",
      "Iter 19 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 20\n",
      "Train accuracy: 68.81111111111112\n",
      "Val accuracy: 68.54285714285714\n",
      "Iter 20 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 21\n",
      "Train accuracy: 70.85396825396826\n",
      "Val accuracy: 70.61428571428571\n",
      "Iter 21 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 22\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 72.0\n",
      "Iter 22 -> sub iter 99 : 73.49206349206358\n",
      "Iteration: 23\n",
      "Train accuracy: 73.44444444444444\n",
      "Val accuracy: 73.17142857142858\n",
      "Iter 23 -> sub iter 99 : 74.44444444444444\n",
      "Iteration: 24\n",
      "Train accuracy: 74.31428571428572\n",
      "Val accuracy: 74.11428571428571\n",
      "Iter 24 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 25\n",
      "Train accuracy: 74.92857142857143\n",
      "Val accuracy: 74.67142857142856\n",
      "Iter 25 -> sub iter 99 : 75.23809523809524\n",
      "Iteration: 26\n",
      "Train accuracy: 75.4968253968254\n",
      "Val accuracy: 75.0\n",
      "Iter 26 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 27\n",
      "Train accuracy: 75.91904761904762\n",
      "Val accuracy: 75.44285714285715\n",
      "Iter 27 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 28\n",
      "Train accuracy: 76.29047619047618\n",
      "Val accuracy: 75.68571428571428\n",
      "Iter 28 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 29\n",
      "Train accuracy: 76.65238095238095\n",
      "Val accuracy: 76.0\n",
      "Iter 29 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 30\n",
      "Train accuracy: 76.95714285714286\n",
      "Val accuracy: 76.31428571428572\n",
      "Iter 30 -> sub iter 99 : 76.34920634920634\n",
      "Iteration: 31\n",
      "Train accuracy: 77.22063492063492\n",
      "Val accuracy: 76.6\n",
      "Iter 31 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 32\n",
      "Train accuracy: 77.4920634920635\n",
      "Val accuracy: 76.92857142857143\n",
      "Iter 32 -> sub iter 99 : 77.14285714285715\n",
      "Iteration: 33\n",
      "Train accuracy: 77.72222222222223\n",
      "Val accuracy: 77.08571428571429\n",
      "Iter 33 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 34\n",
      "Train accuracy: 77.96190476190476\n",
      "Val accuracy: 77.21428571428571\n",
      "Iter 34 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 35\n",
      "Train accuracy: 78.16984126984127\n",
      "Val accuracy: 77.47142857142858\n",
      "Iter 35 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 36\n",
      "Train accuracy: 78.37460317460318\n",
      "Val accuracy: 77.8\n",
      "Iter 36 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 37\n",
      "Train accuracy: 78.57936507936508\n",
      "Val accuracy: 77.91428571428571\n",
      "Iter 37 -> sub iter 99 : 78.09523809523812\n",
      "Iteration: 38\n",
      "Train accuracy: 78.75555555555556\n",
      "Val accuracy: 78.04285714285714\n",
      "Iter 38 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 39\n",
      "Train accuracy: 78.92063492063492\n",
      "Val accuracy: 78.22857142857143\n",
      "Iter 39 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 40\n",
      "Train accuracy: 79.0936507936508\n",
      "Val accuracy: 78.37142857142857\n",
      "Iter 40 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 41\n",
      "Train accuracy: 79.26666666666667\n",
      "Val accuracy: 78.51428571428572\n",
      "Iter 41 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 79.4\n",
      "Val accuracy: 78.58571428571427\n",
      "Iter 42 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 43\n",
      "Train accuracy: 79.52539682539683\n",
      "Val accuracy: 78.77142857142857\n",
      "Iter 43 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 44\n",
      "Train accuracy: 79.62539682539682\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 44 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 45\n",
      "Train accuracy: 79.71587301587302\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 45 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 46\n",
      "Train accuracy: 79.83492063492064\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 46 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 47\n",
      "Train accuracy: 79.95396825396826\n",
      "Val accuracy: 79.37142857142857\n",
      "Iter 47 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 48\n",
      "Train accuracy: 80.04444444444444\n",
      "Val accuracy: 79.45714285714286\n",
      "Iter 48 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 49\n",
      "Train accuracy: 80.13333333333334\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 49 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 50\n",
      "Train accuracy: 80.21111111111111\n",
      "Val accuracy: 79.7\n",
      "Iter 50 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 51\n",
      "Train accuracy: 80.2984126984127\n",
      "Val accuracy: 79.84285714285714\n",
      "Iter 51 -> sub iter 99 : 79.20634920634924\n",
      "Iteration: 52\n",
      "Train accuracy: 80.38253968253967\n",
      "Val accuracy: 79.9\n",
      "Iter 52 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 53\n",
      "Train accuracy: 80.48095238095239\n",
      "Val accuracy: 79.91428571428571\n",
      "Iter 53 -> sub iter 99 : 80.15873015873017\n",
      "Iteration: 54\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 80.05714285714286\n",
      "Iter 54 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 55\n",
      "Train accuracy: 80.64126984126983\n",
      "Val accuracy: 80.12857142857143\n",
      "Iter 55 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 56\n",
      "Train accuracy: 80.70793650793651\n",
      "Val accuracy: 80.21428571428572\n",
      "Iter 56 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 57\n",
      "Train accuracy: 80.78888888888889\n",
      "Val accuracy: 80.32857142857142\n",
      "Iter 57 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 58\n",
      "Train accuracy: 80.85873015873017\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 58 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 59\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.48571428571428\n",
      "Iter 59 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 60\n",
      "Train accuracy: 80.99047619047619\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 60 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 61\n",
      "Train accuracy: 81.06507936507936\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 61 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 62\n",
      "Train accuracy: 81.12857142857143\n",
      "Val accuracy: 80.60000000000001\n",
      "Iter 62 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 63\n",
      "Train accuracy: 81.1920634920635\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 63 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 64\n",
      "Train accuracy: 81.25555555555556\n",
      "Val accuracy: 80.65714285714286\n",
      "Iter 64 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 65\n",
      "Train accuracy: 81.32857142857142\n",
      "Val accuracy: 80.7\n",
      "Iter 65 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 66\n",
      "Train accuracy: 81.37936507936509\n",
      "Val accuracy: 80.78571428571428\n",
      "Iter 66 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 67\n",
      "Train accuracy: 81.43333333333334\n",
      "Val accuracy: 80.80000000000001\n",
      "Iter 67 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 68\n",
      "Train accuracy: 81.48253968253968\n",
      "Val accuracy: 80.84285714285714\n",
      "Iter 68 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 69\n",
      "Train accuracy: 81.52539682539683\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 69 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 70\n",
      "Train accuracy: 81.5904761904762\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 70 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 71\n",
      "Train accuracy: 81.65238095238095\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 71 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 72\n",
      "Train accuracy: 81.70793650793651\n",
      "Val accuracy: 80.95714285714286\n",
      "Iter 72 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 73\n",
      "Train accuracy: 81.76825396825397\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 73 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 74\n",
      "Train accuracy: 81.81904761904762\n",
      "Val accuracy: 81.08571428571429\n",
      "Iter 74 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 75\n",
      "Train accuracy: 81.85238095238095\n",
      "Val accuracy: 81.12857142857143\n",
      "Iter 75 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 76\n",
      "Train accuracy: 81.8968253968254\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 76 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 77\n",
      "Train accuracy: 81.93015873015874\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 77 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 78\n",
      "Train accuracy: 82.0\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 78 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 79\n",
      "Train accuracy: 82.04444444444444\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 79 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 80\n",
      "Train accuracy: 82.1\n",
      "Val accuracy: 81.3\n",
      "Iter 80 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 81\n",
      "Train accuracy: 82.13333333333334\n",
      "Val accuracy: 81.34285714285714\n",
      "Iter 81 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 82\n",
      "Train accuracy: 82.15396825396826\n",
      "Val accuracy: 81.32857142857142\n",
      "Iter 82 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 83\n",
      "Train accuracy: 82.2015873015873\n",
      "Val accuracy: 81.37142857142857\n",
      "Iter 83 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 84\n",
      "Train accuracy: 82.24761904761905\n",
      "Val accuracy: 81.38571428571429\n",
      "Iter 84 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 85\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.41428571428571\n",
      "Iter 85 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 86\n",
      "Train accuracy: 82.33015873015873\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 86 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 87\n",
      "Train accuracy: 82.37301587301587\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 87 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 88\n",
      "Train accuracy: 82.43809523809524\n",
      "Val accuracy: 81.54285714285714\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 82.46507936507936\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 89 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 90\n",
      "Train accuracy: 82.5047619047619\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 90 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 91\n",
      "Train accuracy: 82.53650793650795\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 91 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 92\n",
      "Train accuracy: 82.57142857142857\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 92 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 93\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.74285714285713\n",
      "Iter 93 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 94\n",
      "Train accuracy: 82.62857142857143\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 94 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 95\n",
      "Train accuracy: 82.65714285714286\n",
      "Val accuracy: 81.81428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 82.6920634920635\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 96 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 97\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 97 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 98\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.94285714285714\n",
      "Iter 98 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 99\n",
      "Train accuracy: 82.78095238095237\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 99 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 100\n",
      "Train accuracy: 82.81746031746032\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.84603174603174\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.87777777777777\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 102 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 103\n",
      "Train accuracy: 82.89047619047619\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 104\n",
      "Train accuracy: 82.92380952380952\n",
      "Val accuracy: 82.01428571428572\n",
      "Iter 104 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 105\n",
      "Train accuracy: 82.95555555555556\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 105 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 106\n",
      "Train accuracy: 82.97777777777777\n",
      "Val accuracy: 82.05714285714286\n",
      "Iter 106 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 107\n",
      "Train accuracy: 83.0\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 107 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 108\n",
      "Train accuracy: 83.02539682539683\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 108 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 109\n",
      "Train accuracy: 83.05079365079365\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 109 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 110\n",
      "Train accuracy: 83.07460317460318\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 110 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 111\n",
      "Train accuracy: 83.1\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 111 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 112\n",
      "Train accuracy: 83.13015873015873\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 112 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 113\n",
      "Train accuracy: 83.16190476190476\n",
      "Val accuracy: 82.24285714285713\n",
      "Iter 113 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 114\n",
      "Train accuracy: 83.17936507936507\n",
      "Val accuracy: 82.27142857142857\n",
      "Iter 114 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 115\n",
      "Train accuracy: 83.20476190476191\n",
      "Val accuracy: 82.28571428571428\n",
      "Iter 115 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 116\n",
      "Train accuracy: 83.22698412698412\n",
      "Val accuracy: 82.32857142857142\n",
      "Iter 116 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 117\n",
      "Train accuracy: 83.25396825396825\n",
      "Val accuracy: 82.37142857142857\n",
      "Iter 117 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 118\n",
      "Train accuracy: 83.28253968253968\n",
      "Val accuracy: 82.38571428571429\n",
      "Iter 118 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 119\n",
      "Train accuracy: 83.30634920634921\n",
      "Val accuracy: 82.39999999999999\n",
      "Iter 119 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 120\n",
      "Train accuracy: 83.31904761904761\n",
      "Val accuracy: 82.42857142857143\n",
      "Iter 120 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 121\n",
      "Train accuracy: 83.34603174603174\n",
      "Val accuracy: 82.45714285714286\n",
      "Iter 121 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 122\n",
      "Train accuracy: 83.37460317460318\n",
      "Val accuracy: 82.48571428571428\n",
      "Iter 122 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 123\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.51428571428572\n",
      "Iter 123 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 124\n",
      "Train accuracy: 83.4095238095238\n",
      "Val accuracy: 82.55714285714286\n",
      "Iter 124 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 125\n",
      "Train accuracy: 83.44444444444444\n",
      "Val accuracy: 82.6\n",
      "Iter 125 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 126\n",
      "Train accuracy: 83.46190476190476\n",
      "Val accuracy: 82.62857142857143\n",
      "Iter 126 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 127\n",
      "Train accuracy: 83.49047619047619\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 127 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 128\n",
      "Train accuracy: 83.5079365079365\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 128 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 129\n",
      "Train accuracy: 83.53333333333333\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 129 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 130\n",
      "Train accuracy: 83.54920634920634\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 130 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 131\n",
      "Train accuracy: 83.57460317460318\n",
      "Val accuracy: 82.72857142857143\n",
      "Iter 131 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 132\n",
      "Train accuracy: 83.6015873015873\n",
      "Val accuracy: 82.78571428571428\n",
      "Iter 132 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 133\n",
      "Train accuracy: 83.62857142857143\n",
      "Val accuracy: 82.8\n",
      "Iter 133 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 134\n",
      "Train accuracy: 83.65714285714286\n",
      "Val accuracy: 82.81428571428572\n",
      "Iter 134 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 135\n",
      "Train accuracy: 83.67460317460318\n",
      "Val accuracy: 82.84285714285714\n",
      "Iter 135 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 136\n",
      "Train accuracy: 83.69682539682539\n",
      "Val accuracy: 82.85714285714286\n",
      "Iter 136 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 137\n",
      "Train accuracy: 83.72539682539683\n",
      "Val accuracy: 82.87142857142857\n",
      "Iter 137 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 138\n",
      "Train accuracy: 83.74444444444444\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 139\n",
      "Train accuracy: 83.75873015873016\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 139 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 140\n",
      "Train accuracy: 83.77619047619046\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 140 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 141\n",
      "Train accuracy: 83.7968253968254\n",
      "Val accuracy: 82.94285714285714\n",
      "Iter 141 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 142\n",
      "Train accuracy: 83.82063492063492\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 142 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 143\n",
      "Train accuracy: 83.83809523809524\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 143 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 144\n",
      "Train accuracy: 83.86190476190475\n",
      "Val accuracy: 82.97142857142858\n",
      "Iter 144 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 145\n",
      "Train accuracy: 83.88571428571429\n",
      "Val accuracy: 82.98571428571428\n",
      "Iter 145 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 146\n",
      "Train accuracy: 83.9031746031746\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 146 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 147\n",
      "Train accuracy: 83.91904761904762\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 147 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 148\n",
      "Train accuracy: 83.95238095238096\n",
      "Val accuracy: 83.0\n",
      "Iter 148 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 149\n",
      "Train accuracy: 83.97301587301588\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 149 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 150\n",
      "Train accuracy: 83.97460317460317\n",
      "Val accuracy: 83.04285714285714\n",
      "Training for 0.001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 20.158730158730158\n",
      "Iteration: 1\n",
      "Train accuracy: 16.414285714285715\n",
      "Val accuracy: 15.814285714285713\n",
      "Iter 1 -> sub iter 99 : 30.158730158730158\n",
      "Iteration: 2\n",
      "Train accuracy: 27.836507936507935\n",
      "Val accuracy: 27.15714285714286\n",
      "Iter 2 -> sub iter 99 : 37.936507936507946\n",
      "Iteration: 3\n",
      "Train accuracy: 36.93015873015873\n",
      "Val accuracy: 36.15714285714286\n",
      "Iter 3 -> sub iter 99 : 44.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 42.15238095238095\n",
      "Val accuracy: 41.27142857142857\n",
      "Iter 4 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 5\n",
      "Train accuracy: 46.84285714285714\n",
      "Val accuracy: 45.92857142857143\n",
      "Iter 5 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 6\n",
      "Train accuracy: 51.34603174603175\n",
      "Val accuracy: 50.55714285714286\n",
      "Iter 6 -> sub iter 99 : 54.603174603174605\n",
      "Iteration: 7\n",
      "Train accuracy: 54.85079365079365\n",
      "Val accuracy: 53.51428571428571\n",
      "Iter 7 -> sub iter 99 : 56.825396825396824\n",
      "Iteration: 8\n",
      "Train accuracy: 57.369841269841274\n",
      "Val accuracy: 56.07142857142857\n",
      "Iter 8 -> sub iter 99 : 58.253968253968264\n",
      "Iteration: 9\n",
      "Train accuracy: 59.23492063492064\n",
      "Val accuracy: 57.8\n",
      "Iter 9 -> sub iter 99 : 59.841269841269844\n",
      "Iteration: 10\n",
      "Train accuracy: 60.663492063492065\n",
      "Val accuracy: 59.5\n",
      "Iter 10 -> sub iter 99 : 60.476190476190474\n",
      "Iteration: 11\n",
      "Train accuracy: 61.850793650793655\n",
      "Val accuracy: 60.785714285714285\n",
      "Iter 11 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 12\n",
      "Train accuracy: 62.790476190476184\n",
      "Val accuracy: 62.02857142857143\n",
      "Iter 12 -> sub iter 99 : 62.539682539682545\n",
      "Iteration: 13\n",
      "Train accuracy: 63.549206349206344\n",
      "Val accuracy: 62.94285714285714\n",
      "Iter 13 -> sub iter 99 : 63.492063492063495\n",
      "Iteration: 14\n",
      "Train accuracy: 64.18253968253968\n",
      "Val accuracy: 63.67142857142857\n",
      "Iter 14 -> sub iter 99 : 63.809523809523835\n",
      "Iteration: 15\n",
      "Train accuracy: 64.75555555555556\n",
      "Val accuracy: 64.24285714285715\n",
      "Iter 15 -> sub iter 99 : 64.603174603174615\n",
      "Iteration: 16\n",
      "Train accuracy: 65.28412698412698\n",
      "Val accuracy: 64.85714285714286\n",
      "Iter 16 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 17\n",
      "Train accuracy: 65.6952380952381\n",
      "Val accuracy: 65.18571428571428\n",
      "Iter 17 -> sub iter 99 : 66.031746031746024\n",
      "Iteration: 18\n",
      "Train accuracy: 66.10317460317461\n",
      "Val accuracy: 65.84285714285714\n",
      "Iter 18 -> sub iter 99 : 66.190476190476195\n",
      "Iteration: 19\n",
      "Train accuracy: 66.45555555555556\n",
      "Val accuracy: 66.17142857142856\n",
      "Iter 19 -> sub iter 99 : 66.507936507936514\n",
      "Iteration: 20\n",
      "Train accuracy: 66.79047619047618\n",
      "Val accuracy: 66.57142857142857\n",
      "Iter 20 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 21\n",
      "Train accuracy: 67.14126984126985\n",
      "Val accuracy: 66.8\n",
      "Iter 21 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 22\n",
      "Train accuracy: 67.43492063492063\n",
      "Val accuracy: 67.0\n",
      "Iter 22 -> sub iter 99 : 67.142857142857146\n",
      "Iteration: 23\n",
      "Train accuracy: 67.76507936507936\n",
      "Val accuracy: 67.31428571428572\n",
      "Iter 23 -> sub iter 99 : 67.460317460317474\n",
      "Iteration: 24\n",
      "Train accuracy: 68.03968253968254\n",
      "Val accuracy: 67.52857142857142\n",
      "Iter 24 -> sub iter 99 : 68.09523809523813\n",
      "Iteration: 25\n",
      "Train accuracy: 68.27619047619048\n",
      "Val accuracy: 67.92857142857143\n",
      "Iter 25 -> sub iter 99 : 68.25396825396825\n",
      "Iteration: 26\n",
      "Train accuracy: 68.53174603174604\n",
      "Val accuracy: 68.14285714285714\n",
      "Iter 26 -> sub iter 99 : 68.73015873015873\n",
      "Iteration: 27\n",
      "Train accuracy: 68.77301587301588\n",
      "Val accuracy: 68.27142857142857\n",
      "Iter 27 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 28\n",
      "Train accuracy: 68.98412698412699\n",
      "Val accuracy: 68.51428571428572\n",
      "Iter 28 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 29\n",
      "Train accuracy: 69.19047619047619\n",
      "Val accuracy: 68.77142857142857\n",
      "Iter 29 -> sub iter 99 : 69.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 69.39047619047619\n",
      "Val accuracy: 68.94285714285714\n",
      "Iter 30 -> sub iter 99 : 69.52380952380952\n",
      "Iteration: 31\n",
      "Train accuracy: 69.55238095238096\n",
      "Val accuracy: 69.07142857142857\n",
      "Iter 31 -> sub iter 99 : 70.15873015873015\n",
      "Iteration: 32\n",
      "Train accuracy: 69.72857142857143\n",
      "Val accuracy: 69.22857142857143\n",
      "Iter 32 -> sub iter 99 : 70.31746031746032\n",
      "Iteration: 33\n",
      "Train accuracy: 69.92063492063491\n",
      "Val accuracy: 69.32857142857142\n",
      "Iter 33 -> sub iter 99 : 70.47619047619048\n",
      "Iteration: 34\n",
      "Train accuracy: 70.0904761904762\n",
      "Val accuracy: 69.44285714285714\n",
      "Iter 34 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 35\n",
      "Train accuracy: 70.22380952380952\n",
      "Val accuracy: 69.69999999999999\n",
      "Iter 35 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 36\n",
      "Train accuracy: 70.36984126984127\n",
      "Val accuracy: 69.88571428571429\n",
      "Iter 36 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 37\n",
      "Train accuracy: 70.5\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 37 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 38\n",
      "Train accuracy: 70.63650793650794\n",
      "Val accuracy: 70.02857142857142\n",
      "Iter 38 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 39\n",
      "Train accuracy: 70.74126984126984\n",
      "Val accuracy: 70.11428571428571\n",
      "Iter 39 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 40\n",
      "Train accuracy: 70.84920634920636\n",
      "Val accuracy: 70.15714285714286\n",
      "Iter 40 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 41\n",
      "Train accuracy: 70.92539682539683\n",
      "Val accuracy: 70.3\n",
      "Iter 41 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 42\n",
      "Train accuracy: 71.02539682539683\n",
      "Val accuracy: 70.34285714285714\n",
      "Iter 42 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 43\n",
      "Train accuracy: 71.13650793650793\n",
      "Val accuracy: 70.39999999999999\n",
      "Iter 43 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 44\n",
      "Train accuracy: 71.22222222222221\n",
      "Val accuracy: 70.48571428571428\n",
      "Iter 44 -> sub iter 99 : 71.26984126984127\n",
      "Iteration: 45\n",
      "Train accuracy: 71.28730158730158\n",
      "Val accuracy: 70.55714285714285\n",
      "Iter 45 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 46\n",
      "Train accuracy: 71.36984126984127\n",
      "Val accuracy: 70.71428571428572\n",
      "Iter 46 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 47\n",
      "Train accuracy: 71.44603174603175\n",
      "Val accuracy: 70.84285714285714\n",
      "Iter 47 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 48\n",
      "Train accuracy: 71.5047619047619\n",
      "Val accuracy: 70.94285714285714\n",
      "Iter 48 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 49\n",
      "Train accuracy: 71.56825396825397\n",
      "Val accuracy: 70.98571428571428\n",
      "Iter 49 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 50\n",
      "Train accuracy: 71.615873015873\n",
      "Val accuracy: 71.02857142857142\n",
      "Iter 50 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 51\n",
      "Train accuracy: 71.67301587301587\n",
      "Val accuracy: 71.11428571428571\n",
      "Iter 51 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 52\n",
      "Train accuracy: 71.73650793650793\n",
      "Val accuracy: 71.21428571428572\n",
      "Iter 52 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 53\n",
      "Train accuracy: 71.8047619047619\n",
      "Val accuracy: 71.25714285714285\n",
      "Iter 53 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 54\n",
      "Train accuracy: 71.88253968253969\n",
      "Val accuracy: 71.28571428571429\n",
      "Iter 54 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 55\n",
      "Train accuracy: 71.96984126984127\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 55 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 72.04126984126984\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 56 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 57\n",
      "Train accuracy: 72.0920634920635\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 57 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 58\n",
      "Train accuracy: 72.16984126984127\n",
      "Val accuracy: 71.41428571428573\n",
      "Iter 58 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 59\n",
      "Train accuracy: 72.22698412698414\n",
      "Val accuracy: 71.45714285714286\n",
      "Iter 59 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 60\n",
      "Train accuracy: 72.27936507936508\n",
      "Val accuracy: 71.55714285714285\n",
      "Iter 60 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 61\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 71.64285714285714\n",
      "Iter 61 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 62\n",
      "Train accuracy: 72.38888888888889\n",
      "Val accuracy: 71.68571428571428\n",
      "Iter 62 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 63\n",
      "Train accuracy: 72.45238095238096\n",
      "Val accuracy: 71.7\n",
      "Iter 63 -> sub iter 99 : 72.69841269841276\n",
      "Iteration: 64\n",
      "Train accuracy: 72.52222222222223\n",
      "Val accuracy: 71.75714285714285\n",
      "Iter 64 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 65\n",
      "Train accuracy: 72.59523809523809\n",
      "Val accuracy: 71.77142857142857\n",
      "Iter 65 -> sub iter 99 : 72.69841269841278\n",
      "Iteration: 66\n",
      "Train accuracy: 72.65079365079366\n",
      "Val accuracy: 71.82857142857144\n",
      "Iter 66 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 67\n",
      "Train accuracy: 72.73650793650793\n",
      "Val accuracy: 71.97142857142858\n",
      "Iter 67 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 68\n",
      "Train accuracy: 73.05396825396825\n",
      "Val accuracy: 72.27142857142857\n",
      "Iter 68 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 69\n",
      "Train accuracy: 74.32857142857144\n",
      "Val accuracy: 73.97142857142858\n",
      "Iter 69 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 70\n",
      "Train accuracy: 76.29206349206349\n",
      "Val accuracy: 75.61428571428571\n",
      "Iter 70 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 71\n",
      "Train accuracy: 77.46507936507938\n",
      "Val accuracy: 76.81428571428572\n",
      "Iter 71 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 72\n",
      "Train accuracy: 78.24126984126984\n",
      "Val accuracy: 77.54285714285714\n",
      "Iter 72 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 73\n",
      "Train accuracy: 78.8\n",
      "Val accuracy: 77.95714285714286\n",
      "Iter 73 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 74\n",
      "Train accuracy: 79.25555555555556\n",
      "Val accuracy: 78.38571428571429\n",
      "Iter 74 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 75\n",
      "Train accuracy: 79.56031746031746\n",
      "Val accuracy: 78.7\n",
      "Iter 75 -> sub iter 99 : 80.95238095238095\n",
      "Iteration: 76\n",
      "Train accuracy: 79.81428571428572\n",
      "Val accuracy: 78.95714285714286\n",
      "Iter 76 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 77\n",
      "Train accuracy: 79.98571428571428\n",
      "Val accuracy: 79.11428571428571\n",
      "Iter 77 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 78\n",
      "Train accuracy: 80.18571428571428\n",
      "Val accuracy: 79.34285714285714\n",
      "Iter 78 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 79\n",
      "Train accuracy: 80.33015873015873\n",
      "Val accuracy: 79.48571428571428\n",
      "Iter 79 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 80\n",
      "Train accuracy: 80.43492063492064\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 80 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 81\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 79.72857142857143\n",
      "Iter 81 -> sub iter 99 : 81.58730158730158\n",
      "Iteration: 82\n",
      "Train accuracy: 80.71428571428572\n",
      "Val accuracy: 79.82857142857142\n",
      "Iter 82 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 83\n",
      "Train accuracy: 80.80952380952381\n",
      "Val accuracy: 79.97142857142858\n",
      "Iter 83 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 84\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.07142857142857\n",
      "Iter 84 -> sub iter 99 : 81.90476190476199\n",
      "Iteration: 85\n",
      "Train accuracy: 81.0047619047619\n",
      "Val accuracy: 80.17142857142858\n",
      "Iter 85 -> sub iter 99 : 81.90476190476194\n",
      "Iteration: 86\n",
      "Train accuracy: 81.10952380952381\n",
      "Val accuracy: 80.30000000000001\n",
      "Iter 86 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 87\n",
      "Train accuracy: 81.17460317460318\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 87 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 88\n",
      "Train accuracy: 81.26031746031745\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 81.33968253968254\n",
      "Val accuracy: 80.47142857142858\n",
      "Iter 89 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 90\n",
      "Train accuracy: 81.41428571428571\n",
      "Val accuracy: 80.51428571428572\n",
      "Iter 90 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 91\n",
      "Train accuracy: 81.48571428571428\n",
      "Val accuracy: 80.54285714285714\n",
      "Iter 91 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 92\n",
      "Train accuracy: 81.54444444444444\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 92 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 93\n",
      "Train accuracy: 81.6047619047619\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 93 -> sub iter 99 : 82.69841269841272\n",
      "Iteration: 94\n",
      "Train accuracy: 81.66984126984127\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 94 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 95\n",
      "Train accuracy: 81.72539682539683\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 81.78730158730158\n",
      "Val accuracy: 80.77142857142857\n",
      "Iter 96 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 97\n",
      "Train accuracy: 81.83809523809525\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 97 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 98\n",
      "Train accuracy: 81.87619047619049\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 98 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 99\n",
      "Train accuracy: 81.93333333333334\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 99 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 100\n",
      "Train accuracy: 81.99047619047619\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.02063492063492\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.05873015873016\n",
      "Val accuracy: 81.0\n",
      "Iter 102 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 103\n",
      "Train accuracy: 82.11746031746033\n",
      "Val accuracy: 81.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 104\n",
      "Train accuracy: 82.15714285714286\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 104 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 105\n",
      "Train accuracy: 82.1968253968254\n",
      "Val accuracy: 81.11428571428571\n",
      "Iter 105 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 106\n",
      "Train accuracy: 82.24285714285713\n",
      "Val accuracy: 81.15714285714286\n",
      "Iter 106 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 107\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 107 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 108\n",
      "Train accuracy: 82.32857142857142\n",
      "Val accuracy: 81.17142857142858\n",
      "Iter 108 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 109\n",
      "Train accuracy: 82.37619047619049\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 109 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 110\n",
      "Train accuracy: 82.41746031746032\n",
      "Val accuracy: 81.25714285714287\n",
      "Iter 110 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 111\n",
      "Train accuracy: 82.46190476190476\n",
      "Val accuracy: 81.28571428571428\n",
      "Iter 111 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 112\n",
      "Train accuracy: 82.4952380952381\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 112 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 113\n",
      "Train accuracy: 82.53015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 113 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 114\n",
      "Train accuracy: 82.55396825396826\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 114 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 115\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 115 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 116\n",
      "Train accuracy: 82.63174603174603\n",
      "Val accuracy: 81.39999999999999\n",
      "Iter 116 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 117\n",
      "Train accuracy: 82.67142857142858\n",
      "Val accuracy: 81.42857142857143\n",
      "Iter 117 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 118\n",
      "Train accuracy: 82.7031746031746\n",
      "Val accuracy: 81.47142857142858\n",
      "Iter 118 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 119\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 119 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 120\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 120 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 121\n",
      "Train accuracy: 82.77460317460317\n",
      "Val accuracy: 81.57142857142857\n",
      "Iter 121 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 122\n",
      "Train accuracy: 82.79841269841269\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 122 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 123\n",
      "Train accuracy: 82.82380952380952\n",
      "Val accuracy: 81.6\n",
      "Iter 123 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 124\n",
      "Train accuracy: 82.86825396825397\n",
      "Val accuracy: 81.61428571428571\n",
      "Iter 124 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 125\n",
      "Train accuracy: 82.88412698412698\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 125 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 126\n",
      "Train accuracy: 82.91111111111111\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 126 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 127\n",
      "Train accuracy: 82.93174603174603\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 127 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 128\n",
      "Train accuracy: 82.94920634920635\n",
      "Val accuracy: 81.65714285714286\n",
      "Iter 128 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 129\n",
      "Train accuracy: 82.98253968253968\n",
      "Val accuracy: 81.68571428571428\n",
      "Iter 129 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 130\n",
      "Train accuracy: 83.01746031746032\n",
      "Val accuracy: 81.67142857142858\n",
      "Iter 130 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 131\n",
      "Train accuracy: 83.03968253968253\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 131 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 132\n",
      "Train accuracy: 83.06825396825397\n",
      "Val accuracy: 81.72857142857143\n",
      "Iter 132 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 133\n",
      "Train accuracy: 83.0952380952381\n",
      "Val accuracy: 81.75714285714287\n",
      "Iter 133 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 134\n",
      "Train accuracy: 83.12698412698413\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 134 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 135\n",
      "Train accuracy: 83.15714285714286\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 135 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 136\n",
      "Train accuracy: 83.18412698412698\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 136 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 137\n",
      "Train accuracy: 83.1984126984127\n",
      "Val accuracy: 81.82857142857142\n",
      "Iter 137 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 138\n",
      "Train accuracy: 83.21904761904761\n",
      "Val accuracy: 81.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 139\n",
      "Train accuracy: 83.23968253968253\n",
      "Val accuracy: 81.92857142857143\n",
      "Iter 139 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 140\n",
      "Train accuracy: 83.27460317460319\n",
      "Val accuracy: 81.97142857142858\n",
      "Iter 140 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 141\n",
      "Train accuracy: 83.3015873015873\n",
      "Val accuracy: 82.04285714285714\n",
      "Iter 141 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 142\n",
      "Train accuracy: 83.32698412698413\n",
      "Val accuracy: 82.07142857142857\n",
      "Iter 142 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 143\n",
      "Train accuracy: 83.34444444444445\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 143 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 144\n",
      "Train accuracy: 83.36031746031746\n",
      "Val accuracy: 82.17142857142858\n",
      "Iter 144 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 145\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 145 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 146\n",
      "Train accuracy: 83.40793650793651\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 146 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 147\n",
      "Train accuracy: 83.43015873015874\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 147 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 148\n",
      "Train accuracy: 83.45555555555556\n",
      "Val accuracy: 82.22857142857143\n",
      "Iter 148 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 149\n",
      "Train accuracy: 83.47777777777777\n",
      "Val accuracy: 82.25714285714287\n",
      "Iter 149 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 150\n",
      "Train accuracy: 83.4920634920635\n",
      "Val accuracy: 82.27142857142857\n",
      "Training for 0.0001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 21.904761904761905\n",
      "Iteration: 1\n",
      "Train accuracy: 22.957142857142856\n",
      "Val accuracy: 24.22857142857143\n",
      "Iter 1 -> sub iter 99 : 27.777777777777785\n",
      "Iteration: 2\n",
      "Train accuracy: 29.887301587301586\n",
      "Val accuracy: 31.157142857142855\n",
      "Iter 2 -> sub iter 99 : 31.904761904761975\n",
      "Iteration: 3\n",
      "Train accuracy: 34.33968253968254\n",
      "Val accuracy: 35.65714285714286\n",
      "Iter 3 -> sub iter 99 : 38.571428571428584\n",
      "Iteration: 4\n",
      "Train accuracy: 39.353968253968254\n",
      "Val accuracy: 40.87142857142857\n",
      "Iter 4 -> sub iter 99 : 41.111111111111116\n",
      "Iteration: 5\n",
      "Train accuracy: 42.35714285714286\n",
      "Val accuracy: 43.9\n",
      "Iter 5 -> sub iter 99 : 42.539682539682546\n",
      "Iteration: 6\n",
      "Train accuracy: 44.34603174603174\n",
      "Val accuracy: 46.1\n",
      "Iter 6 -> sub iter 99 : 44.603174603174614\n",
      "Iteration: 7\n",
      "Train accuracy: 45.76190476190476\n",
      "Val accuracy: 47.599999999999994\n",
      "Iter 7 -> sub iter 99 : 46.031746031746034\n",
      "Iteration: 8\n",
      "Train accuracy: 46.68571428571428\n",
      "Val accuracy: 48.22857142857143\n",
      "Iter 8 -> sub iter 99 : 46.825396825396824\n",
      "Iteration: 9\n",
      "Train accuracy: 47.41746031746032\n",
      "Val accuracy: 48.871428571428574\n",
      "Iter 9 -> sub iter 99 : 48.095238095238095\n",
      "Iteration: 10\n",
      "Train accuracy: 48.03809523809524\n",
      "Val accuracy: 49.542857142857144\n",
      "Iter 10 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 11\n",
      "Train accuracy: 48.53174603174603\n",
      "Val accuracy: 49.81428571428572\n",
      "Iter 11 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 12\n",
      "Train accuracy: 48.94761904761904\n",
      "Val accuracy: 50.24285714285715\n",
      "Iter 12 -> sub iter 99 : 48.571428571428575\n",
      "Iteration: 13\n",
      "Train accuracy: 49.32857142857143\n",
      "Val accuracy: 50.5\n",
      "Iter 13 -> sub iter 99 : 49.206349206349296\n",
      "Iteration: 14\n",
      "Train accuracy: 49.71111111111111\n",
      "Val accuracy: 50.91428571428571\n",
      "Iter 14 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 15\n",
      "Train accuracy: 50.00476190476191\n",
      "Val accuracy: 51.28571428571429\n",
      "Iter 15 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 16\n",
      "Train accuracy: 50.3111111111111\n",
      "Val accuracy: 51.4\n",
      "Iter 16 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 17\n",
      "Train accuracy: 50.56349206349206\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 17 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 18\n",
      "Train accuracy: 50.7984126984127\n",
      "Val accuracy: 52.028571428571425\n",
      "Iter 18 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 19\n",
      "Train accuracy: 51.01269841269841\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 19 -> sub iter 99 : 50.158730158730165\n",
      "Iteration: 20\n",
      "Train accuracy: 51.217460317460315\n",
      "Val accuracy: 52.51428571428571\n",
      "Iter 20 -> sub iter 99 : 50.158730158730164\n",
      "Iteration: 21\n",
      "Train accuracy: 51.42539682539683\n",
      "Val accuracy: 52.65714285714286\n",
      "Iter 21 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 22\n",
      "Train accuracy: 51.65238095238095\n",
      "Val accuracy: 52.87142857142857\n",
      "Iter 22 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 23\n",
      "Train accuracy: 51.866666666666674\n",
      "Val accuracy: 53.128571428571426\n",
      "Iter 23 -> sub iter 99 : 50.952380952380956\n",
      "Iteration: 24\n",
      "Train accuracy: 52.05714285714286\n",
      "Val accuracy: 53.38571428571428\n",
      "Iter 24 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 25\n",
      "Train accuracy: 52.23174603174603\n",
      "Val accuracy: 53.57142857142857\n",
      "Iter 25 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 26\n",
      "Train accuracy: 52.38095238095239\n",
      "Val accuracy: 53.7\n",
      "Iter 26 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 27\n",
      "Train accuracy: 52.549206349206344\n",
      "Val accuracy: 53.800000000000004\n",
      "Iter 27 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 28\n",
      "Train accuracy: 52.7079365079365\n",
      "Val accuracy: 54.0\n",
      "Iter 28 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 29\n",
      "Train accuracy: 52.84920634920635\n",
      "Val accuracy: 54.142857142857146\n",
      "Iter 29 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 30\n",
      "Train accuracy: 52.957142857142856\n",
      "Val accuracy: 54.27142857142857\n",
      "Iter 30 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 31\n",
      "Train accuracy: 53.07777777777778\n",
      "Val accuracy: 54.371428571428574\n",
      "Iter 31 -> sub iter 99 : 52.222222222222234\n",
      "Iteration: 32\n",
      "Train accuracy: 53.20952380952381\n",
      "Val accuracy: 54.51428571428571\n",
      "Iter 32 -> sub iter 99 : 52.222222222222235\n",
      "Iteration: 33\n",
      "Train accuracy: 53.31428571428572\n",
      "Val accuracy: 54.65714285714286\n",
      "Iter 33 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 34\n",
      "Train accuracy: 53.42857142857142\n",
      "Val accuracy: 54.75714285714286\n",
      "Iter 34 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 35\n",
      "Train accuracy: 53.51904761904762\n",
      "Val accuracy: 54.95714285714286\n",
      "Iter 35 -> sub iter 99 : 52.698412698412785\n",
      "Iteration: 36\n",
      "Train accuracy: 53.642857142857146\n",
      "Val accuracy: 55.01428571428571\n",
      "Iter 36 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 37\n",
      "Train accuracy: 53.73015873015873\n",
      "Val accuracy: 55.08571428571428\n",
      "Iter 37 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 38\n",
      "Train accuracy: 53.78571428571428\n",
      "Val accuracy: 55.22857142857143\n",
      "Iter 38 -> sub iter 99 : 52.857142857142865\n",
      "Iteration: 39\n",
      "Train accuracy: 53.86825396825397\n",
      "Val accuracy: 55.371428571428574\n",
      "Iter 39 -> sub iter 99 : 52.857142857142866\n",
      "Iteration: 40\n",
      "Train accuracy: 53.955555555555556\n",
      "Val accuracy: 55.385714285714286\n",
      "Iter 40 -> sub iter 99 : 53.174603174603184\n",
      "Iteration: 41\n",
      "Train accuracy: 54.03174603174603\n",
      "Val accuracy: 55.471428571428575\n",
      "Iter 41 -> sub iter 99 : 53.174603174603186\n",
      "Iteration: 42\n",
      "Train accuracy: 54.1\n",
      "Val accuracy: 55.58571428571428\n",
      "Iter 42 -> sub iter 99 : 53.492063492063494\n",
      "Iteration: 43\n",
      "Train accuracy: 54.179365079365084\n",
      "Val accuracy: 55.68571428571428\n",
      "Iter 43 -> sub iter 99 : 53.650793650793654\n",
      "Iteration: 44\n",
      "Train accuracy: 54.250793650793646\n",
      "Val accuracy: 55.74285714285714\n",
      "Iter 44 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 45\n",
      "Train accuracy: 54.31587301587302\n",
      "Val accuracy: 55.84285714285714\n",
      "Iter 45 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 46\n",
      "Train accuracy: 54.37777777777778\n",
      "Val accuracy: 55.871428571428574\n",
      "Iter 46 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 47\n",
      "Train accuracy: 54.44126984126984\n",
      "Val accuracy: 55.91428571428572\n",
      "Iter 47 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 48\n",
      "Train accuracy: 54.50000000000001\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 48 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 49\n",
      "Train accuracy: 54.541269841269845\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 49 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 50\n",
      "Train accuracy: 54.6063492063492\n",
      "Val accuracy: 56.128571428571426\n",
      "Iter 50 -> sub iter 99 : 53.968253968253976\n",
      "Iteration: 51\n",
      "Train accuracy: 54.646031746031746\n",
      "Val accuracy: 56.15714285714286\n",
      "Iter 51 -> sub iter 99 : 54.126984126984136\n",
      "Iteration: 52\n",
      "Train accuracy: 54.68730158730158\n",
      "Val accuracy: 56.214285714285715\n",
      "Iter 52 -> sub iter 99 : 54.285714285714285\n",
      "Iteration: 53\n",
      "Train accuracy: 54.72380952380952\n",
      "Val accuracy: 56.27142857142857\n",
      "Iter 53 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 54\n",
      "Train accuracy: 54.75714285714286\n",
      "Val accuracy: 56.34285714285714\n",
      "Iter 54 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 55\n",
      "Train accuracy: 54.801587301587304\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 55 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 56\n",
      "Train accuracy: 54.82857142857143\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 56 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 57\n",
      "Train accuracy: 54.87301587301587\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 57 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 58\n",
      "Train accuracy: 54.919047619047625\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 58 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 59\n",
      "Train accuracy: 54.96507936507936\n",
      "Val accuracy: 56.442857142857136\n",
      "Iter 59 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 60\n",
      "Train accuracy: 55.00158730158731\n",
      "Val accuracy: 56.49999999999999\n",
      "Iter 60 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 61\n",
      "Train accuracy: 55.05238095238095\n",
      "Val accuracy: 56.51428571428572\n",
      "Iter 61 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 62\n",
      "Train accuracy: 55.093650793650795\n",
      "Val accuracy: 56.528571428571425\n",
      "Iter 62 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 63\n",
      "Train accuracy: 55.141269841269846\n",
      "Val accuracy: 56.557142857142864\n",
      "Iter 63 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 64\n",
      "Train accuracy: 55.18253968253968\n",
      "Val accuracy: 56.58571428571428\n",
      "Iter 64 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 65\n",
      "Train accuracy: 55.21904761904762\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 65 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 66\n",
      "Train accuracy: 55.24444444444444\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 66 -> sub iter 99 : 54.920634920634924\n",
      "Iteration: 67\n",
      "Train accuracy: 55.269841269841265\n",
      "Val accuracy: 56.67142857142857\n",
      "Iter 67 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 68\n",
      "Train accuracy: 55.304761904761904\n",
      "Val accuracy: 56.714285714285715\n",
      "Iter 68 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 69\n",
      "Train accuracy: 55.33968253968254\n",
      "Val accuracy: 56.74285714285714\n",
      "Iter 69 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 70\n",
      "Train accuracy: 55.37460317460317\n",
      "Val accuracy: 56.81428571428572\n",
      "Iter 70 -> sub iter 99 : 55.238095238095244\n",
      "Iteration: 71\n",
      "Train accuracy: 55.4031746031746\n",
      "Val accuracy: 56.84285714285714\n",
      "Iter 71 -> sub iter 99 : 55.396825396825435\n",
      "Iteration: 72\n",
      "Train accuracy: 55.43492063492064\n",
      "Val accuracy: 56.871428571428574\n",
      "Iter 72 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 73\n",
      "Train accuracy: 55.474603174603175\n",
      "Val accuracy: 56.89999999999999\n",
      "Iter 73 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 74\n",
      "Train accuracy: 55.4952380952381\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 74 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 75\n",
      "Train accuracy: 55.52222222222222\n",
      "Val accuracy: 56.92857142857143\n",
      "Iter 75 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 76\n",
      "Train accuracy: 55.55238095238095\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 76 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 77\n",
      "Train accuracy: 55.574603174603176\n",
      "Val accuracy: 56.98571428571428\n",
      "Iter 77 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 78\n",
      "Train accuracy: 55.6063492063492\n",
      "Val accuracy: 56.99999999999999\n",
      "Iter 78 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 79\n",
      "Train accuracy: 55.62222222222222\n",
      "Val accuracy: 57.028571428571425\n",
      "Iter 79 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 80\n",
      "Train accuracy: 55.63809523809524\n",
      "Val accuracy: 57.04285714285714\n",
      "Iter 80 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 81\n",
      "Train accuracy: 55.65714285714286\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 81 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 82\n",
      "Train accuracy: 55.68730158730158\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 82 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 83\n",
      "Train accuracy: 55.70952380952381\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 83 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 84\n",
      "Train accuracy: 55.72380952380952\n",
      "Val accuracy: 57.15714285714286\n",
      "Iter 84 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 85\n",
      "Train accuracy: 55.73968253968255\n",
      "Val accuracy: 57.17142857142857\n",
      "Iter 85 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 86\n",
      "Train accuracy: 55.76190476190476\n",
      "Val accuracy: 57.18571428571428\n",
      "Iter 86 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 87\n",
      "Train accuracy: 55.78412698412698\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 87 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 88\n",
      "Train accuracy: 55.7952380952381\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 88 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 89\n",
      "Train accuracy: 55.80952380952381\n",
      "Val accuracy: 57.24285714285714\n",
      "Iter 89 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 90\n",
      "Train accuracy: 55.822222222222216\n",
      "Val accuracy: 57.25714285714286\n",
      "Iter 90 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 91\n",
      "Train accuracy: 55.83968253968254\n",
      "Val accuracy: 57.27142857142857\n",
      "Iter 91 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 92\n",
      "Train accuracy: 55.85555555555556\n",
      "Val accuracy: 57.285714285714285\n",
      "Iter 92 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 93\n",
      "Train accuracy: 55.87777777777778\n",
      "Val accuracy: 57.3\n",
      "Iter 93 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 94\n",
      "Train accuracy: 55.888888888888886\n",
      "Val accuracy: 57.3\n",
      "Iter 94 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 95\n",
      "Train accuracy: 55.907936507936505\n",
      "Val accuracy: 57.32857142857143\n",
      "Iter 95 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 96\n",
      "Train accuracy: 55.919047619047625\n",
      "Val accuracy: 57.371428571428574\n",
      "Iter 96 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 97\n",
      "Train accuracy: 55.93015873015873\n",
      "Val accuracy: 57.385714285714286\n",
      "Iter 97 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 98\n",
      "Train accuracy: 55.941269841269836\n",
      "Val accuracy: 57.44285714285714\n",
      "Iter 98 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 99\n",
      "Train accuracy: 55.94920634920635\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 99 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 100\n",
      "Train accuracy: 55.96666666666666\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 100 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 101\n",
      "Train accuracy: 55.98888888888889\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 101 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 102\n",
      "Train accuracy: 56.007936507936506\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 102 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 103\n",
      "Train accuracy: 56.019047619047626\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 103 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 104\n",
      "Train accuracy: 56.03174603174603\n",
      "Val accuracy: 57.49999999999999\n",
      "Iter 104 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 105\n",
      "Train accuracy: 56.046031746031744\n",
      "Val accuracy: 57.51428571428572\n",
      "Iter 105 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 106\n",
      "Train accuracy: 56.06666666666666\n",
      "Val accuracy: 57.54285714285714\n",
      "Iter 106 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 107\n",
      "Train accuracy: 56.092063492063495\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 107 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 108\n",
      "Train accuracy: 56.1031746031746\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 108 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 109\n",
      "Train accuracy: 56.12380952380952\n",
      "Val accuracy: 57.57142857142858\n",
      "Iter 109 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 110\n",
      "Train accuracy: 56.14285714285714\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 110 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 111\n",
      "Train accuracy: 56.15555555555556\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 111 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 112\n",
      "Train accuracy: 56.18253968253968\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 112 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 113\n",
      "Train accuracy: 56.19047619047619\n",
      "Val accuracy: 57.61428571428572\n",
      "Iter 113 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 114\n",
      "Train accuracy: 56.2047619047619\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 114 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 115\n",
      "Train accuracy: 56.21587301587302\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 115 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 116\n",
      "Train accuracy: 56.23650793650794\n",
      "Val accuracy: 57.657142857142865\n",
      "Iter 116 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 117\n",
      "Train accuracy: 56.25238095238095\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 117 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 118\n",
      "Train accuracy: 56.268253968253966\n",
      "Val accuracy: 57.68571428571428\n",
      "Iter 118 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 119\n",
      "Train accuracy: 56.29206349206349\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 119 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 120\n",
      "Train accuracy: 56.3031746031746\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 120 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 121\n",
      "Train accuracy: 56.31587301587302\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 121 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 122\n",
      "Train accuracy: 56.33015873015873\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 122 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 123\n",
      "Train accuracy: 56.34603174603174\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 123 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 124\n",
      "Train accuracy: 56.353968253968254\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 124 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 125\n",
      "Train accuracy: 56.35714285714286\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 125 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 126\n",
      "Train accuracy: 56.36190476190476\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 126 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 127\n",
      "Train accuracy: 56.371428571428574\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 127 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 128\n",
      "Train accuracy: 56.38253968253968\n",
      "Val accuracy: 57.74285714285714\n",
      "Iter 128 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 129\n",
      "Train accuracy: 56.3968253968254\n",
      "Val accuracy: 57.77142857142857\n",
      "Iter 129 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 130\n",
      "Train accuracy: 56.4015873015873\n",
      "Val accuracy: 57.785714285714285\n",
      "Iter 130 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 131\n",
      "Train accuracy: 56.41111111111111\n",
      "Val accuracy: 57.8\n",
      "Iter 131 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 132\n",
      "Train accuracy: 56.423809523809524\n",
      "Val accuracy: 57.8\n",
      "Iter 132 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 133\n",
      "Train accuracy: 56.426984126984124\n",
      "Val accuracy: 57.8\n",
      "Iter 133 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 134\n",
      "Train accuracy: 56.442857142857136\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 134 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 135\n",
      "Train accuracy: 56.44603174603174\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 135 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 136\n",
      "Train accuracy: 56.45873015873016\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 136 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 137\n",
      "Train accuracy: 56.461904761904755\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 137 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 138\n",
      "Train accuracy: 56.46984126984127\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 138 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 139\n",
      "Train accuracy: 56.48253968253968\n",
      "Val accuracy: 57.84285714285714\n",
      "Iter 139 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 140\n",
      "Train accuracy: 56.48412698412698\n",
      "Val accuracy: 57.871428571428574\n",
      "Iter 140 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 141\n",
      "Train accuracy: 56.48888888888889\n",
      "Val accuracy: 57.885714285714286\n",
      "Iter 141 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 142\n",
      "Train accuracy: 56.5031746031746\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 142 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 143\n",
      "Train accuracy: 56.50952380952381\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 143 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 144\n",
      "Train accuracy: 56.51587301587302\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 144 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 145\n",
      "Train accuracy: 56.52063492063492\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 145 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 146\n",
      "Train accuracy: 56.526984126984125\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 146 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 147\n",
      "Train accuracy: 56.528571428571425\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 147 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 148\n",
      "Train accuracy: 56.53650793650794\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 148 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 149\n",
      "Train accuracy: 56.54920634920635\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 149 -> sub iter 99 : 55.873015873015875\n",
      "Iteration: 150\n",
      "Train accuracy: 56.56031746031746\n",
      "Val accuracy: 57.92857142857143\n"
     ]
    }
   ],
   "source": [
    "for pert in pertList:\n",
    "    print(f\"Training for {pert}\")\n",
    "    W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.01, pert, 1)\n",
    "    trainAccPertList.append(train_acc)\n",
    "    valAccPertList.append(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 81.73650793650793\n",
      "Val accuracy: 81.02857142857142\n",
      "Iteration: 2\n",
      "Train accuracy: 86.43174603174603\n",
      "Val accuracy: 85.91428571428571\n",
      "Iteration: 3\n",
      "Train accuracy: 88.5984126984127\n",
      "Val accuracy: 88.18571428571428\n",
      "Iteration: 4\n",
      "Train accuracy: 89.78571428571429\n",
      "Val accuracy: 89.44285714285715\n",
      "Iteration: 5\n",
      "Train accuracy: 90.70158730158731\n",
      "Val accuracy: 90.21428571428571\n",
      "Iteration: 6\n",
      "Train accuracy: 91.39206349206349\n",
      "Val accuracy: 90.72857142857143\n",
      "Iteration: 7\n",
      "Train accuracy: 91.88730158730158\n",
      "Val accuracy: 91.15714285714286\n",
      "Iteration: 8\n",
      "Train accuracy: 92.33650793650794\n",
      "Val accuracy: 91.45714285714286\n",
      "Iteration: 9\n",
      "Train accuracy: 92.70793650793651\n",
      "Val accuracy: 91.75714285714285\n",
      "Iteration: 10\n",
      "Train accuracy: 93.04126984126984\n",
      "Val accuracy: 92.08571428571429\n",
      "Iteration: 11\n",
      "Train accuracy: 93.31746031746032\n",
      "Val accuracy: 92.41428571428571\n",
      "Iteration: 12\n",
      "Train accuracy: 93.57460317460318\n",
      "Val accuracy: 92.55714285714286\n",
      "Iteration: 13\n",
      "Train accuracy: 93.7984126984127\n",
      "Val accuracy: 92.84285714285714\n",
      "Iteration: 14\n",
      "Train accuracy: 94.04285714285714\n",
      "Val accuracy: 93.02857142857142\n",
      "Iteration: 15\n",
      "Train accuracy: 94.20952380952382\n",
      "Val accuracy: 93.10000000000001\n",
      "Iteration: 16\n",
      "Train accuracy: 94.3984126984127\n",
      "Val accuracy: 93.30000000000001\n",
      "Iteration: 17\n",
      "Train accuracy: 94.57619047619048\n",
      "Val accuracy: 93.5\n",
      "Iteration: 18\n",
      "Train accuracy: 94.70952380952382\n",
      "Val accuracy: 93.61428571428571\n",
      "Iteration: 19\n",
      "Train accuracy: 94.86507936507937\n",
      "Val accuracy: 93.71428571428572\n",
      "Iteration: 20\n",
      "Train accuracy: 95.01428571428572\n",
      "Val accuracy: 93.75714285714287\n",
      "Iteration: 21\n",
      "Train accuracy: 95.12063492063491\n",
      "Val accuracy: 93.92857142857143\n",
      "Iteration: 22\n",
      "Train accuracy: 95.23809523809523\n",
      "Val accuracy: 94.04285714285714\n",
      "Iteration: 23\n",
      "Train accuracy: 95.36190476190475\n",
      "Val accuracy: 94.18571428571428\n",
      "Iteration: 24\n",
      "Train accuracy: 95.46825396825398\n",
      "Val accuracy: 94.25714285714287\n",
      "Iteration: 25\n",
      "Train accuracy: 95.5936507936508\n",
      "Val accuracy: 94.32857142857142\n",
      "Iteration: 26\n",
      "Train accuracy: 95.72222222222221\n",
      "Val accuracy: 94.38571428571429\n",
      "Iteration: 27\n",
      "Train accuracy: 95.83015873015873\n",
      "Val accuracy: 94.45714285714286\n",
      "Iteration: 28\n",
      "Train accuracy: 95.93015873015874\n",
      "Val accuracy: 94.58571428571429\n",
      "Iteration: 29\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 30\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 31\n",
      "Train accuracy: 96.16031746031746\n",
      "Val accuracy: 94.65714285714286\n",
      "Iteration: 32\n",
      "Train accuracy: 96.23968253968253\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 33\n",
      "Train accuracy: 96.3031746031746\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 34\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 94.77142857142857\n",
      "Iteration: 35\n",
      "Train accuracy: 96.42380952380952\n",
      "Val accuracy: 94.8\n",
      "Iteration: 36\n",
      "Train accuracy: 96.48412698412699\n",
      "Val accuracy: 94.87142857142857\n",
      "Iteration: 37\n",
      "Train accuracy: 96.58412698412698\n",
      "Val accuracy: 94.85714285714286\n",
      "Iteration: 38\n",
      "Train accuracy: 96.65396825396826\n",
      "Val accuracy: 94.95714285714286\n",
      "Iteration: 39\n",
      "Train accuracy: 96.72222222222221\n",
      "Val accuracy: 94.94285714285714\n",
      "Iteration: 40\n",
      "Train accuracy: 96.75555555555555\n",
      "Val accuracy: 95.01428571428572\n",
      "Iteration: 41\n",
      "Train accuracy: 96.81904761904761\n",
      "Val accuracy: 95.07142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 96.88412698412698\n",
      "Val accuracy: 95.1\n",
      "Iteration: 43\n",
      "Train accuracy: 96.93650793650794\n",
      "Val accuracy: 95.11428571428571\n",
      "Iteration: 44\n",
      "Train accuracy: 97.0079365079365\n",
      "Val accuracy: 95.17142857142858\n",
      "Iteration: 45\n",
      "Train accuracy: 97.06190476190476\n",
      "Val accuracy: 95.15714285714286\n",
      "Iteration: 46\n",
      "Train accuracy: 97.12380952380953\n",
      "Val accuracy: 95.21428571428572\n",
      "Iteration: 47\n",
      "Train accuracy: 97.17619047619047\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 48\n",
      "Train accuracy: 97.22380952380952\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 49\n",
      "Train accuracy: 97.27460317460317\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 50\n",
      "Train accuracy: 97.31269841269842\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 51\n",
      "Train accuracy: 97.35079365079365\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 97.38095238095238\n",
      "Val accuracy: 95.3\n",
      "Iteration: 53\n",
      "Train accuracy: 97.42698412698412\n",
      "Val accuracy: 95.32857142857142\n",
      "Iteration: 54\n",
      "Train accuracy: 97.47142857142858\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 55\n",
      "Train accuracy: 97.5047619047619\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 56\n",
      "Train accuracy: 97.55079365079365\n",
      "Val accuracy: 95.31428571428572\n",
      "Iteration: 57\n",
      "Train accuracy: 97.6\n",
      "Val accuracy: 95.37142857142857\n",
      "Iteration: 58\n",
      "Train accuracy: 97.64761904761905\n",
      "Val accuracy: 95.38571428571429\n",
      "Iteration: 59\n",
      "Train accuracy: 97.68730158730159\n",
      "Val accuracy: 95.41428571428571\n",
      "Iteration: 60\n",
      "Train accuracy: 97.73174603174604\n",
      "Val accuracy: 95.42857142857143\n",
      "Iteration: 61\n",
      "Train accuracy: 97.76984126984128\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 62\n",
      "Train accuracy: 97.78730158730158\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 63\n",
      "Train accuracy: 97.82222222222222\n",
      "Val accuracy: 95.5\n",
      "Iteration: 64\n",
      "Train accuracy: 97.85238095238095\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 65\n",
      "Train accuracy: 97.87936507936507\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 66\n",
      "Train accuracy: 97.93015873015874\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 67\n",
      "Train accuracy: 97.96349206349207\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 68\n",
      "Train accuracy: 98.00634920634921\n",
      "Val accuracy: 95.48571428571428\n",
      "Iteration: 69\n",
      "Train accuracy: 98.03809523809524\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 70\n",
      "Train accuracy: 98.08571428571429\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 71\n",
      "Train accuracy: 98.12539682539682\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 72\n",
      "Train accuracy: 98.17142857142858\n",
      "Val accuracy: 95.67142857142858\n",
      "Iteration: 73\n",
      "Train accuracy: 98.1920634920635\n",
      "Val accuracy: 95.71428571428572\n",
      "Iteration: 74\n",
      "Train accuracy: 98.21587301587302\n",
      "Val accuracy: 95.7\n",
      "Iteration: 75\n",
      "Train accuracy: 98.25079365079365\n",
      "Val accuracy: 95.7\n",
      "Iteration: 76\n",
      "Train accuracy: 98.27619047619048\n",
      "Val accuracy: 95.7\n",
      "Iteration: 77\n",
      "Train accuracy: 98.30793650793652\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 78\n",
      "Train accuracy: 98.33333333333333\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 79\n",
      "Train accuracy: 98.35873015873015\n",
      "Val accuracy: 95.74285714285715\n",
      "Iteration: 80\n",
      "Train accuracy: 98.38571428571429\n",
      "Val accuracy: 95.75714285714285\n",
      "Iteration: 81\n",
      "Train accuracy: 98.40476190476191\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 82\n",
      "Train accuracy: 98.41904761904762\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 83\n",
      "Train accuracy: 98.43968253968254\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 84\n",
      "Train accuracy: 98.47142857142858\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 85\n",
      "Train accuracy: 98.4984126984127\n",
      "Val accuracy: 95.8\n",
      "Iteration: 86\n",
      "Train accuracy: 98.52380952380952\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 87\n",
      "Train accuracy: 98.55238095238094\n",
      "Val accuracy: 95.8\n",
      "Iteration: 88\n",
      "Train accuracy: 98.56666666666666\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 89\n",
      "Train accuracy: 98.58888888888889\n",
      "Val accuracy: 95.8\n",
      "Iteration: 90\n",
      "Train accuracy: 98.62222222222222\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 91\n",
      "Train accuracy: 98.64761904761905\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 92\n",
      "Train accuracy: 98.66666666666667\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 93\n",
      "Train accuracy: 98.67936507936508\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 94\n",
      "Train accuracy: 98.70476190476191\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 95\n",
      "Train accuracy: 98.72222222222223\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 96\n",
      "Train accuracy: 98.73333333333333\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 97\n",
      "Train accuracy: 98.74920634920635\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 98\n",
      "Train accuracy: 98.78412698412698\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 99\n",
      "Train accuracy: 98.79523809523809\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 100\n",
      "Train accuracy: 98.80793650793652\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 101\n",
      "Train accuracy: 98.84126984126983\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 102\n",
      "Train accuracy: 98.86984126984127\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 103\n",
      "Train accuracy: 98.88888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 104\n",
      "Train accuracy: 98.89682539682539\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 105\n",
      "Train accuracy: 98.91428571428571\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 106\n",
      "Train accuracy: 98.92380952380952\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 107\n",
      "Train accuracy: 98.94603174603175\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 108\n",
      "Train accuracy: 98.95873015873016\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 109\n",
      "Train accuracy: 98.97142857142858\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 110\n",
      "Train accuracy: 98.98095238095237\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 111\n",
      "Train accuracy: 99.0031746031746\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 112\n",
      "Train accuracy: 99.01269841269841\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 113\n",
      "Train accuracy: 99.02539682539683\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 114\n",
      "Train accuracy: 99.05079365079365\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 115\n",
      "Train accuracy: 99.06031746031746\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 116\n",
      "Train accuracy: 99.06666666666666\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 117\n",
      "Train accuracy: 99.08888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 118\n",
      "Train accuracy: 99.12222222222222\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 119\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 120\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 121\n",
      "Train accuracy: 99.16507936507936\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 122\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 123\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 124\n",
      "Train accuracy: 99.21904761904761\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 125\n",
      "Train accuracy: 99.21111111111112\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 126\n",
      "Train accuracy: 99.22857142857143\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 127\n",
      "Train accuracy: 99.25079365079365\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 128\n",
      "Train accuracy: 99.26031746031747\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 129\n",
      "Train accuracy: 99.27936507936508\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 130\n",
      "Train accuracy: 99.28095238095239\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 131\n",
      "Train accuracy: 99.30952380952381\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 132\n",
      "Train accuracy: 99.32857142857144\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 133\n",
      "Train accuracy: 99.33809523809524\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 134\n",
      "Train accuracy: 99.35238095238094\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 135\n",
      "Train accuracy: 99.35079365079366\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 136\n",
      "Train accuracy: 99.36507936507937\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 137\n",
      "Train accuracy: 99.37777777777778\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 138\n",
      "Train accuracy: 99.38412698412698\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 139\n",
      "Train accuracy: 99.39365079365079\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 140\n",
      "Train accuracy: 99.41587301587302\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 141\n",
      "Train accuracy: 99.42857142857143\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 142\n",
      "Train accuracy: 99.44603174603175\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 143\n",
      "Train accuracy: 99.46031746031746\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 144\n",
      "Train accuracy: 99.46825396825398\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 145\n",
      "Train accuracy: 99.47619047619047\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 146\n",
      "Train accuracy: 99.4904761904762\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 147\n",
      "Train accuracy: 99.4968253968254\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 148\n",
      "Train accuracy: 99.50952380952381\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 149\n",
      "Train accuracy: 99.51746031746032\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 150\n",
      "Train accuracy: 99.52857142857144\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 151\n",
      "Train accuracy: 99.53809523809524\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 152\n",
      "Train accuracy: 99.54126984126984\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 153\n",
      "Train accuracy: 99.55079365079365\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 154\n",
      "Train accuracy: 99.55238095238094\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 155\n",
      "Train accuracy: 99.56825396825397\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 156\n",
      "Train accuracy: 99.57777777777778\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 157\n",
      "Train accuracy: 99.58253968253969\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 158\n",
      "Train accuracy: 99.58571428571429\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 159\n",
      "Train accuracy: 99.6\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 160\n",
      "Train accuracy: 99.60793650793651\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 161\n",
      "Train accuracy: 99.615873015873\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 162\n",
      "Train accuracy: 99.62063492063493\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 163\n",
      "Train accuracy: 99.63492063492063\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 164\n",
      "Train accuracy: 99.63650793650794\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 165\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 166\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 167\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 168\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 169\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 170\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 171\n",
      "Train accuracy: 99.68095238095238\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 172\n",
      "Train accuracy: 99.68253968253968\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 173\n",
      "Train accuracy: 99.69047619047619\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 174\n",
      "Train accuracy: 99.69365079365079\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 175\n",
      "Train accuracy: 99.6984126984127\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 176\n",
      "Train accuracy: 99.70476190476191\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 177\n",
      "Train accuracy: 99.71269841269842\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 178\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 179\n",
      "Train accuracy: 99.72222222222223\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 180\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 181\n",
      "Train accuracy: 99.73174603174603\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 182\n",
      "Train accuracy: 99.73968253968253\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 183\n",
      "Train accuracy: 99.74603174603175\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 184\n",
      "Train accuracy: 99.74444444444444\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 185\n",
      "Train accuracy: 99.75238095238095\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 186\n",
      "Train accuracy: 99.76031746031747\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 187\n",
      "Train accuracy: 99.77460317460317\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 188\n",
      "Train accuracy: 99.77301587301586\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 189\n",
      "Train accuracy: 99.78730158730158\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 190\n",
      "Train accuracy: 99.79206349206349\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 191\n",
      "Train accuracy: 99.7984126984127\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 192\n",
      "Train accuracy: 99.80317460317461\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 193\n",
      "Train accuracy: 99.80952380952381\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 194\n",
      "Train accuracy: 99.81269841269841\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 195\n",
      "Train accuracy: 99.81587301587301\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 196\n",
      "Train accuracy: 99.82857142857144\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 197\n",
      "Train accuracy: 99.82698412698413\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 198\n",
      "Train accuracy: 99.83015873015873\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 199\n",
      "Train accuracy: 99.84126984126985\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 200\n",
      "Train accuracy: 99.84603174603176\n",
      "Val accuracy: 95.95714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_bp, val_acc_bp, train_loss_bp, val_loss_bp, sum_weights_bp = batch_grad_descent(x_train,y_train,epochsToTrain, 0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22a64942500>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP\", \"BP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imposing variability and seeing the effect of variability on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 0.3\n",
    "vDD = 5\n",
    "precision = 10#setting the precision value of the calculations\n",
    "wRange = 3\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization\n",
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVth(mu, sigma, shape):\n",
    "  #last dimension represents the binary rep for each weight\n",
    "  return np.random.normal(loc=mu, scale=sigma, size=shape) #each bit is represented by an sram so we need those many vth values for each mosfet in this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initMosParam(shape, mu, sigma, vDD, precision):\n",
    "    dim1, dim2 = shape\n",
    "    sizeI = (dim1, dim2, precision)\n",
    "\n",
    "    Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "\n",
    "    iOn = ((vDD - Vth)**2)*1e-06#scaling the current according to Ioff values arbitraryfor now!!\n",
    "\n",
    "\n",
    "    iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "\n",
    "\n",
    "    iOff = np.random.uniform(low=0, high=1e-10, size = sizeI)#no negative value\n",
    "    return (iOn, iOnNominal, iOff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, currents, precision, step, discreteSteps):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  \n",
    "  iOn, iOnNominal, iOff = currents\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray += np.sign(weightArray) * np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel)\n",
    "\n",
    "\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descentFPOCBP(X,Y,iter, lr, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    print(\"Z1\")\n",
    "    print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_grad_descentFPOCNP(X,Y,iter, lr, pert, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      \n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train loss {1/63000*np.sum((A3_train-one_hot_encoding(Y))**2)}::Val Loss {1/63000*np.sum((A3_val-one_hot_encoding(y_val))**2)}\")\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/Y.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentOCBP(X,Y,iter, lr, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, X1, Y1)\n",
    "\n",
    "      dW1varoc = weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varoc = weightTransformWithVariability(db1.reshape(db1.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varoc = weightTransformWithVariability(dW2, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varoc = weightTransformWithVariability(db2.reshape(db2.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varoc = weightTransformWithVariability(dW3, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varoc = weightTransformWithVariability(db3.reshape(db3.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1varoc, db1varoc, dW2varoc, db2varoc, dW3varoc, db3varoc, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentOCNP(X,Y,iter, lr, pert, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "  \n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "      #print(np.min(np.abs(W1)), np.min(np.abs(b1)), np.min(np.abs(W2)), np.min(np.abs(b2)), np.min(np.abs(W3)), np.min(np.abs(b3)))\n",
    "\n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n",
    "      #print(db1.shape)\n",
    "\n",
    "      dW1varoc = weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varoc = weightTransformWithVariability(db1.reshape(db1.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varoc = weightTransformWithVariability(dW2, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varoc = weightTransformWithVariability(db2.reshape(db2.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varoc = weightTransformWithVariability(dW3, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varoc = weightTransformWithVariability(db3.reshape(db3.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "      #print(dW3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1varoc, db1varoc, dW2varoc, db2varoc, dW3varoc, db3varoc, lr = lr)\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train loss {1/63000*np.sum((A3_train-one_hot_encoding(Y))**2)}::Val Loss {1/63000*np.sum((A3_val-one_hot_encoding(y_val))**2)}\")\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the effect of variability on the final accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTrainAcc = []\n",
    "finalValAcc = []\n",
    "sigmaList = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 12.15079365079365\n",
      "Val accuracy: 11.928571428571429\n",
      "Iteration: 2\n",
      "Train accuracy: 12.376190476190477\n",
      "Val accuracy: 12.828571428571427\n",
      "Iteration: 3\n",
      "Train accuracy: 12.617460317460317\n",
      "Val accuracy: 12.9\n",
      "Iteration: 4\n",
      "Train accuracy: 12.536507936507938\n",
      "Val accuracy: 12.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m sigma \u001b[39min\u001b[39;00m sigmaList:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=1'>2</a>\u001b[0m     W1, b1, W2, b2, W3, b3, train_acc_bpVar, val_acc_bpVar, train_loss_bpVar, val_loss_bpVar, sum_weights_bpVar \u001b[39m=\u001b[39m batch_grad_descentOCBP(x_train,y_train,\u001b[39m100\u001b[39;49m , \u001b[39m0.0005\u001b[39;49m, mu,sigma, vDD, precision, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=2'>3</a>\u001b[0m     finalTrainAcc\u001b[39m.\u001b[39mappend(train_acc_bpVar[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=3'>4</a>\u001b[0m     finalValAcc\u001b[39m.\u001b[39mappend(val_acc_bpVar[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 18'\u001b[0m in \u001b[0;36mbatch_grad_descentOCBP\u001b[1;34m(X, Y, iter, lr, mu, sigma, vDD, precision, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=40'>41</a>\u001b[0m X1 \u001b[39m=\u001b[39m X1\u001b[39m.\u001b[39mT \u001b[39m#take transpose to match the sizes \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=42'>43</a>\u001b[0m \u001b[39m#startin = time.time()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=43'>44</a>\u001b[0m W1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=44'>45</a>\u001b[0m b1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=45'>46</a>\u001b[0m W2varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 16'\u001b[0m in \u001b[0;36mweightTransformWithVariability\u001b[1;34m(weightArray, currents, precision, step, discreteSteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=14'>15</a>\u001b[0m analogWeightArray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(weightArray, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m bitLevel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(precision):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=17'>18</a>\u001b[0m   analogWeightArray \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(weightArray) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mwhere(np\u001b[39m.\u001b[39;49mbitwise_and(clippedWeightIndexArray, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbitLevel)\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbitLevel)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=21'>22</a>\u001b[0m weightWithVariability \u001b[39m=\u001b[39m (analogWeightArray\u001b[39m/\u001b[39miOnNominal)\u001b[39m*\u001b[39mstep\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m weightWithVariability\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sigma in sigmaList:\n",
    "    W1, b1, W2, b2, W3, b3, train_acc_bpVar, val_acc_bpVar, train_loss_bpVar, val_loss_bpVar, sum_weights_bpVar = batch_grad_descentOCBP(x_train,y_train,100 , 0.0005, mu,sigma, vDD, precision, print_op=1)\n",
    "    finalTrainAcc.append(train_acc_bpVar[-1])\n",
    "    finalValAcc.append(val_acc_bpVar[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(finalTrainAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 48.65238095238095::Val accuracy: 47.65714285714286::Train loss 1.0125491483283615::Val Loss 0.11469425632050993\n",
      "Iter 1 -> sub iter 9 : 47.777777777777785\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=0'>1</a>\u001b[0m W1, b1, W2, b2, W3, b3, train_acc_npVarAll, val_acc_npVarAll, train_loss_npVarAll, val_loss_npVarAll, sum_weights_npVarAll \u001b[39m=\u001b[39m batch_grad_descentOCNP(X\u001b[39m=\u001b[39;49mx_train,Y\u001b[39m=\u001b[39;49my_train,\u001b[39miter\u001b[39;49m \u001b[39m=\u001b[39;49m epochsToTrain, lr\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, pert\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m, mu\u001b[39m=\u001b[39;49mmu, sigma\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, vDD \u001b[39m=\u001b[39;49m vDD, precision \u001b[39m=\u001b[39;49m precision, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mbatch_grad_descentOCNP\u001b[1;34m(X, Y, iter, lr, pert, mu, sigma, vDD, precision, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=60'>61</a>\u001b[0m lossBeforePert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((A3\u001b[39m-\u001b[39mone_hot_encoding(Y1))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=62'>63</a>\u001b[0m \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=63'>64</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=64'>65</a>\u001b[0m dW1, db1, dW2, db2, dW3, db3 \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=65'>66</a>\u001b[0m \u001b[39m#print(db1.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=67'>68</a>\u001b[0m dW1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=53'>54</a>\u001b[0m A1pert \u001b[39m=\u001b[39m relu(Z1pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=55'>56</a>\u001b[0m Z2pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2,A1pert) \u001b[39m+\u001b[39m b2 \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=56'>57</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=58'>59</a>\u001b[0m Z3pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W3,A2pert) \u001b[39m+\u001b[39m b3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=59'>60</a>\u001b[0m A3pert \u001b[39m=\u001b[39m softmax(Z3pert)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=31'>32</a>\u001b[0m   \u001b[39m#Z4 = np.matmul(W4,A3) + b4\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=32'>33</a>\u001b[0m   \u001b[39m#A4 = relu(Z4)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=33'>34</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=41'>42</a>\u001b[0m   \u001b[39m#A2 is 10*m, final predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=42'>43</a>\u001b[0m   \u001b[39m# print(\"Fp Done\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=44'>45</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m Z1, A1, Z2, A2, Z3, A3\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=48'>49</a>\u001b[0m    \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmaximum(x,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(Z):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=52'>53</a>\u001b[0m   \u001b[39m#return np.exp(Z) / np.sum(np.exp(Z),0)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_npVarAll, val_acc_npVarAll, train_loss_npVarAll, val_loss_npVarAll, sum_weights_npVarAll = batch_grad_descentOCNP(X=x_train,Y=y_train,iter = epochsToTrain, lr=0.5, pert=0.0001, mu=mu, sigma=0.1, vDD = vDD, precision = precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 13.200000000000001::Val accuracy: 13.314285714285715::Train loss 1.566857139916198::Val Loss 0.17345653254052462\n",
      "Iteration: 2::Train accuracy: 21.284126984126985::Val accuracy: 21.37142857142857::Train loss 1.4363407935332517::Val Loss 0.1594937005890703\n",
      "Iteration: 3::Train accuracy: 25.63015873015873::Val accuracy: 25.142857142857146::Train loss 1.3699294923959326::Val Loss 0.15311371716078398\n",
      "Iteration: 4::Train accuracy: 28.40793650793651::Val accuracy: 27.885714285714286::Train loss 1.3242668092887602::Val Loss 0.1484728716058764\n",
      "Iteration: 5::Train accuracy: 30.631746031746033::Val accuracy: 30.028571428571425::Train loss 1.2878223717344564::Val Loss 0.1446851014398137\n",
      "Iteration: 6::Train accuracy: 32.05396825396825::Val accuracy: 31.2::Train loss 1.263716840929155::Val Loss 0.1423615132114519\n",
      "Iteration: 7::Train accuracy: 33.46825396825397::Val accuracy: 32.471428571428575::Train loss 1.2398404015421536::Val Loss 0.13983673487543052\n",
      "Iteration: 8::Train accuracy: 34.4968253968254::Val accuracy: 33.614285714285714::Train loss 1.2220524340238694::Val Loss 0.13791989162570473\n",
      "Iteration: 9::Train accuracy: 35.20634920634921::Val accuracy: 34.385714285714286::Train loss 1.207064330270723::Val Loss 0.13606595934210522\n",
      "Iteration: 10::Train accuracy: 36.7968253968254::Val accuracy: 35.542857142857144::Train loss 1.1673545220101735::Val Loss 0.1325893568270379\n",
      "Iteration: 11::Train accuracy: 38.58412698412698::Val accuracy: 36.957142857142856::Train loss 1.127328240093724::Val Loss 0.12889188314924008\n",
      "Iteration: 12::Train accuracy: 40.10793650793651::Val accuracy: 38.08571428571428::Train loss 1.0979546352344713::Val Loss 0.12574812070687272\n",
      "Iteration: 13::Train accuracy: 41.31269841269841::Val accuracy: 39.55714285714286::Train loss 1.0744097788738356::Val Loss 0.12298202407682617\n",
      "Iteration: 14::Train accuracy: 42.12698412698413::Val accuracy: 40.52857142857143::Train loss 1.0559987977357943::Val Loss 0.12082054188286893\n",
      "Iteration: 15::Train accuracy: 42.87619047619048::Val accuracy: 41.214285714285715::Train loss 1.0409342705764992::Val Loss 0.11908680334024636\n",
      "Iteration: 16::Train accuracy: 43.40952380952381::Val accuracy: 41.77142857142857::Train loss 1.0280037255057215::Val Loss 0.11762459499191995\n",
      "Iteration: 17::Train accuracy: 43.99523809523809::Val accuracy: 42.44285714285714::Train loss 1.008443673565804::Val Loss 0.11548025444037016\n",
      "Iteration: 18::Train accuracy: 46.08253968253968::Val accuracy: 44.628571428571426::Train loss 0.9476629664236439::Val Loss 0.1082331064889093\n",
      "Iteration: 19::Train accuracy: 48.86825396825397::Val accuracy: 47.08571428571429::Train loss 0.8924231179696367::Val Loss 0.10237569442578502\n",
      "Iteration: 20::Train accuracy: 51.36190476190477::Val accuracy: 49.7::Train loss 0.8484831582996054::Val Loss 0.0971278127885064\n",
      "Iteration: 21::Train accuracy: 53.353968253968254::Val accuracy: 51.82857142857142::Train loss 0.8151814632890194::Val Loss 0.09304330935881679\n",
      "Iteration: 22::Train accuracy: 54.85079365079365::Val accuracy: 53.7::Train loss 0.7901488132958926::Val Loss 0.09035121809806243\n",
      "Iteration: 23::Train accuracy: 56.01746031746032::Val accuracy: 54.91428571428572::Train loss 0.7726854876540288::Val Loss 0.0885385309859091\n",
      "Iteration: 24::Train accuracy: 56.941269841269836::Val accuracy: 55.7::Train loss 0.7592634175166658::Val Loss 0.08703825972935011\n",
      "Iteration: 25::Train accuracy: 57.66031746031746::Val accuracy: 56.48571428571428::Train loss 0.7486318367256443::Val Loss 0.0858822341144995\n",
      "Iteration: 26::Train accuracy: 58.20952380952381::Val accuracy: 56.92857142857143::Train loss 0.7400621705266568::Val Loss 0.08495708822996367\n",
      "Iteration: 27::Train accuracy: 58.66825396825397::Val accuracy: 57.15714285714286::Train loss 0.7327846887543837::Val Loss 0.08423344310401153\n",
      "Iteration: 28::Train accuracy: 59.02857142857143::Val accuracy: 57.61428571428572::Train loss 0.7262426895187616::Val Loss 0.08359608727630793\n",
      "Iteration: 29::Train accuracy: 59.34603174603175::Val accuracy: 57.74285714285714::Train loss 0.7210538013914125::Val Loss 0.0831341752165388\n",
      "Iteration: 30::Train accuracy: 59.63650793650793::Val accuracy: 57.94285714285714::Train loss 0.7159276283956159::Val Loss 0.08263832455559401\n",
      "Iteration: 31::Train accuracy: 59.871428571428574::Val accuracy: 58.199999999999996::Train loss 0.7112034275209694::Val Loss 0.08218186965566306\n",
      "Iteration: 32::Train accuracy: 60.109523809523814::Val accuracy: 58.542857142857144::Train loss 0.7068308798673936::Val Loss 0.08175110523584024\n",
      "Iteration: 33::Train accuracy: 60.303174603174604::Val accuracy: 58.9::Train loss 0.7025330249648811::Val Loss 0.08134857028133265\n",
      "Iteration: 34::Train accuracy: 60.52222222222222::Val accuracy: 59.042857142857144::Train loss 0.6982700487345431::Val Loss 0.08100981276000896\n",
      "Iteration: 35::Train accuracy: 60.68730158730159::Val accuracy: 59.07142857142858::Train loss 0.6942341406417082::Val Loss 0.08067336818930004\n",
      "Iteration: 36::Train accuracy: 60.87619047619047::Val accuracy: 59.15714285714285::Train loss 0.6902779816163724::Val Loss 0.08028398806431553\n",
      "Iteration: 37::Train accuracy: 61.06349206349206::Val accuracy: 59.3::Train loss 0.6861637202575336::Val Loss 0.07981914929236583\n",
      "Iteration: 38::Train accuracy: 61.31111111111112::Val accuracy: 59.71428571428572::Train loss 0.6818478344841853::Val Loss 0.07925897747850831\n",
      "Iteration: 39::Train accuracy: 61.60634920634921::Val accuracy: 59.971428571428575::Train loss 0.6759474864697054::Val Loss 0.07860031272778223\n",
      "Iteration: 40::Train accuracy: 61.93333333333333::Val accuracy: 60.34285714285714::Train loss 0.6694505649192561::Val Loss 0.07786240869673679\n",
      "Iteration: 41::Train accuracy: 62.25555555555555::Val accuracy: 60.61428571428571::Train loss 0.6624677723564442::Val Loss 0.07705512375113802\n",
      "Iteration: 42::Train accuracy: 62.56507936507937::Val accuracy: 60.94285714285714::Train loss 0.6560501841817448::Val Loss 0.07619817115595598\n",
      "Iteration: 43::Train accuracy: 63.05555555555556::Val accuracy: 61.27142857142858::Train loss 0.6473395098219097::Val Loss 0.07520433829909705\n",
      "Iteration: 44::Train accuracy: 63.48412698412699::Val accuracy: 61.74285714285715::Train loss 0.6389574081105465::Val Loss 0.07430330453825759\n",
      "Iteration: 45::Train accuracy: 63.993650793650794::Val accuracy: 62.28571428571429::Train loss 0.6304632062725702::Val Loss 0.0733821225251423\n",
      "Iteration: 46::Train accuracy: 64.40476190476191::Val accuracy: 62.771428571428565::Train loss 0.6212307151427998::Val Loss 0.07238706124263183\n",
      "Iteration: 47::Train accuracy: 64.9031746031746::Val accuracy: 63.37142857142857::Train loss 0.6118972230571534::Val Loss 0.07133069847968727\n",
      "Iteration: 48::Train accuracy: 65.37460317460318::Val accuracy: 63.82857142857142::Train loss 0.602534636254433::Val Loss 0.0703437798880992\n",
      "Iteration: 49::Train accuracy: 65.86507936507935::Val accuracy: 64.18571428571428::Train loss 0.5924183853812914::Val Loss 0.0693357245971397\n",
      "Iteration: 50::Train accuracy: 66.33333333333333::Val accuracy: 64.61428571428571::Train loss 0.5832160986345554::Val Loss 0.06843089820634923\n",
      "Iteration: 51::Train accuracy: 66.7::Val accuracy: 64.91428571428571::Train loss 0.5743728419503267::Val Loss 0.06750626935496203\n",
      "Iteration: 52::Train accuracy: 67.05873015873016::Val accuracy: 65.28571428571428::Train loss 0.5658968898188959::Val Loss 0.06654167669859441\n",
      "Iteration: 53::Train accuracy: 67.5079365079365::Val accuracy: 65.60000000000001::Train loss 0.5557929883868066::Val Loss 0.06547782624049356\n",
      "Iteration: 54::Train accuracy: 68.10000000000001::Val accuracy: 66.28571428571428::Train loss 0.5405740099324423::Val Loss 0.06372877661198328\n",
      "Iteration: 55::Train accuracy: 68.96349206349205::Val accuracy: 67.08571428571429::Train loss 0.5230621874696225::Val Loss 0.061585221413256376\n",
      "Iteration: 56::Train accuracy: 69.77936507936508::Val accuracy: 68.01428571428572::Train loss 0.5067061800288143::Val Loss 0.05939179810132212\n",
      "Iteration: 57::Train accuracy: 70.6936507936508::Val accuracy: 69.32857142857142::Train loss 0.489210367574694::Val Loss 0.05724441181936453\n",
      "Iteration: 58::Train accuracy: 71.52539682539683::Val accuracy: 70.0::Train loss 0.4744003446947302::Val Loss 0.05544247607969578\n",
      "Iteration: 59::Train accuracy: 72.17936507936507::Val accuracy: 71.1::Train loss 0.4623977945309487::Val Loss 0.05392138917870933\n",
      "Iteration: 60::Train accuracy: 72.75555555555555::Val accuracy: 71.61428571428571::Train loss 0.4522700737808248::Val Loss 0.052731181590657816\n",
      "Iteration: 61::Train accuracy: 73.23015873015873::Val accuracy: 72.11428571428571::Train loss 0.44308804971709287::Val Loss 0.051632713808782794\n",
      "Iteration: 62::Train accuracy: 73.63809523809523::Val accuracy: 72.57142857142857::Train loss 0.4348761643848828::Val Loss 0.050673730365417904\n",
      "Iteration: 63::Train accuracy: 74.07460317460317::Val accuracy: 73.0::Train loss 0.4272682491363391::Val Loss 0.04974865868531299\n",
      "Iteration: 64::Train accuracy: 74.3873015873016::Val accuracy: 73.34285714285714::Train loss 0.4213311027409983::Val Loss 0.04900178174948363\n",
      "Iteration: 65::Train accuracy: 74.71746031746032::Val accuracy: 73.61428571428571::Train loss 0.4158759552442644::Val Loss 0.04841171393602029\n",
      "Iteration: 66::Train accuracy: 74.98253968253968::Val accuracy: 74.0142857142857::Train loss 0.41123739271511395::Val Loss 0.047847580511822856\n",
      "Iteration: 67::Train accuracy: 75.14444444444445::Val accuracy: 74.17142857142856::Train loss 0.40750603546743264::Val Loss 0.0473729781176053\n",
      "Iteration: 68::Train accuracy: 75.20476190476191::Val accuracy: 74.32857142857144::Train loss 0.4044411712381762::Val Loss 0.04701874269508109\n",
      "Iteration: 69::Train accuracy: 75.35079365079365::Val accuracy: 74.52857142857144::Train loss 0.40080949912923075::Val Loss 0.04662500559570586\n",
      "Iteration: 70::Train accuracy: 75.67301587301587::Val accuracy: 74.67142857142856::Train loss 0.39606091452170117::Val Loss 0.04612529895336065\n",
      "Iteration: 71::Train accuracy: 75.80634920634921::Val accuracy: 74.8::Train loss 0.3930276834723899::Val Loss 0.045768092346855783\n",
      "Iteration: 72::Train accuracy: 75.93968253968254::Val accuracy: 74.88571428571429::Train loss 0.38975023840666617::Val Loss 0.04536594516067845\n",
      "Iteration: 73::Train accuracy: 76.05238095238094::Val accuracy: 75.07142857142857::Train loss 0.387058850964811::Val Loss 0.045067949489097474\n",
      "Iteration: 74::Train accuracy: 76.16825396825396::Val accuracy: 75.08571428571429::Train loss 0.38443934984364236::Val Loss 0.04478501293285217\n",
      "Iteration: 75::Train accuracy: 76.23174603174603::Val accuracy: 75.0::Train loss 0.38248895885768064::Val Loss 0.044630863254052806\n",
      "Iteration: 76::Train accuracy: 76.3015873015873::Val accuracy: 74.94285714285715::Train loss 0.3803032954024225::Val Loss 0.04441068170828668\n",
      "Iteration: 77::Train accuracy: 76.25396825396825::Val accuracy: 75.0::Train loss 0.37955029563788056::Val Loss 0.044375198855480434\n",
      "Iteration: 78::Train accuracy: 76.23968253968255::Val accuracy: 74.84285714285714::Train loss 0.3786147323187425::Val Loss 0.044212330638647865\n",
      "Iteration: 79::Train accuracy: 76.19206349206348::Val accuracy: 74.9857142857143::Train loss 0.37756385385093755::Val Loss 0.04403500870330437\n",
      "Iteration: 80::Train accuracy: 76.21904761904761::Val accuracy: 75.1::Train loss 0.3761446652932145::Val Loss 0.043805003571116226\n",
      "Iteration: 81::Train accuracy: 76.13968253968254::Val accuracy: 75.08571428571429::Train loss 0.3755324485036911::Val Loss 0.043685519491015935\n",
      "Iteration: 82::Train accuracy: 76.0::Val accuracy: 74.9857142857143::Train loss 0.37603545454433085::Val Loss 0.04366540601855181\n",
      "Iteration: 83::Train accuracy: 75.86349206349206::Val accuracy: 74.9::Train loss 0.3759558638146004::Val Loss 0.043555407449570926\n",
      "Iteration: 84::Train accuracy: 75.72380952380952::Val accuracy: 74.75714285714285::Train loss 0.3764619718391046::Val Loss 0.04354496903240951\n",
      "Iteration: 85::Train accuracy: 75.45396825396826::Val accuracy: 74.37142857142857::Train loss 0.3774222717049735::Val Loss 0.04363045513779789\n",
      "Iteration: 86::Train accuracy: 75.28730158730158::Val accuracy: 74.14285714285714::Train loss 0.37806966257343666::Val Loss 0.04367095285227896\n",
      "Iteration: 87::Train accuracy: 75.24444444444444::Val accuracy: 74.27142857142857::Train loss 0.37679004982275616::Val Loss 0.04346263023202824\n",
      "Iteration: 88::Train accuracy: 75.36190476190477::Val accuracy: 74.44285714285715::Train loss 0.37431357331506143::Val Loss 0.043159011203238035\n",
      "Iteration: 89::Train accuracy: 75.37936507936507::Val accuracy: 74.4::Train loss 0.372442377330091::Val Loss 0.04294066130588854\n",
      "Iteration: 90::Train accuracy: 75.4095238095238::Val accuracy: 74.37142857142857::Train loss 0.3703551843358487::Val Loss 0.0426511653072721\n",
      "Iteration: 91::Train accuracy: 75.38095238095238::Val accuracy: 74.44285714285715::Train loss 0.37001127005786494::Val Loss 0.04253241198347452\n",
      "Iteration: 92::Train accuracy: 75.33492063492064::Val accuracy: 74.52857142857144::Train loss 0.36979248763980616::Val Loss 0.0424579559239301\n",
      "Iteration: 93::Train accuracy: 75.46031746031746::Val accuracy: 74.74285714285715::Train loss 0.36694780598842563::Val Loss 0.042105558351730035\n",
      "Iteration: 94::Train accuracy: 75.61746031746031::Val accuracy: 75.02857142857144::Train loss 0.3646148428776769::Val Loss 0.041790042860175146\n",
      "Iteration: 95::Train accuracy: 75.59047619047618::Val accuracy: 75.05714285714285::Train loss 0.3648352164631686::Val Loss 0.041721667197602764\n",
      "Iteration: 96::Train accuracy: 75.75238095238095::Val accuracy: 75.34285714285714::Train loss 0.3626116211144255::Val Loss 0.041392339687590145\n",
      "Iteration: 97::Train accuracy: 75.95079365079364::Val accuracy: 75.4857142857143::Train loss 0.36068142544299947::Val Loss 0.04116651226151273\n",
      "Iteration: 98::Train accuracy: 76.15079365079364::Val accuracy: 75.52857142857144::Train loss 0.3587569371636391::Val Loss 0.040938139728600546\n",
      "Iteration: 99::Train accuracy: 76.17936507936508::Val accuracy: 75.67142857142856::Train loss 0.35855645099895916::Val Loss 0.040898363110898193\n",
      "Iteration: 100::Train accuracy: 76.14126984126985::Val accuracy: 75.7::Train loss 0.3603090228730898::Val Loss 0.041091918678338925\n",
      "Iteration: 101::Train accuracy: 76.05873015873016::Val accuracy: 75.67142857142856::Train loss 0.3633811688095412::Val Loss 0.04139103917980882\n",
      "Iteration: 102::Train accuracy: 75.77460317460317::Val accuracy: 75.41428571428571::Train loss 0.3697073784403775::Val Loss 0.04203141035073541\n",
      "Iteration: 103::Train accuracy: 75.59047619047618::Val accuracy: 75.37142857142857::Train loss 0.37553841262084997::Val Loss 0.042614337691663305\n",
      "Iteration: 104::Train accuracy: 74.65555555555555::Val accuracy: 74.5142857142857::Train loss 0.3925318499161501::Val Loss 0.04453789074917866\n",
      "Iteration: 105::Train accuracy: 74.03492063492064::Val accuracy: 73.52857142857144::Train loss 0.4062282198761424::Val Loss 0.04606551445734629\n",
      "Iteration: 106::Train accuracy: 73.50952380952381::Val accuracy: 73.15714285714286::Train loss 0.4167716790389977::Val Loss 0.047250658743916846\n",
      "Iteration: 107::Train accuracy: 72.92063492063492::Val accuracy: 71.98571428571428::Train loss 0.4285144158104274::Val Loss 0.04861148137111578\n",
      "Iteration: 108::Train accuracy: 72.52539682539683::Val accuracy: 71.62857142857143::Train loss 0.4360526724749092::Val Loss 0.049494780643807565\n",
      "Iteration: 109::Train accuracy: 72.03015873015873::Val accuracy: 70.98571428571428::Train loss 0.44381307483809895::Val Loss 0.050411663839491076\n",
      "Iteration: 110::Train accuracy: 71.44126984126984::Val accuracy: 70.12857142857143::Train loss 0.45285981429068334::Val Loss 0.05145296990772659\n",
      "Iteration: 111::Train accuracy: 71.21269841269842::Val accuracy: 69.89999999999999::Train loss 0.45768231069119786::Val Loss 0.05197435486593842\n",
      "Iteration: 112::Train accuracy: 71.4015873015873::Val accuracy: 70.31428571428572::Train loss 0.4555815011716835::Val Loss 0.051729342107397505\n",
      "Iteration: 113::Train accuracy: 71.58095238095238::Val accuracy: 70.67142857142858::Train loss 0.4518406348077735::Val Loss 0.05122947627832551\n",
      "Iteration: 114::Train accuracy: 71.63968253968254::Val accuracy: 70.75714285714285::Train loss 0.44970297789928254::Val Loss 0.051001722800778686\n",
      "Iteration: 115::Train accuracy: 71.63174603174603::Val accuracy: 70.8::Train loss 0.44879977180704705::Val Loss 0.05087280964761152\n",
      "Iteration: 116::Train accuracy: 71.82063492063492::Val accuracy: 70.85714285714285::Train loss 0.4448732883339239::Val Loss 0.05042981349158813\n",
      "Iteration: 117::Train accuracy: 72.11746031746031::Val accuracy: 71.11428571428571::Train loss 0.43914083614618565::Val Loss 0.049753420922208226\n",
      "Iteration: 118::Train accuracy: 72.51587301587301::Val accuracy: 71.67142857142858::Train loss 0.4313158543088888::Val Loss 0.04886907058081938\n",
      "Iteration: 119::Train accuracy: 72.63968253968254::Val accuracy: 71.72857142857143::Train loss 0.4264904555005334::Val Loss 0.04833908911282878\n",
      "Iteration: 120::Train accuracy: 72.53968253968253::Val accuracy: 71.65714285714286::Train loss 0.42508536502553856::Val Loss 0.04815584908342543\n",
      "Iteration: 121::Train accuracy: 72.46190476190476::Val accuracy: 71.6::Train loss 0.42569663546838377::Val Loss 0.048192561405076814\n",
      "Iteration: 122::Train accuracy: 72.45238095238096::Val accuracy: 71.61428571428571::Train loss 0.424480185093886::Val Loss 0.04800480277468351\n",
      "Iteration: 123::Train accuracy: 72.52380952380952::Val accuracy: 71.75714285714285::Train loss 0.42212436637617207::Val Loss 0.047714394392754746\n",
      "Iteration: 124::Train accuracy: 72.93174603174603::Val accuracy: 72.25714285714285::Train loss 0.41469491666851543::Val Loss 0.04688845646547452\n",
      "Iteration: 125::Train accuracy: 73.54603174603174::Val accuracy: 73.14285714285714::Train loss 0.40487448253409136::Val Loss 0.04580463974639918\n",
      "Iteration: 126::Train accuracy: 74.07777777777778::Val accuracy: 73.57142857142858::Train loss 0.39510412135947726::Val Loss 0.04471319419426244\n",
      "Iteration: 127::Train accuracy: 74.65238095238095::Val accuracy: 74.04285714285714::Train loss 0.3853637250331926::Val Loss 0.04365159928588187\n",
      "Iteration: 128::Train accuracy: 75.2031746031746::Val accuracy: 74.67142857142856::Train loss 0.3757434750632165::Val Loss 0.0425925081682317\n",
      "Iteration: 129::Train accuracy: 75.57460317460317::Val accuracy: 75.08571428571429::Train loss 0.36784123346279224::Val Loss 0.0416964991065021\n",
      "Iteration: 130::Train accuracy: 76.07142857142857::Val accuracy: 75.57142857142857::Train loss 0.3598113027001118::Val Loss 0.040767845470462416\n",
      "Iteration: 131::Train accuracy: 76.36984126984126::Val accuracy: 75.95714285714286::Train loss 0.3537747876799183::Val Loss 0.04008755263143746\n",
      "Iteration: 132::Train accuracy: 76.66349206349207::Val accuracy: 76.25714285714285::Train loss 0.34922555188499865::Val Loss 0.039578890011053344\n",
      "Iteration: 133::Train accuracy: 76.7936507936508::Val accuracy: 76.41428571428571::Train loss 0.34673503298157565::Val Loss 0.03930649675222413\n",
      "Iteration: 134::Train accuracy: 76.78253968253969::Val accuracy: 76.72857142857143::Train loss 0.3459664410310996::Val Loss 0.03920809186034574\n",
      "Iteration: 135::Train accuracy: 76.92063492063492::Val accuracy: 76.52857142857142::Train loss 0.34436680543312437::Val Loss 0.03901479362611946\n",
      "Iteration: 136::Train accuracy: 77.10952380952381::Val accuracy: 76.72857142857143::Train loss 0.3415524614446109::Val Loss 0.038651525677897515\n",
      "Iteration: 137::Train accuracy: 77.36825396825397::Val accuracy: 77.21428571428571::Train loss 0.33734795861190325::Val Loss 0.038155595535838374\n",
      "Iteration: 138::Train accuracy: 77.53015873015873::Val accuracy: 77.15714285714286::Train loss 0.33505144723164754::Val Loss 0.03794106830924203\n",
      "Iteration: 139::Train accuracy: 77.45396825396826::Val accuracy: 77.31428571428572::Train loss 0.33630277638174577::Val Loss 0.038031622766569385\n",
      "Iteration: 140::Train accuracy: 77.54761904761904::Val accuracy: 77.42857142857143::Train loss 0.33528901383066934::Val Loss 0.037870626064066125\n",
      "Iteration: 141::Train accuracy: 77.81428571428572::Val accuracy: 77.68571428571428::Train loss 0.33150462171464873::Val Loss 0.03740308288645637\n",
      "Iteration: 142::Train accuracy: 78.07936507936508::Val accuracy: 77.94285714285715::Train loss 0.32871417032806655::Val Loss 0.037064373185121624\n",
      "Iteration: 143::Train accuracy: 78.21904761904761::Val accuracy: 78.15714285714286::Train loss 0.3261954556600593::Val Loss 0.03677323974413577\n",
      "Iteration: 144::Train accuracy: 78.33809523809524::Val accuracy: 78.31428571428572::Train loss 0.3242875501261326::Val Loss 0.036519788594353085\n",
      "Iteration: 145::Train accuracy: 78.37619047619047::Val accuracy: 78.38571428571429::Train loss 0.3239869610894462::Val Loss 0.036417311865783904\n",
      "Iteration: 146::Train accuracy: 78.36190476190477::Val accuracy: 78.5::Train loss 0.324959401773729::Val Loss 0.036505361445708304\n",
      "Iteration: 147::Train accuracy: 78.34761904761905::Val accuracy: 78.54285714285714::Train loss 0.3254484462736096::Val Loss 0.03655231390633435\n",
      "Iteration: 148::Train accuracy: 78.37142857142857::Val accuracy: 78.54285714285714::Train loss 0.3256102718955764::Val Loss 0.036533326113977624\n",
      "Iteration: 149::Train accuracy: 78.45238095238095::Val accuracy: 78.65714285714286::Train loss 0.3251646934241628::Val Loss 0.036422419681394126\n",
      "Iteration: 150::Train accuracy: 78.5936507936508::Val accuracy: 78.72857142857143::Train loss 0.32370404433659367::Val Loss 0.03621963779096084\n",
      "Iteration: 151::Train accuracy: 78.58253968253969::Val accuracy: 78.85714285714286::Train loss 0.3240790397305::Val Loss 0.036224410761862245\n",
      "Iteration: 152::Train accuracy: 78.64920634920635::Val accuracy: 78.97142857142858::Train loss 0.32330584704680626::Val Loss 0.0361126041143741\n",
      "Iteration: 153::Train accuracy: 78.89047619047619::Val accuracy: 79.14285714285715::Train loss 0.3204246674001256::Val Loss 0.03574646816727893\n",
      "Iteration: 154::Train accuracy: 78.74761904761904::Val accuracy: 79.2::Train loss 0.3220850491178458::Val Loss 0.03589316612966283\n",
      "Iteration: 155::Train accuracy: 78.62698412698413::Val accuracy: 78.97142857142858::Train loss 0.3250543649492729::Val Loss 0.03620275354140952\n",
      "Iteration: 156::Train accuracy: 78.52539682539683::Val accuracy: 78.87142857142857::Train loss 0.32852065861238544::Val Loss 0.036581647934342224\n",
      "Iteration: 157::Train accuracy: 78.45238095238095::Val accuracy: 78.64285714285715::Train loss 0.3318121158214347::Val Loss 0.03695303111799949\n",
      "Iteration: 158::Train accuracy: 78.38730158730158::Val accuracy: 78.62857142857142::Train loss 0.3346306283671121::Val Loss 0.03726064014163791\n",
      "Iteration: 159::Train accuracy: 78.32857142857142::Val accuracy: 78.54285714285714::Train loss 0.33655155103070145::Val Loss 0.037436661178767094\n",
      "Iteration: 160::Train accuracy: 78.16349206349207::Val accuracy: 78.41428571428571::Train loss 0.33886458197589714::Val Loss 0.03766531181859757\n",
      "Iteration: 161::Train accuracy: 78.13968253968254::Val accuracy: 78.4::Train loss 0.3407794255564295::Val Loss 0.03786916876837811\n",
      "Iteration: 162::Train accuracy: 77.98412698412699::Val accuracy: 78.25714285714285::Train loss 0.34345365552643736::Val Loss 0.03815579489451571\n",
      "Iteration: 163::Train accuracy: 77.83809523809524::Val accuracy: 78.11428571428571::Train loss 0.34602319912082796::Val Loss 0.03842016905505417\n",
      "Iteration: 164::Train accuracy: 77.77777777777779::Val accuracy: 78.04285714285714::Train loss 0.34726307782610605::Val Loss 0.038534055245543875\n",
      "Iteration: 165::Train accuracy: 77.76984126984127::Val accuracy: 78.10000000000001::Train loss 0.34628173877473833::Val Loss 0.03842883998671159\n",
      "Iteration: 166::Train accuracy: 77.91746031746032::Val accuracy: 78.37142857142857::Train loss 0.34413120918183876::Val Loss 0.03819404277077673\n",
      "Iteration: 167::Train accuracy: 77.94285714285715::Val accuracy: 78.45714285714286::Train loss 0.34314926698357623::Val Loss 0.038081409818027157\n",
      "Iteration: 168::Train accuracy: 78.04444444444445::Val accuracy: 78.58571428571427::Train loss 0.34123772318660534::Val Loss 0.03787181293365782\n",
      "Iteration: 169::Train accuracy: 78.12222222222222::Val accuracy: 78.57142857142857::Train loss 0.33894717898953663::Val Loss 0.03761846890296244\n",
      "Iteration: 170::Train accuracy: 78.17301587301587::Val accuracy: 78.68571428571428::Train loss 0.3377868220684938::Val Loss 0.03748346525812862\n",
      "Iteration: 171::Train accuracy: 78.25873015873016::Val accuracy: 78.72857142857143::Train loss 0.336310023576885::Val Loss 0.037317685113656886\n",
      "Iteration: 172::Train accuracy: 78.4095238095238::Val accuracy: 78.94285714285715::Train loss 0.33328583024875486::Val Loss 0.03699458590116936\n",
      "Iteration: 173::Train accuracy: 78.5111111111111::Val accuracy: 79.01428571428572::Train loss 0.33089000785910594::Val Loss 0.03673326558427378\n",
      "Iteration: 174::Train accuracy: 78.53809523809524::Val accuracy: 79.02857142857142::Train loss 0.32980022848665386::Val Loss 0.036610338643806736\n",
      "Iteration: 175::Train accuracy: 78.54285714285714::Val accuracy: 79.10000000000001::Train loss 0.3295164433700145::Val Loss 0.03657247949066407\n",
      "Iteration: 176::Train accuracy: 78.38888888888889::Val accuracy: 79.01428571428572::Train loss 0.33197709907562056::Val Loss 0.03684312594536459\n",
      "Iteration: 177::Train accuracy: 78.29206349206349::Val accuracy: 78.92857142857143::Train loss 0.33434511142684253::Val Loss 0.03709414356653327\n",
      "Iteration: 178::Train accuracy: 78.21746031746032::Val accuracy: 78.77142857142857::Train loss 0.33581264962256324::Val Loss 0.03726263784901043\n",
      "Iteration: 179::Train accuracy: 78.27936507936508::Val accuracy: 78.9::Train loss 0.3341776367978907::Val Loss 0.03709022848702856\n",
      "Iteration: 180::Train accuracy: 78.31269841269841::Val accuracy: 78.92857142857143::Train loss 0.3330168555892107::Val Loss 0.036975167605223916\n",
      "Iteration: 181::Train accuracy: 78.40158730158731::Val accuracy: 79.04285714285714::Train loss 0.33084605089704655::Val Loss 0.03674369046078428\n",
      "Iteration: 182::Train accuracy: 78.45079365079364::Val accuracy: 79.04285714285714::Train loss 0.33000111945201394::Val Loss 0.03665620677420437\n",
      "Iteration: 183::Train accuracy: 78.44444444444446::Val accuracy: 79.01428571428572::Train loss 0.3299851497575845::Val Loss 0.036658530639535974\n",
      "Iteration: 184::Train accuracy: 78.52380952380953::Val accuracy: 79.07142857142857::Train loss 0.32887066082956407::Val Loss 0.036536264208544\n",
      "Iteration: 185::Train accuracy: 78.72380952380954::Val accuracy: 79.17142857142856::Train loss 0.3255459727355709::Val Loss 0.036176031131740394\n",
      "Iteration: 186::Train accuracy: 78.91746031746032::Val accuracy: 79.24285714285715::Train loss 0.32218005726354465::Val Loss 0.035809655896149976\n",
      "Iteration: 187::Train accuracy: 79.05714285714286::Val accuracy: 79.4::Train loss 0.31863161791168565::Val Loss 0.035416493538705976\n",
      "Iteration: 188::Train accuracy: 79.24603174603175::Val accuracy: 79.60000000000001::Train loss 0.3148895627857311::Val Loss 0.03500817729271245\n",
      "Iteration: 189::Train accuracy: 79.45238095238095::Val accuracy: 79.84285714285714::Train loss 0.3113584861719522::Val Loss 0.03462124728093009\n",
      "Iteration: 190::Train accuracy: 79.6031746031746::Val accuracy: 79.87142857142857::Train loss 0.3084282786433664::Val Loss 0.03429779423033516\n",
      "Iteration: 191::Train accuracy: 79.76190476190477::Val accuracy: 79.98571428571428::Train loss 0.30600708401886934::Val Loss 0.03403197561652687\n",
      "Iteration: 192::Train accuracy: 79.87936507936509::Val accuracy: 79.97142857142858::Train loss 0.3043254949741518::Val Loss 0.0338513075143452\n",
      "Iteration: 193::Train accuracy: 79.97301587301587::Val accuracy: 80.05714285714286::Train loss 0.3027831966961689::Val Loss 0.03368597072694213\n",
      "Iteration: 194::Train accuracy: 80.04761904761905::Val accuracy: 79.98571428571428::Train loss 0.3018773511778084::Val Loss 0.033600222099888104\n",
      "Iteration: 195::Train accuracy: 80.10000000000001::Val accuracy: 80.02857142857142::Train loss 0.300726502523443::Val Loss 0.033481602841493166\n",
      "Iteration: 196::Train accuracy: 80.13650793650794::Val accuracy: 80.10000000000001::Train loss 0.3008678712281815::Val Loss 0.0334981984007354\n",
      "Iteration: 197::Train accuracy: 80.30000000000001::Val accuracy: 80.32857142857142::Train loss 0.29818347838176096::Val Loss 0.033199616393923104\n",
      "Iteration: 198::Train accuracy: 80.4063492063492::Val accuracy: 80.38571428571429::Train loss 0.2963257647810298::Val Loss 0.03299437540784002\n",
      "Iteration: 199::Train accuracy: 80.51904761904763::Val accuracy: 80.5::Train loss 0.2943972000115268::Val Loss 0.0327829922011792\n",
      "Iteration: 200::Train accuracy: 80.5968253968254::Val accuracy: 80.65714285714286::Train loss 0.2928825318239694::Val Loss 0.03261730544325987\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_npVar, val_acc_npVar, train_loss_npVar, val_loss_npVar, sum_weights_npVar = batch_grad_descentFPOCNP(X=x_train,Y=y_train,iter = epochsToTrain, lr=0.005, pert=0.5, mu=mu, sigma=sigma, vDD = vDD, precision = precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22aa7a7dbd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.plot(train_acc_bpVar, \"c^\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP(software)\", \"BP(software)\", \"BP(with variability)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.5873015873016"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc_bpVar[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the software NP and BP algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  #print(Z3.shape)\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i, :] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    lossArrayAfterPertZ3[i, :] += np.sum(np.square(A3pert-one_hot_encoding(Y1)), axis=0)\n",
    "\n",
    "\n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "\n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    lossArrayAfterPertZ2[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "    A1pert = relu(Z1pert)\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    \n",
    "    lossArrayAfterPertZ1[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "    \n",
    "  #print(lossArrayAfterPertZ1)\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDComp(X,Y,iter, lrBP, lrNP, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "\n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1np, b1np, W2np,b2np, W3np, b3np) \n",
    "      print(f\"NP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1np, b1np, W2np, b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr = lrNP)\n",
    "      #print(W1)\n",
    "\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1bp, b1bp, W2bp,b2bp, W3bp, b3bp) \n",
    "      print(f\"                                                                        BP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1bp, W2bp, W3bp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1bp, b1bp, W2bp, b2bp, W3bp, b3bp, dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp, lr = lrBP)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    lrNP = lrNP*np.exp(-0.01)\n",
    "    lrBP = lrBP*np.exp(-0.01)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train Acc BP::{accuracy(predictions(A3_train_bp), Y)} Val Acc BP::{accuracy(predictions(A3_val_bp), y_val)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "1.0000000000000004\n",
      "NP Iter 0 -> sub iter 15 : 42.857142857142854                           BP Iter 0 -> sub iter 14 : 53.968253968253975\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=0'>1</a>\u001b[0m w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _ \u001b[39m=\u001b[39m batchGDComp(x_train,y_train,\u001b[39m20\u001b[39;49m, \u001b[39m0.1\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m0.0000000000001\u001b[39;49m, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 38\u001b[0m in \u001b[0;36mbatchGDComp\u001b[1;34m(X, Y, iter, lrBP, lrNP, pert, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=53'>54</a>\u001b[0m lossBeforePert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((A3\u001b[39m-\u001b[39mone_hot_encoding(Y1))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=55'>56</a>\u001b[0m \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=56'>57</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=57'>58</a>\u001b[0m dW1np, db1np, dW2np, db2np, dW3np, db3np \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=58'>59</a>\u001b[0m \u001b[39m#print(f\"iter in iter{j}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=59'>60</a>\u001b[0m W1np, b1np, W2np, b2np, W3np, b3np \u001b[39m=\u001b[39m param_update(W1np, b1np, W2np, b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr \u001b[39m=\u001b[39m lrNP)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 38\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=47'>48</a>\u001b[0m lossArrayAfterPertZ1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(Z1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Z1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=49'>50</a>\u001b[0m   Z1pert \u001b[39m=\u001b[39m Z1\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=50'>51</a>\u001b[0m   Z1pert[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pert\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=51'>52</a>\u001b[0m   \u001b[39m#print(np.array_equal(Z1, Z1pert))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _ = batchGDComp(x_train,y_train,20, 0.1, 0.5, 0.0000000000001, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainBP = [i[0] for i in trainAccBoth]\n",
    "trainNP = [i[1] for i in trainAccBoth]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainBP, 'rx')\n",
    "plt.plot(trainNP, 'g^')\n",
    "plt.plot(trainBP, 'r', linewidth=5,alpha=0.3)\n",
    "plt.plot(trainNP, 'g', linewidth=5,alpha=0.3)\n",
    "plt.legend([\"Back propagation\", \"Node Perturbation\"])\n",
    "plt.title(\"Accuracy vs epochs\", size=15)\n",
    "plt.xlabel(\"Epochs ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "0.9999999999999997\n",
      "Iteration: 1::Train accuracy: 9.846031746031745::Val accuracy: 9.971428571428572::Train Acc BP::9.844444444444445 Val Acc BP::9.185714285714287\n",
      "0.9999995164409428\n",
      "Iteration: 2::Train accuracy: 9.974603174603175::Val accuracy: 10.085714285714285::Train Acc BP::11.007936507936508 Val Acc BP::10.4\n",
      "0.9999985844600019\n",
      "Iteration: 3::Train accuracy: 10.096825396825396::Val accuracy: 10.4::Train Acc BP::12.304761904761904 Val Acc BP::11.942857142857143\n",
      "0.9999975219493955\n",
      "Iteration: 4::Train accuracy: 10.217460317460317::Val accuracy: 10.585714285714285::Train Acc BP::13.561904761904762 Val Acc BP::13.385714285714286\n",
      "0.9999965073297103\n",
      "Iteration: 5::Train accuracy: 10.353968253968254::Val accuracy: 10.557142857142857::Train Acc BP::14.844444444444443 Val Acc BP::14.657142857142858\n",
      "0.9999955591551105\n",
      "Iteration: 6::Train accuracy: 10.47936507936508::Val accuracy: 10.614285714285714::Train Acc BP::15.946031746031746 Val Acc BP::15.814285714285713\n",
      "0.9999946449000549\n",
      "Iteration: 7::Train accuracy: 10.617460317460317::Val accuracy: 10.757142857142856::Train Acc BP::17.114285714285714 Val Acc BP::17.085714285714285\n",
      "0.9999937375289455\n",
      "Iteration: 8::Train accuracy: 10.769841269841269::Val accuracy: 11.042857142857143::Train Acc BP::18.252380952380953 Val Acc BP::18.0\n",
      "0.9999928258591593\n",
      "Iteration: 9::Train accuracy: 10.96031746031746::Val accuracy: 11.314285714285715::Train Acc BP::19.28095238095238 Val Acc BP::19.3\n",
      "0.9999919064576464\n",
      "Iteration: 10::Train accuracy: 11.192063492063491::Val accuracy: 11.514285714285714::Train Acc BP::20.247619047619047 Val Acc BP::20.3\n",
      "0.9999909854383202\n",
      "Iteration: 11::Train accuracy: 11.420634920634921::Val accuracy: 11.600000000000001::Train Acc BP::21.22222222222222 Val Acc BP::21.585714285714285\n",
      "0.9999900701356013\n",
      "Iteration: 12::Train accuracy: 11.65079365079365::Val accuracy: 11.714285714285715::Train Acc BP::22.174603174603174 Val Acc BP::22.7\n",
      "0.9999891657135772\n",
      "Iteration: 13::Train accuracy: 11.912698412698413::Val accuracy: 11.899999999999999::Train Acc BP::23.153968253968255 Val Acc BP::23.614285714285714\n",
      "0.9999882744471709\n",
      "Iteration: 14::Train accuracy: 12.115873015873015::Val accuracy: 12.0::Train Acc BP::24.08730158730159 Val Acc BP::24.6\n",
      "0.9999873954286769\n",
      "Iteration: 15::Train accuracy: 12.268253968253969::Val accuracy: 12.2::Train Acc BP::25.023809523809526 Val Acc BP::25.585714285714285\n",
      "0.9999865263059154\n",
      "Iteration: 16::Train accuracy: 12.457142857142857::Val accuracy: 12.385714285714286::Train Acc BP::25.98412698412698 Val Acc BP::26.385714285714286\n",
      "0.9999856644582005\n",
      "Iteration: 17::Train accuracy: 12.598412698412698::Val accuracy: 12.514285714285714::Train Acc BP::26.81269841269841 Val Acc BP::27.400000000000002\n",
      "0.9999848086325619\n",
      "Iteration: 18::Train accuracy: 12.760317460317461::Val accuracy: 12.614285714285714::Train Acc BP::27.750793650793646 Val Acc BP::28.199999999999996\n",
      "0.9999839573215036\n",
      "Iteration: 19::Train accuracy: 12.917460317460316::Val accuracy: 12.757142857142856::Train Acc BP::28.61269841269841 Val Acc BP::28.985714285714288\n",
      "0.9999831105250694\n",
      "Iteration: 20::Train accuracy: 13.080952380952382::Val accuracy: 13.0::Train Acc BP::29.44126984126984 Val Acc BP::29.814285714285717\n",
      "0.9999822680356802\n",
      "Iteration: 21::Train accuracy: 13.206349206349207::Val accuracy: 13.17142857142857::Train Acc BP::30.319047619047616 Val Acc BP::30.7\n",
      "0.9999814288380513\n",
      "Iteration: 22::Train accuracy: 13.37936507936508::Val accuracy: 13.242857142857142::Train Acc BP::31.17936507936508 Val Acc BP::31.514285714285716\n",
      "0.999980594434652\n",
      "Iteration: 23::Train accuracy: 13.476190476190474::Val accuracy: 13.428571428571429::Train Acc BP::31.998412698412697 Val Acc BP::32.142857142857146\n",
      "0.9999797662027383\n",
      "Iteration: 24::Train accuracy: 13.587301587301587::Val accuracy: 13.571428571428571::Train Acc BP::32.768253968253966 Val Acc BP::32.9\n",
      "0.9999789425223171\n",
      "Iteration: 25::Train accuracy: 13.674603174603176::Val accuracy: 13.742857142857142::Train Acc BP::33.5984126984127 Val Acc BP::33.72857142857143\n",
      "0.9999781252683322\n",
      "Iteration: 26::Train accuracy: 13.803174603174604::Val accuracy: 13.87142857142857::Train Acc BP::34.336507936507935 Val Acc BP::34.31428571428572\n",
      "0.999977314945538\n",
      "Iteration: 27::Train accuracy: 13.922222222222222::Val accuracy: 13.942857142857143::Train Acc BP::35.09206349206349 Val Acc BP::35.12857142857143\n",
      "0.9999765119253564\n",
      "Iteration: 28::Train accuracy: 14.026984126984127::Val accuracy: 14.028571428571428::Train Acc BP::35.84444444444445 Val Acc BP::35.84285714285714\n",
      "0.999975716400569\n",
      "Iteration: 29::Train accuracy: 14.126984126984127::Val accuracy: 14.099999999999998::Train Acc BP::36.56507936507937 Val Acc BP::36.542857142857144\n",
      "0.9999749283161928\n",
      "Iteration: 30::Train accuracy: 14.238095238095239::Val accuracy: 14.2::Train Acc BP::37.161904761904765 Val Acc BP::37.128571428571426\n",
      "0.9999741475838191\n",
      "Iteration: 31::Train accuracy: 14.355555555555554::Val accuracy: 14.314285714285715::Train Acc BP::37.83015873015873 Val Acc BP::37.81428571428572\n",
      "0.9999733747001303\n",
      "Iteration: 32::Train accuracy: 14.463492063492064::Val accuracy: 14.371428571428572::Train Acc BP::38.477777777777774 Val Acc BP::38.32857142857143\n",
      "0.9999726096738659\n",
      "Iteration: 33::Train accuracy: 14.561904761904762::Val accuracy: 14.457142857142857::Train Acc BP::39.10634920634921 Val Acc BP::38.971428571428575\n",
      "0.9999718536795815\n",
      "Iteration: 34::Train accuracy: 14.63968253968254::Val accuracy: 14.499999999999998::Train Acc BP::39.75396825396825 Val Acc BP::39.614285714285714\n",
      "0.9999711057358021\n",
      "Iteration: 35::Train accuracy: 14.746031746031745::Val accuracy: 14.557142857142857::Train Acc BP::40.34285714285714 Val Acc BP::40.05714285714286\n",
      "0.9999703651328559\n",
      "Iteration: 36::Train accuracy: 14.853968253968253::Val accuracy: 14.657142857142858::Train Acc BP::40.87936507936508 Val Acc BP::40.68571428571428\n",
      "0.9999696329784439\n",
      "Iteration: 37::Train accuracy: 14.947619047619048::Val accuracy: 14.771428571428572::Train Acc BP::41.474603174603175 Val Acc BP::41.199999999999996\n",
      "0.999968908798597\n",
      "Iteration: 38::Train accuracy: 15.03015873015873::Val accuracy: 14.857142857142858::Train Acc BP::42.03650793650794 Val Acc BP::41.72857142857143\n",
      "0.9999681927068615\n",
      "Iteration: 39::Train accuracy: 15.10793650793651::Val accuracy: 14.985714285714286::Train Acc BP::42.592063492063495 Val Acc BP::42.35714285714286\n",
      "0.999967484938454\n",
      "Iteration: 40::Train accuracy: 15.214285714285714::Val accuracy: 15.1::Train Acc BP::43.13492063492064 Val Acc BP::43.042857142857144\n",
      "0.9999667850215108\n",
      "Iteration: 41::Train accuracy: 15.306349206349207::Val accuracy: 15.228571428571428::Train Acc BP::43.67936507936508 Val Acc BP::43.528571428571425\n",
      "0.9999660927731331\n",
      "Iteration: 42::Train accuracy: 15.403174603174602::Val accuracy: 15.285714285714286::Train Acc BP::44.13492063492063 Val Acc BP::43.885714285714286\n",
      "0.9999654079231636\n",
      "Iteration: 43::Train accuracy: 15.507936507936506::Val accuracy: 15.385714285714286::Train Acc BP::44.641269841269846 Val Acc BP::44.285714285714285\n",
      "0.9999647307105496\n",
      "Iteration: 44::Train accuracy: 15.6::Val accuracy: 15.442857142857141::Train Acc BP::45.141269841269846 Val Acc BP::44.871428571428574\n",
      "0.9999640606468017\n",
      "Iteration: 45::Train accuracy: 15.711111111111112::Val accuracy: 15.571428571428573::Train Acc BP::45.63809523809524 Val Acc BP::45.385714285714286\n",
      "0.9999633973773256\n",
      "Iteration: 46::Train accuracy: 15.807936507936507::Val accuracy: 15.714285714285714::Train Acc BP::46.10634920634921 Val Acc BP::45.77142857142857\n",
      "0.9999627415245088\n",
      "Iteration: 47::Train accuracy: 15.898412698412697::Val accuracy: 15.771428571428572::Train Acc BP::46.56984126984127 Val Acc BP::46.41428571428571\n",
      "0.9999620928307383\n",
      "Iteration: 48::Train accuracy: 15.971428571428573::Val accuracy: 15.871428571428572::Train Acc BP::47.08571428571429 Val Acc BP::46.84285714285714\n",
      "0.9999614504288715\n",
      "Iteration: 49::Train accuracy: 16.06190476190476::Val accuracy: 15.928571428571429::Train Acc BP::47.50793650793651 Val Acc BP::47.385714285714286\n",
      "0.9999608156836131\n",
      "Iteration: 50::Train accuracy: 16.157142857142855::Val accuracy: 16.057142857142857::Train Acc BP::47.923809523809524 Val Acc BP::47.699999999999996\n",
      "0.9999601876799649\n",
      "Iteration: 51::Train accuracy: 16.255555555555556::Val accuracy: 16.114285714285714::Train Acc BP::48.32857142857143 Val Acc BP::48.142857142857146\n",
      "0.9999595663700823\n",
      "Iteration: 52::Train accuracy: 16.347619047619048::Val accuracy: 16.214285714285715::Train Acc BP::48.67936507936508 Val Acc BP::48.528571428571425\n",
      "0.9999589509619153\n",
      "Iteration: 53::Train accuracy: 16.466666666666665::Val accuracy: 16.242857142857144::Train Acc BP::49.047619047619044 Val Acc BP::48.91428571428572\n",
      "0.9999583424616668\n",
      "Iteration: 54::Train accuracy: 16.553968253968254::Val accuracy: 16.314285714285713::Train Acc BP::49.42857142857143 Val Acc BP::49.371428571428574\n",
      "0.9999577407988471\n",
      "Iteration: 55::Train accuracy: 16.63174603174603::Val accuracy: 16.37142857142857::Train Acc BP::49.834920634920636 Val Acc BP::49.72857142857143\n",
      "0.9999571454566911\n",
      "Iteration: 56::Train accuracy: 16.71904761904762::Val accuracy: 16.428571428571427::Train Acc BP::50.14761904761905 Val Acc BP::50.18571428571429\n",
      "0.9999565562271636\n",
      "Iteration: 57::Train accuracy: 16.825396825396826::Val accuracy: 16.52857142857143::Train Acc BP::50.4984126984127 Val Acc BP::50.6\n",
      "0.9999559728266323\n",
      "Iteration: 58::Train accuracy: 16.914285714285715::Val accuracy: 16.685714285714287::Train Acc BP::50.828571428571436 Val Acc BP::50.957142857142856\n",
      "0.9999553948606779\n",
      "Iteration: 59::Train accuracy: 17.02063492063492::Val accuracy: 16.8::Train Acc BP::51.16666666666667 Val Acc BP::51.37142857142857\n",
      "0.9999548227235879\n",
      "Iteration: 60::Train accuracy: 17.125396825396823::Val accuracy: 16.957142857142856::Train Acc BP::51.487301587301594 Val Acc BP::51.67142857142857\n",
      "0.9999542562479857\n",
      "Iteration: 61::Train accuracy: 17.201587301587303::Val accuracy: 17.057142857142857::Train Acc BP::51.82222222222222 Val Acc BP::51.97142857142857\n",
      "0.9999536955407411\n",
      "Iteration: 62::Train accuracy: 17.287301587301588::Val accuracy: 17.12857142857143::Train Acc BP::52.15396825396825 Val Acc BP::52.300000000000004\n",
      "0.9999531413684388\n",
      "Iteration: 63::Train accuracy: 17.377777777777776::Val accuracy: 17.15714285714286::Train Acc BP::52.46349206349207 Val Acc BP::52.55714285714286\n",
      "0.9999525924454208\n",
      "Iteration: 64::Train accuracy: 17.44761904761905::Val accuracy: 17.228571428571428::Train Acc BP::52.77460317460317 Val Acc BP::52.900000000000006\n",
      "0.9999520489978656\n",
      "Iteration: 65::Train accuracy: 17.51904761904762::Val accuracy: 17.285714285714285::Train Acc BP::53.076190476190476 Val Acc BP::53.25714285714286\n",
      "0.999951511083581\n",
      "Iteration: 66::Train accuracy: 17.584126984126986::Val accuracy: 17.299999999999997::Train Acc BP::53.41428571428571 Val Acc BP::53.614285714285714\n",
      "0.9999509780341992\n",
      "Iteration: 67::Train accuracy: 17.679365079365077::Val accuracy: 17.457142857142856::Train Acc BP::53.71111111111111 Val Acc BP::53.957142857142856\n",
      "0.9999504498641594\n",
      "Iteration: 68::Train accuracy: 17.744444444444444::Val accuracy: 17.5::Train Acc BP::53.98412698412698 Val Acc BP::54.22857142857143\n",
      "0.9999499262870707\n",
      "Iteration: 69::Train accuracy: 17.814285714285713::Val accuracy: 17.57142857142857::Train Acc BP::54.27301587301587 Val Acc BP::54.45714285714286\n",
      "0.999949408067556\n",
      "Iteration: 70::Train accuracy: 17.90793650793651::Val accuracy: 17.757142857142856::Train Acc BP::54.4984126984127 Val Acc BP::54.74285714285714\n",
      "0.9999488943777787\n",
      "Iteration: 71::Train accuracy: 17.995238095238093::Val accuracy: 17.87142857142857::Train Acc BP::54.72857142857143 Val Acc BP::55.08571428571428\n",
      "0.9999483853001079\n",
      "Iteration: 72::Train accuracy: 18.073015873015873::Val accuracy: 17.92857142857143::Train Acc BP::54.96825396825397 Val Acc BP::55.35714285714286\n",
      "0.9999478810652096\n",
      "Iteration: 73::Train accuracy: 18.152380952380952::Val accuracy: 18.0::Train Acc BP::55.20952380952381 Val Acc BP::55.614285714285714\n",
      "0.9999473813761124\n",
      "Iteration: 74::Train accuracy: 18.226984126984128::Val accuracy: 18.085714285714285::Train Acc BP::55.46984126984127 Val Acc BP::55.92857142857143\n",
      "0.9999468858518142\n",
      "Iteration: 75::Train accuracy: 18.29206349206349::Val accuracy: 18.142857142857142::Train Acc BP::55.76190476190476 Val Acc BP::56.24285714285714\n",
      "0.9999463948937064\n",
      "Iteration: 76::Train accuracy: 18.376190476190477::Val accuracy: 18.242857142857144::Train Acc BP::55.980952380952374 Val Acc BP::56.51428571428572\n",
      "0.9999459085023812\n",
      "Iteration: 77::Train accuracy: 18.455555555555556::Val accuracy: 18.357142857142858::Train Acc BP::56.27619047619048 Val Acc BP::56.72857142857143\n",
      "0.999945425648125\n",
      "Iteration: 78::Train accuracy: 18.530158730158732::Val accuracy: 18.385714285714286::Train Acc BP::56.4968253968254 Val Acc BP::56.99999999999999\n",
      "0.9999449471112105\n",
      "Iteration: 79::Train accuracy: 18.604761904761904::Val accuracy: 18.414285714285715::Train Acc BP::56.73809523809524 Val Acc BP::57.32857142857143\n",
      "0.9999444722440929\n",
      "Iteration: 80::Train accuracy: 18.66190476190476::Val accuracy: 18.45714285714286::Train Acc BP::56.938095238095244 Val Acc BP::57.61428571428572\n",
      "0.999944001361166\n",
      "Iteration: 81::Train accuracy: 18.73333333333333::Val accuracy: 18.52857142857143::Train Acc BP::57.16031746031746 Val Acc BP::57.8\n",
      "0.9999435347080066\n",
      "Iteration: 82::Train accuracy: 18.788888888888888::Val accuracy: 18.6::Train Acc BP::57.38412698412698 Val Acc BP::58.099999999999994\n",
      "0.9999430719304538\n",
      "Iteration: 83::Train accuracy: 18.86031746031746::Val accuracy: 18.628571428571426::Train Acc BP::57.5968253968254 Val Acc BP::58.32857142857143\n",
      "0.9999426125479574\n",
      "Iteration: 84::Train accuracy: 18.93015873015873::Val accuracy: 18.7::Train Acc BP::57.784126984126985 Val Acc BP::58.471428571428575\n",
      "0.9999421570189035\n",
      "Iteration: 85::Train accuracy: 19.001587301587303::Val accuracy: 18.771428571428572::Train Acc BP::58.025396825396825 Val Acc BP::58.81428571428572\n",
      "0.9999417050961348\n",
      "Iteration: 86::Train accuracy: 19.076190476190476::Val accuracy: 18.871428571428574::Train Acc BP::58.219047619047615 Val Acc BP::58.98571428571429\n",
      "0.9999412567016195\n",
      "Iteration: 87::Train accuracy: 19.14920634920635::Val accuracy: 18.95714285714286::Train Acc BP::58.439682539682536 Val Acc BP::59.15714285714285\n",
      "0.9999408117134039\n",
      "Iteration: 88::Train accuracy: 19.233333333333334::Val accuracy: 19.0::Train Acc BP::58.6031746031746 Val Acc BP::59.357142857142854\n",
      "0.999940369872262\n",
      "Iteration: 89::Train accuracy: 19.303174603174604::Val accuracy: 19.02857142857143::Train Acc BP::58.7952380952381 Val Acc BP::59.55714285714285\n",
      "0.9999399308941685\n",
      "Iteration: 90::Train accuracy: 19.37936507936508::Val accuracy: 19.085714285714285::Train Acc BP::58.99682539682539 Val Acc BP::59.74285714285714\n",
      "0.9999394949886351\n",
      "Iteration: 91::Train accuracy: 19.446031746031746::Val accuracy: 19.157142857142855::Train Acc BP::59.16825396825397 Val Acc BP::59.785714285714285\n",
      "0.9999390622922886\n",
      "Iteration: 92::Train accuracy: 19.523809523809526::Val accuracy: 19.257142857142856::Train Acc BP::59.357142857142854 Val Acc BP::60.014285714285705\n",
      "0.999938632335762\n",
      "Iteration: 93::Train accuracy: 19.58888888888889::Val accuracy: 19.357142857142858::Train Acc BP::59.56825396825397 Val Acc BP::60.21428571428571\n",
      "0.999938205310331\n",
      "Iteration: 94::Train accuracy: 19.63968253968254::Val accuracy: 19.371428571428574::Train Acc BP::59.738095238095234 Val Acc BP::60.385714285714286\n",
      "0.9999377804614717\n",
      "Iteration: 95::Train accuracy: 19.70952380952381::Val accuracy: 19.442857142857143::Train Acc BP::59.91111111111111 Val Acc BP::60.542857142857144\n",
      "0.9999373585399819\n",
      "Iteration: 96::Train accuracy: 19.8015873015873::Val accuracy: 19.485714285714288::Train Acc BP::60.098412698412695 Val Acc BP::60.699999999999996\n",
      "0.9999369390548026\n",
      "Iteration: 97::Train accuracy: 19.877777777777776::Val accuracy: 19.57142857142857::Train Acc BP::60.25873015873016 Val Acc BP::60.871428571428574\n",
      "0.9999365226654353\n",
      "Iteration: 98::Train accuracy: 19.942857142857143::Val accuracy: 19.685714285714287::Train Acc BP::60.42857142857143 Val Acc BP::61.1\n",
      "0.9999361085592147\n",
      "Iteration: 99::Train accuracy: 20.017460317460316::Val accuracy: 19.757142857142856::Train Acc BP::60.592063492063495 Val Acc BP::61.24285714285714\n",
      "0.9999356970216483\n",
      "Iteration: 100::Train accuracy: 20.06984126984127::Val accuracy: 19.8::Train Acc BP::60.73492063492063 Val Acc BP::61.357142857142854\n"
     ]
    }
   ],
   "source": [
    "w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _ = batchGDComp(x_train,y_train,100, 0.0001, 0.000005, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainBP = [i[0] for i in trainAccBoth]\n",
    "trainNP = [i[1] for i in trainAccBoth]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainBP, 'r.')\n",
    "plt.plot(trainBP, 'r', linewidth=5,alpha=0.3)\n",
    "plt.plot(trainNP, 'g^')\n",
    "plt.plot(trainNP, 'g', linewidth=5,alpha=0.3)\n",
    "plt.legend([\"Back propagation\", \"Node Perturbation\"])\n",
    "plt.title(\"Accuracy vs epochs\", size=15)\n",
    "plt.xlabel(\"Epochs ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cosine Similiarity of the NP and BP updates')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(w1Sim)\n",
    "plt.plot(b1Sim)\n",
    "plt.plot(w2Sim)\n",
    "plt.plot(b2Sim)\n",
    "plt.plot(w3Sim)\n",
    "plt.plot(b3Sim)\n",
    "plt.legend([\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"])\n",
    "plt.title(\"Cosine similarity of the weights of NP and BP with the epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cosine Similiarity of the NP and BP updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance of BP and NP with the full variability play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "  #print(\"b3 shape = \",b3.shape, \" A2fp shape = \", A2.shape,\" and W3 shape = \", W3.shape)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDCompOC(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      W1varocnp = weightTransformWithVariability(W1np, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocnp = weightTransformWithVariability(b1np, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocnp = weightTransformWithVariability(W2np, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocnp = weightTransformWithVariability(b2np, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocnp = weightTransformWithVariability(W3np, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocnp = weightTransformWithVariability(b3np, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      #print(W3varocnp.shape)\n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocnp, b1varocnp, W2varocnp,b2varocnp, W3varocnp, b3varocnp) \n",
    "      print(f\"NP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varocnp, W2varocnp, W3varocnp,b1varocnp, b2varocnp, b3varocnp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      dW1varocnp = weightTransformWithVariability(dW1np, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varocnp = weightTransformWithVariability(db1np.reshape(db1np.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varocnp = weightTransformWithVariability(dW2np, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varocnp = weightTransformWithVariability(db2np.reshape(db2np.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varocnp = weightTransformWithVariability(dW3np, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varocnp = weightTransformWithVariability(db3np.reshape(db3np.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1varocnp, b1varocnp, W2varocnp,b2varocnp, W3varocnp, b3varocnp, dW1varocnp, db1varocnp, dW2varocnp, db2varocnp, dW3varocnp, db3varocnp, lr = lrNP)\n",
    "      #print(W1)\n",
    "\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "\n",
    "      W1varocbp = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocbp = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocbp = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocbp = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocbp = weightTransformWithVariability(W3bp, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocbp = weightTransformWithVariability(b3bp, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocbp, b1varocbp, W2varocbp,b2varocbp, W3varocbp, b3varocbp) \n",
    "      print(f\"                                                                        BP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1varocbp, W2varocbp, W3varocbp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      dW1varocbp = weightTransformWithVariability(dW1bp, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varocbp = weightTransformWithVariability(db1bp.reshape(db1bp.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varocbp = weightTransformWithVariability(dW2bp, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varocbp = weightTransformWithVariability(db2bp.reshape(db2bp.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varocbp = weightTransformWithVariability(dW3bp, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varocbp = weightTransformWithVariability(db3bp.reshape(db3bp.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1varocbp, b1varocbp, W2varocbp, b2varocbp, W3varocbp, b3varocbp, dW1varocbp, db1varocbp, dW2varocbp, db2varocbp, dW3varocbp, db3varocbp, lr = lrBP)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    lrNP = lrNP*np.exp(-1)\n",
    "    lrBP = lrBP*np.exp(-0.1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {round(train_score, 3)}::Val accuracy: {round(val_score, 3)}::Train Acc BP::{round(accuracy(predictions(A3_train_bp), Y), 3)} Val Acc BP::{round(accuracy(predictions(A3_val_bp), y_val), 3)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter=10\n",
    "lrBP=0.05\n",
    "lrNP=0.5\n",
    "pert=0.00001\n",
    "mu = 0.7\n",
    "sigma = 0.01\n",
    "vDD = 5\n",
    "precision = 10#setting the precision value of the calculations\n",
    "wRange = 3\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization\n",
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "1.0\n",
      "Iteration: 1::Train accuracy: 83.202::Val accuracy: 82.443::Train Acc BP::73.298 Val Acc BP::72.8: 73.015873015873015\n",
      "0.9785838609145416\n",
      "Iteration: 2::Train accuracy: 79.692::Val accuracy: 78.757::Train Acc BP::69.163 Val Acc BP::68.64370.79365079365089\n",
      "0.9609362444239804\n",
      "Iteration: 3::Train accuracy: 71.957::Val accuracy: 71.657::Train Acc BP::61.19 Val Acc BP::60.171 61.587301587301595\n",
      "0.9480933813322582\n",
      "Iteration: 4::Train accuracy: 58.235::Val accuracy: 58.7::Train Acc BP::55.235 Val Acc BP::54.429: 55.873015873015874\n",
      "0.9368475799652576\n",
      "Iteration: 5::Train accuracy: 46.732::Val accuracy: 47.686::Train Acc BP::48.032 Val Acc BP::48.24348.095238095238095\n",
      "0.926332755112817\n",
      "NP Iter 5 -> sub iter 13 : 44.920634920634924                           BP Iter 5 -> sub iter 12 : 48.412698412698414\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=0'>1</a>\u001b[0m batchGDCompOC(x_train,y_train,\u001b[39miter\u001b[39;49m, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 42\u001b[0m in \u001b[0;36mbatchGDCompOC\u001b[1;34m(X, Y, iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=86'>87</a>\u001b[0m dW1varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(dW1np, dW1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=87'>88</a>\u001b[0m db1varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(db1np\u001b[39m.\u001b[39mreshape(db1np\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m), db1Currents, precision, step, discreteSteps)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=88'>89</a>\u001b[0m dW2varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(dW2np, dW2Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=89'>90</a>\u001b[0m db2varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(db2np\u001b[39m.\u001b[39mreshape(db2np\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m), db2Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=90'>91</a>\u001b[0m dW3varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(dW3np, dW3Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 42\u001b[0m in \u001b[0;36mweightTransformWithVariability\u001b[1;34m(weightArray, currents, precision, step, discreteSteps)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=1'>2</a>\u001b[0m dim1, dim2 \u001b[39m=\u001b[39m weightArray\u001b[39m.\u001b[39mshape\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=2'>3</a>\u001b[0m sizeI \u001b[39m=\u001b[39m (dim1, dim2, precision)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=4'>5</a>\u001b[0m clippedWeightIndexArray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdigitize(np\u001b[39m.\u001b[39;49mabs(weightArray), discreteSteps) \u001b[39m#finds the index value of the weights\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=6'>7</a>\u001b[0m \u001b[39m#vDD = 5\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=7'>8</a>\u001b[0m \u001b[39m#mu = 0.7#mean of the distribution\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=8'>9</a>\u001b[0m \u001b[39m#sigma = 0.00001\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=9'>10</a>\u001b[0m \u001b[39m#! work with sigma/mu\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=11'>12</a>\u001b[0m iOn, iOnNominal, iOff \u001b[39m=\u001b[39m currents\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\lib\\function_base.py:5555\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   5553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(bins) \u001b[39m-\u001b[39m _nx\u001b[39m.\u001b[39msearchsorted(bins[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], x, side\u001b[39m=\u001b[39mside)\n\u001b[0;32m   5554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 5555\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49msearchsorted(bins, x, side\u001b[39m=\u001b[39;49mside)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:1387\u001b[0m, in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[0;32m   1320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearchsorted\u001b[39m(a, v, side\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, sorter\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1321\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m \u001b[39m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \n\u001b[0;32m   1386\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39msearchsorted\u001b[39;49m\u001b[39m'\u001b[39;49m, v, side\u001b[39m=\u001b[39;49mside, sorter\u001b[39m=\u001b[39;49msorter)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batchGDCompOC(x_train,y_train,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
