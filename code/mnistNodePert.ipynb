{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init(seed):\n",
    "\n",
    "  np.random.seed(seed)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  #print(\"BP \\n \", dZ3)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z1\")\n",
    "    #print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "def batchGDNP(X,Y,iter, lr, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "      #print(W1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsToTrain = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertList = [1,0.1, 0.01, 0.001, 0.0001]\n",
    "trainAccPertList = []\n",
    "valAccPertList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 69.841269841269834\n",
      "Iteration: 1\n",
      "Train accuracy: 70.33809523809524\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 1 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 2\n",
      "Train accuracy: 79.0079365079365\n",
      "Val accuracy: 78.08571428571427\n",
      "Iter 2 -> sub iter 99 : 82.69841269841271\n",
      "Iteration: 3\n",
      "Train accuracy: 82.1079365079365\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 3 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 4\n",
      "Train accuracy: 84.05238095238096\n",
      "Val accuracy: 83.52857142857142\n",
      "Iter 4 -> sub iter 99 : 85.23809523809524\n",
      "Iteration: 5\n",
      "Train accuracy: 85.35396825396825\n",
      "Val accuracy: 84.88571428571429\n",
      "Iter 5 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 6\n",
      "Train accuracy: 86.41904761904762\n",
      "Val accuracy: 85.97142857142858\n",
      "Iter 6 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 7\n",
      "Train accuracy: 87.17777777777778\n",
      "Val accuracy: 86.62857142857143\n",
      "Iter 7 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 8\n",
      "Train accuracy: 87.86825396825397\n",
      "Val accuracy: 87.41428571428571\n",
      "Iter 8 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 9\n",
      "Train accuracy: 88.39047619047619\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 9 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 10\n",
      "Train accuracy: 88.84761904761905\n",
      "Val accuracy: 88.25714285714285\n",
      "Iter 10 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 11\n",
      "Train accuracy: 89.2095238095238\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 11 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 12\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 12 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 13\n",
      "Train accuracy: 89.89047619047619\n",
      "Val accuracy: 89.4\n",
      "Iter 13 -> sub iter 99 : 90.01111111111111\n",
      "Iteration: 14\n",
      "Train accuracy: 90.15555555555555\n",
      "Val accuracy: 89.64285714285715\n",
      "Iter 14 -> sub iter 99 : 90.08730158730158\n",
      "Iteration: 15\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.84285714285714\n",
      "Iter 15 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 16\n",
      "Train accuracy: 90.62857142857142\n",
      "Val accuracy: 90.17142857142856\n",
      "Iter 16 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 17\n",
      "Train accuracy: 90.86349206349207\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 17 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 18\n",
      "Train accuracy: 91.09047619047618\n",
      "Val accuracy: 90.64285714285715\n",
      "Iter 18 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 19\n",
      "Train accuracy: 91.25079365079365\n",
      "Val accuracy: 90.8\n",
      "Iter 19 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 20\n",
      "Train accuracy: 91.42857142857143\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 20 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 21\n",
      "Train accuracy: 91.58730158730158\n",
      "Val accuracy: 91.01428571428572\n",
      "Iter 21 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 22\n",
      "Train accuracy: 91.73174603174603\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 22 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 23\n",
      "Train accuracy: 91.87777777777778\n",
      "Val accuracy: 91.42857142857143\n",
      "Iter 23 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 24\n",
      "Train accuracy: 91.99841269841271\n",
      "Val accuracy: 91.41428571428571\n",
      "Iter 24 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 25\n",
      "Train accuracy: 92.0984126984127\n",
      "Val accuracy: 91.54285714285714\n",
      "Iter 25 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 26\n",
      "Train accuracy: 92.2031746031746\n",
      "Val accuracy: 91.60000000000001\n",
      "Iter 26 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 27\n",
      "Train accuracy: 92.3079365079365\n",
      "Val accuracy: 91.75714285714285\n",
      "Iter 27 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 28\n",
      "Train accuracy: 92.41428571428571\n",
      "Val accuracy: 91.88571428571429\n",
      "Iter 28 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 29\n",
      "Train accuracy: 92.5063492063492\n",
      "Val accuracy: 92.01428571428572\n",
      "Iter 29 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 30\n",
      "Train accuracy: 92.57619047619048\n",
      "Val accuracy: 92.02857142857142\n",
      "Iter 30 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 31\n",
      "Train accuracy: 92.67301587301587\n",
      "Val accuracy: 92.12857142857143\n",
      "Iter 31 -> sub iter 99 : 91.90476190476191\n",
      "Iteration: 32\n",
      "Train accuracy: 92.74761904761904\n",
      "Val accuracy: 92.14285714285714\n",
      "Iter 32 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 33\n",
      "Train accuracy: 92.83809523809524\n",
      "Val accuracy: 92.21428571428572\n",
      "Iter 33 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 34\n",
      "Train accuracy: 92.93174603174603\n",
      "Val accuracy: 92.34285714285714\n",
      "Iter 34 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 35\n",
      "Train accuracy: 92.9968253968254\n",
      "Val accuracy: 92.4\n",
      "Iter 35 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 36\n",
      "Train accuracy: 93.06349206349206\n",
      "Val accuracy: 92.5\n",
      "Iter 36 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 37\n",
      "Train accuracy: 93.12222222222222\n",
      "Val accuracy: 92.51428571428572\n",
      "Iter 37 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 38\n",
      "Train accuracy: 93.16349206349206\n",
      "Val accuracy: 92.58571428571429\n",
      "Iter 38 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 39\n",
      "Train accuracy: 93.2047619047619\n",
      "Val accuracy: 92.62857142857143\n",
      "Iter 39 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 40\n",
      "Train accuracy: 93.25873015873016\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 40 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 41\n",
      "Train accuracy: 93.32222222222222\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 41 -> sub iter 99 : 92.69841269841273\n",
      "Iteration: 42\n",
      "Train accuracy: 93.38253968253967\n",
      "Val accuracy: 92.77142857142857\n",
      "Iter 42 -> sub iter 99 : 93.01587301587301\n",
      "Iteration: 43\n",
      "Train accuracy: 93.42857142857143\n",
      "Val accuracy: 92.85714285714286\n",
      "Iter 43 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 44\n",
      "Train accuracy: 93.4888888888889\n",
      "Val accuracy: 92.95714285714286\n",
      "Iter 44 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 45\n",
      "Train accuracy: 93.55079365079365\n",
      "Val accuracy: 93.05714285714286\n",
      "Iter 45 -> sub iter 99 : 93.49206349206356\n",
      "Iteration: 46\n",
      "Train accuracy: 93.6015873015873\n",
      "Val accuracy: 93.12857142857143\n",
      "Iter 46 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 47\n",
      "Train accuracy: 93.66507936507936\n",
      "Val accuracy: 93.18571428571428\n",
      "Iter 47 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 48\n",
      "Train accuracy: 93.72222222222221\n",
      "Val accuracy: 93.22857142857143\n",
      "Iter 48 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 49\n",
      "Train accuracy: 93.76984126984127\n",
      "Val accuracy: 93.27142857142857\n",
      "Iter 49 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 50\n",
      "Train accuracy: 93.83968253968254\n",
      "Val accuracy: 93.31428571428572\n",
      "Iter 50 -> sub iter 99 : 94.12698412698413\n",
      "Iteration: 51\n",
      "Train accuracy: 93.88571428571429\n",
      "Val accuracy: 93.32857142857142\n",
      "Iter 51 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 93.92380952380952\n",
      "Val accuracy: 93.34285714285714\n",
      "Iter 52 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 53\n",
      "Train accuracy: 93.97777777777779\n",
      "Val accuracy: 93.38571428571429\n",
      "Iter 53 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 54\n",
      "Train accuracy: 94.01428571428572\n",
      "Val accuracy: 93.41428571428571\n",
      "Iter 54 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 55\n",
      "Train accuracy: 94.07142857142857\n",
      "Val accuracy: 93.4\n",
      "Iter 55 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 56\n",
      "Train accuracy: 94.12063492063491\n",
      "Val accuracy: 93.45714285714286\n",
      "Iter 56 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 57\n",
      "Train accuracy: 94.13650793650794\n",
      "Val accuracy: 93.47142857142858\n",
      "Iter 57 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 58\n",
      "Train accuracy: 94.16349206349206\n",
      "Val accuracy: 93.54285714285714\n",
      "Iter 58 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 59\n",
      "Train accuracy: 94.22222222222221\n",
      "Val accuracy: 93.58571428571429\n",
      "Iter 59 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 60\n",
      "Train accuracy: 94.24285714285713\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 60 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 61\n",
      "Train accuracy: 94.28412698412698\n",
      "Val accuracy: 93.57142857142857\n",
      "Iter 61 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 62\n",
      "Train accuracy: 94.33174603174604\n",
      "Val accuracy: 93.61428571428571\n",
      "Iter 62 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 63\n",
      "Train accuracy: 94.4031746031746\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 63 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 64\n",
      "Train accuracy: 94.43492063492064\n",
      "Val accuracy: 93.62857142857143\n",
      "Iter 64 -> sub iter 99 : 94.60317460317463\n",
      "Iteration: 65\n",
      "Train accuracy: 94.46190476190476\n",
      "Val accuracy: 93.64285714285714\n",
      "Iter 65 -> sub iter 99 : 94.60317460317467\n",
      "Iteration: 66\n",
      "Train accuracy: 94.4888888888889\n",
      "Val accuracy: 93.68571428571428\n",
      "Iter 66 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 94.51587301587303\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 67 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 68\n",
      "Train accuracy: 94.54444444444444\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 68 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 69\n",
      "Train accuracy: 94.57301587301588\n",
      "Val accuracy: 93.8\n",
      "Iter 69 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 70\n",
      "Train accuracy: 94.61587301587302\n",
      "Val accuracy: 93.8\n",
      "Iter 70 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 71\n",
      "Train accuracy: 94.63492063492063\n",
      "Val accuracy: 93.84285714285714\n",
      "Iter 71 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 72\n",
      "Train accuracy: 94.65396825396826\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 72 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 73\n",
      "Train accuracy: 94.67301587301587\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 73 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 74\n",
      "Train accuracy: 94.71269841269842\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 74 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 75\n",
      "Train accuracy: 94.74126984126984\n",
      "Val accuracy: 93.91428571428571\n",
      "Iter 75 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 76\n",
      "Train accuracy: 94.77777777777779\n",
      "Val accuracy: 93.92857142857143\n",
      "Iter 76 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 77\n",
      "Train accuracy: 94.80952380952381\n",
      "Val accuracy: 93.95714285714286\n",
      "Iter 77 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 78\n",
      "Train accuracy: 94.82698412698413\n",
      "Val accuracy: 93.98571428571428\n",
      "Iter 78 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 79\n",
      "Train accuracy: 94.83968253968254\n",
      "Val accuracy: 94.01428571428572\n",
      "Iter 79 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 80\n",
      "Train accuracy: 94.86825396825397\n",
      "Val accuracy: 94.04285714285714\n",
      "Iter 80 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 81\n",
      "Train accuracy: 94.89999999999999\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 81 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 82\n",
      "Train accuracy: 94.92063492063491\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 82 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 83\n",
      "Train accuracy: 94.94920634920635\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 83 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 84\n",
      "Train accuracy: 94.98253968253968\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 84 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 85\n",
      "Train accuracy: 95.0031746031746\n",
      "Val accuracy: 94.1\n",
      "Iter 85 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 86\n",
      "Train accuracy: 95.03174603174604\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 86 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 87\n",
      "Train accuracy: 95.06190476190476\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 87 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 88\n",
      "Train accuracy: 95.09206349206349\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 88 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 89\n",
      "Train accuracy: 95.11904761904762\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 89 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 90\n",
      "Train accuracy: 95.13174603174603\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 90 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 91\n",
      "Train accuracy: 95.18095238095238\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 91 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 92\n",
      "Train accuracy: 95.21111111111111\n",
      "Val accuracy: 94.15714285714286\n",
      "Iter 92 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 93\n",
      "Train accuracy: 95.24603174603175\n",
      "Val accuracy: 94.18571428571428\n",
      "Iter 93 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 94\n",
      "Train accuracy: 95.25396825396825\n",
      "Val accuracy: 94.21428571428572\n",
      "Iter 94 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 95\n",
      "Train accuracy: 95.3\n",
      "Val accuracy: 94.28571428571428\n",
      "Iter 95 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 96\n",
      "Train accuracy: 95.31904761904761\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 96 -> sub iter 99 : 95.39682539682549\n",
      "Iteration: 97\n",
      "Train accuracy: 95.33015873015873\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 97 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 98\n",
      "Train accuracy: 95.34444444444445\n",
      "Val accuracy: 94.22857142857143\n",
      "Iter 98 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 99\n",
      "Train accuracy: 95.36666666666666\n",
      "Val accuracy: 94.3\n",
      "Iter 99 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 100\n",
      "Train accuracy: 95.3920634920635\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 100 -> sub iter 99 : 95.71428571428572\n",
      "Iteration: 101\n",
      "Train accuracy: 95.41428571428571\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 101 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 102\n",
      "Train accuracy: 95.42222222222222\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 102 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 103\n",
      "Train accuracy: 95.43333333333334\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 103 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 104\n",
      "Train accuracy: 95.46507936507936\n",
      "Val accuracy: 94.38571428571429\n",
      "Iter 104 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 105\n",
      "Train accuracy: 95.47936507936508\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 105 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 106\n",
      "Train accuracy: 95.5079365079365\n",
      "Val accuracy: 94.39999999999999\n",
      "Iter 106 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 107\n",
      "Train accuracy: 95.51587301587303\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 107 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 108\n",
      "Train accuracy: 95.52857142857142\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 108 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 109\n",
      "Train accuracy: 95.54126984126984\n",
      "Val accuracy: 94.42857142857143\n",
      "Iter 109 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 110\n",
      "Train accuracy: 95.56031746031746\n",
      "Val accuracy: 94.47142857142858\n",
      "Iter 110 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 111\n",
      "Train accuracy: 95.55873015873016\n",
      "Val accuracy: 94.48571428571428\n",
      "Iter 111 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 112\n",
      "Train accuracy: 95.57460317460318\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 112 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 113\n",
      "Train accuracy: 95.6047619047619\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 113 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 114\n",
      "Train accuracy: 95.62380952380953\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 114 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 115\n",
      "Train accuracy: 95.63968253968254\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 115 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 116\n",
      "Train accuracy: 95.65079365079366\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 116 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 117\n",
      "Train accuracy: 95.67142857142858\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 117 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 118\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 118 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 119\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 119 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 120\n",
      "Train accuracy: 95.71111111111111\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 120 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 121\n",
      "Train accuracy: 95.72063492063492\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 121 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 122\n",
      "Train accuracy: 95.72539682539683\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 122 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 123\n",
      "Train accuracy: 95.73174603174604\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 123 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 124\n",
      "Train accuracy: 95.74920634920635\n",
      "Val accuracy: 94.6\n",
      "Iter 124 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 125\n",
      "Train accuracy: 95.74603174603175\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 125 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 126\n",
      "Train accuracy: 95.75396825396825\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 126 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 127\n",
      "Train accuracy: 95.75873015873016\n",
      "Val accuracy: 94.6\n",
      "Iter 127 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 128\n",
      "Train accuracy: 95.78571428571429\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 128 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 129\n",
      "Train accuracy: 95.80634920634921\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 129 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 130\n",
      "Train accuracy: 95.82380952380952\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 130 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 131\n",
      "Train accuracy: 95.83650793650794\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 131 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 132\n",
      "Train accuracy: 95.85555555555555\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 132 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 133\n",
      "Train accuracy: 95.86031746031746\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 133 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 134\n",
      "Train accuracy: 95.87142857142858\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 134 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 135\n",
      "Train accuracy: 95.87301587301587\n",
      "Val accuracy: 94.65714285714286\n",
      "Iter 135 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 136\n",
      "Train accuracy: 95.8873015873016\n",
      "Val accuracy: 94.67142857142858\n",
      "Iter 136 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 137\n",
      "Train accuracy: 95.91428571428573\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 137 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 138\n",
      "Train accuracy: 95.92222222222222\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 138 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 139\n",
      "Train accuracy: 95.94126984126984\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 139 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 140\n",
      "Train accuracy: 95.95714285714286\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 140 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 141\n",
      "Train accuracy: 95.97777777777777\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 141 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 142\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.72857142857143\n",
      "Iter 142 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 143\n",
      "Train accuracy: 96.02857142857142\n",
      "Val accuracy: 94.77142857142857\n",
      "Iter 143 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 144\n",
      "Train accuracy: 96.04285714285714\n",
      "Val accuracy: 94.8\n",
      "Iter 144 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 145\n",
      "Train accuracy: 96.06507936507937\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 145 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 146\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 146 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 147\n",
      "Train accuracy: 96.0936507936508\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 147 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 148\n",
      "Train accuracy: 96.10634920634921\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 148 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 149\n",
      "Train accuracy: 96.13174603174603\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 149 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 150\n",
      "Train accuracy: 96.15079365079366\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 150 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 151\n",
      "Train accuracy: 96.16666666666667\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 151 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 152\n",
      "Train accuracy: 96.18253968253968\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 152 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 153\n",
      "Train accuracy: 96.18571428571428\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 153 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 154\n",
      "Train accuracy: 96.2\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 154 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 155\n",
      "Train accuracy: 96.20793650793651\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 155 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 156\n",
      "Train accuracy: 96.21428571428572\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 156 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 157\n",
      "Train accuracy: 96.22539682539683\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 157 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 158\n",
      "Train accuracy: 96.22857142857143\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 158 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 159\n",
      "Train accuracy: 96.24126984126984\n",
      "Val accuracy: 94.91428571428571\n",
      "Iter 159 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 160\n",
      "Train accuracy: 96.24444444444444\n",
      "Val accuracy: 94.94285714285714\n",
      "Iter 160 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 161\n",
      "Train accuracy: 96.25396825396825\n",
      "Val accuracy: 94.97142857142858\n",
      "Iter 161 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 162\n",
      "Train accuracy: 96.27142857142857\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 162 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 163\n",
      "Train accuracy: 96.27777777777777\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 163 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 164\n",
      "Train accuracy: 96.3047619047619\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 164 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 165\n",
      "Train accuracy: 96.32063492063492\n",
      "Val accuracy: 95.0\n",
      "Iter 165 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 166\n",
      "Train accuracy: 96.33809523809524\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 166 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 167\n",
      "Train accuracy: 96.34126984126983\n",
      "Val accuracy: 95.04285714285714\n",
      "Iter 167 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 168\n",
      "Train accuracy: 96.34761904761905\n",
      "Val accuracy: 95.08571428571429\n",
      "Iter 168 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 169\n",
      "Train accuracy: 96.35873015873015\n",
      "Val accuracy: 95.12857142857143\n",
      "Iter 169 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 170\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 170 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 171\n",
      "Train accuracy: 96.3873015873016\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 171 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 172\n",
      "Train accuracy: 96.3984126984127\n",
      "Val accuracy: 95.15714285714286\n",
      "Iter 172 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 173\n",
      "Train accuracy: 96.41428571428573\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 173 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 174\n",
      "Train accuracy: 96.42857142857143\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 174 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 175\n",
      "Train accuracy: 96.44285714285714\n",
      "Val accuracy: 95.21428571428572\n",
      "Iter 175 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 176\n",
      "Train accuracy: 96.45396825396826\n",
      "Val accuracy: 95.22857142857143\n",
      "Iter 176 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 177\n",
      "Train accuracy: 96.46031746031746\n",
      "Val accuracy: 95.27142857142857\n",
      "Iter 177 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 178\n",
      "Train accuracy: 96.46666666666667\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 178 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 179\n",
      "Train accuracy: 96.46825396825398\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 179 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 180\n",
      "Train accuracy: 96.48095238095237\n",
      "Val accuracy: 95.3\n",
      "Iter 180 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 181\n",
      "Train accuracy: 96.49682539682539\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 181 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 182\n",
      "Train accuracy: 96.50634920634921\n",
      "Val accuracy: 95.3\n",
      "Iter 182 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 183\n",
      "Train accuracy: 96.52222222222223\n",
      "Val accuracy: 95.3\n",
      "Iter 183 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 184\n",
      "Train accuracy: 96.53015873015873\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 184 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 185\n",
      "Train accuracy: 96.54444444444444\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 185 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 186\n",
      "Train accuracy: 96.54761904761905\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 186 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 187\n",
      "Train accuracy: 96.56349206349206\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 187 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 188\n",
      "Train accuracy: 96.56031746031746\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 188 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 189\n",
      "Train accuracy: 96.56825396825397\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 189 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 190\n",
      "Train accuracy: 96.58571428571429\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 190 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 191\n",
      "Train accuracy: 96.5952380952381\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 191 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 192\n",
      "Train accuracy: 96.60317460317461\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 192 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 193\n",
      "Train accuracy: 96.61111111111111\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 193 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 194\n",
      "Train accuracy: 96.62380952380953\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 194 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 195\n",
      "Train accuracy: 96.62857142857143\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 195 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 196\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 196 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 197\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 197 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 198\n",
      "Train accuracy: 96.63492063492065\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 198 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 199\n",
      "Train accuracy: 96.63809523809523\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 199 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 200\n",
      "Train accuracy: 96.65238095238095\n",
      "Val accuracy: 95.35714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 23.333333333333332\n",
      "Iteration: 1\n",
      "Train accuracy: 21.192063492063493\n",
      "Val accuracy: 21.185714285714287\n",
      "Iter 1 -> sub iter 99 : 32.380952380952387\n",
      "Iteration: 2\n",
      "Train accuracy: 30.338095238095235\n",
      "Val accuracy: 30.185714285714287\n",
      "Iter 2 -> sub iter 99 : 39.365079365079374\n",
      "Iteration: 3\n",
      "Train accuracy: 36.630158730158726\n",
      "Val accuracy: 35.885714285714286\n",
      "Iter 3 -> sub iter 99 : 43.333333333333336\n",
      "Iteration: 4\n",
      "Train accuracy: 40.7015873015873\n",
      "Val accuracy: 39.957142857142856\n",
      "Iter 4 -> sub iter 99 : 45.873015873015874\n",
      "Iteration: 5\n",
      "Train accuracy: 43.32222222222222\n",
      "Val accuracy: 43.042857142857144\n",
      "Iter 5 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 6\n",
      "Train accuracy: 45.303174603174604\n",
      "Val accuracy: 44.94285714285714\n",
      "Iter 6 -> sub iter 99 : 49.365079365079376\n",
      "Iteration: 7\n",
      "Train accuracy: 48.33174603174603\n",
      "Val accuracy: 47.3\n",
      "Iter 7 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 8\n",
      "Train accuracy: 53.77301587301587\n",
      "Val accuracy: 52.67142857142857\n",
      "Iter 8 -> sub iter 99 : 58.888888888888896\n",
      "Iteration: 9\n",
      "Train accuracy: 57.51904761904761\n",
      "Val accuracy: 56.471428571428575\n",
      "Iter 9 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 10\n",
      "Train accuracy: 60.73968253968254\n",
      "Val accuracy: 59.68571428571428\n",
      "Iter 10 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 11\n",
      "Train accuracy: 63.834920634920636\n",
      "Val accuracy: 62.97142857142857\n",
      "Iter 11 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 12\n",
      "Train accuracy: 66.26349206349207\n",
      "Val accuracy: 65.65714285714286\n",
      "Iter 12 -> sub iter 99 : 66.349206349206345\n",
      "Iteration: 13\n",
      "Train accuracy: 68.01904761904763\n",
      "Val accuracy: 67.37142857142857\n",
      "Iter 13 -> sub iter 99 : 67.619047619047625\n",
      "Iteration: 14\n",
      "Train accuracy: 69.34920634920636\n",
      "Val accuracy: 68.45714285714286\n",
      "Iter 14 -> sub iter 99 : 68.57142857142857\n",
      "Iteration: 15\n",
      "Train accuracy: 70.35714285714286\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 15 -> sub iter 99 : 69.68253968253968\n",
      "Iteration: 16\n",
      "Train accuracy: 71.15873015873015\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 16 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 17\n",
      "Train accuracy: 71.80952380952381\n",
      "Val accuracy: 70.6\n",
      "Iter 17 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 18\n",
      "Train accuracy: 72.47777777777777\n",
      "Val accuracy: 71.31428571428572\n",
      "Iter 18 -> sub iter 99 : 72.22222222222221\n",
      "Iteration: 19\n",
      "Train accuracy: 73.23015873015873\n",
      "Val accuracy: 72.17142857142858\n",
      "Iter 19 -> sub iter 99 : 73.49206349206354\n",
      "Iteration: 20\n",
      "Train accuracy: 74.12222222222222\n",
      "Val accuracy: 73.08571428571429\n",
      "Iter 20 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 21\n",
      "Train accuracy: 75.04603174603174\n",
      "Val accuracy: 73.75714285714285\n",
      "Iter 21 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 22\n",
      "Train accuracy: 75.86190476190477\n",
      "Val accuracy: 74.5142857142857\n",
      "Iter 22 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 23\n",
      "Train accuracy: 76.63809523809523\n",
      "Val accuracy: 74.9857142857143\n",
      "Iter 23 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 24\n",
      "Train accuracy: 77.25555555555556\n",
      "Val accuracy: 75.74285714285715\n",
      "Iter 24 -> sub iter 99 : 77.30158730158733\n",
      "Iteration: 25\n",
      "Train accuracy: 77.86031746031746\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 25 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 26\n",
      "Train accuracy: 78.41269841269842\n",
      "Val accuracy: 76.91428571428571\n",
      "Iter 26 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 27\n",
      "Train accuracy: 78.94285714285715\n",
      "Val accuracy: 77.58571428571429\n",
      "Iter 27 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 28\n",
      "Train accuracy: 79.43650793650794\n",
      "Val accuracy: 78.01428571428572\n",
      "Iter 28 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 29\n",
      "Train accuracy: 79.9\n",
      "Val accuracy: 78.65714285714286\n",
      "Iter 29 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 80.32857142857142\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 30 -> sub iter 99 : 80.07460317460318\n",
      "Iteration: 31\n",
      "Train accuracy: 80.68095238095238\n",
      "Val accuracy: 79.31428571428572\n",
      "Iter 31 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 32\n",
      "Train accuracy: 81.01428571428572\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 32 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 33\n",
      "Train accuracy: 81.36984126984127\n",
      "Val accuracy: 79.94285714285714\n",
      "Iter 33 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 34\n",
      "Train accuracy: 81.67619047619048\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 34 -> sub iter 99 : 81.90476190476196\n",
      "Iteration: 35\n",
      "Train accuracy: 81.95396825396826\n",
      "Val accuracy: 80.4\n",
      "Iter 35 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 36\n",
      "Train accuracy: 82.28095238095237\n",
      "Val accuracy: 80.7\n",
      "Iter 36 -> sub iter 99 : 81.90476190476198\n",
      "Iteration: 37\n",
      "Train accuracy: 82.53809523809524\n",
      "Val accuracy: 81.10000000000001\n",
      "Iter 37 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 38\n",
      "Train accuracy: 82.73492063492064\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 38 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 39\n",
      "Train accuracy: 82.9968253968254\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 39 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 40\n",
      "Train accuracy: 83.2\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 40 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 41\n",
      "Train accuracy: 83.46507936507936\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 41 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 42\n",
      "Train accuracy: 83.66349206349206\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 42 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 43\n",
      "Train accuracy: 83.87142857142858\n",
      "Val accuracy: 82.47142857142858\n",
      "Iter 43 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 44\n",
      "Train accuracy: 84.04761904761905\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 44 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 45\n",
      "Train accuracy: 84.21111111111111\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 45 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 46\n",
      "Train accuracy: 84.38571428571429\n",
      "Val accuracy: 83.05714285714285\n",
      "Iter 46 -> sub iter 99 : 85.07936507936508\n",
      "Iteration: 47\n",
      "Train accuracy: 84.57301587301588\n",
      "Val accuracy: 83.12857142857143\n",
      "Iter 47 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 48\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 83.34285714285714\n",
      "Iter 48 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 49\n",
      "Train accuracy: 84.91746031746031\n",
      "Val accuracy: 83.5\n",
      "Iter 49 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 50\n",
      "Train accuracy: 85.06984126984128\n",
      "Val accuracy: 83.61428571428571\n",
      "Iter 50 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 51\n",
      "Train accuracy: 85.22222222222223\n",
      "Val accuracy: 83.78571428571429\n",
      "Iter 51 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 52\n",
      "Train accuracy: 85.39999999999999\n",
      "Val accuracy: 83.84285714285714\n",
      "Iter 52 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 53\n",
      "Train accuracy: 85.54603174603174\n",
      "Val accuracy: 83.92857142857143\n",
      "Iter 53 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 54\n",
      "Train accuracy: 85.66984126984127\n",
      "Val accuracy: 84.12857142857143\n",
      "Iter 54 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 55\n",
      "Train accuracy: 85.8\n",
      "Val accuracy: 84.2\n",
      "Iter 55 -> sub iter 99 : 86.03174603174604\n",
      "Iteration: 56\n",
      "Train accuracy: 85.91587301587302\n",
      "Val accuracy: 84.31428571428572\n",
      "Iter 56 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 57\n",
      "Train accuracy: 85.99365079365079\n",
      "Val accuracy: 84.41428571428573\n",
      "Iter 57 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 58\n",
      "Train accuracy: 86.1\n",
      "Val accuracy: 84.52857142857142\n",
      "Iter 58 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 59\n",
      "Train accuracy: 86.23015873015873\n",
      "Val accuracy: 84.61428571428571\n",
      "Iter 59 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 60\n",
      "Train accuracy: 86.34444444444445\n",
      "Val accuracy: 84.68571428571428\n",
      "Iter 60 -> sub iter 99 : 86.50793650793652\n",
      "Iteration: 61\n",
      "Train accuracy: 86.42857142857143\n",
      "Val accuracy: 84.72857142857143\n",
      "Iter 61 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 62\n",
      "Train accuracy: 86.53968253968254\n",
      "Val accuracy: 84.85714285714285\n",
      "Iter 62 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 63\n",
      "Train accuracy: 86.65555555555555\n",
      "Val accuracy: 85.02857142857142\n",
      "Iter 63 -> sub iter 99 : 86.98412698412699\n",
      "Iteration: 64\n",
      "Train accuracy: 86.76984126984128\n",
      "Val accuracy: 85.08571428571429\n",
      "Iter 64 -> sub iter 99 : 87.14285714285714\n",
      "Iteration: 65\n",
      "Train accuracy: 86.89206349206349\n",
      "Val accuracy: 85.21428571428571\n",
      "Iter 65 -> sub iter 99 : 87.30158730158735\n",
      "Iteration: 66\n",
      "Train accuracy: 86.9920634920635\n",
      "Val accuracy: 85.3\n",
      "Iter 66 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 67\n",
      "Train accuracy: 87.06349206349206\n",
      "Val accuracy: 85.45714285714286\n",
      "Iter 67 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 68\n",
      "Train accuracy: 87.14603174603175\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 68 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 69\n",
      "Train accuracy: 87.1984126984127\n",
      "Val accuracy: 85.58571428571429\n",
      "Iter 69 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 70\n",
      "Train accuracy: 87.27936507936508\n",
      "Val accuracy: 85.8\n",
      "Iter 70 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 71\n",
      "Train accuracy: 87.34603174603176\n",
      "Val accuracy: 85.91428571428571\n",
      "Iter 71 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 72\n",
      "Train accuracy: 87.44761904761906\n",
      "Val accuracy: 86.02857142857144\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 87.55238095238094\n",
      "Val accuracy: 86.11428571428571\n",
      "Iter 73 -> sub iter 99 : 87.77777777777777\n",
      "Iteration: 74\n",
      "Train accuracy: 87.61904761904762\n",
      "Val accuracy: 86.18571428571428\n",
      "Iter 74 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 75\n",
      "Train accuracy: 87.69206349206348\n",
      "Val accuracy: 86.25714285714285\n",
      "Iter 75 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 76\n",
      "Train accuracy: 87.7968253968254\n",
      "Val accuracy: 86.34285714285714\n",
      "Iter 76 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 77\n",
      "Train accuracy: 87.87460317460317\n",
      "Val accuracy: 86.37142857142858\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.96666666666667\n",
      "Val accuracy: 86.5142857142857\n",
      "Iter 78 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 79\n",
      "Train accuracy: 88.00634920634921\n",
      "Val accuracy: 86.61428571428571\n",
      "Iter 79 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 80\n",
      "Train accuracy: 88.07301587301588\n",
      "Val accuracy: 86.71428571428571\n",
      "Iter 80 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 81\n",
      "Train accuracy: 88.16190476190476\n",
      "Val accuracy: 86.81428571428572\n",
      "Iter 81 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 82\n",
      "Train accuracy: 88.22539682539683\n",
      "Val accuracy: 86.88571428571429\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.27301587301586\n",
      "Val accuracy: 87.02857142857144\n",
      "Iter 83 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 84\n",
      "Train accuracy: 88.33333333333333\n",
      "Val accuracy: 87.08571428571429\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.38571428571429\n",
      "Val accuracy: 87.1\n",
      "Iter 85 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 86\n",
      "Train accuracy: 88.43174603174603\n",
      "Val accuracy: 87.14285714285714\n",
      "Iter 86 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 87\n",
      "Train accuracy: 88.47301587301587\n",
      "Val accuracy: 87.25714285714285\n",
      "Iter 87 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 88\n",
      "Train accuracy: 88.50634920634921\n",
      "Val accuracy: 87.37142857142857\n",
      "Iter 88 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 89\n",
      "Train accuracy: 88.55079365079365\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 89 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 90\n",
      "Train accuracy: 88.60317460317461\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 90 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 91\n",
      "Train accuracy: 88.66349206349207\n",
      "Val accuracy: 87.42857142857143\n",
      "Iter 91 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 92\n",
      "Train accuracy: 88.70476190476191\n",
      "Val accuracy: 87.44285714285715\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 88.75714285714285\n",
      "Val accuracy: 87.5142857142857\n",
      "Iter 93 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 94\n",
      "Train accuracy: 88.81269841269841\n",
      "Val accuracy: 87.55714285714285\n",
      "Iter 94 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 95\n",
      "Train accuracy: 88.87142857142857\n",
      "Val accuracy: 87.6\n",
      "Iter 95 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 96\n",
      "Train accuracy: 88.93015873015872\n",
      "Val accuracy: 87.64285714285714\n",
      "Iter 96 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 97\n",
      "Train accuracy: 89.0047619047619\n",
      "Val accuracy: 87.71428571428571\n",
      "Iter 97 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 98\n",
      "Train accuracy: 89.04761904761904\n",
      "Val accuracy: 87.78571428571429\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 87.81428571428572\n",
      "Iter 99 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 100\n",
      "Train accuracy: 89.13650793650794\n",
      "Val accuracy: 87.84285714285714\n",
      "Iter 100 -> sub iter 99 : 90.02222222222223\n",
      "Iteration: 101\n",
      "Train accuracy: 89.17142857142856\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 101 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 102\n",
      "Train accuracy: 89.23968253968255\n",
      "Val accuracy: 87.97142857142856\n",
      "Iter 102 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 103\n",
      "Train accuracy: 89.28888888888889\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 103 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 104\n",
      "Train accuracy: 89.33174603174604\n",
      "Val accuracy: 88.12857142857143\n",
      "Iter 104 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 105\n",
      "Train accuracy: 89.36666666666667\n",
      "Val accuracy: 88.2\n",
      "Iter 105 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 106\n",
      "Train accuracy: 89.4015873015873\n",
      "Val accuracy: 88.27142857142857\n",
      "Iter 106 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 107\n",
      "Train accuracy: 89.43968253968254\n",
      "Val accuracy: 88.34285714285714\n",
      "Iter 107 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 108\n",
      "Train accuracy: 89.46666666666667\n",
      "Val accuracy: 88.35714285714286\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 89.5047619047619\n",
      "Val accuracy: 88.38571428571429\n",
      "Iter 109 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 110\n",
      "Train accuracy: 89.55396825396825\n",
      "Val accuracy: 88.44285714285715\n",
      "Iter 110 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 111\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 88.54285714285714\n",
      "Iter 111 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 112\n",
      "Train accuracy: 89.64444444444445\n",
      "Val accuracy: 88.6\n",
      "Iter 112 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 113\n",
      "Train accuracy: 89.67936507936508\n",
      "Val accuracy: 88.62857142857142\n",
      "Iter 113 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 114\n",
      "Train accuracy: 89.72698412698412\n",
      "Val accuracy: 88.67142857142856\n",
      "Iter 114 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 115\n",
      "Train accuracy: 89.78095238095239\n",
      "Val accuracy: 88.68571428571428\n",
      "Iter 115 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 116\n",
      "Train accuracy: 89.8079365079365\n",
      "Val accuracy: 88.77142857142857\n",
      "Iter 116 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 117\n",
      "Train accuracy: 89.83968253968254\n",
      "Val accuracy: 88.81428571428572\n",
      "Iter 117 -> sub iter 99 : 90.79365079365088\n",
      "Iteration: 118\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 88.87142857142857\n",
      "Iter 118 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 119\n",
      "Train accuracy: 89.91428571428571\n",
      "Val accuracy: 88.88571428571429\n",
      "Iter 119 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 120\n",
      "Train accuracy: 89.95714285714286\n",
      "Val accuracy: 88.91428571428571\n",
      "Iter 120 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 121\n",
      "Train accuracy: 89.97777777777777\n",
      "Val accuracy: 88.97142857142856\n",
      "Iter 121 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 122\n",
      "Train accuracy: 90.0079365079365\n",
      "Val accuracy: 89.07142857142857\n",
      "Iter 122 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 123\n",
      "Train accuracy: 90.04126984126984\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 123 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 124\n",
      "Train accuracy: 90.07619047619048\n",
      "Val accuracy: 89.22857142857143\n",
      "Iter 124 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 125\n",
      "Train accuracy: 90.11587301587302\n",
      "Val accuracy: 89.24285714285715\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 90.14761904761905\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 90.16507936507936\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 90.1968253968254\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 90.23492063492064\n",
      "Val accuracy: 89.31428571428572\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 90.26190476190476\n",
      "Val accuracy: 89.37142857142857\n",
      "Iter 130 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 131\n",
      "Train accuracy: 90.27936507936508\n",
      "Val accuracy: 89.34285714285714\n",
      "Iter 131 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 132\n",
      "Train accuracy: 90.32063492063493\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 132 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 133\n",
      "Train accuracy: 90.34920634920634\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 133 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 134\n",
      "Train accuracy: 90.36825396825397\n",
      "Val accuracy: 89.45714285714286\n",
      "Iter 134 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 135\n",
      "Train accuracy: 90.4\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 135 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 136\n",
      "Train accuracy: 90.42857142857143\n",
      "Val accuracy: 89.60000000000001\n",
      "Iter 136 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 137\n",
      "Train accuracy: 90.44761904761904\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 137 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 138\n",
      "Train accuracy: 90.46825396825396\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 138 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 139\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 139 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 140\n",
      "Train accuracy: 90.4952380952381\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 140 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 141\n",
      "Train accuracy: 90.51269841269841\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 141 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 142\n",
      "Train accuracy: 90.52222222222223\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 142 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 143\n",
      "Train accuracy: 90.54603174603174\n",
      "Val accuracy: 89.6857142857143\n",
      "Iter 143 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 144\n",
      "Train accuracy: 90.57777777777778\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 144 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 145\n",
      "Train accuracy: 90.6079365079365\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 145 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 146\n",
      "Train accuracy: 90.62222222222222\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 146 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 147\n",
      "Train accuracy: 90.63015873015873\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 147 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 148\n",
      "Train accuracy: 90.65396825396826\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 148 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 149\n",
      "Train accuracy: 90.67936507936508\n",
      "Val accuracy: 89.8\n",
      "Iter 149 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 150\n",
      "Train accuracy: 90.6920634920635\n",
      "Val accuracy: 89.81428571428572\n",
      "Training for 0.1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 13.650793650793653\n",
      "Iteration: 1\n",
      "Train accuracy: 13.998412698412698\n",
      "Val accuracy: 14.32857142857143\n",
      "Iter 1 -> sub iter 99 : 19.365079365079367\n",
      "Iteration: 2\n",
      "Train accuracy: 20.185714285714283\n",
      "Val accuracy: 20.314285714285717\n",
      "Iter 2 -> sub iter 99 : 22.380952380952383\n",
      "Iteration: 3\n",
      "Train accuracy: 23.184126984126983\n",
      "Val accuracy: 23.32857142857143\n",
      "Iter 3 -> sub iter 99 : 25.555555555555554\n",
      "Iteration: 4\n",
      "Train accuracy: 26.05396825396825\n",
      "Val accuracy: 26.285714285714285\n",
      "Iter 4 -> sub iter 99 : 29.682539682539684\n",
      "Iteration: 5\n",
      "Train accuracy: 29.965079365079365\n",
      "Val accuracy: 30.542857142857144\n",
      "Iter 5 -> sub iter 99 : 39.841269841269845\n",
      "Iteration: 6\n",
      "Train accuracy: 39.268253968253966\n",
      "Val accuracy: 39.628571428571426\n",
      "Iter 6 -> sub iter 99 : 45.714285714285715\n",
      "Iteration: 7\n",
      "Train accuracy: 44.371428571428574\n",
      "Val accuracy: 44.800000000000004\n",
      "Iter 7 -> sub iter 99 : 48.888888888888886\n",
      "Iteration: 8\n",
      "Train accuracy: 47.52222222222222\n",
      "Val accuracy: 47.92857142857142\n",
      "Iter 8 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 9\n",
      "Train accuracy: 49.804761904761904\n",
      "Val accuracy: 50.2\n",
      "Iter 9 -> sub iter 99 : 53.174603174603185\n",
      "Iteration: 10\n",
      "Train accuracy: 51.74126984126984\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 10 -> sub iter 99 : 54.444444444444444\n",
      "Iteration: 11\n",
      "Train accuracy: 53.179365079365084\n",
      "Val accuracy: 53.214285714285715\n",
      "Iter 11 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 12\n",
      "Train accuracy: 54.36984126984127\n",
      "Val accuracy: 54.38571428571428\n",
      "Iter 12 -> sub iter 99 : 57.301587301587396\n",
      "Iteration: 13\n",
      "Train accuracy: 55.74761904761905\n",
      "Val accuracy: 55.51428571428572\n",
      "Iter 13 -> sub iter 99 : 60.793650793650794\n",
      "Iteration: 14\n",
      "Train accuracy: 58.53174603174603\n",
      "Val accuracy: 58.08571428571428\n",
      "Iter 14 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 15\n",
      "Train accuracy: 61.68730158730159\n",
      "Val accuracy: 61.01428571428571\n",
      "Iter 15 -> sub iter 99 : 66.507936507936544\n",
      "Iteration: 16\n",
      "Train accuracy: 63.53809523809524\n",
      "Val accuracy: 63.15714285714286\n",
      "Iter 16 -> sub iter 99 : 67.619047619047626\n",
      "Iteration: 17\n",
      "Train accuracy: 64.57619047619048\n",
      "Val accuracy: 64.31428571428572\n",
      "Iter 17 -> sub iter 99 : 68.730158730158735\n",
      "Iteration: 18\n",
      "Train accuracy: 65.37777777777778\n",
      "Val accuracy: 65.11428571428571\n",
      "Iter 18 -> sub iter 99 : 68.888888888888895\n",
      "Iteration: 19\n",
      "Train accuracy: 66.02698412698412\n",
      "Val accuracy: 65.75714285714285\n",
      "Iter 19 -> sub iter 99 : 69.523809523809526\n",
      "Iteration: 20\n",
      "Train accuracy: 66.6126984126984\n",
      "Val accuracy: 66.4\n",
      "Iter 20 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 21\n",
      "Train accuracy: 67.1952380952381\n",
      "Val accuracy: 66.81428571428572\n",
      "Iter 21 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 22\n",
      "Train accuracy: 68.56507936507936\n",
      "Val accuracy: 68.37142857142857\n",
      "Iter 22 -> sub iter 99 : 73.33333333333333\n",
      "Iteration: 23\n",
      "Train accuracy: 70.21904761904761\n",
      "Val accuracy: 70.35714285714286\n",
      "Iter 23 -> sub iter 99 : 73.80952380952381\n",
      "Iteration: 24\n",
      "Train accuracy: 71.43174603174603\n",
      "Val accuracy: 71.8\n",
      "Iter 24 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 25\n",
      "Train accuracy: 72.41587301587302\n",
      "Val accuracy: 73.2\n",
      "Iter 25 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 26\n",
      "Train accuracy: 73.25714285714285\n",
      "Val accuracy: 73.87142857142858\n",
      "Iter 26 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 27\n",
      "Train accuracy: 73.98095238095237\n",
      "Val accuracy: 74.58571428571429\n",
      "Iter 27 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 28\n",
      "Train accuracy: 74.55555555555556\n",
      "Val accuracy: 75.17142857142856\n",
      "Iter 28 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 29\n",
      "Train accuracy: 75.05873015873016\n",
      "Val accuracy: 75.72857142857143\n",
      "Iter 29 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 30\n",
      "Train accuracy: 75.51904761904763\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 30 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 31\n",
      "Train accuracy: 75.84444444444445\n",
      "Val accuracy: 76.5142857142857\n",
      "Iter 31 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 32\n",
      "Train accuracy: 76.22698412698414\n",
      "Val accuracy: 76.84285714285714\n",
      "Iter 32 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 33\n",
      "Train accuracy: 76.47460317460317\n",
      "Val accuracy: 77.14285714285715\n",
      "Iter 33 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 34\n",
      "Train accuracy: 76.77777777777777\n",
      "Val accuracy: 77.4\n",
      "Iter 34 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 35\n",
      "Train accuracy: 77.0015873015873\n",
      "Val accuracy: 77.65714285714286\n",
      "Iter 35 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 36\n",
      "Train accuracy: 77.21904761904761\n",
      "Val accuracy: 77.81428571428572\n",
      "Iter 36 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 37\n",
      "Train accuracy: 77.44920634920635\n",
      "Val accuracy: 78.02857142857142\n",
      "Iter 37 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 38\n",
      "Train accuracy: 77.64603174603174\n",
      "Val accuracy: 78.18571428571428\n",
      "Iter 38 -> sub iter 99 : 79.52380952380952\n",
      "Iteration: 39\n",
      "Train accuracy: 77.83333333333333\n",
      "Val accuracy: 78.3\n",
      "Iter 39 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 40\n",
      "Train accuracy: 78.01904761904763\n",
      "Val accuracy: 78.54285714285714\n",
      "Iter 40 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 41\n",
      "Train accuracy: 78.1984126984127\n",
      "Val accuracy: 78.74285714285715\n",
      "Iter 41 -> sub iter 99 : 80.00634920634929\n",
      "Iteration: 42\n",
      "Train accuracy: 78.37936507936509\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 42 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 43\n",
      "Train accuracy: 78.54603174603174\n",
      "Val accuracy: 78.98571428571428\n",
      "Iter 43 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 44\n",
      "Train accuracy: 78.71904761904761\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 44 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 45\n",
      "Train accuracy: 78.84761904761905\n",
      "Val accuracy: 79.27142857142857\n",
      "Iter 45 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 46\n",
      "Train accuracy: 78.98730158730159\n",
      "Val accuracy: 79.47142857142858\n",
      "Iter 46 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 47\n",
      "Train accuracy: 79.11587301587302\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 47 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 48\n",
      "Train accuracy: 79.23492063492064\n",
      "Val accuracy: 79.74285714285713\n",
      "Iter 48 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 49\n",
      "Train accuracy: 79.34126984126985\n",
      "Val accuracy: 79.87142857142857\n",
      "Iter 49 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 50\n",
      "Train accuracy: 79.43015873015872\n",
      "Val accuracy: 79.88571428571429\n",
      "Iter 50 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 51\n",
      "Train accuracy: 79.53492063492064\n",
      "Val accuracy: 80.01428571428572\n",
      "Iter 51 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 52\n",
      "Train accuracy: 79.62857142857143\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 52 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 53\n",
      "Train accuracy: 79.74285714285713\n",
      "Val accuracy: 80.27142857142857\n",
      "Iter 53 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 54\n",
      "Train accuracy: 79.84126984126985\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 54 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 55\n",
      "Train accuracy: 79.94920634920635\n",
      "Val accuracy: 80.37142857142857\n",
      "Iter 55 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 80.04603174603174\n",
      "Val accuracy: 80.44285714285714\n",
      "Iter 56 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 57\n",
      "Train accuracy: 80.12698412698413\n",
      "Val accuracy: 80.52857142857142\n",
      "Iter 57 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 58\n",
      "Train accuracy: 80.22857142857143\n",
      "Val accuracy: 80.57142857142857\n",
      "Iter 58 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 59\n",
      "Train accuracy: 80.32380952380952\n",
      "Val accuracy: 80.64285714285714\n",
      "Iter 59 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 60\n",
      "Train accuracy: 80.4047619047619\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 60 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 61\n",
      "Train accuracy: 80.51428571428572\n",
      "Val accuracy: 80.85714285714286\n",
      "Iter 61 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 62\n",
      "Train accuracy: 80.61111111111111\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 62 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 63\n",
      "Train accuracy: 80.72857142857143\n",
      "Val accuracy: 80.94285714285714\n",
      "Iter 63 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 64\n",
      "Train accuracy: 80.81428571428572\n",
      "Val accuracy: 81.01428571428572\n",
      "Iter 64 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 65\n",
      "Train accuracy: 80.95079365079366\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 65 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 66\n",
      "Train accuracy: 81.2047619047619\n",
      "Val accuracy: 81.21428571428572\n",
      "Iter 66 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 81.55238095238096\n",
      "Val accuracy: 81.55714285714286\n",
      "Iter 67 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 68\n",
      "Train accuracy: 82.17142857142858\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 68 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 69\n",
      "Train accuracy: 82.97301587301588\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 69 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 70\n",
      "Train accuracy: 83.9015873015873\n",
      "Val accuracy: 83.6\n",
      "Iter 70 -> sub iter 99 : 87.30158730158731\n",
      "Iteration: 71\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 84.48571428571428\n",
      "Iter 71 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 72\n",
      "Train accuracy: 85.50952380952381\n",
      "Val accuracy: 85.28571428571429\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 86.03650793650793\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 73 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 74\n",
      "Train accuracy: 86.52063492063492\n",
      "Val accuracy: 85.94285714285715\n",
      "Iter 74 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 75\n",
      "Train accuracy: 86.90952380952382\n",
      "Val accuracy: 86.52857142857144\n",
      "Iter 75 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 76\n",
      "Train accuracy: 87.25714285714285\n",
      "Val accuracy: 86.92857142857143\n",
      "Iter 76 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 77\n",
      "Train accuracy: 87.59523809523809\n",
      "Val accuracy: 87.22857142857143\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.83492063492064\n",
      "Val accuracy: 87.4\n",
      "Iter 78 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 79\n",
      "Train accuracy: 88.07460317460317\n",
      "Val accuracy: 87.62857142857143\n",
      "Iter 79 -> sub iter 99 : 88.41269841269849\n",
      "Iteration: 80\n",
      "Train accuracy: 88.23968253968253\n",
      "Val accuracy: 87.8\n",
      "Iter 80 -> sub iter 99 : 88.41269841269842\n",
      "Iteration: 81\n",
      "Train accuracy: 88.4063492063492\n",
      "Val accuracy: 87.94285714285715\n",
      "Iter 81 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 82\n",
      "Train accuracy: 88.58095238095238\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.73174603174603\n",
      "Val accuracy: 88.15714285714286\n",
      "Iter 83 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 84\n",
      "Train accuracy: 88.86349206349206\n",
      "Val accuracy: 88.3\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.97142857142856\n",
      "Val accuracy: 88.37142857142857\n",
      "Iter 85 -> sub iter 99 : 88.41269841269845\n",
      "Iteration: 86\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 88.45714285714286\n",
      "Iter 86 -> sub iter 99 : 88.41269841269844\n",
      "Iteration: 87\n",
      "Train accuracy: 89.19365079365079\n",
      "Val accuracy: 88.52857142857142\n",
      "Iter 87 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 88\n",
      "Train accuracy: 89.29206349206349\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 88 -> sub iter 99 : 88.41269841269848\n",
      "Iteration: 89\n",
      "Train accuracy: 89.40476190476191\n",
      "Val accuracy: 88.8\n",
      "Iter 89 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 90\n",
      "Train accuracy: 89.51269841269841\n",
      "Val accuracy: 88.9857142857143\n",
      "Iter 90 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 91\n",
      "Train accuracy: 89.63333333333333\n",
      "Val accuracy: 89.1\n",
      "Iter 91 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 92\n",
      "Train accuracy: 89.71746031746032\n",
      "Val accuracy: 89.17142857142856\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 89.80317460317461\n",
      "Val accuracy: 89.2\n",
      "Iter 93 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 94\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 94 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 95\n",
      "Train accuracy: 89.94920634920635\n",
      "Val accuracy: 89.35714285714286\n",
      "Iter 95 -> sub iter 99 : 89.52380952380953\n",
      "Iteration: 96\n",
      "Train accuracy: 90.01904761904763\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 96 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 97\n",
      "Train accuracy: 90.09206349206349\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 97 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 98\n",
      "Train accuracy: 90.14126984126985\n",
      "Val accuracy: 89.47142857142858\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 90.2095238095238\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 99 -> sub iter 99 : 90.02857142857143\n",
      "Iteration: 100\n",
      "Train accuracy: 90.2936507936508\n",
      "Val accuracy: 89.58571428571429\n",
      "Iter 100 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 101\n",
      "Train accuracy: 90.36031746031746\n",
      "Val accuracy: 89.67142857142856\n",
      "Iter 101 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 102\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.75714285714285\n",
      "Iter 102 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 103\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.81428571428572\n",
      "Iter 103 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 104\n",
      "Train accuracy: 90.54920634920634\n",
      "Val accuracy: 89.87142857142857\n",
      "Iter 104 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 105\n",
      "Train accuracy: 90.60952380952381\n",
      "Val accuracy: 89.9\n",
      "Iter 105 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 106\n",
      "Train accuracy: 90.65873015873017\n",
      "Val accuracy: 89.97142857142858\n",
      "Iter 106 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 107\n",
      "Train accuracy: 90.7031746031746\n",
      "Val accuracy: 90.04285714285714\n",
      "Iter 107 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 108\n",
      "Train accuracy: 90.73174603174603\n",
      "Val accuracy: 90.08571428571429\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 90.78412698412698\n",
      "Val accuracy: 90.12857142857142\n",
      "Iter 109 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 110\n",
      "Train accuracy: 90.84603174603174\n",
      "Val accuracy: 90.15714285714286\n",
      "Iter 110 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 111\n",
      "Train accuracy: 90.89365079365079\n",
      "Val accuracy: 90.2\n",
      "Iter 111 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 112\n",
      "Train accuracy: 90.94920634920635\n",
      "Val accuracy: 90.24285714285715\n",
      "Iter 112 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 113\n",
      "Train accuracy: 91.0079365079365\n",
      "Val accuracy: 90.25714285714285\n",
      "Iter 113 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 114\n",
      "Train accuracy: 91.06349206349206\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 114 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 115\n",
      "Train accuracy: 91.10000000000001\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 115 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 116\n",
      "Train accuracy: 91.14603174603174\n",
      "Val accuracy: 90.31428571428572\n",
      "Iter 116 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 117\n",
      "Train accuracy: 91.1952380952381\n",
      "Val accuracy: 90.34285714285714\n",
      "Iter 117 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 118\n",
      "Train accuracy: 91.23968253968255\n",
      "Val accuracy: 90.4\n",
      "Iter 118 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 119\n",
      "Train accuracy: 91.28571428571428\n",
      "Val accuracy: 90.4\n",
      "Iter 119 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 120\n",
      "Train accuracy: 91.33015873015873\n",
      "Val accuracy: 90.42857142857143\n",
      "Iter 120 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 121\n",
      "Train accuracy: 91.35555555555555\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 121 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 122\n",
      "Train accuracy: 91.41269841269842\n",
      "Val accuracy: 90.52857142857142\n",
      "Iter 122 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 123\n",
      "Train accuracy: 91.43968253968254\n",
      "Val accuracy: 90.58571428571427\n",
      "Iter 123 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 124\n",
      "Train accuracy: 91.48412698412697\n",
      "Val accuracy: 90.60000000000001\n",
      "Iter 124 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 125\n",
      "Train accuracy: 91.53333333333333\n",
      "Val accuracy: 90.62857142857142\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 91.57142857142857\n",
      "Val accuracy: 90.65714285714286\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 91.60634920634921\n",
      "Val accuracy: 90.68571428571428\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 91.63650793650794\n",
      "Val accuracy: 90.71428571428571\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 91.66349206349206\n",
      "Val accuracy: 90.74285714285715\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 91.70158730158731\n",
      "Val accuracy: 90.8\n",
      "Iter 130 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 131\n",
      "Train accuracy: 91.72063492063492\n",
      "Val accuracy: 90.82857142857142\n",
      "Iter 131 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 132\n",
      "Train accuracy: 91.74285714285715\n",
      "Val accuracy: 90.84285714285714\n",
      "Iter 132 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 133\n",
      "Train accuracy: 91.76507936507936\n",
      "Val accuracy: 90.87142857142857\n",
      "Iter 133 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 134\n",
      "Train accuracy: 91.8015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 134 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 135\n",
      "Train accuracy: 91.83015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 135 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 136\n",
      "Train accuracy: 91.86666666666666\n",
      "Val accuracy: 90.9\n",
      "Iter 136 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 137\n",
      "Train accuracy: 91.91746031746032\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 137 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 138\n",
      "Train accuracy: 91.94761904761904\n",
      "Val accuracy: 90.97142857142858\n",
      "Iter 138 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 139\n",
      "Train accuracy: 91.98888888888888\n",
      "Val accuracy: 91.05714285714286\n",
      "Iter 139 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 140\n",
      "Train accuracy: 92.02698412698412\n",
      "Val accuracy: 91.11428571428571\n",
      "Iter 140 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 141\n",
      "Train accuracy: 92.05238095238096\n",
      "Val accuracy: 91.12857142857142\n",
      "Iter 141 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 142\n",
      "Train accuracy: 92.08571428571429\n",
      "Val accuracy: 91.15714285714286\n",
      "Iter 142 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 143\n",
      "Train accuracy: 92.1079365079365\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 143 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 144\n",
      "Train accuracy: 92.13809523809525\n",
      "Val accuracy: 91.21428571428571\n",
      "Iter 144 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 145\n",
      "Train accuracy: 92.17142857142858\n",
      "Val accuracy: 91.24285714285715\n",
      "Iter 145 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 146\n",
      "Train accuracy: 92.1968253968254\n",
      "Val accuracy: 91.25714285714285\n",
      "Iter 146 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 147\n",
      "Train accuracy: 92.23174603174603\n",
      "Val accuracy: 91.27142857142857\n",
      "Iter 147 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 148\n",
      "Train accuracy: 92.25873015873016\n",
      "Val accuracy: 91.3\n",
      "Iter 148 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 149\n",
      "Train accuracy: 92.28253968253968\n",
      "Val accuracy: 91.3\n",
      "Iter 149 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 150\n",
      "Train accuracy: 92.3015873015873\n",
      "Val accuracy: 91.3\n",
      "Training for 0.01\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 24.285714285714285\n",
      "Iteration: 1\n",
      "Train accuracy: 21.285714285714285\n",
      "Val accuracy: 20.42857142857143\n",
      "Iter 1 -> sub iter 99 : 26.666666666666668\n",
      "Iteration: 2\n",
      "Train accuracy: 24.23968253968254\n",
      "Val accuracy: 23.414285714285715\n",
      "Iter 2 -> sub iter 99 : 29.523809523809526\n",
      "Iteration: 3\n",
      "Train accuracy: 26.51904761904762\n",
      "Val accuracy: 25.785714285714285\n",
      "Iter 3 -> sub iter 99 : 34.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 30.831746031746032\n",
      "Val accuracy: 30.242857142857144\n",
      "Iter 4 -> sub iter 99 : 37.301587301587304\n",
      "Iteration: 5\n",
      "Train accuracy: 33.77460317460317\n",
      "Val accuracy: 33.15714285714286\n",
      "Iter 5 -> sub iter 99 : 39.047619047619054\n",
      "Iteration: 6\n",
      "Train accuracy: 35.75238095238095\n",
      "Val accuracy: 35.25714285714286\n",
      "Iter 6 -> sub iter 99 : 39.523809523809526\n",
      "Iteration: 7\n",
      "Train accuracy: 37.7\n",
      "Val accuracy: 37.25714285714285\n",
      "Iter 7 -> sub iter 99 : 42.698412698412696\n",
      "Iteration: 8\n",
      "Train accuracy: 41.15555555555556\n",
      "Val accuracy: 40.34285714285714\n",
      "Iter 8 -> sub iter 99 : 46.190476190476195\n",
      "Iteration: 9\n",
      "Train accuracy: 44.84444444444444\n",
      "Val accuracy: 43.67142857142857\n",
      "Iter 9 -> sub iter 99 : 51.904761904761916\n",
      "Iteration: 10\n",
      "Train accuracy: 52.49206349206349\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 10 -> sub iter 99 : 55.238095238095246\n",
      "Iteration: 11\n",
      "Train accuracy: 56.10793650793651\n",
      "Val accuracy: 55.442857142857136\n",
      "Iter 11 -> sub iter 99 : 58.412698412698425\n",
      "Iteration: 12\n",
      "Train accuracy: 58.439682539682536\n",
      "Val accuracy: 57.72857142857143\n",
      "Iter 12 -> sub iter 99 : 60.317460317460316\n",
      "Iteration: 13\n",
      "Train accuracy: 60.098412698412695\n",
      "Val accuracy: 59.61428571428572\n",
      "Iter 13 -> sub iter 99 : 62.698412698412696\n",
      "Iteration: 14\n",
      "Train accuracy: 61.66031746031746\n",
      "Val accuracy: 61.42857142857143\n",
      "Iter 14 -> sub iter 99 : 65.238095238095245\n",
      "Iteration: 15\n",
      "Train accuracy: 63.32698412698413\n",
      "Val accuracy: 62.857142857142854\n",
      "Iter 15 -> sub iter 99 : 66.190476190476196\n",
      "Iteration: 16\n",
      "Train accuracy: 64.60317460317461\n",
      "Val accuracy: 64.14285714285714\n",
      "Iter 16 -> sub iter 99 : 66.507936507936545\n",
      "Iteration: 17\n",
      "Train accuracy: 65.4952380952381\n",
      "Val accuracy: 65.17142857142856\n",
      "Iter 17 -> sub iter 99 : 67.777777777777796\n",
      "Iteration: 18\n",
      "Train accuracy: 66.21904761904761\n",
      "Val accuracy: 65.9\n",
      "Iter 18 -> sub iter 99 : 68.730158730158736\n",
      "Iteration: 19\n",
      "Train accuracy: 66.91111111111111\n",
      "Val accuracy: 66.60000000000001\n",
      "Iter 19 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 20\n",
      "Train accuracy: 68.81111111111112\n",
      "Val accuracy: 68.54285714285714\n",
      "Iter 20 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 21\n",
      "Train accuracy: 70.85396825396826\n",
      "Val accuracy: 70.61428571428571\n",
      "Iter 21 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 22\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 72.0\n",
      "Iter 22 -> sub iter 99 : 73.49206349206358\n",
      "Iteration: 23\n",
      "Train accuracy: 73.44444444444444\n",
      "Val accuracy: 73.17142857142858\n",
      "Iter 23 -> sub iter 99 : 74.44444444444444\n",
      "Iteration: 24\n",
      "Train accuracy: 74.31428571428572\n",
      "Val accuracy: 74.11428571428571\n",
      "Iter 24 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 25\n",
      "Train accuracy: 74.92857142857143\n",
      "Val accuracy: 74.67142857142856\n",
      "Iter 25 -> sub iter 99 : 75.23809523809524\n",
      "Iteration: 26\n",
      "Train accuracy: 75.4968253968254\n",
      "Val accuracy: 75.0\n",
      "Iter 26 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 27\n",
      "Train accuracy: 75.91904761904762\n",
      "Val accuracy: 75.44285714285715\n",
      "Iter 27 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 28\n",
      "Train accuracy: 76.29047619047618\n",
      "Val accuracy: 75.68571428571428\n",
      "Iter 28 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 29\n",
      "Train accuracy: 76.65238095238095\n",
      "Val accuracy: 76.0\n",
      "Iter 29 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 30\n",
      "Train accuracy: 76.95714285714286\n",
      "Val accuracy: 76.31428571428572\n",
      "Iter 30 -> sub iter 99 : 76.34920634920634\n",
      "Iteration: 31\n",
      "Train accuracy: 77.22063492063492\n",
      "Val accuracy: 76.6\n",
      "Iter 31 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 32\n",
      "Train accuracy: 77.4920634920635\n",
      "Val accuracy: 76.92857142857143\n",
      "Iter 32 -> sub iter 99 : 77.14285714285715\n",
      "Iteration: 33\n",
      "Train accuracy: 77.72222222222223\n",
      "Val accuracy: 77.08571428571429\n",
      "Iter 33 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 34\n",
      "Train accuracy: 77.96190476190476\n",
      "Val accuracy: 77.21428571428571\n",
      "Iter 34 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 35\n",
      "Train accuracy: 78.16984126984127\n",
      "Val accuracy: 77.47142857142858\n",
      "Iter 35 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 36\n",
      "Train accuracy: 78.37460317460318\n",
      "Val accuracy: 77.8\n",
      "Iter 36 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 37\n",
      "Train accuracy: 78.57936507936508\n",
      "Val accuracy: 77.91428571428571\n",
      "Iter 37 -> sub iter 99 : 78.09523809523812\n",
      "Iteration: 38\n",
      "Train accuracy: 78.75555555555556\n",
      "Val accuracy: 78.04285714285714\n",
      "Iter 38 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 39\n",
      "Train accuracy: 78.92063492063492\n",
      "Val accuracy: 78.22857142857143\n",
      "Iter 39 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 40\n",
      "Train accuracy: 79.0936507936508\n",
      "Val accuracy: 78.37142857142857\n",
      "Iter 40 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 41\n",
      "Train accuracy: 79.26666666666667\n",
      "Val accuracy: 78.51428571428572\n",
      "Iter 41 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 79.4\n",
      "Val accuracy: 78.58571428571427\n",
      "Iter 42 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 43\n",
      "Train accuracy: 79.52539682539683\n",
      "Val accuracy: 78.77142857142857\n",
      "Iter 43 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 44\n",
      "Train accuracy: 79.62539682539682\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 44 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 45\n",
      "Train accuracy: 79.71587301587302\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 45 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 46\n",
      "Train accuracy: 79.83492063492064\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 46 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 47\n",
      "Train accuracy: 79.95396825396826\n",
      "Val accuracy: 79.37142857142857\n",
      "Iter 47 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 48\n",
      "Train accuracy: 80.04444444444444\n",
      "Val accuracy: 79.45714285714286\n",
      "Iter 48 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 49\n",
      "Train accuracy: 80.13333333333334\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 49 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 50\n",
      "Train accuracy: 80.21111111111111\n",
      "Val accuracy: 79.7\n",
      "Iter 50 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 51\n",
      "Train accuracy: 80.2984126984127\n",
      "Val accuracy: 79.84285714285714\n",
      "Iter 51 -> sub iter 99 : 79.20634920634924\n",
      "Iteration: 52\n",
      "Train accuracy: 80.38253968253967\n",
      "Val accuracy: 79.9\n",
      "Iter 52 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 53\n",
      "Train accuracy: 80.48095238095239\n",
      "Val accuracy: 79.91428571428571\n",
      "Iter 53 -> sub iter 99 : 80.15873015873017\n",
      "Iteration: 54\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 80.05714285714286\n",
      "Iter 54 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 55\n",
      "Train accuracy: 80.64126984126983\n",
      "Val accuracy: 80.12857142857143\n",
      "Iter 55 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 56\n",
      "Train accuracy: 80.70793650793651\n",
      "Val accuracy: 80.21428571428572\n",
      "Iter 56 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 57\n",
      "Train accuracy: 80.78888888888889\n",
      "Val accuracy: 80.32857142857142\n",
      "Iter 57 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 58\n",
      "Train accuracy: 80.85873015873017\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 58 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 59\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.48571428571428\n",
      "Iter 59 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 60\n",
      "Train accuracy: 80.99047619047619\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 60 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 61\n",
      "Train accuracy: 81.06507936507936\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 61 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 62\n",
      "Train accuracy: 81.12857142857143\n",
      "Val accuracy: 80.60000000000001\n",
      "Iter 62 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 63\n",
      "Train accuracy: 81.1920634920635\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 63 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 64\n",
      "Train accuracy: 81.25555555555556\n",
      "Val accuracy: 80.65714285714286\n",
      "Iter 64 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 65\n",
      "Train accuracy: 81.32857142857142\n",
      "Val accuracy: 80.7\n",
      "Iter 65 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 66\n",
      "Train accuracy: 81.37936507936509\n",
      "Val accuracy: 80.78571428571428\n",
      "Iter 66 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 67\n",
      "Train accuracy: 81.43333333333334\n",
      "Val accuracy: 80.80000000000001\n",
      "Iter 67 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 68\n",
      "Train accuracy: 81.48253968253968\n",
      "Val accuracy: 80.84285714285714\n",
      "Iter 68 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 69\n",
      "Train accuracy: 81.52539682539683\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 69 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 70\n",
      "Train accuracy: 81.5904761904762\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 70 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 71\n",
      "Train accuracy: 81.65238095238095\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 71 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 72\n",
      "Train accuracy: 81.70793650793651\n",
      "Val accuracy: 80.95714285714286\n",
      "Iter 72 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 73\n",
      "Train accuracy: 81.76825396825397\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 73 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 74\n",
      "Train accuracy: 81.81904761904762\n",
      "Val accuracy: 81.08571428571429\n",
      "Iter 74 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 75\n",
      "Train accuracy: 81.85238095238095\n",
      "Val accuracy: 81.12857142857143\n",
      "Iter 75 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 76\n",
      "Train accuracy: 81.8968253968254\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 76 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 77\n",
      "Train accuracy: 81.93015873015874\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 77 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 78\n",
      "Train accuracy: 82.0\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 78 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 79\n",
      "Train accuracy: 82.04444444444444\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 79 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 80\n",
      "Train accuracy: 82.1\n",
      "Val accuracy: 81.3\n",
      "Iter 80 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 81\n",
      "Train accuracy: 82.13333333333334\n",
      "Val accuracy: 81.34285714285714\n",
      "Iter 81 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 82\n",
      "Train accuracy: 82.15396825396826\n",
      "Val accuracy: 81.32857142857142\n",
      "Iter 82 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 83\n",
      "Train accuracy: 82.2015873015873\n",
      "Val accuracy: 81.37142857142857\n",
      "Iter 83 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 84\n",
      "Train accuracy: 82.24761904761905\n",
      "Val accuracy: 81.38571428571429\n",
      "Iter 84 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 85\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.41428571428571\n",
      "Iter 85 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 86\n",
      "Train accuracy: 82.33015873015873\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 86 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 87\n",
      "Train accuracy: 82.37301587301587\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 87 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 88\n",
      "Train accuracy: 82.43809523809524\n",
      "Val accuracy: 81.54285714285714\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 82.46507936507936\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 89 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 90\n",
      "Train accuracy: 82.5047619047619\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 90 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 91\n",
      "Train accuracy: 82.53650793650795\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 91 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 92\n",
      "Train accuracy: 82.57142857142857\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 92 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 93\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.74285714285713\n",
      "Iter 93 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 94\n",
      "Train accuracy: 82.62857142857143\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 94 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 95\n",
      "Train accuracy: 82.65714285714286\n",
      "Val accuracy: 81.81428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 82.6920634920635\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 96 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 97\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 97 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 98\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.94285714285714\n",
      "Iter 98 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 99\n",
      "Train accuracy: 82.78095238095237\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 99 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 100\n",
      "Train accuracy: 82.81746031746032\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.84603174603174\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.87777777777777\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 102 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 103\n",
      "Train accuracy: 82.89047619047619\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 104\n",
      "Train accuracy: 82.92380952380952\n",
      "Val accuracy: 82.01428571428572\n",
      "Iter 104 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 105\n",
      "Train accuracy: 82.95555555555556\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 105 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 106\n",
      "Train accuracy: 82.97777777777777\n",
      "Val accuracy: 82.05714285714286\n",
      "Iter 106 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 107\n",
      "Train accuracy: 83.0\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 107 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 108\n",
      "Train accuracy: 83.02539682539683\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 108 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 109\n",
      "Train accuracy: 83.05079365079365\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 109 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 110\n",
      "Train accuracy: 83.07460317460318\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 110 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 111\n",
      "Train accuracy: 83.1\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 111 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 112\n",
      "Train accuracy: 83.13015873015873\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 112 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 113\n",
      "Train accuracy: 83.16190476190476\n",
      "Val accuracy: 82.24285714285713\n",
      "Iter 113 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 114\n",
      "Train accuracy: 83.17936507936507\n",
      "Val accuracy: 82.27142857142857\n",
      "Iter 114 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 115\n",
      "Train accuracy: 83.20476190476191\n",
      "Val accuracy: 82.28571428571428\n",
      "Iter 115 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 116\n",
      "Train accuracy: 83.22698412698412\n",
      "Val accuracy: 82.32857142857142\n",
      "Iter 116 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 117\n",
      "Train accuracy: 83.25396825396825\n",
      "Val accuracy: 82.37142857142857\n",
      "Iter 117 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 118\n",
      "Train accuracy: 83.28253968253968\n",
      "Val accuracy: 82.38571428571429\n",
      "Iter 118 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 119\n",
      "Train accuracy: 83.30634920634921\n",
      "Val accuracy: 82.39999999999999\n",
      "Iter 119 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 120\n",
      "Train accuracy: 83.31904761904761\n",
      "Val accuracy: 82.42857142857143\n",
      "Iter 120 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 121\n",
      "Train accuracy: 83.34603174603174\n",
      "Val accuracy: 82.45714285714286\n",
      "Iter 121 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 122\n",
      "Train accuracy: 83.37460317460318\n",
      "Val accuracy: 82.48571428571428\n",
      "Iter 122 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 123\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.51428571428572\n",
      "Iter 123 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 124\n",
      "Train accuracy: 83.4095238095238\n",
      "Val accuracy: 82.55714285714286\n",
      "Iter 124 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 125\n",
      "Train accuracy: 83.44444444444444\n",
      "Val accuracy: 82.6\n",
      "Iter 125 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 126\n",
      "Train accuracy: 83.46190476190476\n",
      "Val accuracy: 82.62857142857143\n",
      "Iter 126 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 127\n",
      "Train accuracy: 83.49047619047619\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 127 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 128\n",
      "Train accuracy: 83.5079365079365\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 128 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 129\n",
      "Train accuracy: 83.53333333333333\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 129 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 130\n",
      "Train accuracy: 83.54920634920634\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 130 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 131\n",
      "Train accuracy: 83.57460317460318\n",
      "Val accuracy: 82.72857142857143\n",
      "Iter 131 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 132\n",
      "Train accuracy: 83.6015873015873\n",
      "Val accuracy: 82.78571428571428\n",
      "Iter 132 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 133\n",
      "Train accuracy: 83.62857142857143\n",
      "Val accuracy: 82.8\n",
      "Iter 133 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 134\n",
      "Train accuracy: 83.65714285714286\n",
      "Val accuracy: 82.81428571428572\n",
      "Iter 134 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 135\n",
      "Train accuracy: 83.67460317460318\n",
      "Val accuracy: 82.84285714285714\n",
      "Iter 135 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 136\n",
      "Train accuracy: 83.69682539682539\n",
      "Val accuracy: 82.85714285714286\n",
      "Iter 136 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 137\n",
      "Train accuracy: 83.72539682539683\n",
      "Val accuracy: 82.87142857142857\n",
      "Iter 137 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 138\n",
      "Train accuracy: 83.74444444444444\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 139\n",
      "Train accuracy: 83.75873015873016\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 139 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 140\n",
      "Train accuracy: 83.77619047619046\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 140 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 141\n",
      "Train accuracy: 83.7968253968254\n",
      "Val accuracy: 82.94285714285714\n",
      "Iter 141 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 142\n",
      "Train accuracy: 83.82063492063492\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 142 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 143\n",
      "Train accuracy: 83.83809523809524\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 143 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 144\n",
      "Train accuracy: 83.86190476190475\n",
      "Val accuracy: 82.97142857142858\n",
      "Iter 144 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 145\n",
      "Train accuracy: 83.88571428571429\n",
      "Val accuracy: 82.98571428571428\n",
      "Iter 145 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 146\n",
      "Train accuracy: 83.9031746031746\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 146 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 147\n",
      "Train accuracy: 83.91904761904762\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 147 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 148\n",
      "Train accuracy: 83.95238095238096\n",
      "Val accuracy: 83.0\n",
      "Iter 148 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 149\n",
      "Train accuracy: 83.97301587301588\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 149 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 150\n",
      "Train accuracy: 83.97460317460317\n",
      "Val accuracy: 83.04285714285714\n",
      "Training for 0.001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 20.158730158730158\n",
      "Iteration: 1\n",
      "Train accuracy: 16.414285714285715\n",
      "Val accuracy: 15.814285714285713\n",
      "Iter 1 -> sub iter 99 : 30.158730158730158\n",
      "Iteration: 2\n",
      "Train accuracy: 27.836507936507935\n",
      "Val accuracy: 27.15714285714286\n",
      "Iter 2 -> sub iter 99 : 37.936507936507946\n",
      "Iteration: 3\n",
      "Train accuracy: 36.93015873015873\n",
      "Val accuracy: 36.15714285714286\n",
      "Iter 3 -> sub iter 99 : 44.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 42.15238095238095\n",
      "Val accuracy: 41.27142857142857\n",
      "Iter 4 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 5\n",
      "Train accuracy: 46.84285714285714\n",
      "Val accuracy: 45.92857142857143\n",
      "Iter 5 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 6\n",
      "Train accuracy: 51.34603174603175\n",
      "Val accuracy: 50.55714285714286\n",
      "Iter 6 -> sub iter 99 : 54.603174603174605\n",
      "Iteration: 7\n",
      "Train accuracy: 54.85079365079365\n",
      "Val accuracy: 53.51428571428571\n",
      "Iter 7 -> sub iter 99 : 56.825396825396824\n",
      "Iteration: 8\n",
      "Train accuracy: 57.369841269841274\n",
      "Val accuracy: 56.07142857142857\n",
      "Iter 8 -> sub iter 99 : 58.253968253968264\n",
      "Iteration: 9\n",
      "Train accuracy: 59.23492063492064\n",
      "Val accuracy: 57.8\n",
      "Iter 9 -> sub iter 99 : 59.841269841269844\n",
      "Iteration: 10\n",
      "Train accuracy: 60.663492063492065\n",
      "Val accuracy: 59.5\n",
      "Iter 10 -> sub iter 99 : 60.476190476190474\n",
      "Iteration: 11\n",
      "Train accuracy: 61.850793650793655\n",
      "Val accuracy: 60.785714285714285\n",
      "Iter 11 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 12\n",
      "Train accuracy: 62.790476190476184\n",
      "Val accuracy: 62.02857142857143\n",
      "Iter 12 -> sub iter 99 : 62.539682539682545\n",
      "Iteration: 13\n",
      "Train accuracy: 63.549206349206344\n",
      "Val accuracy: 62.94285714285714\n",
      "Iter 13 -> sub iter 99 : 63.492063492063495\n",
      "Iteration: 14\n",
      "Train accuracy: 64.18253968253968\n",
      "Val accuracy: 63.67142857142857\n",
      "Iter 14 -> sub iter 99 : 63.809523809523835\n",
      "Iteration: 15\n",
      "Train accuracy: 64.75555555555556\n",
      "Val accuracy: 64.24285714285715\n",
      "Iter 15 -> sub iter 99 : 64.603174603174615\n",
      "Iteration: 16\n",
      "Train accuracy: 65.28412698412698\n",
      "Val accuracy: 64.85714285714286\n",
      "Iter 16 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 17\n",
      "Train accuracy: 65.6952380952381\n",
      "Val accuracy: 65.18571428571428\n",
      "Iter 17 -> sub iter 99 : 66.031746031746024\n",
      "Iteration: 18\n",
      "Train accuracy: 66.10317460317461\n",
      "Val accuracy: 65.84285714285714\n",
      "Iter 18 -> sub iter 99 : 66.190476190476195\n",
      "Iteration: 19\n",
      "Train accuracy: 66.45555555555556\n",
      "Val accuracy: 66.17142857142856\n",
      "Iter 19 -> sub iter 99 : 66.507936507936514\n",
      "Iteration: 20\n",
      "Train accuracy: 66.79047619047618\n",
      "Val accuracy: 66.57142857142857\n",
      "Iter 20 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 21\n",
      "Train accuracy: 67.14126984126985\n",
      "Val accuracy: 66.8\n",
      "Iter 21 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 22\n",
      "Train accuracy: 67.43492063492063\n",
      "Val accuracy: 67.0\n",
      "Iter 22 -> sub iter 99 : 67.142857142857146\n",
      "Iteration: 23\n",
      "Train accuracy: 67.76507936507936\n",
      "Val accuracy: 67.31428571428572\n",
      "Iter 23 -> sub iter 99 : 67.460317460317474\n",
      "Iteration: 24\n",
      "Train accuracy: 68.03968253968254\n",
      "Val accuracy: 67.52857142857142\n",
      "Iter 24 -> sub iter 99 : 68.09523809523813\n",
      "Iteration: 25\n",
      "Train accuracy: 68.27619047619048\n",
      "Val accuracy: 67.92857142857143\n",
      "Iter 25 -> sub iter 99 : 68.25396825396825\n",
      "Iteration: 26\n",
      "Train accuracy: 68.53174603174604\n",
      "Val accuracy: 68.14285714285714\n",
      "Iter 26 -> sub iter 99 : 68.73015873015873\n",
      "Iteration: 27\n",
      "Train accuracy: 68.77301587301588\n",
      "Val accuracy: 68.27142857142857\n",
      "Iter 27 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 28\n",
      "Train accuracy: 68.98412698412699\n",
      "Val accuracy: 68.51428571428572\n",
      "Iter 28 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 29\n",
      "Train accuracy: 69.19047619047619\n",
      "Val accuracy: 68.77142857142857\n",
      "Iter 29 -> sub iter 99 : 69.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 69.39047619047619\n",
      "Val accuracy: 68.94285714285714\n",
      "Iter 30 -> sub iter 99 : 69.52380952380952\n",
      "Iteration: 31\n",
      "Train accuracy: 69.55238095238096\n",
      "Val accuracy: 69.07142857142857\n",
      "Iter 31 -> sub iter 99 : 70.15873015873015\n",
      "Iteration: 32\n",
      "Train accuracy: 69.72857142857143\n",
      "Val accuracy: 69.22857142857143\n",
      "Iter 32 -> sub iter 99 : 70.31746031746032\n",
      "Iteration: 33\n",
      "Train accuracy: 69.92063492063491\n",
      "Val accuracy: 69.32857142857142\n",
      "Iter 33 -> sub iter 99 : 70.47619047619048\n",
      "Iteration: 34\n",
      "Train accuracy: 70.0904761904762\n",
      "Val accuracy: 69.44285714285714\n",
      "Iter 34 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 35\n",
      "Train accuracy: 70.22380952380952\n",
      "Val accuracy: 69.69999999999999\n",
      "Iter 35 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 36\n",
      "Train accuracy: 70.36984126984127\n",
      "Val accuracy: 69.88571428571429\n",
      "Iter 36 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 37\n",
      "Train accuracy: 70.5\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 37 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 38\n",
      "Train accuracy: 70.63650793650794\n",
      "Val accuracy: 70.02857142857142\n",
      "Iter 38 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 39\n",
      "Train accuracy: 70.74126984126984\n",
      "Val accuracy: 70.11428571428571\n",
      "Iter 39 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 40\n",
      "Train accuracy: 70.84920634920636\n",
      "Val accuracy: 70.15714285714286\n",
      "Iter 40 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 41\n",
      "Train accuracy: 70.92539682539683\n",
      "Val accuracy: 70.3\n",
      "Iter 41 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 42\n",
      "Train accuracy: 71.02539682539683\n",
      "Val accuracy: 70.34285714285714\n",
      "Iter 42 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 43\n",
      "Train accuracy: 71.13650793650793\n",
      "Val accuracy: 70.39999999999999\n",
      "Iter 43 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 44\n",
      "Train accuracy: 71.22222222222221\n",
      "Val accuracy: 70.48571428571428\n",
      "Iter 44 -> sub iter 99 : 71.26984126984127\n",
      "Iteration: 45\n",
      "Train accuracy: 71.28730158730158\n",
      "Val accuracy: 70.55714285714285\n",
      "Iter 45 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 46\n",
      "Train accuracy: 71.36984126984127\n",
      "Val accuracy: 70.71428571428572\n",
      "Iter 46 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 47\n",
      "Train accuracy: 71.44603174603175\n",
      "Val accuracy: 70.84285714285714\n",
      "Iter 47 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 48\n",
      "Train accuracy: 71.5047619047619\n",
      "Val accuracy: 70.94285714285714\n",
      "Iter 48 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 49\n",
      "Train accuracy: 71.56825396825397\n",
      "Val accuracy: 70.98571428571428\n",
      "Iter 49 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 50\n",
      "Train accuracy: 71.615873015873\n",
      "Val accuracy: 71.02857142857142\n",
      "Iter 50 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 51\n",
      "Train accuracy: 71.67301587301587\n",
      "Val accuracy: 71.11428571428571\n",
      "Iter 51 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 52\n",
      "Train accuracy: 71.73650793650793\n",
      "Val accuracy: 71.21428571428572\n",
      "Iter 52 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 53\n",
      "Train accuracy: 71.8047619047619\n",
      "Val accuracy: 71.25714285714285\n",
      "Iter 53 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 54\n",
      "Train accuracy: 71.88253968253969\n",
      "Val accuracy: 71.28571428571429\n",
      "Iter 54 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 55\n",
      "Train accuracy: 71.96984126984127\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 55 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 72.04126984126984\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 56 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 57\n",
      "Train accuracy: 72.0920634920635\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 57 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 58\n",
      "Train accuracy: 72.16984126984127\n",
      "Val accuracy: 71.41428571428573\n",
      "Iter 58 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 59\n",
      "Train accuracy: 72.22698412698414\n",
      "Val accuracy: 71.45714285714286\n",
      "Iter 59 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 60\n",
      "Train accuracy: 72.27936507936508\n",
      "Val accuracy: 71.55714285714285\n",
      "Iter 60 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 61\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 71.64285714285714\n",
      "Iter 61 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 62\n",
      "Train accuracy: 72.38888888888889\n",
      "Val accuracy: 71.68571428571428\n",
      "Iter 62 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 63\n",
      "Train accuracy: 72.45238095238096\n",
      "Val accuracy: 71.7\n",
      "Iter 63 -> sub iter 99 : 72.69841269841276\n",
      "Iteration: 64\n",
      "Train accuracy: 72.52222222222223\n",
      "Val accuracy: 71.75714285714285\n",
      "Iter 64 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 65\n",
      "Train accuracy: 72.59523809523809\n",
      "Val accuracy: 71.77142857142857\n",
      "Iter 65 -> sub iter 99 : 72.69841269841278\n",
      "Iteration: 66\n",
      "Train accuracy: 72.65079365079366\n",
      "Val accuracy: 71.82857142857144\n",
      "Iter 66 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 67\n",
      "Train accuracy: 72.73650793650793\n",
      "Val accuracy: 71.97142857142858\n",
      "Iter 67 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 68\n",
      "Train accuracy: 73.05396825396825\n",
      "Val accuracy: 72.27142857142857\n",
      "Iter 68 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 69\n",
      "Train accuracy: 74.32857142857144\n",
      "Val accuracy: 73.97142857142858\n",
      "Iter 69 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 70\n",
      "Train accuracy: 76.29206349206349\n",
      "Val accuracy: 75.61428571428571\n",
      "Iter 70 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 71\n",
      "Train accuracy: 77.46507936507938\n",
      "Val accuracy: 76.81428571428572\n",
      "Iter 71 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 72\n",
      "Train accuracy: 78.24126984126984\n",
      "Val accuracy: 77.54285714285714\n",
      "Iter 72 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 73\n",
      "Train accuracy: 78.8\n",
      "Val accuracy: 77.95714285714286\n",
      "Iter 73 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 74\n",
      "Train accuracy: 79.25555555555556\n",
      "Val accuracy: 78.38571428571429\n",
      "Iter 74 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 75\n",
      "Train accuracy: 79.56031746031746\n",
      "Val accuracy: 78.7\n",
      "Iter 75 -> sub iter 99 : 80.95238095238095\n",
      "Iteration: 76\n",
      "Train accuracy: 79.81428571428572\n",
      "Val accuracy: 78.95714285714286\n",
      "Iter 76 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 77\n",
      "Train accuracy: 79.98571428571428\n",
      "Val accuracy: 79.11428571428571\n",
      "Iter 77 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 78\n",
      "Train accuracy: 80.18571428571428\n",
      "Val accuracy: 79.34285714285714\n",
      "Iter 78 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 79\n",
      "Train accuracy: 80.33015873015873\n",
      "Val accuracy: 79.48571428571428\n",
      "Iter 79 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 80\n",
      "Train accuracy: 80.43492063492064\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 80 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 81\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 79.72857142857143\n",
      "Iter 81 -> sub iter 99 : 81.58730158730158\n",
      "Iteration: 82\n",
      "Train accuracy: 80.71428571428572\n",
      "Val accuracy: 79.82857142857142\n",
      "Iter 82 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 83\n",
      "Train accuracy: 80.80952380952381\n",
      "Val accuracy: 79.97142857142858\n",
      "Iter 83 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 84\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.07142857142857\n",
      "Iter 84 -> sub iter 99 : 81.90476190476199\n",
      "Iteration: 85\n",
      "Train accuracy: 81.0047619047619\n",
      "Val accuracy: 80.17142857142858\n",
      "Iter 85 -> sub iter 99 : 81.90476190476194\n",
      "Iteration: 86\n",
      "Train accuracy: 81.10952380952381\n",
      "Val accuracy: 80.30000000000001\n",
      "Iter 86 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 87\n",
      "Train accuracy: 81.17460317460318\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 87 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 88\n",
      "Train accuracy: 81.26031746031745\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 81.33968253968254\n",
      "Val accuracy: 80.47142857142858\n",
      "Iter 89 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 90\n",
      "Train accuracy: 81.41428571428571\n",
      "Val accuracy: 80.51428571428572\n",
      "Iter 90 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 91\n",
      "Train accuracy: 81.48571428571428\n",
      "Val accuracy: 80.54285714285714\n",
      "Iter 91 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 92\n",
      "Train accuracy: 81.54444444444444\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 92 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 93\n",
      "Train accuracy: 81.6047619047619\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 93 -> sub iter 99 : 82.69841269841272\n",
      "Iteration: 94\n",
      "Train accuracy: 81.66984126984127\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 94 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 95\n",
      "Train accuracy: 81.72539682539683\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 81.78730158730158\n",
      "Val accuracy: 80.77142857142857\n",
      "Iter 96 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 97\n",
      "Train accuracy: 81.83809523809525\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 97 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 98\n",
      "Train accuracy: 81.87619047619049\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 98 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 99\n",
      "Train accuracy: 81.93333333333334\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 99 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 100\n",
      "Train accuracy: 81.99047619047619\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.02063492063492\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.05873015873016\n",
      "Val accuracy: 81.0\n",
      "Iter 102 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 103\n",
      "Train accuracy: 82.11746031746033\n",
      "Val accuracy: 81.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 104\n",
      "Train accuracy: 82.15714285714286\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 104 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 105\n",
      "Train accuracy: 82.1968253968254\n",
      "Val accuracy: 81.11428571428571\n",
      "Iter 105 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 106\n",
      "Train accuracy: 82.24285714285713\n",
      "Val accuracy: 81.15714285714286\n",
      "Iter 106 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 107\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 107 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 108\n",
      "Train accuracy: 82.32857142857142\n",
      "Val accuracy: 81.17142857142858\n",
      "Iter 108 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 109\n",
      "Train accuracy: 82.37619047619049\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 109 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 110\n",
      "Train accuracy: 82.41746031746032\n",
      "Val accuracy: 81.25714285714287\n",
      "Iter 110 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 111\n",
      "Train accuracy: 82.46190476190476\n",
      "Val accuracy: 81.28571428571428\n",
      "Iter 111 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 112\n",
      "Train accuracy: 82.4952380952381\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 112 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 113\n",
      "Train accuracy: 82.53015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 113 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 114\n",
      "Train accuracy: 82.55396825396826\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 114 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 115\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 115 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 116\n",
      "Train accuracy: 82.63174603174603\n",
      "Val accuracy: 81.39999999999999\n",
      "Iter 116 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 117\n",
      "Train accuracy: 82.67142857142858\n",
      "Val accuracy: 81.42857142857143\n",
      "Iter 117 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 118\n",
      "Train accuracy: 82.7031746031746\n",
      "Val accuracy: 81.47142857142858\n",
      "Iter 118 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 119\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 119 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 120\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 120 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 121\n",
      "Train accuracy: 82.77460317460317\n",
      "Val accuracy: 81.57142857142857\n",
      "Iter 121 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 122\n",
      "Train accuracy: 82.79841269841269\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 122 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 123\n",
      "Train accuracy: 82.82380952380952\n",
      "Val accuracy: 81.6\n",
      "Iter 123 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 124\n",
      "Train accuracy: 82.86825396825397\n",
      "Val accuracy: 81.61428571428571\n",
      "Iter 124 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 125\n",
      "Train accuracy: 82.88412698412698\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 125 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 126\n",
      "Train accuracy: 82.91111111111111\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 126 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 127\n",
      "Train accuracy: 82.93174603174603\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 127 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 128\n",
      "Train accuracy: 82.94920634920635\n",
      "Val accuracy: 81.65714285714286\n",
      "Iter 128 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 129\n",
      "Train accuracy: 82.98253968253968\n",
      "Val accuracy: 81.68571428571428\n",
      "Iter 129 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 130\n",
      "Train accuracy: 83.01746031746032\n",
      "Val accuracy: 81.67142857142858\n",
      "Iter 130 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 131\n",
      "Train accuracy: 83.03968253968253\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 131 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 132\n",
      "Train accuracy: 83.06825396825397\n",
      "Val accuracy: 81.72857142857143\n",
      "Iter 132 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 133\n",
      "Train accuracy: 83.0952380952381\n",
      "Val accuracy: 81.75714285714287\n",
      "Iter 133 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 134\n",
      "Train accuracy: 83.12698412698413\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 134 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 135\n",
      "Train accuracy: 83.15714285714286\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 135 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 136\n",
      "Train accuracy: 83.18412698412698\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 136 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 137\n",
      "Train accuracy: 83.1984126984127\n",
      "Val accuracy: 81.82857142857142\n",
      "Iter 137 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 138\n",
      "Train accuracy: 83.21904761904761\n",
      "Val accuracy: 81.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 139\n",
      "Train accuracy: 83.23968253968253\n",
      "Val accuracy: 81.92857142857143\n",
      "Iter 139 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 140\n",
      "Train accuracy: 83.27460317460319\n",
      "Val accuracy: 81.97142857142858\n",
      "Iter 140 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 141\n",
      "Train accuracy: 83.3015873015873\n",
      "Val accuracy: 82.04285714285714\n",
      "Iter 141 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 142\n",
      "Train accuracy: 83.32698412698413\n",
      "Val accuracy: 82.07142857142857\n",
      "Iter 142 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 143\n",
      "Train accuracy: 83.34444444444445\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 143 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 144\n",
      "Train accuracy: 83.36031746031746\n",
      "Val accuracy: 82.17142857142858\n",
      "Iter 144 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 145\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 145 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 146\n",
      "Train accuracy: 83.40793650793651\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 146 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 147\n",
      "Train accuracy: 83.43015873015874\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 147 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 148\n",
      "Train accuracy: 83.45555555555556\n",
      "Val accuracy: 82.22857142857143\n",
      "Iter 148 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 149\n",
      "Train accuracy: 83.47777777777777\n",
      "Val accuracy: 82.25714285714287\n",
      "Iter 149 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 150\n",
      "Train accuracy: 83.4920634920635\n",
      "Val accuracy: 82.27142857142857\n",
      "Training for 0.0001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 21.904761904761905\n",
      "Iteration: 1\n",
      "Train accuracy: 22.957142857142856\n",
      "Val accuracy: 24.22857142857143\n",
      "Iter 1 -> sub iter 99 : 27.777777777777785\n",
      "Iteration: 2\n",
      "Train accuracy: 29.887301587301586\n",
      "Val accuracy: 31.157142857142855\n",
      "Iter 2 -> sub iter 99 : 31.904761904761975\n",
      "Iteration: 3\n",
      "Train accuracy: 34.33968253968254\n",
      "Val accuracy: 35.65714285714286\n",
      "Iter 3 -> sub iter 99 : 38.571428571428584\n",
      "Iteration: 4\n",
      "Train accuracy: 39.353968253968254\n",
      "Val accuracy: 40.87142857142857\n",
      "Iter 4 -> sub iter 99 : 41.111111111111116\n",
      "Iteration: 5\n",
      "Train accuracy: 42.35714285714286\n",
      "Val accuracy: 43.9\n",
      "Iter 5 -> sub iter 99 : 42.539682539682546\n",
      "Iteration: 6\n",
      "Train accuracy: 44.34603174603174\n",
      "Val accuracy: 46.1\n",
      "Iter 6 -> sub iter 99 : 44.603174603174614\n",
      "Iteration: 7\n",
      "Train accuracy: 45.76190476190476\n",
      "Val accuracy: 47.599999999999994\n",
      "Iter 7 -> sub iter 99 : 46.031746031746034\n",
      "Iteration: 8\n",
      "Train accuracy: 46.68571428571428\n",
      "Val accuracy: 48.22857142857143\n",
      "Iter 8 -> sub iter 99 : 46.825396825396824\n",
      "Iteration: 9\n",
      "Train accuracy: 47.41746031746032\n",
      "Val accuracy: 48.871428571428574\n",
      "Iter 9 -> sub iter 99 : 48.095238095238095\n",
      "Iteration: 10\n",
      "Train accuracy: 48.03809523809524\n",
      "Val accuracy: 49.542857142857144\n",
      "Iter 10 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 11\n",
      "Train accuracy: 48.53174603174603\n",
      "Val accuracy: 49.81428571428572\n",
      "Iter 11 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 12\n",
      "Train accuracy: 48.94761904761904\n",
      "Val accuracy: 50.24285714285715\n",
      "Iter 12 -> sub iter 99 : 48.571428571428575\n",
      "Iteration: 13\n",
      "Train accuracy: 49.32857142857143\n",
      "Val accuracy: 50.5\n",
      "Iter 13 -> sub iter 99 : 49.206349206349296\n",
      "Iteration: 14\n",
      "Train accuracy: 49.71111111111111\n",
      "Val accuracy: 50.91428571428571\n",
      "Iter 14 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 15\n",
      "Train accuracy: 50.00476190476191\n",
      "Val accuracy: 51.28571428571429\n",
      "Iter 15 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 16\n",
      "Train accuracy: 50.3111111111111\n",
      "Val accuracy: 51.4\n",
      "Iter 16 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 17\n",
      "Train accuracy: 50.56349206349206\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 17 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 18\n",
      "Train accuracy: 50.7984126984127\n",
      "Val accuracy: 52.028571428571425\n",
      "Iter 18 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 19\n",
      "Train accuracy: 51.01269841269841\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 19 -> sub iter 99 : 50.158730158730165\n",
      "Iteration: 20\n",
      "Train accuracy: 51.217460317460315\n",
      "Val accuracy: 52.51428571428571\n",
      "Iter 20 -> sub iter 99 : 50.158730158730164\n",
      "Iteration: 21\n",
      "Train accuracy: 51.42539682539683\n",
      "Val accuracy: 52.65714285714286\n",
      "Iter 21 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 22\n",
      "Train accuracy: 51.65238095238095\n",
      "Val accuracy: 52.87142857142857\n",
      "Iter 22 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 23\n",
      "Train accuracy: 51.866666666666674\n",
      "Val accuracy: 53.128571428571426\n",
      "Iter 23 -> sub iter 99 : 50.952380952380956\n",
      "Iteration: 24\n",
      "Train accuracy: 52.05714285714286\n",
      "Val accuracy: 53.38571428571428\n",
      "Iter 24 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 25\n",
      "Train accuracy: 52.23174603174603\n",
      "Val accuracy: 53.57142857142857\n",
      "Iter 25 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 26\n",
      "Train accuracy: 52.38095238095239\n",
      "Val accuracy: 53.7\n",
      "Iter 26 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 27\n",
      "Train accuracy: 52.549206349206344\n",
      "Val accuracy: 53.800000000000004\n",
      "Iter 27 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 28\n",
      "Train accuracy: 52.7079365079365\n",
      "Val accuracy: 54.0\n",
      "Iter 28 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 29\n",
      "Train accuracy: 52.84920634920635\n",
      "Val accuracy: 54.142857142857146\n",
      "Iter 29 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 30\n",
      "Train accuracy: 52.957142857142856\n",
      "Val accuracy: 54.27142857142857\n",
      "Iter 30 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 31\n",
      "Train accuracy: 53.07777777777778\n",
      "Val accuracy: 54.371428571428574\n",
      "Iter 31 -> sub iter 99 : 52.222222222222234\n",
      "Iteration: 32\n",
      "Train accuracy: 53.20952380952381\n",
      "Val accuracy: 54.51428571428571\n",
      "Iter 32 -> sub iter 99 : 52.222222222222235\n",
      "Iteration: 33\n",
      "Train accuracy: 53.31428571428572\n",
      "Val accuracy: 54.65714285714286\n",
      "Iter 33 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 34\n",
      "Train accuracy: 53.42857142857142\n",
      "Val accuracy: 54.75714285714286\n",
      "Iter 34 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 35\n",
      "Train accuracy: 53.51904761904762\n",
      "Val accuracy: 54.95714285714286\n",
      "Iter 35 -> sub iter 99 : 52.698412698412785\n",
      "Iteration: 36\n",
      "Train accuracy: 53.642857142857146\n",
      "Val accuracy: 55.01428571428571\n",
      "Iter 36 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 37\n",
      "Train accuracy: 53.73015873015873\n",
      "Val accuracy: 55.08571428571428\n",
      "Iter 37 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 38\n",
      "Train accuracy: 53.78571428571428\n",
      "Val accuracy: 55.22857142857143\n",
      "Iter 38 -> sub iter 99 : 52.857142857142865\n",
      "Iteration: 39\n",
      "Train accuracy: 53.86825396825397\n",
      "Val accuracy: 55.371428571428574\n",
      "Iter 39 -> sub iter 99 : 52.857142857142866\n",
      "Iteration: 40\n",
      "Train accuracy: 53.955555555555556\n",
      "Val accuracy: 55.385714285714286\n",
      "Iter 40 -> sub iter 99 : 53.174603174603184\n",
      "Iteration: 41\n",
      "Train accuracy: 54.03174603174603\n",
      "Val accuracy: 55.471428571428575\n",
      "Iter 41 -> sub iter 99 : 53.174603174603186\n",
      "Iteration: 42\n",
      "Train accuracy: 54.1\n",
      "Val accuracy: 55.58571428571428\n",
      "Iter 42 -> sub iter 99 : 53.492063492063494\n",
      "Iteration: 43\n",
      "Train accuracy: 54.179365079365084\n",
      "Val accuracy: 55.68571428571428\n",
      "Iter 43 -> sub iter 99 : 53.650793650793654\n",
      "Iteration: 44\n",
      "Train accuracy: 54.250793650793646\n",
      "Val accuracy: 55.74285714285714\n",
      "Iter 44 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 45\n",
      "Train accuracy: 54.31587301587302\n",
      "Val accuracy: 55.84285714285714\n",
      "Iter 45 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 46\n",
      "Train accuracy: 54.37777777777778\n",
      "Val accuracy: 55.871428571428574\n",
      "Iter 46 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 47\n",
      "Train accuracy: 54.44126984126984\n",
      "Val accuracy: 55.91428571428572\n",
      "Iter 47 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 48\n",
      "Train accuracy: 54.50000000000001\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 48 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 49\n",
      "Train accuracy: 54.541269841269845\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 49 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 50\n",
      "Train accuracy: 54.6063492063492\n",
      "Val accuracy: 56.128571428571426\n",
      "Iter 50 -> sub iter 99 : 53.968253968253976\n",
      "Iteration: 51\n",
      "Train accuracy: 54.646031746031746\n",
      "Val accuracy: 56.15714285714286\n",
      "Iter 51 -> sub iter 99 : 54.126984126984136\n",
      "Iteration: 52\n",
      "Train accuracy: 54.68730158730158\n",
      "Val accuracy: 56.214285714285715\n",
      "Iter 52 -> sub iter 99 : 54.285714285714285\n",
      "Iteration: 53\n",
      "Train accuracy: 54.72380952380952\n",
      "Val accuracy: 56.27142857142857\n",
      "Iter 53 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 54\n",
      "Train accuracy: 54.75714285714286\n",
      "Val accuracy: 56.34285714285714\n",
      "Iter 54 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 55\n",
      "Train accuracy: 54.801587301587304\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 55 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 56\n",
      "Train accuracy: 54.82857142857143\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 56 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 57\n",
      "Train accuracy: 54.87301587301587\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 57 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 58\n",
      "Train accuracy: 54.919047619047625\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 58 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 59\n",
      "Train accuracy: 54.96507936507936\n",
      "Val accuracy: 56.442857142857136\n",
      "Iter 59 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 60\n",
      "Train accuracy: 55.00158730158731\n",
      "Val accuracy: 56.49999999999999\n",
      "Iter 60 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 61\n",
      "Train accuracy: 55.05238095238095\n",
      "Val accuracy: 56.51428571428572\n",
      "Iter 61 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 62\n",
      "Train accuracy: 55.093650793650795\n",
      "Val accuracy: 56.528571428571425\n",
      "Iter 62 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 63\n",
      "Train accuracy: 55.141269841269846\n",
      "Val accuracy: 56.557142857142864\n",
      "Iter 63 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 64\n",
      "Train accuracy: 55.18253968253968\n",
      "Val accuracy: 56.58571428571428\n",
      "Iter 64 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 65\n",
      "Train accuracy: 55.21904761904762\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 65 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 66\n",
      "Train accuracy: 55.24444444444444\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 66 -> sub iter 99 : 54.920634920634924\n",
      "Iteration: 67\n",
      "Train accuracy: 55.269841269841265\n",
      "Val accuracy: 56.67142857142857\n",
      "Iter 67 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 68\n",
      "Train accuracy: 55.304761904761904\n",
      "Val accuracy: 56.714285714285715\n",
      "Iter 68 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 69\n",
      "Train accuracy: 55.33968253968254\n",
      "Val accuracy: 56.74285714285714\n",
      "Iter 69 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 70\n",
      "Train accuracy: 55.37460317460317\n",
      "Val accuracy: 56.81428571428572\n",
      "Iter 70 -> sub iter 99 : 55.238095238095244\n",
      "Iteration: 71\n",
      "Train accuracy: 55.4031746031746\n",
      "Val accuracy: 56.84285714285714\n",
      "Iter 71 -> sub iter 99 : 55.396825396825435\n",
      "Iteration: 72\n",
      "Train accuracy: 55.43492063492064\n",
      "Val accuracy: 56.871428571428574\n",
      "Iter 72 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 73\n",
      "Train accuracy: 55.474603174603175\n",
      "Val accuracy: 56.89999999999999\n",
      "Iter 73 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 74\n",
      "Train accuracy: 55.4952380952381\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 74 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 75\n",
      "Train accuracy: 55.52222222222222\n",
      "Val accuracy: 56.92857142857143\n",
      "Iter 75 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 76\n",
      "Train accuracy: 55.55238095238095\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 76 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 77\n",
      "Train accuracy: 55.574603174603176\n",
      "Val accuracy: 56.98571428571428\n",
      "Iter 77 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 78\n",
      "Train accuracy: 55.6063492063492\n",
      "Val accuracy: 56.99999999999999\n",
      "Iter 78 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 79\n",
      "Train accuracy: 55.62222222222222\n",
      "Val accuracy: 57.028571428571425\n",
      "Iter 79 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 80\n",
      "Train accuracy: 55.63809523809524\n",
      "Val accuracy: 57.04285714285714\n",
      "Iter 80 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 81\n",
      "Train accuracy: 55.65714285714286\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 81 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 82\n",
      "Train accuracy: 55.68730158730158\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 82 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 83\n",
      "Train accuracy: 55.70952380952381\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 83 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 84\n",
      "Train accuracy: 55.72380952380952\n",
      "Val accuracy: 57.15714285714286\n",
      "Iter 84 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 85\n",
      "Train accuracy: 55.73968253968255\n",
      "Val accuracy: 57.17142857142857\n",
      "Iter 85 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 86\n",
      "Train accuracy: 55.76190476190476\n",
      "Val accuracy: 57.18571428571428\n",
      "Iter 86 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 87\n",
      "Train accuracy: 55.78412698412698\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 87 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 88\n",
      "Train accuracy: 55.7952380952381\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 88 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 89\n",
      "Train accuracy: 55.80952380952381\n",
      "Val accuracy: 57.24285714285714\n",
      "Iter 89 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 90\n",
      "Train accuracy: 55.822222222222216\n",
      "Val accuracy: 57.25714285714286\n",
      "Iter 90 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 91\n",
      "Train accuracy: 55.83968253968254\n",
      "Val accuracy: 57.27142857142857\n",
      "Iter 91 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 92\n",
      "Train accuracy: 55.85555555555556\n",
      "Val accuracy: 57.285714285714285\n",
      "Iter 92 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 93\n",
      "Train accuracy: 55.87777777777778\n",
      "Val accuracy: 57.3\n",
      "Iter 93 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 94\n",
      "Train accuracy: 55.888888888888886\n",
      "Val accuracy: 57.3\n",
      "Iter 94 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 95\n",
      "Train accuracy: 55.907936507936505\n",
      "Val accuracy: 57.32857142857143\n",
      "Iter 95 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 96\n",
      "Train accuracy: 55.919047619047625\n",
      "Val accuracy: 57.371428571428574\n",
      "Iter 96 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 97\n",
      "Train accuracy: 55.93015873015873\n",
      "Val accuracy: 57.385714285714286\n",
      "Iter 97 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 98\n",
      "Train accuracy: 55.941269841269836\n",
      "Val accuracy: 57.44285714285714\n",
      "Iter 98 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 99\n",
      "Train accuracy: 55.94920634920635\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 99 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 100\n",
      "Train accuracy: 55.96666666666666\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 100 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 101\n",
      "Train accuracy: 55.98888888888889\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 101 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 102\n",
      "Train accuracy: 56.007936507936506\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 102 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 103\n",
      "Train accuracy: 56.019047619047626\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 103 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 104\n",
      "Train accuracy: 56.03174603174603\n",
      "Val accuracy: 57.49999999999999\n",
      "Iter 104 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 105\n",
      "Train accuracy: 56.046031746031744\n",
      "Val accuracy: 57.51428571428572\n",
      "Iter 105 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 106\n",
      "Train accuracy: 56.06666666666666\n",
      "Val accuracy: 57.54285714285714\n",
      "Iter 106 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 107\n",
      "Train accuracy: 56.092063492063495\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 107 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 108\n",
      "Train accuracy: 56.1031746031746\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 108 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 109\n",
      "Train accuracy: 56.12380952380952\n",
      "Val accuracy: 57.57142857142858\n",
      "Iter 109 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 110\n",
      "Train accuracy: 56.14285714285714\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 110 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 111\n",
      "Train accuracy: 56.15555555555556\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 111 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 112\n",
      "Train accuracy: 56.18253968253968\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 112 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 113\n",
      "Train accuracy: 56.19047619047619\n",
      "Val accuracy: 57.61428571428572\n",
      "Iter 113 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 114\n",
      "Train accuracy: 56.2047619047619\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 114 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 115\n",
      "Train accuracy: 56.21587301587302\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 115 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 116\n",
      "Train accuracy: 56.23650793650794\n",
      "Val accuracy: 57.657142857142865\n",
      "Iter 116 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 117\n",
      "Train accuracy: 56.25238095238095\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 117 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 118\n",
      "Train accuracy: 56.268253968253966\n",
      "Val accuracy: 57.68571428571428\n",
      "Iter 118 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 119\n",
      "Train accuracy: 56.29206349206349\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 119 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 120\n",
      "Train accuracy: 56.3031746031746\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 120 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 121\n",
      "Train accuracy: 56.31587301587302\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 121 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 122\n",
      "Train accuracy: 56.33015873015873\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 122 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 123\n",
      "Train accuracy: 56.34603174603174\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 123 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 124\n",
      "Train accuracy: 56.353968253968254\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 124 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 125\n",
      "Train accuracy: 56.35714285714286\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 125 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 126\n",
      "Train accuracy: 56.36190476190476\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 126 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 127\n",
      "Train accuracy: 56.371428571428574\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 127 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 128\n",
      "Train accuracy: 56.38253968253968\n",
      "Val accuracy: 57.74285714285714\n",
      "Iter 128 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 129\n",
      "Train accuracy: 56.3968253968254\n",
      "Val accuracy: 57.77142857142857\n",
      "Iter 129 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 130\n",
      "Train accuracy: 56.4015873015873\n",
      "Val accuracy: 57.785714285714285\n",
      "Iter 130 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 131\n",
      "Train accuracy: 56.41111111111111\n",
      "Val accuracy: 57.8\n",
      "Iter 131 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 132\n",
      "Train accuracy: 56.423809523809524\n",
      "Val accuracy: 57.8\n",
      "Iter 132 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 133\n",
      "Train accuracy: 56.426984126984124\n",
      "Val accuracy: 57.8\n",
      "Iter 133 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 134\n",
      "Train accuracy: 56.442857142857136\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 134 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 135\n",
      "Train accuracy: 56.44603174603174\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 135 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 136\n",
      "Train accuracy: 56.45873015873016\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 136 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 137\n",
      "Train accuracy: 56.461904761904755\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 137 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 138\n",
      "Train accuracy: 56.46984126984127\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 138 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 139\n",
      "Train accuracy: 56.48253968253968\n",
      "Val accuracy: 57.84285714285714\n",
      "Iter 139 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 140\n",
      "Train accuracy: 56.48412698412698\n",
      "Val accuracy: 57.871428571428574\n",
      "Iter 140 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 141\n",
      "Train accuracy: 56.48888888888889\n",
      "Val accuracy: 57.885714285714286\n",
      "Iter 141 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 142\n",
      "Train accuracy: 56.5031746031746\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 142 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 143\n",
      "Train accuracy: 56.50952380952381\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 143 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 144\n",
      "Train accuracy: 56.51587301587302\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 144 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 145\n",
      "Train accuracy: 56.52063492063492\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 145 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 146\n",
      "Train accuracy: 56.526984126984125\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 146 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 147\n",
      "Train accuracy: 56.528571428571425\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 147 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 148\n",
      "Train accuracy: 56.53650793650794\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 148 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 149\n",
      "Train accuracy: 56.54920634920635\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 149 -> sub iter 99 : 55.873015873015875\n",
      "Iteration: 150\n",
      "Train accuracy: 56.56031746031746\n",
      "Val accuracy: 57.92857142857143\n"
     ]
    }
   ],
   "source": [
    "for pert in pertList:\n",
    "    print(f\"Training for {pert}\")\n",
    "    W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.01, pert, 1)\n",
    "    trainAccPertList.append(train_acc)\n",
    "    valAccPertList.append(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 81.73650793650793\n",
      "Val accuracy: 81.02857142857142\n",
      "Iteration: 2\n",
      "Train accuracy: 86.43174603174603\n",
      "Val accuracy: 85.91428571428571\n",
      "Iteration: 3\n",
      "Train accuracy: 88.5984126984127\n",
      "Val accuracy: 88.18571428571428\n",
      "Iteration: 4\n",
      "Train accuracy: 89.78571428571429\n",
      "Val accuracy: 89.44285714285715\n",
      "Iteration: 5\n",
      "Train accuracy: 90.70158730158731\n",
      "Val accuracy: 90.21428571428571\n",
      "Iteration: 6\n",
      "Train accuracy: 91.39206349206349\n",
      "Val accuracy: 90.72857142857143\n",
      "Iteration: 7\n",
      "Train accuracy: 91.88730158730158\n",
      "Val accuracy: 91.15714285714286\n",
      "Iteration: 8\n",
      "Train accuracy: 92.33650793650794\n",
      "Val accuracy: 91.45714285714286\n",
      "Iteration: 9\n",
      "Train accuracy: 92.70793650793651\n",
      "Val accuracy: 91.75714285714285\n",
      "Iteration: 10\n",
      "Train accuracy: 93.04126984126984\n",
      "Val accuracy: 92.08571428571429\n",
      "Iteration: 11\n",
      "Train accuracy: 93.31746031746032\n",
      "Val accuracy: 92.41428571428571\n",
      "Iteration: 12\n",
      "Train accuracy: 93.57460317460318\n",
      "Val accuracy: 92.55714285714286\n",
      "Iteration: 13\n",
      "Train accuracy: 93.7984126984127\n",
      "Val accuracy: 92.84285714285714\n",
      "Iteration: 14\n",
      "Train accuracy: 94.04285714285714\n",
      "Val accuracy: 93.02857142857142\n",
      "Iteration: 15\n",
      "Train accuracy: 94.20952380952382\n",
      "Val accuracy: 93.10000000000001\n",
      "Iteration: 16\n",
      "Train accuracy: 94.3984126984127\n",
      "Val accuracy: 93.30000000000001\n",
      "Iteration: 17\n",
      "Train accuracy: 94.57619047619048\n",
      "Val accuracy: 93.5\n",
      "Iteration: 18\n",
      "Train accuracy: 94.70952380952382\n",
      "Val accuracy: 93.61428571428571\n",
      "Iteration: 19\n",
      "Train accuracy: 94.86507936507937\n",
      "Val accuracy: 93.71428571428572\n",
      "Iteration: 20\n",
      "Train accuracy: 95.01428571428572\n",
      "Val accuracy: 93.75714285714287\n",
      "Iteration: 21\n",
      "Train accuracy: 95.12063492063491\n",
      "Val accuracy: 93.92857142857143\n",
      "Iteration: 22\n",
      "Train accuracy: 95.23809523809523\n",
      "Val accuracy: 94.04285714285714\n",
      "Iteration: 23\n",
      "Train accuracy: 95.36190476190475\n",
      "Val accuracy: 94.18571428571428\n",
      "Iteration: 24\n",
      "Train accuracy: 95.46825396825398\n",
      "Val accuracy: 94.25714285714287\n",
      "Iteration: 25\n",
      "Train accuracy: 95.5936507936508\n",
      "Val accuracy: 94.32857142857142\n",
      "Iteration: 26\n",
      "Train accuracy: 95.72222222222221\n",
      "Val accuracy: 94.38571428571429\n",
      "Iteration: 27\n",
      "Train accuracy: 95.83015873015873\n",
      "Val accuracy: 94.45714285714286\n",
      "Iteration: 28\n",
      "Train accuracy: 95.93015873015874\n",
      "Val accuracy: 94.58571428571429\n",
      "Iteration: 29\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 30\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 31\n",
      "Train accuracy: 96.16031746031746\n",
      "Val accuracy: 94.65714285714286\n",
      "Iteration: 32\n",
      "Train accuracy: 96.23968253968253\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 33\n",
      "Train accuracy: 96.3031746031746\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 34\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 94.77142857142857\n",
      "Iteration: 35\n",
      "Train accuracy: 96.42380952380952\n",
      "Val accuracy: 94.8\n",
      "Iteration: 36\n",
      "Train accuracy: 96.48412698412699\n",
      "Val accuracy: 94.87142857142857\n",
      "Iteration: 37\n",
      "Train accuracy: 96.58412698412698\n",
      "Val accuracy: 94.85714285714286\n",
      "Iteration: 38\n",
      "Train accuracy: 96.65396825396826\n",
      "Val accuracy: 94.95714285714286\n",
      "Iteration: 39\n",
      "Train accuracy: 96.72222222222221\n",
      "Val accuracy: 94.94285714285714\n",
      "Iteration: 40\n",
      "Train accuracy: 96.75555555555555\n",
      "Val accuracy: 95.01428571428572\n",
      "Iteration: 41\n",
      "Train accuracy: 96.81904761904761\n",
      "Val accuracy: 95.07142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 96.88412698412698\n",
      "Val accuracy: 95.1\n",
      "Iteration: 43\n",
      "Train accuracy: 96.93650793650794\n",
      "Val accuracy: 95.11428571428571\n",
      "Iteration: 44\n",
      "Train accuracy: 97.0079365079365\n",
      "Val accuracy: 95.17142857142858\n",
      "Iteration: 45\n",
      "Train accuracy: 97.06190476190476\n",
      "Val accuracy: 95.15714285714286\n",
      "Iteration: 46\n",
      "Train accuracy: 97.12380952380953\n",
      "Val accuracy: 95.21428571428572\n",
      "Iteration: 47\n",
      "Train accuracy: 97.17619047619047\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 48\n",
      "Train accuracy: 97.22380952380952\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 49\n",
      "Train accuracy: 97.27460317460317\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 50\n",
      "Train accuracy: 97.31269841269842\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 51\n",
      "Train accuracy: 97.35079365079365\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 97.38095238095238\n",
      "Val accuracy: 95.3\n",
      "Iteration: 53\n",
      "Train accuracy: 97.42698412698412\n",
      "Val accuracy: 95.32857142857142\n",
      "Iteration: 54\n",
      "Train accuracy: 97.47142857142858\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 55\n",
      "Train accuracy: 97.5047619047619\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 56\n",
      "Train accuracy: 97.55079365079365\n",
      "Val accuracy: 95.31428571428572\n",
      "Iteration: 57\n",
      "Train accuracy: 97.6\n",
      "Val accuracy: 95.37142857142857\n",
      "Iteration: 58\n",
      "Train accuracy: 97.64761904761905\n",
      "Val accuracy: 95.38571428571429\n",
      "Iteration: 59\n",
      "Train accuracy: 97.68730158730159\n",
      "Val accuracy: 95.41428571428571\n",
      "Iteration: 60\n",
      "Train accuracy: 97.73174603174604\n",
      "Val accuracy: 95.42857142857143\n",
      "Iteration: 61\n",
      "Train accuracy: 97.76984126984128\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 62\n",
      "Train accuracy: 97.78730158730158\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 63\n",
      "Train accuracy: 97.82222222222222\n",
      "Val accuracy: 95.5\n",
      "Iteration: 64\n",
      "Train accuracy: 97.85238095238095\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 65\n",
      "Train accuracy: 97.87936507936507\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 66\n",
      "Train accuracy: 97.93015873015874\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 67\n",
      "Train accuracy: 97.96349206349207\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 68\n",
      "Train accuracy: 98.00634920634921\n",
      "Val accuracy: 95.48571428571428\n",
      "Iteration: 69\n",
      "Train accuracy: 98.03809523809524\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 70\n",
      "Train accuracy: 98.08571428571429\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 71\n",
      "Train accuracy: 98.12539682539682\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 72\n",
      "Train accuracy: 98.17142857142858\n",
      "Val accuracy: 95.67142857142858\n",
      "Iteration: 73\n",
      "Train accuracy: 98.1920634920635\n",
      "Val accuracy: 95.71428571428572\n",
      "Iteration: 74\n",
      "Train accuracy: 98.21587301587302\n",
      "Val accuracy: 95.7\n",
      "Iteration: 75\n",
      "Train accuracy: 98.25079365079365\n",
      "Val accuracy: 95.7\n",
      "Iteration: 76\n",
      "Train accuracy: 98.27619047619048\n",
      "Val accuracy: 95.7\n",
      "Iteration: 77\n",
      "Train accuracy: 98.30793650793652\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 78\n",
      "Train accuracy: 98.33333333333333\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 79\n",
      "Train accuracy: 98.35873015873015\n",
      "Val accuracy: 95.74285714285715\n",
      "Iteration: 80\n",
      "Train accuracy: 98.38571428571429\n",
      "Val accuracy: 95.75714285714285\n",
      "Iteration: 81\n",
      "Train accuracy: 98.40476190476191\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 82\n",
      "Train accuracy: 98.41904761904762\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 83\n",
      "Train accuracy: 98.43968253968254\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 84\n",
      "Train accuracy: 98.47142857142858\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 85\n",
      "Train accuracy: 98.4984126984127\n",
      "Val accuracy: 95.8\n",
      "Iteration: 86\n",
      "Train accuracy: 98.52380952380952\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 87\n",
      "Train accuracy: 98.55238095238094\n",
      "Val accuracy: 95.8\n",
      "Iteration: 88\n",
      "Train accuracy: 98.56666666666666\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 89\n",
      "Train accuracy: 98.58888888888889\n",
      "Val accuracy: 95.8\n",
      "Iteration: 90\n",
      "Train accuracy: 98.62222222222222\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 91\n",
      "Train accuracy: 98.64761904761905\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 92\n",
      "Train accuracy: 98.66666666666667\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 93\n",
      "Train accuracy: 98.67936507936508\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 94\n",
      "Train accuracy: 98.70476190476191\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 95\n",
      "Train accuracy: 98.72222222222223\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 96\n",
      "Train accuracy: 98.73333333333333\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 97\n",
      "Train accuracy: 98.74920634920635\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 98\n",
      "Train accuracy: 98.78412698412698\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 99\n",
      "Train accuracy: 98.79523809523809\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 100\n",
      "Train accuracy: 98.80793650793652\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 101\n",
      "Train accuracy: 98.84126984126983\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 102\n",
      "Train accuracy: 98.86984126984127\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 103\n",
      "Train accuracy: 98.88888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 104\n",
      "Train accuracy: 98.89682539682539\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 105\n",
      "Train accuracy: 98.91428571428571\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 106\n",
      "Train accuracy: 98.92380952380952\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 107\n",
      "Train accuracy: 98.94603174603175\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 108\n",
      "Train accuracy: 98.95873015873016\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 109\n",
      "Train accuracy: 98.97142857142858\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 110\n",
      "Train accuracy: 98.98095238095237\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 111\n",
      "Train accuracy: 99.0031746031746\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 112\n",
      "Train accuracy: 99.01269841269841\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 113\n",
      "Train accuracy: 99.02539682539683\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 114\n",
      "Train accuracy: 99.05079365079365\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 115\n",
      "Train accuracy: 99.06031746031746\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 116\n",
      "Train accuracy: 99.06666666666666\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 117\n",
      "Train accuracy: 99.08888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 118\n",
      "Train accuracy: 99.12222222222222\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 119\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 120\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 121\n",
      "Train accuracy: 99.16507936507936\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 122\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 123\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 124\n",
      "Train accuracy: 99.21904761904761\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 125\n",
      "Train accuracy: 99.21111111111112\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 126\n",
      "Train accuracy: 99.22857142857143\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 127\n",
      "Train accuracy: 99.25079365079365\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 128\n",
      "Train accuracy: 99.26031746031747\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 129\n",
      "Train accuracy: 99.27936507936508\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 130\n",
      "Train accuracy: 99.28095238095239\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 131\n",
      "Train accuracy: 99.30952380952381\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 132\n",
      "Train accuracy: 99.32857142857144\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 133\n",
      "Train accuracy: 99.33809523809524\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 134\n",
      "Train accuracy: 99.35238095238094\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 135\n",
      "Train accuracy: 99.35079365079366\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 136\n",
      "Train accuracy: 99.36507936507937\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 137\n",
      "Train accuracy: 99.37777777777778\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 138\n",
      "Train accuracy: 99.38412698412698\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 139\n",
      "Train accuracy: 99.39365079365079\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 140\n",
      "Train accuracy: 99.41587301587302\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 141\n",
      "Train accuracy: 99.42857142857143\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 142\n",
      "Train accuracy: 99.44603174603175\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 143\n",
      "Train accuracy: 99.46031746031746\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 144\n",
      "Train accuracy: 99.46825396825398\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 145\n",
      "Train accuracy: 99.47619047619047\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 146\n",
      "Train accuracy: 99.4904761904762\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 147\n",
      "Train accuracy: 99.4968253968254\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 148\n",
      "Train accuracy: 99.50952380952381\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 149\n",
      "Train accuracy: 99.51746031746032\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 150\n",
      "Train accuracy: 99.52857142857144\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 151\n",
      "Train accuracy: 99.53809523809524\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 152\n",
      "Train accuracy: 99.54126984126984\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 153\n",
      "Train accuracy: 99.55079365079365\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 154\n",
      "Train accuracy: 99.55238095238094\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 155\n",
      "Train accuracy: 99.56825396825397\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 156\n",
      "Train accuracy: 99.57777777777778\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 157\n",
      "Train accuracy: 99.58253968253969\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 158\n",
      "Train accuracy: 99.58571428571429\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 159\n",
      "Train accuracy: 99.6\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 160\n",
      "Train accuracy: 99.60793650793651\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 161\n",
      "Train accuracy: 99.615873015873\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 162\n",
      "Train accuracy: 99.62063492063493\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 163\n",
      "Train accuracy: 99.63492063492063\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 164\n",
      "Train accuracy: 99.63650793650794\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 165\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 166\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 167\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 168\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 169\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 170\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 171\n",
      "Train accuracy: 99.68095238095238\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 172\n",
      "Train accuracy: 99.68253968253968\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 173\n",
      "Train accuracy: 99.69047619047619\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 174\n",
      "Train accuracy: 99.69365079365079\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 175\n",
      "Train accuracy: 99.6984126984127\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 176\n",
      "Train accuracy: 99.70476190476191\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 177\n",
      "Train accuracy: 99.71269841269842\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 178\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 179\n",
      "Train accuracy: 99.72222222222223\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 180\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 181\n",
      "Train accuracy: 99.73174603174603\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 182\n",
      "Train accuracy: 99.73968253968253\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 183\n",
      "Train accuracy: 99.74603174603175\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 184\n",
      "Train accuracy: 99.74444444444444\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 185\n",
      "Train accuracy: 99.75238095238095\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 186\n",
      "Train accuracy: 99.76031746031747\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 187\n",
      "Train accuracy: 99.77460317460317\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 188\n",
      "Train accuracy: 99.77301587301586\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 189\n",
      "Train accuracy: 99.78730158730158\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 190\n",
      "Train accuracy: 99.79206349206349\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 191\n",
      "Train accuracy: 99.7984126984127\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 192\n",
      "Train accuracy: 99.80317460317461\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 193\n",
      "Train accuracy: 99.80952380952381\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 194\n",
      "Train accuracy: 99.81269841269841\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 195\n",
      "Train accuracy: 99.81587301587301\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 196\n",
      "Train accuracy: 99.82857142857144\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 197\n",
      "Train accuracy: 99.82698412698413\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 198\n",
      "Train accuracy: 99.83015873015873\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 199\n",
      "Train accuracy: 99.84126984126985\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 200\n",
      "Train accuracy: 99.84603174603176\n",
      "Val accuracy: 95.95714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_bp, val_acc_bp, train_loss_bp, val_loss_bp, sum_weights_bp = batch_grad_descent(x_train,y_train,epochsToTrain, 0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22a64942500>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP\", \"BP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imposing variability and seeing the effect of variability on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 0.01\n",
    "vDD = 5\n",
    "precision = 10#setting the precision value of the calculations\n",
    "wRange = 3\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization\n",
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVth(mu, sigma, shape):\n",
    "  #last dimension represents the binary rep for each weight\n",
    "  return np.random.normal(loc=mu, scale=sigma, size=shape) #each bit is represented by an sram so we need those many vth values for each mosfet in this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initMosParam(shape, mu, sigma, vDD, precision):\n",
    "    dim1, dim2 = shape\n",
    "    sizeI = (dim1, dim2, precision)\n",
    "\n",
    "    Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "\n",
    "    iOn = ((vDD - Vth)**2)*1e-06#scaling the current according to Ioff values arbitraryfor now!!\n",
    "\n",
    "\n",
    "    iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "\n",
    "\n",
    "    iOff = np.random.uniform(low=0, high=1e-10, size = sizeI)#no negative value\n",
    "    return (iOn, iOnNominal, iOff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, currents, precision, step, discreteSteps):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  \n",
    "  iOn, iOnNominal, iOff = currents\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray += np.multiply(np.sign(weightArray),  np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel))\n",
    "\n",
    "\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "# def params_init():\n",
    "\n",
    "#   #np.random.seed(2)\n",
    "#   W1 = np.random.rand(200,784) - 0.5\n",
    "#   b1 = np.random.rand(200,1) - 0.5\n",
    "#   W2 = np.random.rand(50,200) - 0.5\n",
    "#   b2 = np.random.rand(50,1) - 0.5\n",
    "#   W3 = np.random.rand(10,50) - 0.5 \n",
    "#   b3 = np.random.rand(10,1) - 0.5\n",
    "#   #W4 = np.random.rand(50,200) - 0.5   \n",
    "#   #b4 = np.random.rand(50,1) - 0.5    \n",
    "#   #W5 = np.random.rand(10,50) - 0.5  \n",
    "#   #b5 = np.random.rand(10,1) - 0.5    \n",
    "#   print(\"Params Initialised\")\n",
    "\n",
    "#   return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descentFPOCBP(X,Y,iter, lr, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    print(\"Z1\")\n",
    "    print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_grad_descentFPOCNP(X,Y,iter, lr, pert, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      \n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train loss {1/63000*np.sum((A3_train-one_hot_encoding(Y))**2)}::Val Loss {1/63000*np.sum((A3_val-one_hot_encoding(y_val))**2)}\")\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/Y.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentOCBP(X,Y,iter, lr, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, X1, Y1)\n",
    "\n",
    "      dW1varoc = weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varoc = weightTransformWithVariability(db1.reshape(db1.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varoc = weightTransformWithVariability(dW2, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varoc = weightTransformWithVariability(db2.reshape(db2.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varoc = weightTransformWithVariability(dW3, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varoc = weightTransformWithVariability(db3.reshape(db3.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1varoc, db1varoc, dW2varoc, db2varoc, dW3varoc, db3varoc, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentOCNP(X,Y,iter, lr, pert, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "  \n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "      #print(np.min(np.abs(W1)), np.min(np.abs(b1)), np.min(np.abs(W2)), np.min(np.abs(b2)), np.min(np.abs(W3)), np.min(np.abs(b3)))\n",
    "\n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n",
    "      #print(db1.shape)\n",
    "\n",
    "      dW1varoc = weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varoc = weightTransformWithVariability(db1.reshape(db1.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varoc = weightTransformWithVariability(dW2, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varoc = weightTransformWithVariability(db2.reshape(db2.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varoc = weightTransformWithVariability(dW3, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varoc = weightTransformWithVariability(db3.reshape(db3.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "      #print(dW3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1varoc, db1varoc, dW2varoc, db2varoc, dW3varoc, db3varoc, lr = lr)\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train loss {1/63000*np.sum((A3_train-one_hot_encoding(Y))**2)}::Val Loss {1/63000*np.sum((A3_val-one_hot_encoding(y_val))**2)}\")\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the effect of variability on the final accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTrainAcc = []\n",
    "finalValAcc = []\n",
    "sigmaList = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 12.15079365079365\n",
      "Val accuracy: 11.928571428571429\n",
      "Iteration: 2\n",
      "Train accuracy: 12.376190476190477\n",
      "Val accuracy: 12.828571428571427\n",
      "Iteration: 3\n",
      "Train accuracy: 12.617460317460317\n",
      "Val accuracy: 12.9\n",
      "Iteration: 4\n",
      "Train accuracy: 12.536507936507938\n",
      "Val accuracy: 12.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m sigma \u001b[39min\u001b[39;00m sigmaList:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=1'>2</a>\u001b[0m     W1, b1, W2, b2, W3, b3, train_acc_bpVar, val_acc_bpVar, train_loss_bpVar, val_loss_bpVar, sum_weights_bpVar \u001b[39m=\u001b[39m batch_grad_descentOCBP(x_train,y_train,\u001b[39m100\u001b[39;49m , \u001b[39m0.0005\u001b[39;49m, mu,sigma, vDD, precision, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=2'>3</a>\u001b[0m     finalTrainAcc\u001b[39m.\u001b[39mappend(train_acc_bpVar[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=3'>4</a>\u001b[0m     finalValAcc\u001b[39m.\u001b[39mappend(val_acc_bpVar[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 18'\u001b[0m in \u001b[0;36mbatch_grad_descentOCBP\u001b[1;34m(X, Y, iter, lr, mu, sigma, vDD, precision, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=40'>41</a>\u001b[0m X1 \u001b[39m=\u001b[39m X1\u001b[39m.\u001b[39mT \u001b[39m#take transpose to match the sizes \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=42'>43</a>\u001b[0m \u001b[39m#startin = time.time()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=43'>44</a>\u001b[0m W1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=44'>45</a>\u001b[0m b1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=45'>46</a>\u001b[0m W2varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 16'\u001b[0m in \u001b[0;36mweightTransformWithVariability\u001b[1;34m(weightArray, currents, precision, step, discreteSteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=14'>15</a>\u001b[0m analogWeightArray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(weightArray, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m bitLevel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(precision):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=17'>18</a>\u001b[0m   analogWeightArray \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(weightArray) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mwhere(np\u001b[39m.\u001b[39;49mbitwise_and(clippedWeightIndexArray, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbitLevel)\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbitLevel)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=21'>22</a>\u001b[0m weightWithVariability \u001b[39m=\u001b[39m (analogWeightArray\u001b[39m/\u001b[39miOnNominal)\u001b[39m*\u001b[39mstep\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m weightWithVariability\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sigma in sigmaList:\n",
    "    W1, b1, W2, b2, W3, b3, train_acc_bpVar, val_acc_bpVar, train_loss_bpVar, val_loss_bpVar, sum_weights_bpVar = batch_grad_descentOCBP(x_train,y_train,100 , 0.0005, mu,sigma, vDD, precision, print_op=1)\n",
    "    finalTrainAcc.append(train_acc_bpVar[-1])\n",
    "    finalValAcc.append(val_acc_bpVar[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(finalTrainAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 48.65238095238095::Val accuracy: 47.65714285714286::Train loss 1.0125491483283615::Val Loss 0.11469425632050993\n",
      "Iter 1 -> sub iter 9 : 47.777777777777785\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=0'>1</a>\u001b[0m W1, b1, W2, b2, W3, b3, train_acc_npVarAll, val_acc_npVarAll, train_loss_npVarAll, val_loss_npVarAll, sum_weights_npVarAll \u001b[39m=\u001b[39m batch_grad_descentOCNP(X\u001b[39m=\u001b[39;49mx_train,Y\u001b[39m=\u001b[39;49my_train,\u001b[39miter\u001b[39;49m \u001b[39m=\u001b[39;49m epochsToTrain, lr\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, pert\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m, mu\u001b[39m=\u001b[39;49mmu, sigma\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, vDD \u001b[39m=\u001b[39;49m vDD, precision \u001b[39m=\u001b[39;49m precision, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mbatch_grad_descentOCNP\u001b[1;34m(X, Y, iter, lr, pert, mu, sigma, vDD, precision, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=60'>61</a>\u001b[0m lossBeforePert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((A3\u001b[39m-\u001b[39mone_hot_encoding(Y1))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=62'>63</a>\u001b[0m \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=63'>64</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=64'>65</a>\u001b[0m dW1, db1, dW2, db2, dW3, db3 \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=65'>66</a>\u001b[0m \u001b[39m#print(db1.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=67'>68</a>\u001b[0m dW1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=53'>54</a>\u001b[0m A1pert \u001b[39m=\u001b[39m relu(Z1pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=55'>56</a>\u001b[0m Z2pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2,A1pert) \u001b[39m+\u001b[39m b2 \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=56'>57</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=58'>59</a>\u001b[0m Z3pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W3,A2pert) \u001b[39m+\u001b[39m b3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=59'>60</a>\u001b[0m A3pert \u001b[39m=\u001b[39m softmax(Z3pert)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=31'>32</a>\u001b[0m   \u001b[39m#Z4 = np.matmul(W4,A3) + b4\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=32'>33</a>\u001b[0m   \u001b[39m#A4 = relu(Z4)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=33'>34</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=41'>42</a>\u001b[0m   \u001b[39m#A2 is 10*m, final predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=42'>43</a>\u001b[0m   \u001b[39m# print(\"Fp Done\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=44'>45</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m Z1, A1, Z2, A2, Z3, A3\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=48'>49</a>\u001b[0m    \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmaximum(x,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(Z):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=52'>53</a>\u001b[0m   \u001b[39m#return np.exp(Z) / np.sum(np.exp(Z),0)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_npVarAll, val_acc_npVarAll, train_loss_npVarAll, val_loss_npVarAll, sum_weights_npVarAll = batch_grad_descentOCNP(X=x_train,Y=y_train,iter = epochsToTrain, lr=0.5, pert=0.0001, mu=mu, sigma=0.1, vDD = vDD, precision = precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 13.200000000000001::Val accuracy: 13.314285714285715::Train loss 1.566857139916198::Val Loss 0.17345653254052462\n",
      "Iteration: 2::Train accuracy: 21.284126984126985::Val accuracy: 21.37142857142857::Train loss 1.4363407935332517::Val Loss 0.1594937005890703\n",
      "Iteration: 3::Train accuracy: 25.63015873015873::Val accuracy: 25.142857142857146::Train loss 1.3699294923959326::Val Loss 0.15311371716078398\n",
      "Iteration: 4::Train accuracy: 28.40793650793651::Val accuracy: 27.885714285714286::Train loss 1.3242668092887602::Val Loss 0.1484728716058764\n",
      "Iteration: 5::Train accuracy: 30.631746031746033::Val accuracy: 30.028571428571425::Train loss 1.2878223717344564::Val Loss 0.1446851014398137\n",
      "Iteration: 6::Train accuracy: 32.05396825396825::Val accuracy: 31.2::Train loss 1.263716840929155::Val Loss 0.1423615132114519\n",
      "Iteration: 7::Train accuracy: 33.46825396825397::Val accuracy: 32.471428571428575::Train loss 1.2398404015421536::Val Loss 0.13983673487543052\n",
      "Iteration: 8::Train accuracy: 34.4968253968254::Val accuracy: 33.614285714285714::Train loss 1.2220524340238694::Val Loss 0.13791989162570473\n",
      "Iteration: 9::Train accuracy: 35.20634920634921::Val accuracy: 34.385714285714286::Train loss 1.207064330270723::Val Loss 0.13606595934210522\n",
      "Iteration: 10::Train accuracy: 36.7968253968254::Val accuracy: 35.542857142857144::Train loss 1.1673545220101735::Val Loss 0.1325893568270379\n",
      "Iteration: 11::Train accuracy: 38.58412698412698::Val accuracy: 36.957142857142856::Train loss 1.127328240093724::Val Loss 0.12889188314924008\n",
      "Iteration: 12::Train accuracy: 40.10793650793651::Val accuracy: 38.08571428571428::Train loss 1.0979546352344713::Val Loss 0.12574812070687272\n",
      "Iteration: 13::Train accuracy: 41.31269841269841::Val accuracy: 39.55714285714286::Train loss 1.0744097788738356::Val Loss 0.12298202407682617\n",
      "Iteration: 14::Train accuracy: 42.12698412698413::Val accuracy: 40.52857142857143::Train loss 1.0559987977357943::Val Loss 0.12082054188286893\n",
      "Iteration: 15::Train accuracy: 42.87619047619048::Val accuracy: 41.214285714285715::Train loss 1.0409342705764992::Val Loss 0.11908680334024636\n",
      "Iteration: 16::Train accuracy: 43.40952380952381::Val accuracy: 41.77142857142857::Train loss 1.0280037255057215::Val Loss 0.11762459499191995\n",
      "Iteration: 17::Train accuracy: 43.99523809523809::Val accuracy: 42.44285714285714::Train loss 1.008443673565804::Val Loss 0.11548025444037016\n",
      "Iteration: 18::Train accuracy: 46.08253968253968::Val accuracy: 44.628571428571426::Train loss 0.9476629664236439::Val Loss 0.1082331064889093\n",
      "Iteration: 19::Train accuracy: 48.86825396825397::Val accuracy: 47.08571428571429::Train loss 0.8924231179696367::Val Loss 0.10237569442578502\n",
      "Iteration: 20::Train accuracy: 51.36190476190477::Val accuracy: 49.7::Train loss 0.8484831582996054::Val Loss 0.0971278127885064\n",
      "Iteration: 21::Train accuracy: 53.353968253968254::Val accuracy: 51.82857142857142::Train loss 0.8151814632890194::Val Loss 0.09304330935881679\n",
      "Iteration: 22::Train accuracy: 54.85079365079365::Val accuracy: 53.7::Train loss 0.7901488132958926::Val Loss 0.09035121809806243\n",
      "Iteration: 23::Train accuracy: 56.01746031746032::Val accuracy: 54.91428571428572::Train loss 0.7726854876540288::Val Loss 0.0885385309859091\n",
      "Iteration: 24::Train accuracy: 56.941269841269836::Val accuracy: 55.7::Train loss 0.7592634175166658::Val Loss 0.08703825972935011\n",
      "Iteration: 25::Train accuracy: 57.66031746031746::Val accuracy: 56.48571428571428::Train loss 0.7486318367256443::Val Loss 0.0858822341144995\n",
      "Iteration: 26::Train accuracy: 58.20952380952381::Val accuracy: 56.92857142857143::Train loss 0.7400621705266568::Val Loss 0.08495708822996367\n",
      "Iteration: 27::Train accuracy: 58.66825396825397::Val accuracy: 57.15714285714286::Train loss 0.7327846887543837::Val Loss 0.08423344310401153\n",
      "Iteration: 28::Train accuracy: 59.02857142857143::Val accuracy: 57.61428571428572::Train loss 0.7262426895187616::Val Loss 0.08359608727630793\n",
      "Iteration: 29::Train accuracy: 59.34603174603175::Val accuracy: 57.74285714285714::Train loss 0.7210538013914125::Val Loss 0.0831341752165388\n",
      "Iteration: 30::Train accuracy: 59.63650793650793::Val accuracy: 57.94285714285714::Train loss 0.7159276283956159::Val Loss 0.08263832455559401\n",
      "Iteration: 31::Train accuracy: 59.871428571428574::Val accuracy: 58.199999999999996::Train loss 0.7112034275209694::Val Loss 0.08218186965566306\n",
      "Iteration: 32::Train accuracy: 60.109523809523814::Val accuracy: 58.542857142857144::Train loss 0.7068308798673936::Val Loss 0.08175110523584024\n",
      "Iteration: 33::Train accuracy: 60.303174603174604::Val accuracy: 58.9::Train loss 0.7025330249648811::Val Loss 0.08134857028133265\n",
      "Iteration: 34::Train accuracy: 60.52222222222222::Val accuracy: 59.042857142857144::Train loss 0.6982700487345431::Val Loss 0.08100981276000896\n",
      "Iteration: 35::Train accuracy: 60.68730158730159::Val accuracy: 59.07142857142858::Train loss 0.6942341406417082::Val Loss 0.08067336818930004\n",
      "Iteration: 36::Train accuracy: 60.87619047619047::Val accuracy: 59.15714285714285::Train loss 0.6902779816163724::Val Loss 0.08028398806431553\n",
      "Iteration: 37::Train accuracy: 61.06349206349206::Val accuracy: 59.3::Train loss 0.6861637202575336::Val Loss 0.07981914929236583\n",
      "Iteration: 38::Train accuracy: 61.31111111111112::Val accuracy: 59.71428571428572::Train loss 0.6818478344841853::Val Loss 0.07925897747850831\n",
      "Iteration: 39::Train accuracy: 61.60634920634921::Val accuracy: 59.971428571428575::Train loss 0.6759474864697054::Val Loss 0.07860031272778223\n",
      "Iteration: 40::Train accuracy: 61.93333333333333::Val accuracy: 60.34285714285714::Train loss 0.6694505649192561::Val Loss 0.07786240869673679\n",
      "Iteration: 41::Train accuracy: 62.25555555555555::Val accuracy: 60.61428571428571::Train loss 0.6624677723564442::Val Loss 0.07705512375113802\n",
      "Iteration: 42::Train accuracy: 62.56507936507937::Val accuracy: 60.94285714285714::Train loss 0.6560501841817448::Val Loss 0.07619817115595598\n",
      "Iteration: 43::Train accuracy: 63.05555555555556::Val accuracy: 61.27142857142858::Train loss 0.6473395098219097::Val Loss 0.07520433829909705\n",
      "Iteration: 44::Train accuracy: 63.48412698412699::Val accuracy: 61.74285714285715::Train loss 0.6389574081105465::Val Loss 0.07430330453825759\n",
      "Iteration: 45::Train accuracy: 63.993650793650794::Val accuracy: 62.28571428571429::Train loss 0.6304632062725702::Val Loss 0.0733821225251423\n",
      "Iteration: 46::Train accuracy: 64.40476190476191::Val accuracy: 62.771428571428565::Train loss 0.6212307151427998::Val Loss 0.07238706124263183\n",
      "Iteration: 47::Train accuracy: 64.9031746031746::Val accuracy: 63.37142857142857::Train loss 0.6118972230571534::Val Loss 0.07133069847968727\n",
      "Iteration: 48::Train accuracy: 65.37460317460318::Val accuracy: 63.82857142857142::Train loss 0.602534636254433::Val Loss 0.0703437798880992\n",
      "Iteration: 49::Train accuracy: 65.86507936507935::Val accuracy: 64.18571428571428::Train loss 0.5924183853812914::Val Loss 0.0693357245971397\n",
      "Iteration: 50::Train accuracy: 66.33333333333333::Val accuracy: 64.61428571428571::Train loss 0.5832160986345554::Val Loss 0.06843089820634923\n",
      "Iteration: 51::Train accuracy: 66.7::Val accuracy: 64.91428571428571::Train loss 0.5743728419503267::Val Loss 0.06750626935496203\n",
      "Iteration: 52::Train accuracy: 67.05873015873016::Val accuracy: 65.28571428571428::Train loss 0.5658968898188959::Val Loss 0.06654167669859441\n",
      "Iteration: 53::Train accuracy: 67.5079365079365::Val accuracy: 65.60000000000001::Train loss 0.5557929883868066::Val Loss 0.06547782624049356\n",
      "Iteration: 54::Train accuracy: 68.10000000000001::Val accuracy: 66.28571428571428::Train loss 0.5405740099324423::Val Loss 0.06372877661198328\n",
      "Iteration: 55::Train accuracy: 68.96349206349205::Val accuracy: 67.08571428571429::Train loss 0.5230621874696225::Val Loss 0.061585221413256376\n",
      "Iteration: 56::Train accuracy: 69.77936507936508::Val accuracy: 68.01428571428572::Train loss 0.5067061800288143::Val Loss 0.05939179810132212\n",
      "Iteration: 57::Train accuracy: 70.6936507936508::Val accuracy: 69.32857142857142::Train loss 0.489210367574694::Val Loss 0.05724441181936453\n",
      "Iteration: 58::Train accuracy: 71.52539682539683::Val accuracy: 70.0::Train loss 0.4744003446947302::Val Loss 0.05544247607969578\n",
      "Iteration: 59::Train accuracy: 72.17936507936507::Val accuracy: 71.1::Train loss 0.4623977945309487::Val Loss 0.05392138917870933\n",
      "Iteration: 60::Train accuracy: 72.75555555555555::Val accuracy: 71.61428571428571::Train loss 0.4522700737808248::Val Loss 0.052731181590657816\n",
      "Iteration: 61::Train accuracy: 73.23015873015873::Val accuracy: 72.11428571428571::Train loss 0.44308804971709287::Val Loss 0.051632713808782794\n",
      "Iteration: 62::Train accuracy: 73.63809523809523::Val accuracy: 72.57142857142857::Train loss 0.4348761643848828::Val Loss 0.050673730365417904\n",
      "Iteration: 63::Train accuracy: 74.07460317460317::Val accuracy: 73.0::Train loss 0.4272682491363391::Val Loss 0.04974865868531299\n",
      "Iteration: 64::Train accuracy: 74.3873015873016::Val accuracy: 73.34285714285714::Train loss 0.4213311027409983::Val Loss 0.04900178174948363\n",
      "Iteration: 65::Train accuracy: 74.71746031746032::Val accuracy: 73.61428571428571::Train loss 0.4158759552442644::Val Loss 0.04841171393602029\n",
      "Iteration: 66::Train accuracy: 74.98253968253968::Val accuracy: 74.0142857142857::Train loss 0.41123739271511395::Val Loss 0.047847580511822856\n",
      "Iteration: 67::Train accuracy: 75.14444444444445::Val accuracy: 74.17142857142856::Train loss 0.40750603546743264::Val Loss 0.0473729781176053\n",
      "Iteration: 68::Train accuracy: 75.20476190476191::Val accuracy: 74.32857142857144::Train loss 0.4044411712381762::Val Loss 0.04701874269508109\n",
      "Iteration: 69::Train accuracy: 75.35079365079365::Val accuracy: 74.52857142857144::Train loss 0.40080949912923075::Val Loss 0.04662500559570586\n",
      "Iteration: 70::Train accuracy: 75.67301587301587::Val accuracy: 74.67142857142856::Train loss 0.39606091452170117::Val Loss 0.04612529895336065\n",
      "Iteration: 71::Train accuracy: 75.80634920634921::Val accuracy: 74.8::Train loss 0.3930276834723899::Val Loss 0.045768092346855783\n",
      "Iteration: 72::Train accuracy: 75.93968253968254::Val accuracy: 74.88571428571429::Train loss 0.38975023840666617::Val Loss 0.04536594516067845\n",
      "Iteration: 73::Train accuracy: 76.05238095238094::Val accuracy: 75.07142857142857::Train loss 0.387058850964811::Val Loss 0.045067949489097474\n",
      "Iteration: 74::Train accuracy: 76.16825396825396::Val accuracy: 75.08571428571429::Train loss 0.38443934984364236::Val Loss 0.04478501293285217\n",
      "Iteration: 75::Train accuracy: 76.23174603174603::Val accuracy: 75.0::Train loss 0.38248895885768064::Val Loss 0.044630863254052806\n",
      "Iteration: 76::Train accuracy: 76.3015873015873::Val accuracy: 74.94285714285715::Train loss 0.3803032954024225::Val Loss 0.04441068170828668\n",
      "Iteration: 77::Train accuracy: 76.25396825396825::Val accuracy: 75.0::Train loss 0.37955029563788056::Val Loss 0.044375198855480434\n",
      "Iteration: 78::Train accuracy: 76.23968253968255::Val accuracy: 74.84285714285714::Train loss 0.3786147323187425::Val Loss 0.044212330638647865\n",
      "Iteration: 79::Train accuracy: 76.19206349206348::Val accuracy: 74.9857142857143::Train loss 0.37756385385093755::Val Loss 0.04403500870330437\n",
      "Iteration: 80::Train accuracy: 76.21904761904761::Val accuracy: 75.1::Train loss 0.3761446652932145::Val Loss 0.043805003571116226\n",
      "Iteration: 81::Train accuracy: 76.13968253968254::Val accuracy: 75.08571428571429::Train loss 0.3755324485036911::Val Loss 0.043685519491015935\n",
      "Iteration: 82::Train accuracy: 76.0::Val accuracy: 74.9857142857143::Train loss 0.37603545454433085::Val Loss 0.04366540601855181\n",
      "Iteration: 83::Train accuracy: 75.86349206349206::Val accuracy: 74.9::Train loss 0.3759558638146004::Val Loss 0.043555407449570926\n",
      "Iteration: 84::Train accuracy: 75.72380952380952::Val accuracy: 74.75714285714285::Train loss 0.3764619718391046::Val Loss 0.04354496903240951\n",
      "Iteration: 85::Train accuracy: 75.45396825396826::Val accuracy: 74.37142857142857::Train loss 0.3774222717049735::Val Loss 0.04363045513779789\n",
      "Iteration: 86::Train accuracy: 75.28730158730158::Val accuracy: 74.14285714285714::Train loss 0.37806966257343666::Val Loss 0.04367095285227896\n",
      "Iteration: 87::Train accuracy: 75.24444444444444::Val accuracy: 74.27142857142857::Train loss 0.37679004982275616::Val Loss 0.04346263023202824\n",
      "Iteration: 88::Train accuracy: 75.36190476190477::Val accuracy: 74.44285714285715::Train loss 0.37431357331506143::Val Loss 0.043159011203238035\n",
      "Iteration: 89::Train accuracy: 75.37936507936507::Val accuracy: 74.4::Train loss 0.372442377330091::Val Loss 0.04294066130588854\n",
      "Iteration: 90::Train accuracy: 75.4095238095238::Val accuracy: 74.37142857142857::Train loss 0.3703551843358487::Val Loss 0.0426511653072721\n",
      "Iteration: 91::Train accuracy: 75.38095238095238::Val accuracy: 74.44285714285715::Train loss 0.37001127005786494::Val Loss 0.04253241198347452\n",
      "Iteration: 92::Train accuracy: 75.33492063492064::Val accuracy: 74.52857142857144::Train loss 0.36979248763980616::Val Loss 0.0424579559239301\n",
      "Iteration: 93::Train accuracy: 75.46031746031746::Val accuracy: 74.74285714285715::Train loss 0.36694780598842563::Val Loss 0.042105558351730035\n",
      "Iteration: 94::Train accuracy: 75.61746031746031::Val accuracy: 75.02857142857144::Train loss 0.3646148428776769::Val Loss 0.041790042860175146\n",
      "Iteration: 95::Train accuracy: 75.59047619047618::Val accuracy: 75.05714285714285::Train loss 0.3648352164631686::Val Loss 0.041721667197602764\n",
      "Iteration: 96::Train accuracy: 75.75238095238095::Val accuracy: 75.34285714285714::Train loss 0.3626116211144255::Val Loss 0.041392339687590145\n",
      "Iteration: 97::Train accuracy: 75.95079365079364::Val accuracy: 75.4857142857143::Train loss 0.36068142544299947::Val Loss 0.04116651226151273\n",
      "Iteration: 98::Train accuracy: 76.15079365079364::Val accuracy: 75.52857142857144::Train loss 0.3587569371636391::Val Loss 0.040938139728600546\n",
      "Iteration: 99::Train accuracy: 76.17936507936508::Val accuracy: 75.67142857142856::Train loss 0.35855645099895916::Val Loss 0.040898363110898193\n",
      "Iteration: 100::Train accuracy: 76.14126984126985::Val accuracy: 75.7::Train loss 0.3603090228730898::Val Loss 0.041091918678338925\n",
      "Iteration: 101::Train accuracy: 76.05873015873016::Val accuracy: 75.67142857142856::Train loss 0.3633811688095412::Val Loss 0.04139103917980882\n",
      "Iteration: 102::Train accuracy: 75.77460317460317::Val accuracy: 75.41428571428571::Train loss 0.3697073784403775::Val Loss 0.04203141035073541\n",
      "Iteration: 103::Train accuracy: 75.59047619047618::Val accuracy: 75.37142857142857::Train loss 0.37553841262084997::Val Loss 0.042614337691663305\n",
      "Iteration: 104::Train accuracy: 74.65555555555555::Val accuracy: 74.5142857142857::Train loss 0.3925318499161501::Val Loss 0.04453789074917866\n",
      "Iteration: 105::Train accuracy: 74.03492063492064::Val accuracy: 73.52857142857144::Train loss 0.4062282198761424::Val Loss 0.04606551445734629\n",
      "Iteration: 106::Train accuracy: 73.50952380952381::Val accuracy: 73.15714285714286::Train loss 0.4167716790389977::Val Loss 0.047250658743916846\n",
      "Iteration: 107::Train accuracy: 72.92063492063492::Val accuracy: 71.98571428571428::Train loss 0.4285144158104274::Val Loss 0.04861148137111578\n",
      "Iteration: 108::Train accuracy: 72.52539682539683::Val accuracy: 71.62857142857143::Train loss 0.4360526724749092::Val Loss 0.049494780643807565\n",
      "Iteration: 109::Train accuracy: 72.03015873015873::Val accuracy: 70.98571428571428::Train loss 0.44381307483809895::Val Loss 0.050411663839491076\n",
      "Iteration: 110::Train accuracy: 71.44126984126984::Val accuracy: 70.12857142857143::Train loss 0.45285981429068334::Val Loss 0.05145296990772659\n",
      "Iteration: 111::Train accuracy: 71.21269841269842::Val accuracy: 69.89999999999999::Train loss 0.45768231069119786::Val Loss 0.05197435486593842\n",
      "Iteration: 112::Train accuracy: 71.4015873015873::Val accuracy: 70.31428571428572::Train loss 0.4555815011716835::Val Loss 0.051729342107397505\n",
      "Iteration: 113::Train accuracy: 71.58095238095238::Val accuracy: 70.67142857142858::Train loss 0.4518406348077735::Val Loss 0.05122947627832551\n",
      "Iteration: 114::Train accuracy: 71.63968253968254::Val accuracy: 70.75714285714285::Train loss 0.44970297789928254::Val Loss 0.051001722800778686\n",
      "Iteration: 115::Train accuracy: 71.63174603174603::Val accuracy: 70.8::Train loss 0.44879977180704705::Val Loss 0.05087280964761152\n",
      "Iteration: 116::Train accuracy: 71.82063492063492::Val accuracy: 70.85714285714285::Train loss 0.4448732883339239::Val Loss 0.05042981349158813\n",
      "Iteration: 117::Train accuracy: 72.11746031746031::Val accuracy: 71.11428571428571::Train loss 0.43914083614618565::Val Loss 0.049753420922208226\n",
      "Iteration: 118::Train accuracy: 72.51587301587301::Val accuracy: 71.67142857142858::Train loss 0.4313158543088888::Val Loss 0.04886907058081938\n",
      "Iteration: 119::Train accuracy: 72.63968253968254::Val accuracy: 71.72857142857143::Train loss 0.4264904555005334::Val Loss 0.04833908911282878\n",
      "Iteration: 120::Train accuracy: 72.53968253968253::Val accuracy: 71.65714285714286::Train loss 0.42508536502553856::Val Loss 0.04815584908342543\n",
      "Iteration: 121::Train accuracy: 72.46190476190476::Val accuracy: 71.6::Train loss 0.42569663546838377::Val Loss 0.048192561405076814\n",
      "Iteration: 122::Train accuracy: 72.45238095238096::Val accuracy: 71.61428571428571::Train loss 0.424480185093886::Val Loss 0.04800480277468351\n",
      "Iteration: 123::Train accuracy: 72.52380952380952::Val accuracy: 71.75714285714285::Train loss 0.42212436637617207::Val Loss 0.047714394392754746\n",
      "Iteration: 124::Train accuracy: 72.93174603174603::Val accuracy: 72.25714285714285::Train loss 0.41469491666851543::Val Loss 0.04688845646547452\n",
      "Iteration: 125::Train accuracy: 73.54603174603174::Val accuracy: 73.14285714285714::Train loss 0.40487448253409136::Val Loss 0.04580463974639918\n",
      "Iteration: 126::Train accuracy: 74.07777777777778::Val accuracy: 73.57142857142858::Train loss 0.39510412135947726::Val Loss 0.04471319419426244\n",
      "Iteration: 127::Train accuracy: 74.65238095238095::Val accuracy: 74.04285714285714::Train loss 0.3853637250331926::Val Loss 0.04365159928588187\n",
      "Iteration: 128::Train accuracy: 75.2031746031746::Val accuracy: 74.67142857142856::Train loss 0.3757434750632165::Val Loss 0.0425925081682317\n",
      "Iteration: 129::Train accuracy: 75.57460317460317::Val accuracy: 75.08571428571429::Train loss 0.36784123346279224::Val Loss 0.0416964991065021\n",
      "Iteration: 130::Train accuracy: 76.07142857142857::Val accuracy: 75.57142857142857::Train loss 0.3598113027001118::Val Loss 0.040767845470462416\n",
      "Iteration: 131::Train accuracy: 76.36984126984126::Val accuracy: 75.95714285714286::Train loss 0.3537747876799183::Val Loss 0.04008755263143746\n",
      "Iteration: 132::Train accuracy: 76.66349206349207::Val accuracy: 76.25714285714285::Train loss 0.34922555188499865::Val Loss 0.039578890011053344\n",
      "Iteration: 133::Train accuracy: 76.7936507936508::Val accuracy: 76.41428571428571::Train loss 0.34673503298157565::Val Loss 0.03930649675222413\n",
      "Iteration: 134::Train accuracy: 76.78253968253969::Val accuracy: 76.72857142857143::Train loss 0.3459664410310996::Val Loss 0.03920809186034574\n",
      "Iteration: 135::Train accuracy: 76.92063492063492::Val accuracy: 76.52857142857142::Train loss 0.34436680543312437::Val Loss 0.03901479362611946\n",
      "Iteration: 136::Train accuracy: 77.10952380952381::Val accuracy: 76.72857142857143::Train loss 0.3415524614446109::Val Loss 0.038651525677897515\n",
      "Iteration: 137::Train accuracy: 77.36825396825397::Val accuracy: 77.21428571428571::Train loss 0.33734795861190325::Val Loss 0.038155595535838374\n",
      "Iteration: 138::Train accuracy: 77.53015873015873::Val accuracy: 77.15714285714286::Train loss 0.33505144723164754::Val Loss 0.03794106830924203\n",
      "Iteration: 139::Train accuracy: 77.45396825396826::Val accuracy: 77.31428571428572::Train loss 0.33630277638174577::Val Loss 0.038031622766569385\n",
      "Iteration: 140::Train accuracy: 77.54761904761904::Val accuracy: 77.42857142857143::Train loss 0.33528901383066934::Val Loss 0.037870626064066125\n",
      "Iteration: 141::Train accuracy: 77.81428571428572::Val accuracy: 77.68571428571428::Train loss 0.33150462171464873::Val Loss 0.03740308288645637\n",
      "Iteration: 142::Train accuracy: 78.07936507936508::Val accuracy: 77.94285714285715::Train loss 0.32871417032806655::Val Loss 0.037064373185121624\n",
      "Iteration: 143::Train accuracy: 78.21904761904761::Val accuracy: 78.15714285714286::Train loss 0.3261954556600593::Val Loss 0.03677323974413577\n",
      "Iteration: 144::Train accuracy: 78.33809523809524::Val accuracy: 78.31428571428572::Train loss 0.3242875501261326::Val Loss 0.036519788594353085\n",
      "Iteration: 145::Train accuracy: 78.37619047619047::Val accuracy: 78.38571428571429::Train loss 0.3239869610894462::Val Loss 0.036417311865783904\n",
      "Iteration: 146::Train accuracy: 78.36190476190477::Val accuracy: 78.5::Train loss 0.324959401773729::Val Loss 0.036505361445708304\n",
      "Iteration: 147::Train accuracy: 78.34761904761905::Val accuracy: 78.54285714285714::Train loss 0.3254484462736096::Val Loss 0.03655231390633435\n",
      "Iteration: 148::Train accuracy: 78.37142857142857::Val accuracy: 78.54285714285714::Train loss 0.3256102718955764::Val Loss 0.036533326113977624\n",
      "Iteration: 149::Train accuracy: 78.45238095238095::Val accuracy: 78.65714285714286::Train loss 0.3251646934241628::Val Loss 0.036422419681394126\n",
      "Iteration: 150::Train accuracy: 78.5936507936508::Val accuracy: 78.72857142857143::Train loss 0.32370404433659367::Val Loss 0.03621963779096084\n",
      "Iteration: 151::Train accuracy: 78.58253968253969::Val accuracy: 78.85714285714286::Train loss 0.3240790397305::Val Loss 0.036224410761862245\n",
      "Iteration: 152::Train accuracy: 78.64920634920635::Val accuracy: 78.97142857142858::Train loss 0.32330584704680626::Val Loss 0.0361126041143741\n",
      "Iteration: 153::Train accuracy: 78.89047619047619::Val accuracy: 79.14285714285715::Train loss 0.3204246674001256::Val Loss 0.03574646816727893\n",
      "Iteration: 154::Train accuracy: 78.74761904761904::Val accuracy: 79.2::Train loss 0.3220850491178458::Val Loss 0.03589316612966283\n",
      "Iteration: 155::Train accuracy: 78.62698412698413::Val accuracy: 78.97142857142858::Train loss 0.3250543649492729::Val Loss 0.03620275354140952\n",
      "Iteration: 156::Train accuracy: 78.52539682539683::Val accuracy: 78.87142857142857::Train loss 0.32852065861238544::Val Loss 0.036581647934342224\n",
      "Iteration: 157::Train accuracy: 78.45238095238095::Val accuracy: 78.64285714285715::Train loss 0.3318121158214347::Val Loss 0.03695303111799949\n",
      "Iteration: 158::Train accuracy: 78.38730158730158::Val accuracy: 78.62857142857142::Train loss 0.3346306283671121::Val Loss 0.03726064014163791\n",
      "Iteration: 159::Train accuracy: 78.32857142857142::Val accuracy: 78.54285714285714::Train loss 0.33655155103070145::Val Loss 0.037436661178767094\n",
      "Iteration: 160::Train accuracy: 78.16349206349207::Val accuracy: 78.41428571428571::Train loss 0.33886458197589714::Val Loss 0.03766531181859757\n",
      "Iteration: 161::Train accuracy: 78.13968253968254::Val accuracy: 78.4::Train loss 0.3407794255564295::Val Loss 0.03786916876837811\n",
      "Iteration: 162::Train accuracy: 77.98412698412699::Val accuracy: 78.25714285714285::Train loss 0.34345365552643736::Val Loss 0.03815579489451571\n",
      "Iteration: 163::Train accuracy: 77.83809523809524::Val accuracy: 78.11428571428571::Train loss 0.34602319912082796::Val Loss 0.03842016905505417\n",
      "Iteration: 164::Train accuracy: 77.77777777777779::Val accuracy: 78.04285714285714::Train loss 0.34726307782610605::Val Loss 0.038534055245543875\n",
      "Iteration: 165::Train accuracy: 77.76984126984127::Val accuracy: 78.10000000000001::Train loss 0.34628173877473833::Val Loss 0.03842883998671159\n",
      "Iteration: 166::Train accuracy: 77.91746031746032::Val accuracy: 78.37142857142857::Train loss 0.34413120918183876::Val Loss 0.03819404277077673\n",
      "Iteration: 167::Train accuracy: 77.94285714285715::Val accuracy: 78.45714285714286::Train loss 0.34314926698357623::Val Loss 0.038081409818027157\n",
      "Iteration: 168::Train accuracy: 78.04444444444445::Val accuracy: 78.58571428571427::Train loss 0.34123772318660534::Val Loss 0.03787181293365782\n",
      "Iteration: 169::Train accuracy: 78.12222222222222::Val accuracy: 78.57142857142857::Train loss 0.33894717898953663::Val Loss 0.03761846890296244\n",
      "Iteration: 170::Train accuracy: 78.17301587301587::Val accuracy: 78.68571428571428::Train loss 0.3377868220684938::Val Loss 0.03748346525812862\n",
      "Iteration: 171::Train accuracy: 78.25873015873016::Val accuracy: 78.72857142857143::Train loss 0.336310023576885::Val Loss 0.037317685113656886\n",
      "Iteration: 172::Train accuracy: 78.4095238095238::Val accuracy: 78.94285714285715::Train loss 0.33328583024875486::Val Loss 0.03699458590116936\n",
      "Iteration: 173::Train accuracy: 78.5111111111111::Val accuracy: 79.01428571428572::Train loss 0.33089000785910594::Val Loss 0.03673326558427378\n",
      "Iteration: 174::Train accuracy: 78.53809523809524::Val accuracy: 79.02857142857142::Train loss 0.32980022848665386::Val Loss 0.036610338643806736\n",
      "Iteration: 175::Train accuracy: 78.54285714285714::Val accuracy: 79.10000000000001::Train loss 0.3295164433700145::Val Loss 0.03657247949066407\n",
      "Iteration: 176::Train accuracy: 78.38888888888889::Val accuracy: 79.01428571428572::Train loss 0.33197709907562056::Val Loss 0.03684312594536459\n",
      "Iteration: 177::Train accuracy: 78.29206349206349::Val accuracy: 78.92857142857143::Train loss 0.33434511142684253::Val Loss 0.03709414356653327\n",
      "Iteration: 178::Train accuracy: 78.21746031746032::Val accuracy: 78.77142857142857::Train loss 0.33581264962256324::Val Loss 0.03726263784901043\n",
      "Iteration: 179::Train accuracy: 78.27936507936508::Val accuracy: 78.9::Train loss 0.3341776367978907::Val Loss 0.03709022848702856\n",
      "Iteration: 180::Train accuracy: 78.31269841269841::Val accuracy: 78.92857142857143::Train loss 0.3330168555892107::Val Loss 0.036975167605223916\n",
      "Iteration: 181::Train accuracy: 78.40158730158731::Val accuracy: 79.04285714285714::Train loss 0.33084605089704655::Val Loss 0.03674369046078428\n",
      "Iteration: 182::Train accuracy: 78.45079365079364::Val accuracy: 79.04285714285714::Train loss 0.33000111945201394::Val Loss 0.03665620677420437\n",
      "Iteration: 183::Train accuracy: 78.44444444444446::Val accuracy: 79.01428571428572::Train loss 0.3299851497575845::Val Loss 0.036658530639535974\n",
      "Iteration: 184::Train accuracy: 78.52380952380953::Val accuracy: 79.07142857142857::Train loss 0.32887066082956407::Val Loss 0.036536264208544\n",
      "Iteration: 185::Train accuracy: 78.72380952380954::Val accuracy: 79.17142857142856::Train loss 0.3255459727355709::Val Loss 0.036176031131740394\n",
      "Iteration: 186::Train accuracy: 78.91746031746032::Val accuracy: 79.24285714285715::Train loss 0.32218005726354465::Val Loss 0.035809655896149976\n",
      "Iteration: 187::Train accuracy: 79.05714285714286::Val accuracy: 79.4::Train loss 0.31863161791168565::Val Loss 0.035416493538705976\n",
      "Iteration: 188::Train accuracy: 79.24603174603175::Val accuracy: 79.60000000000001::Train loss 0.3148895627857311::Val Loss 0.03500817729271245\n",
      "Iteration: 189::Train accuracy: 79.45238095238095::Val accuracy: 79.84285714285714::Train loss 0.3113584861719522::Val Loss 0.03462124728093009\n",
      "Iteration: 190::Train accuracy: 79.6031746031746::Val accuracy: 79.87142857142857::Train loss 0.3084282786433664::Val Loss 0.03429779423033516\n",
      "Iteration: 191::Train accuracy: 79.76190476190477::Val accuracy: 79.98571428571428::Train loss 0.30600708401886934::Val Loss 0.03403197561652687\n",
      "Iteration: 192::Train accuracy: 79.87936507936509::Val accuracy: 79.97142857142858::Train loss 0.3043254949741518::Val Loss 0.0338513075143452\n",
      "Iteration: 193::Train accuracy: 79.97301587301587::Val accuracy: 80.05714285714286::Train loss 0.3027831966961689::Val Loss 0.03368597072694213\n",
      "Iteration: 194::Train accuracy: 80.04761904761905::Val accuracy: 79.98571428571428::Train loss 0.3018773511778084::Val Loss 0.033600222099888104\n",
      "Iteration: 195::Train accuracy: 80.10000000000001::Val accuracy: 80.02857142857142::Train loss 0.300726502523443::Val Loss 0.033481602841493166\n",
      "Iteration: 196::Train accuracy: 80.13650793650794::Val accuracy: 80.10000000000001::Train loss 0.3008678712281815::Val Loss 0.0334981984007354\n",
      "Iteration: 197::Train accuracy: 80.30000000000001::Val accuracy: 80.32857142857142::Train loss 0.29818347838176096::Val Loss 0.033199616393923104\n",
      "Iteration: 198::Train accuracy: 80.4063492063492::Val accuracy: 80.38571428571429::Train loss 0.2963257647810298::Val Loss 0.03299437540784002\n",
      "Iteration: 199::Train accuracy: 80.51904761904763::Val accuracy: 80.5::Train loss 0.2943972000115268::Val Loss 0.0327829922011792\n",
      "Iteration: 200::Train accuracy: 80.5968253968254::Val accuracy: 80.65714285714286::Train loss 0.2928825318239694::Val Loss 0.03261730544325987\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_npVar, val_acc_npVar, train_loss_npVar, val_loss_npVar, sum_weights_npVar = batch_grad_descentFPOCNP(X=x_train,Y=y_train,iter = epochsToTrain, lr=0.005, pert=0.5, mu=mu, sigma=sigma, vDD = vDD, precision = precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22aa7a7dbd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.plot(train_acc_bpVar, \"c^\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP(software)\", \"BP(software)\", \"BP(with variability)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.5873015873016"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc_bpVar[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the software NP and BP algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(y,y_pre):\n",
    "  loss=-np.sum(np.multiply(y, np.log(y_pre)), axis = 0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #print(W2[3])\n",
    "  #first approximate dZ3\n",
    "  m = Y1.shape[0]\n",
    "  #print(Z3.shape)\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i, :] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #lossArrayAfterPertZ3[i, :] += np.sum(np.square(A3pert-one_hot_encoding(Y1)), axis=0)\n",
    "    lossArrayAfterPertZ3[i, :] += crossEntropy(one_hot_encoding(Y1), A3pert)\n",
    "\n",
    "\n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "\n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #lossArrayAfterPertZ2[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "    lossArrayAfterPertZ2[i, :] += crossEntropy(one_hot_encoding(Y1), A3pert)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "    A1pert = relu(Z1pert)\n",
    "    \n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    \n",
    "    #lossArrayAfterPertZ1[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "    lossArrayAfterPertZ1[i, :] += crossEntropy(one_hot_encoding(Y1), A3pert)\n",
    "\n",
    "    \n",
    "  #print(lossArrayAfterPertZ1)\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDComp(X,Y,iter, lrBP, lrNP, pert, seed=2,print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init(seed)\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    #print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "\n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1np, b1np, W2np,b2np, W3np, b3np) \n",
    "      print(f\"NP Iter {i+1} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "      lossBeforePert = crossEntropy(one_hot_encoding(Y1), A3)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1np, b1np, W2np, b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr = lrNP)\n",
    "      #print(W1)\n",
    "\n",
    "\n",
    "      #print(f\"Max W1={np.max(np.abs(W1np))},b1={np.max(np.abs(b1np))},W2={np.max(np.abs(W2np))},b2={np.max(np.abs(b2np))},W3={np.max(np.abs(W3np))},b3={np.max(np.abs(b3np))}\", end= 'r', flush=True)\n",
    "\n",
    "\n",
    "\n",
    "      ##print(f\"Max W1={np.max(np.abs(W1np))},b1={np.max(np.abs(b1np))},W2={np.max(np.abs(W2np))},b2={np.max(np.abs(b2np))},W3={np.max(np.abs(W3np))},b3={np.max(np.abs(b3np))}\", end= 'r', flush=True)\n",
    "      #doing the back propagation for the same data set sample\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1bp, b1bp, W2bp,b2bp, W3bp, b3bp) \n",
    "      print(f\"                                                                        BP Iter {i+1} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1bp, W2bp, W3bp, X1, Y1)\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1bp, b1bp, W2bp, b2bp, W3bp, b3bp, dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp, lr = lrBP)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    #lrNP = lrNP*np.exp(-0.01)\n",
    "    #lrBP = lrBP*np.exp(-0.01)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      \n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train Acc BP::{accuracy(predictions(A3_train_bp), Y)} Val Acc BP::{accuracy(predictions(A3_val_bp), y_val)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_acc, val_acc, train_loss, val_loss, sum_weights, W3np, dW1bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 80.57936507936509::Val accuracy: 79.95714285714286::Train Acc BP::81.9063492063492 Val Acc BP::81.28571428571428\n",
      "Iteration: 2::Train accuracy: 85.39523809523808::Val accuracy: 85.0142857142857::Train Acc BP::86.77936507936508 Val Acc BP::86.4857142857143\n",
      "Iteration: 3::Train accuracy: 87.64126984126985::Val accuracy: 86.95714285714286::Train Acc BP::89.09047619047618 Val Acc BP::88.58571428571429\n",
      "Iteration: 4::Train accuracy: 88.98253968253968::Val accuracy: 88.57142857142857::Train Acc BP::90.35079365079365 Val Acc BP::89.7\n",
      "Iteration: 5::Train accuracy: 89.92698412698412::Val accuracy: 89.51428571428572::Train Acc BP::91.16984126984127 Val Acc BP::90.67142857142856\n",
      "Iteration: 6::Train accuracy: 90.65079365079364::Val accuracy: 90.14285714285715::Train Acc BP::91.84126984126985 Val Acc BP::91.05714285714286\n",
      "Iteration: 7::Train accuracy: 91.21111111111111::Val accuracy: 90.84285714285714::Train Acc BP::92.4 Val Acc BP::91.51428571428572\n",
      "Iteration: 8::Train accuracy: 91.65555555555555::Val accuracy: 91.27142857142857::Train Acc BP::92.9095238095238 Val Acc BP::91.98571428571428\n",
      "Iteration: 9::Train accuracy: 92.03809523809524::Val accuracy: 91.57142857142857::Train Acc BP::93.26984126984127 Val Acc BP::92.28571428571428\n",
      "Iteration: 10::Train accuracy: 92.37301587301587::Val accuracy: 91.78571428571428::Train Acc BP::93.6079365079365 Val Acc BP::92.57142857142857\n",
      "Iteration: 11::Train accuracy: 92.64920634920635::Val accuracy: 92.0::Train Acc BP::93.86031746031746 Val Acc BP::92.80000000000001\n",
      "Iteration: 12::Train accuracy: 92.88412698412698::Val accuracy: 92.18571428571428::Train Acc BP::94.11269841269842 Val Acc BP::92.97142857142858\n",
      "Iteration: 13::Train accuracy: 93.0968253968254::Val accuracy: 92.30000000000001::Train Acc BP::94.33015873015873 Val Acc BP::93.12857142857143\n",
      "Iteration: 14::Train accuracy: 93.30952380952381::Val accuracy: 92.48571428571428::Train Acc BP::94.54920634920634 Val Acc BP::93.2\n",
      "Iteration: 15::Train accuracy: 93.5063492063492::Val accuracy: 92.7::Train Acc BP::94.73174603174603 Val Acc BP::93.32857142857142\n",
      "Iteration: 16::Train accuracy: 93.65238095238095::Val accuracy: 93.10000000000001::Train Acc BP::94.89999999999999 Val Acc BP::93.44285714285714\n",
      "Iteration: 17::Train accuracy: 93.8::Val accuracy: 93.10000000000001::Train Acc BP::95.05238095238096 Val Acc BP::93.65714285714286\n",
      "Iteration: 18::Train accuracy: 93.93968253968255::Val accuracy: 93.2::Train Acc BP::95.17936507936507 Val Acc BP::93.77142857142857\n",
      "Iteration: 19::Train accuracy: 94.02380952380952::Val accuracy: 93.28571428571428::Train Acc BP::95.32857142857142 Val Acc BP::93.94285714285714\n",
      "Iteration: 20::Train accuracy: 94.17777777777778::Val accuracy: 93.37142857142857::Train Acc BP::95.45238095238095 Val Acc BP::94.02857142857142\n",
      "Iteration: 21::Train accuracy: 94.29206349206349::Val accuracy: 93.45714285714286::Train Acc BP::95.56666666666666 Val Acc BP::94.05714285714286\n",
      "Iteration: 22::Train accuracy: 94.40793650793651::Val accuracy: 93.57142857142857::Train Acc BP::95.66190476190476 Val Acc BP::94.14285714285714\n",
      "Iteration: 23::Train accuracy: 94.5047619047619::Val accuracy: 93.7::Train Acc BP::95.76190476190476 Val Acc BP::94.21428571428572\n",
      "Iteration: 24::Train accuracy: 94.61269841269842::Val accuracy: 93.74285714285713::Train Acc BP::95.85238095238095 Val Acc BP::94.28571428571428\n",
      "Iteration: 25::Train accuracy: 94.7063492063492::Val accuracy: 93.8::Train Acc BP::95.94126984126984 Val Acc BP::94.32857142857142\n",
      "Iteration: 26::Train accuracy: 94.82063492063492::Val accuracy: 93.78571428571428::Train Acc BP::96.04761904761905 Val Acc BP::94.44285714285714\n",
      "Iteration: 27::Train accuracy: 94.8920634920635::Val accuracy: 93.91428571428571::Train Acc BP::96.13809523809523 Val Acc BP::94.51428571428572\n",
      "Iteration: 28::Train accuracy: 94.97460317460317::Val accuracy: 94.08571428571429::Train Acc BP::96.1920634920635 Val Acc BP::94.64285714285714\n",
      "Iteration: 29::Train accuracy: 95.05396825396826::Val accuracy: 94.1::Train Acc BP::96.27460317460319 Val Acc BP::94.71428571428572\n",
      "Iteration: 30::Train accuracy: 95.12222222222222::Val accuracy: 94.12857142857143::Train Acc BP::96.37301587301587 Val Acc BP::94.77142857142857\n",
      "Pert 1 complete\n",
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 81.77460317460317::Val accuracy: 81.32857142857142::Train Acc BP::81.9063492063492 Val Acc BP::81.28571428571428\n",
      "Iteration: 2::Train accuracy: 86.64126984126985::Val accuracy: 86.24285714285715::Train Acc BP::86.77936507936508 Val Acc BP::86.4857142857143\n",
      "Iteration: 3::Train accuracy: 88.98095238095239::Val accuracy: 88.3::Train Acc BP::89.09047619047618 Val Acc BP::88.58571428571429\n",
      "Iteration: 4::Train accuracy: 90.2968253968254::Val accuracy: 89.72857142857143::Train Acc BP::90.35079365079365 Val Acc BP::89.7\n",
      "Iteration: 5::Train accuracy: 91.11428571428571::Val accuracy: 90.4::Train Acc BP::91.16984126984127 Val Acc BP::90.67142857142856\n",
      "Iteration: 6::Train accuracy: 91.77301587301588::Val accuracy: 91.04285714285714::Train Acc BP::91.84126984126985 Val Acc BP::91.05714285714286\n",
      "Iteration: 7::Train accuracy: 92.2936507936508::Val accuracy: 91.51428571428572::Train Acc BP::92.4 Val Acc BP::91.51428571428572\n",
      "Iteration: 8::Train accuracy: 92.77142857142857::Val accuracy: 91.9::Train Acc BP::92.9095238095238 Val Acc BP::91.98571428571428\n",
      "Iteration: 9::Train accuracy: 93.17777777777778::Val accuracy: 92.24285714285713::Train Acc BP::93.26984126984127 Val Acc BP::92.28571428571428\n",
      "Iteration: 10::Train accuracy: 93.50158730158729::Val accuracy: 92.47142857142858::Train Acc BP::93.6079365079365 Val Acc BP::92.57142857142857\n",
      "Iteration: 11::Train accuracy: 93.81587301587302::Val accuracy: 92.68571428571428::Train Acc BP::93.86031746031746 Val Acc BP::92.80000000000001\n",
      "Iteration: 12::Train accuracy: 94.04603174603174::Val accuracy: 92.87142857142857::Train Acc BP::94.11269841269842 Val Acc BP::92.97142857142858\n",
      "Iteration: 13::Train accuracy: 94.25238095238096::Val accuracy: 92.97142857142858::Train Acc BP::94.33015873015873 Val Acc BP::93.12857142857143\n",
      "Iteration: 14::Train accuracy: 94.46031746031746::Val accuracy: 93.10000000000001::Train Acc BP::94.54920634920634 Val Acc BP::93.2\n",
      "Iteration: 15::Train accuracy: 94.63650793650794::Val accuracy: 93.28571428571428::Train Acc BP::94.73174603174603 Val Acc BP::93.32857142857142\n",
      "Iteration: 16::Train accuracy: 94.7952380952381::Val accuracy: 93.42857142857143::Train Acc BP::94.89999999999999 Val Acc BP::93.44285714285714\n",
      "Iteration: 17::Train accuracy: 94.92539682539683::Val accuracy: 93.48571428571428::Train Acc BP::95.05238095238096 Val Acc BP::93.65714285714286\n",
      "Iteration: 18::Train accuracy: 95.06031746031745::Val accuracy: 93.57142857142857::Train Acc BP::95.17936507936507 Val Acc BP::93.77142857142857\n",
      "Iteration: 19::Train accuracy: 95.16507936507936::Val accuracy: 93.65714285714286::Train Acc BP::95.32857142857142 Val Acc BP::93.94285714285714\n",
      "Iteration: 20::Train accuracy: 95.2904761904762::Val accuracy: 93.81428571428572::Train Acc BP::95.45238095238095 Val Acc BP::94.02857142857142\n",
      "Iteration: 21::Train accuracy: 95.41269841269842::Val accuracy: 93.87142857142857::Train Acc BP::95.56666666666666 Val Acc BP::94.05714285714286\n",
      "Iteration: 22::Train accuracy: 95.51904761904761::Val accuracy: 93.97142857142858::Train Acc BP::95.66190476190476 Val Acc BP::94.14285714285714\n",
      "Iteration: 23::Train accuracy: 95.63174603174603::Val accuracy: 94.07142857142857::Train Acc BP::95.76190476190476 Val Acc BP::94.21428571428572\n",
      "Iteration: 24::Train accuracy: 95.71428571428572::Val accuracy: 94.17142857142858::Train Acc BP::95.85238095238095 Val Acc BP::94.28571428571428\n",
      "Iteration: 25::Train accuracy: 95.81269841269841::Val accuracy: 94.19999999999999::Train Acc BP::95.94126984126984 Val Acc BP::94.32857142857142\n",
      "Iteration: 26::Train accuracy: 95.91904761904762::Val accuracy: 94.25714285714287::Train Acc BP::96.04761904761905 Val Acc BP::94.44285714285714\n",
      "Iteration: 27::Train accuracy: 96.0031746031746::Val accuracy: 94.32857142857142::Train Acc BP::96.13809523809523 Val Acc BP::94.51428571428572\n",
      "Iteration: 28::Train accuracy: 96.0952380952381::Val accuracy: 94.44285714285714::Train Acc BP::96.1920634920635 Val Acc BP::94.64285714285714\n",
      "Iteration: 29::Train accuracy: 96.17301587301587::Val accuracy: 94.47142857142858::Train Acc BP::96.27460317460319 Val Acc BP::94.71428571428572\n",
      "Iteration: 30::Train accuracy: 96.25396825396825::Val accuracy: 94.5::Train Acc BP::96.37301587301587 Val Acc BP::94.77142857142857\n",
      "Pert 0.1 complete\n",
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 81.88412698412698::Val accuracy: 81.24285714285713::Train Acc BP::81.9063492063492 Val Acc BP::81.28571428571428\n",
      "Iteration: 2::Train accuracy: 86.76349206349207::Val accuracy: 86.38571428571429::Train Acc BP::86.77936507936508 Val Acc BP::86.4857142857143\n",
      "Iteration: 3::Train accuracy: 89.08253968253969::Val accuracy: 88.54285714285714::Train Acc BP::89.09047619047618 Val Acc BP::88.58571428571429\n",
      "Iteration: 4::Train accuracy: 90.35238095238095::Val accuracy: 89.81428571428572::Train Acc BP::90.35079365079365 Val Acc BP::89.7\n",
      "Iteration: 5::Train accuracy: 91.18095238095239::Val accuracy: 90.58571428571427::Train Acc BP::91.16984126984127 Val Acc BP::90.67142857142856\n",
      "Iteration: 6::Train accuracy: 91.85714285714286::Val accuracy: 91.01428571428572::Train Acc BP::91.84126984126985 Val Acc BP::91.05714285714286\n",
      "Iteration: 7::Train accuracy: 92.39047619047619::Val accuracy: 91.47142857142858::Train Acc BP::92.4 Val Acc BP::91.51428571428572\n",
      "Iteration: 8::Train accuracy: 92.87619047619047::Val accuracy: 91.94285714285715::Train Acc BP::92.9095238095238 Val Acc BP::91.98571428571428\n",
      "Iteration: 9::Train accuracy: 93.25079365079365::Val accuracy: 92.31428571428572::Train Acc BP::93.26984126984127 Val Acc BP::92.28571428571428\n",
      "Iteration: 10::Train accuracy: 93.57460317460318::Val accuracy: 92.51428571428572::Train Acc BP::93.6079365079365 Val Acc BP::92.57142857142857\n",
      "Iteration: 11::Train accuracy: 93.85079365079365::Val accuracy: 92.78571428571428::Train Acc BP::93.86031746031746 Val Acc BP::92.80000000000001\n",
      "Iteration: 12::Train accuracy: 94.1031746031746::Val accuracy: 93.02857142857142::Train Acc BP::94.11269841269842 Val Acc BP::92.97142857142858\n",
      "Iteration: 13::Train accuracy: 94.33333333333334::Val accuracy: 93.10000000000001::Train Acc BP::94.33015873015873 Val Acc BP::93.12857142857143\n",
      "Iteration: 14::Train accuracy: 94.54603174603174::Val accuracy: 93.24285714285713::Train Acc BP::94.54920634920634 Val Acc BP::93.2\n",
      "Iteration: 15::Train accuracy: 94.72698412698412::Val accuracy: 93.34285714285714::Train Acc BP::94.73174603174603 Val Acc BP::93.32857142857142\n",
      "Iteration: 16::Train accuracy: 94.8873015873016::Val accuracy: 93.51428571428572::Train Acc BP::94.89999999999999 Val Acc BP::93.44285714285714\n",
      "Iteration: 17::Train accuracy: 95.03333333333333::Val accuracy: 93.62857142857143::Train Acc BP::95.05238095238096 Val Acc BP::93.65714285714286\n",
      "Iteration: 18::Train accuracy: 95.17619047619048::Val accuracy: 93.75714285714287::Train Acc BP::95.17936507936507 Val Acc BP::93.77142857142857\n",
      "Iteration: 19::Train accuracy: 95.31428571428572::Val accuracy: 93.88571428571429::Train Acc BP::95.32857142857142 Val Acc BP::93.94285714285714\n",
      "Iteration: 20::Train accuracy: 95.43650793650794::Val accuracy: 93.97142857142858::Train Acc BP::95.45238095238095 Val Acc BP::94.02857142857142\n",
      "Iteration: 21::Train accuracy: 95.54444444444444::Val accuracy: 94.07142857142857::Train Acc BP::95.56666666666666 Val Acc BP::94.05714285714286\n",
      "Iteration: 22::Train accuracy: 95.63809523809523::Val accuracy: 94.11428571428571::Train Acc BP::95.66190476190476 Val Acc BP::94.14285714285714\n",
      "Iteration: 23::Train accuracy: 95.72698412698412::Val accuracy: 94.22857142857143::Train Acc BP::95.76190476190476 Val Acc BP::94.21428571428572\n",
      "Iteration: 24::Train accuracy: 95.83333333333334::Val accuracy: 94.24285714285713::Train Acc BP::95.85238095238095 Val Acc BP::94.28571428571428\n",
      "Iteration: 25::Train accuracy: 95.93015873015874::Val accuracy: 94.31428571428572::Train Acc BP::95.94126984126984 Val Acc BP::94.32857142857142\n",
      "Iteration: 26::Train accuracy: 96.01904761904761::Val accuracy: 94.38571428571429::Train Acc BP::96.04761904761905 Val Acc BP::94.44285714285714\n",
      "Iteration: 27::Train accuracy: 96.10634920634921::Val accuracy: 94.5::Train Acc BP::96.13809523809523 Val Acc BP::94.51428571428572\n",
      "Iteration: 28::Train accuracy: 96.17619047619047::Val accuracy: 94.61428571428571::Train Acc BP::96.1920634920635 Val Acc BP::94.64285714285714\n",
      "Iteration: 29::Train accuracy: 96.26031746031745::Val accuracy: 94.67142857142858::Train Acc BP::96.27460317460319 Val Acc BP::94.71428571428572\n",
      "Iteration: 30::Train accuracy: 96.34920634920636::Val accuracy: 94.77142857142857::Train Acc BP::96.37301587301587 Val Acc BP::94.77142857142857\n",
      "Pert 0.01 complete\n",
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 81.91269841269842::Val accuracy: 81.32857142857142::Train Acc BP::81.9063492063492 Val Acc BP::81.28571428571428\n",
      "Iteration: 2::Train accuracy: 86.7904761904762::Val accuracy: 86.5142857142857::Train Acc BP::86.77936507936508 Val Acc BP::86.4857142857143\n",
      "Iteration: 3::Train accuracy: 89.08412698412698::Val accuracy: 88.54285714285714::Train Acc BP::89.09047619047618 Val Acc BP::88.58571428571429\n",
      "Iteration: 4::Train accuracy: 90.36666666666666::Val accuracy: 89.6857142857143::Train Acc BP::90.35079365079365 Val Acc BP::89.7\n",
      "Iteration: 5::Train accuracy: 91.16825396825396::Val accuracy: 90.58571428571427::Train Acc BP::91.16984126984127 Val Acc BP::90.67142857142856\n",
      "Iteration: 6::Train accuracy: 91.85238095238095::Val accuracy: 91.04285714285714::Train Acc BP::91.84126984126985 Val Acc BP::91.05714285714286\n",
      "Iteration: 7::Train accuracy: 92.40158730158731::Val accuracy: 91.60000000000001::Train Acc BP::92.4 Val Acc BP::91.51428571428572\n",
      "Iteration: 8::Train accuracy: 92.8968253968254::Val accuracy: 91.98571428571428::Train Acc BP::92.9095238095238 Val Acc BP::91.98571428571428\n",
      "Iteration: 9::Train accuracy: 93.27301587301588::Val accuracy: 92.34285714285714::Train Acc BP::93.26984126984127 Val Acc BP::92.28571428571428\n",
      "Iteration: 10::Train accuracy: 93.62222222222222::Val accuracy: 92.61428571428571::Train Acc BP::93.6079365079365 Val Acc BP::92.57142857142857\n",
      "Iteration: 11::Train accuracy: 93.87142857142857::Val accuracy: 92.81428571428572::Train Acc BP::93.86031746031746 Val Acc BP::92.80000000000001\n",
      "Iteration: 12::Train accuracy: 94.13492063492063::Val accuracy: 92.97142857142858::Train Acc BP::94.11269841269842 Val Acc BP::92.97142857142858\n",
      "Iteration: 13::Train accuracy: 94.34126984126983::Val accuracy: 93.08571428571429::Train Acc BP::94.33015873015873 Val Acc BP::93.12857142857143\n",
      "Iteration: 14::Train accuracy: 94.54126984126984::Val accuracy: 93.2::Train Acc BP::94.54920634920634 Val Acc BP::93.2\n",
      "Iteration: 15::Train accuracy: 94.73015873015873::Val accuracy: 93.31428571428572::Train Acc BP::94.73174603174603 Val Acc BP::93.32857142857142\n",
      "Iteration: 16::Train accuracy: 94.9015873015873::Val accuracy: 93.47142857142858::Train Acc BP::94.89999999999999 Val Acc BP::93.44285714285714\n",
      "Iteration: 17::Train accuracy: 95.04444444444444::Val accuracy: 93.65714285714286::Train Acc BP::95.05238095238096 Val Acc BP::93.65714285714286\n",
      "Iteration: 18::Train accuracy: 95.1952380952381::Val accuracy: 93.78571428571428::Train Acc BP::95.17936507936507 Val Acc BP::93.77142857142857\n",
      "Iteration: 19::Train accuracy: 95.34761904761905::Val accuracy: 93.88571428571429::Train Acc BP::95.32857142857142 Val Acc BP::93.94285714285714\n",
      "Iteration: 20::Train accuracy: 95.44761904761904::Val accuracy: 93.97142857142858::Train Acc BP::95.45238095238095 Val Acc BP::94.02857142857142\n",
      "Iteration: 21::Train accuracy: 95.56825396825397::Val accuracy: 94.04285714285714::Train Acc BP::95.56666666666666 Val Acc BP::94.05714285714286\n",
      "Iteration: 22::Train accuracy: 95.67619047619048::Val accuracy: 94.1::Train Acc BP::95.66190476190476 Val Acc BP::94.14285714285714\n",
      "Iteration: 23::Train accuracy: 95.76507936507936::Val accuracy: 94.27142857142857::Train Acc BP::95.76190476190476 Val Acc BP::94.21428571428572\n",
      "Iteration: 24::Train accuracy: 95.86190476190475::Val accuracy: 94.32857142857142::Train Acc BP::95.85238095238095 Val Acc BP::94.28571428571428\n",
      "Iteration: 25::Train accuracy: 95.95079365079366::Val accuracy: 94.38571428571429::Train Acc BP::95.94126984126984 Val Acc BP::94.32857142857142\n",
      "Iteration: 26::Train accuracy: 96.04444444444444::Val accuracy: 94.52857142857142::Train Acc BP::96.04761904761905 Val Acc BP::94.44285714285714\n",
      "Iteration: 27::Train accuracy: 96.13492063492063::Val accuracy: 94.55714285714286::Train Acc BP::96.13809523809523 Val Acc BP::94.51428571428572\n",
      "Iteration: 28::Train accuracy: 96.2015873015873::Val accuracy: 94.64285714285714::Train Acc BP::96.1920634920635 Val Acc BP::94.64285714285714\n",
      "Iteration: 29::Train accuracy: 96.29206349206349::Val accuracy: 94.69999999999999::Train Acc BP::96.27460317460319 Val Acc BP::94.71428571428572\n",
      "Iteration: 30::Train accuracy: 96.37936507936507::Val accuracy: 94.8::Train Acc BP::96.37301587301587 Val Acc BP::94.77142857142857\n",
      "Pert 0.001 complete\n",
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 81.87619047619049::Val accuracy: 81.28571428571428::Train Acc BP::81.9063492063492 Val Acc BP::81.28571428571428\n",
      "Iteration: 2::Train accuracy: 86.7904761904762::Val accuracy: 86.37142857142858::Train Acc BP::86.77936507936508 Val Acc BP::86.4857142857143\n",
      "Iteration: 3::Train accuracy: 89.09047619047618::Val accuracy: 88.6::Train Acc BP::89.09047619047618 Val Acc BP::88.58571428571429\n",
      "Iteration: 4::Train accuracy: 90.35238095238095::Val accuracy: 89.74285714285715::Train Acc BP::90.35079365079365 Val Acc BP::89.7\n",
      "Iteration: 5::Train accuracy: 91.14603174603174::Val accuracy: 90.62857142857142::Train Acc BP::91.16984126984127 Val Acc BP::90.67142857142856\n",
      "Iteration: 6::Train accuracy: 91.84761904761905::Val accuracy: 91.02857142857142::Train Acc BP::91.84126984126985 Val Acc BP::91.05714285714286\n",
      "Iteration: 7::Train accuracy: 92.38571428571429::Val accuracy: 91.48571428571428::Train Acc BP::92.4 Val Acc BP::91.51428571428572\n",
      "Iteration: 8::Train accuracy: 92.9031746031746::Val accuracy: 91.87142857142857::Train Acc BP::92.9095238095238 Val Acc BP::91.98571428571428\n",
      "Iteration: 9::Train accuracy: 93.26666666666667::Val accuracy: 92.24285714285713::Train Acc BP::93.26984126984127 Val Acc BP::92.28571428571428\n",
      "Iteration: 10::Train accuracy: 93.62222222222222::Val accuracy: 92.58571428571429::Train Acc BP::93.6079365079365 Val Acc BP::92.57142857142857\n",
      "Iteration: 11::Train accuracy: 93.84126984126983::Val accuracy: 92.81428571428572::Train Acc BP::93.86031746031746 Val Acc BP::92.80000000000001\n",
      "Iteration: 12::Train accuracy: 94.13015873015873::Val accuracy: 92.95714285714286::Train Acc BP::94.11269841269842 Val Acc BP::92.97142857142858\n",
      "Iteration: 13::Train accuracy: 94.34285714285714::Val accuracy: 93.08571428571429::Train Acc BP::94.33015873015873 Val Acc BP::93.12857142857143\n",
      "Iteration: 14::Train accuracy: 94.54761904761905::Val accuracy: 93.22857142857143::Train Acc BP::94.54920634920634 Val Acc BP::93.2\n",
      "Iteration: 15::Train accuracy: 94.72857142857143::Val accuracy: 93.31428571428572::Train Acc BP::94.73174603174603 Val Acc BP::93.32857142857142\n",
      "Iteration: 16::Train accuracy: 94.91269841269842::Val accuracy: 93.48571428571428::Train Acc BP::94.89999999999999 Val Acc BP::93.44285714285714\n",
      "Iteration: 17::Train accuracy: 95.05396825396826::Val accuracy: 93.67142857142858::Train Acc BP::95.05238095238096 Val Acc BP::93.65714285714286\n",
      "Iteration: 18::Train accuracy: 95.21746031746032::Val accuracy: 93.75714285714287::Train Acc BP::95.17936507936507 Val Acc BP::93.77142857142857\n",
      "Iteration: 19::Train accuracy: 95.33492063492064::Val accuracy: 94.0::Train Acc BP::95.32857142857142 Val Acc BP::93.94285714285714\n",
      "Iteration: 20::Train accuracy: 95.45238095238095::Val accuracy: 94.04285714285714::Train Acc BP::95.45238095238095 Val Acc BP::94.02857142857142\n",
      "Iteration: 21::Train accuracy: 95.56349206349206::Val accuracy: 94.17142857142858::Train Acc BP::95.56666666666666 Val Acc BP::94.05714285714286\n",
      "Iteration: 22::Train accuracy: 95.66031746031746::Val accuracy: 94.18571428571428::Train Acc BP::95.66190476190476 Val Acc BP::94.14285714285714\n",
      "Iteration: 23::Train accuracy: 95.75714285714285::Val accuracy: 94.28571428571428::Train Acc BP::95.76190476190476 Val Acc BP::94.21428571428572\n",
      "Iteration: 24::Train accuracy: 95.86507936507937::Val accuracy: 94.35714285714286::Train Acc BP::95.85238095238095 Val Acc BP::94.28571428571428\n",
      "Iteration: 25::Train accuracy: 95.94920634920635::Val accuracy: 94.34285714285714::Train Acc BP::95.94126984126984 Val Acc BP::94.32857142857142\n",
      "Iteration: 26::Train accuracy: 96.04761904761905::Val accuracy: 94.41428571428571::Train Acc BP::96.04761904761905 Val Acc BP::94.44285714285714\n",
      "Iteration: 27::Train accuracy: 96.13333333333334::Val accuracy: 94.51428571428572::Train Acc BP::96.13809523809523 Val Acc BP::94.51428571428572\n",
      "Iteration: 28::Train accuracy: 96.2::Val accuracy: 94.62857142857143::Train Acc BP::96.1920634920635 Val Acc BP::94.64285714285714\n",
      "Iteration: 29::Train accuracy: 96.29206349206349::Val accuracy: 94.71428571428572::Train Acc BP::96.27460317460319 Val Acc BP::94.71428571428572\n",
      "Iteration: 30::Train accuracy: 96.3920634920635::Val accuracy: 94.78571428571428::Train Acc BP::96.37301587301587 Val Acc BP::94.77142857142857\n",
      "Pert 0.0001 complete\n"
     ]
    }
   ],
   "source": [
    "perturbation = [1, 0.1, 0.01, 0.001, 0.0001] #, 0.00001, 0.000001, 0.0000001]\n",
    "trainBPList = []\n",
    "trainNPList = []\n",
    "for pert in perturbation:\n",
    "    w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _, dW1npt, dW1bpt = batchGDComp(x_train,y_train,30, 0.1, 0.1,pert, print_op=1);\n",
    "    print(f\"Pert {pert} complete\")\n",
    "    trainBPTemp= [i[0] for i in trainAccBoth]\n",
    "    trainNPTemp = [i[1] for i in trainAccBoth]\n",
    "    trainBPList.append(trainBPTemp)\n",
    "    trainNPList.append(trainNPTemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting for different perturbation values (software)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "format = ['bx', 'g.', 'r^', 'kd', 'c+']\n",
    "col = ['b', 'g', 'r', 'k', 'c']\n",
    "leg = []\n",
    "plt.figure(figsize=(15,10))\n",
    "for _ in range(len(perturbation)):\n",
    "    plt.plot(trainNPList[_], col[_], linewidth=2,alpha=0.3)\n",
    "plt.plot(trainBPList[0], 'm', linewidth=2,alpha=0.3)\n",
    "for _ in range(len(perturbation)):\n",
    "    leg.append(f\"Node pertubation with pert = {perturbation[_]}\")\n",
    "    plt.plot(trainNPList[_], format[_])\n",
    "    plt.plot(trainNPList[_], col[_], linewidth=2,alpha=0.3)\n",
    "plt.plot(trainBPList[0], 'm*')\n",
    "leg.append(\"Back propagation\")\n",
    "plt.legend(leg)\n",
    "\n",
    "plt.title(\"Accuracy vs epochs for the  different perturbation\", size=15)\n",
    "plt.xlabel(\"Epochs ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "finAccuracies = [i[-1] for i in trainNPList]\n",
    "plt.axhline(y = trainBPList[0][-1], color='r')\n",
    "plt.plot(np.log10(np.array(perturbation)), finAccuracies, 'go')\n",
    "for i in np.log10(np.array(perturbation)):\n",
    "    plt.axvline(x = i, linestyle='--', linewidth = 1, alpha=0.3)\n",
    "plt.legend([\"Back propagation training accuracy\", \"NP training accuracy\"])\n",
    "plt.title(\"Final Accuracies for BP and NP with different perturbation\", size=15)\n",
    "plt.xlabel(\"Perturbation(log10 scale) ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "0.9999999999999997\n",
      "Iteration: 1::Train accuracy: 9.846031746031745::Val accuracy: 9.971428571428572::Train Acc BP::9.844444444444445 Val Acc BP::9.185714285714287\n",
      "0.9999995164409428\n",
      "Iteration: 2::Train accuracy: 9.974603174603175::Val accuracy: 10.085714285714285::Train Acc BP::11.007936507936508 Val Acc BP::10.4\n",
      "0.9999985844600019\n",
      "Iteration: 3::Train accuracy: 10.096825396825396::Val accuracy: 10.4::Train Acc BP::12.304761904761904 Val Acc BP::11.942857142857143\n",
      "0.9999975219493955\n",
      "Iteration: 4::Train accuracy: 10.217460317460317::Val accuracy: 10.585714285714285::Train Acc BP::13.561904761904762 Val Acc BP::13.385714285714286\n",
      "0.9999965073297103\n",
      "Iteration: 5::Train accuracy: 10.353968253968254::Val accuracy: 10.557142857142857::Train Acc BP::14.844444444444443 Val Acc BP::14.657142857142858\n",
      "0.9999955591551105\n",
      "Iteration: 6::Train accuracy: 10.47936507936508::Val accuracy: 10.614285714285714::Train Acc BP::15.946031746031746 Val Acc BP::15.814285714285713\n",
      "0.9999946449000549\n",
      "Iteration: 7::Train accuracy: 10.617460317460317::Val accuracy: 10.757142857142856::Train Acc BP::17.114285714285714 Val Acc BP::17.085714285714285\n",
      "0.9999937375289455\n",
      "Iteration: 8::Train accuracy: 10.769841269841269::Val accuracy: 11.042857142857143::Train Acc BP::18.252380952380953 Val Acc BP::18.0\n",
      "0.9999928258591593\n",
      "Iteration: 9::Train accuracy: 10.96031746031746::Val accuracy: 11.314285714285715::Train Acc BP::19.28095238095238 Val Acc BP::19.3\n",
      "0.9999919064576464\n",
      "Iteration: 10::Train accuracy: 11.192063492063491::Val accuracy: 11.514285714285714::Train Acc BP::20.247619047619047 Val Acc BP::20.3\n",
      "0.9999909854383202\n",
      "Iteration: 11::Train accuracy: 11.420634920634921::Val accuracy: 11.600000000000001::Train Acc BP::21.22222222222222 Val Acc BP::21.585714285714285\n",
      "0.9999900701356013\n",
      "Iteration: 12::Train accuracy: 11.65079365079365::Val accuracy: 11.714285714285715::Train Acc BP::22.174603174603174 Val Acc BP::22.7\n",
      "0.9999891657135772\n",
      "Iteration: 13::Train accuracy: 11.912698412698413::Val accuracy: 11.899999999999999::Train Acc BP::23.153968253968255 Val Acc BP::23.614285714285714\n",
      "0.9999882744471709\n",
      "Iteration: 14::Train accuracy: 12.115873015873015::Val accuracy: 12.0::Train Acc BP::24.08730158730159 Val Acc BP::24.6\n",
      "0.9999873954286769\n",
      "Iteration: 15::Train accuracy: 12.268253968253969::Val accuracy: 12.2::Train Acc BP::25.023809523809526 Val Acc BP::25.585714285714285\n",
      "0.9999865263059154\n",
      "Iteration: 16::Train accuracy: 12.457142857142857::Val accuracy: 12.385714285714286::Train Acc BP::25.98412698412698 Val Acc BP::26.385714285714286\n",
      "0.9999856644582005\n",
      "Iteration: 17::Train accuracy: 12.598412698412698::Val accuracy: 12.514285714285714::Train Acc BP::26.81269841269841 Val Acc BP::27.400000000000002\n",
      "0.9999848086325619\n",
      "Iteration: 18::Train accuracy: 12.760317460317461::Val accuracy: 12.614285714285714::Train Acc BP::27.750793650793646 Val Acc BP::28.199999999999996\n",
      "0.9999839573215036\n",
      "Iteration: 19::Train accuracy: 12.917460317460316::Val accuracy: 12.757142857142856::Train Acc BP::28.61269841269841 Val Acc BP::28.985714285714288\n",
      "0.9999831105250694\n",
      "Iteration: 20::Train accuracy: 13.080952380952382::Val accuracy: 13.0::Train Acc BP::29.44126984126984 Val Acc BP::29.814285714285717\n",
      "0.9999822680356802\n",
      "Iteration: 21::Train accuracy: 13.206349206349207::Val accuracy: 13.17142857142857::Train Acc BP::30.319047619047616 Val Acc BP::30.7\n",
      "0.9999814288380513\n",
      "Iteration: 22::Train accuracy: 13.37936507936508::Val accuracy: 13.242857142857142::Train Acc BP::31.17936507936508 Val Acc BP::31.514285714285716\n",
      "0.999980594434652\n",
      "Iteration: 23::Train accuracy: 13.476190476190474::Val accuracy: 13.428571428571429::Train Acc BP::31.998412698412697 Val Acc BP::32.142857142857146\n",
      "0.9999797662027383\n",
      "Iteration: 24::Train accuracy: 13.587301587301587::Val accuracy: 13.571428571428571::Train Acc BP::32.768253968253966 Val Acc BP::32.9\n",
      "0.9999789425223171\n",
      "Iteration: 25::Train accuracy: 13.674603174603176::Val accuracy: 13.742857142857142::Train Acc BP::33.5984126984127 Val Acc BP::33.72857142857143\n",
      "0.9999781252683322\n",
      "Iteration: 26::Train accuracy: 13.803174603174604::Val accuracy: 13.87142857142857::Train Acc BP::34.336507936507935 Val Acc BP::34.31428571428572\n",
      "0.999977314945538\n",
      "Iteration: 27::Train accuracy: 13.922222222222222::Val accuracy: 13.942857142857143::Train Acc BP::35.09206349206349 Val Acc BP::35.12857142857143\n",
      "0.9999765119253564\n",
      "Iteration: 28::Train accuracy: 14.026984126984127::Val accuracy: 14.028571428571428::Train Acc BP::35.84444444444445 Val Acc BP::35.84285714285714\n",
      "0.999975716400569\n",
      "Iteration: 29::Train accuracy: 14.126984126984127::Val accuracy: 14.099999999999998::Train Acc BP::36.56507936507937 Val Acc BP::36.542857142857144\n",
      "0.9999749283161928\n",
      "Iteration: 30::Train accuracy: 14.238095238095239::Val accuracy: 14.2::Train Acc BP::37.161904761904765 Val Acc BP::37.128571428571426\n",
      "0.9999741475838191\n",
      "Iteration: 31::Train accuracy: 14.355555555555554::Val accuracy: 14.314285714285715::Train Acc BP::37.83015873015873 Val Acc BP::37.81428571428572\n",
      "0.9999733747001303\n",
      "Iteration: 32::Train accuracy: 14.463492063492064::Val accuracy: 14.371428571428572::Train Acc BP::38.477777777777774 Val Acc BP::38.32857142857143\n",
      "0.9999726096738659\n",
      "Iteration: 33::Train accuracy: 14.561904761904762::Val accuracy: 14.457142857142857::Train Acc BP::39.10634920634921 Val Acc BP::38.971428571428575\n",
      "0.9999718536795815\n",
      "Iteration: 34::Train accuracy: 14.63968253968254::Val accuracy: 14.499999999999998::Train Acc BP::39.75396825396825 Val Acc BP::39.614285714285714\n",
      "0.9999711057358021\n",
      "Iteration: 35::Train accuracy: 14.746031746031745::Val accuracy: 14.557142857142857::Train Acc BP::40.34285714285714 Val Acc BP::40.05714285714286\n",
      "0.9999703651328559\n",
      "Iteration: 36::Train accuracy: 14.853968253968253::Val accuracy: 14.657142857142858::Train Acc BP::40.87936507936508 Val Acc BP::40.68571428571428\n",
      "0.9999696329784439\n",
      "Iteration: 37::Train accuracy: 14.947619047619048::Val accuracy: 14.771428571428572::Train Acc BP::41.474603174603175 Val Acc BP::41.199999999999996\n",
      "0.999968908798597\n",
      "Iteration: 38::Train accuracy: 15.03015873015873::Val accuracy: 14.857142857142858::Train Acc BP::42.03650793650794 Val Acc BP::41.72857142857143\n",
      "0.9999681927068615\n",
      "Iteration: 39::Train accuracy: 15.10793650793651::Val accuracy: 14.985714285714286::Train Acc BP::42.592063492063495 Val Acc BP::42.35714285714286\n",
      "0.999967484938454\n",
      "Iteration: 40::Train accuracy: 15.214285714285714::Val accuracy: 15.1::Train Acc BP::43.13492063492064 Val Acc BP::43.042857142857144\n",
      "0.9999667850215108\n",
      "Iteration: 41::Train accuracy: 15.306349206349207::Val accuracy: 15.228571428571428::Train Acc BP::43.67936507936508 Val Acc BP::43.528571428571425\n",
      "0.9999660927731331\n",
      "Iteration: 42::Train accuracy: 15.403174603174602::Val accuracy: 15.285714285714286::Train Acc BP::44.13492063492063 Val Acc BP::43.885714285714286\n",
      "0.9999654079231636\n",
      "Iteration: 43::Train accuracy: 15.507936507936506::Val accuracy: 15.385714285714286::Train Acc BP::44.641269841269846 Val Acc BP::44.285714285714285\n",
      "0.9999647307105496\n",
      "Iteration: 44::Train accuracy: 15.6::Val accuracy: 15.442857142857141::Train Acc BP::45.141269841269846 Val Acc BP::44.871428571428574\n",
      "0.9999640606468017\n",
      "Iteration: 45::Train accuracy: 15.711111111111112::Val accuracy: 15.571428571428573::Train Acc BP::45.63809523809524 Val Acc BP::45.385714285714286\n",
      "0.9999633973773256\n",
      "Iteration: 46::Train accuracy: 15.807936507936507::Val accuracy: 15.714285714285714::Train Acc BP::46.10634920634921 Val Acc BP::45.77142857142857\n",
      "0.9999627415245088\n",
      "Iteration: 47::Train accuracy: 15.898412698412697::Val accuracy: 15.771428571428572::Train Acc BP::46.56984126984127 Val Acc BP::46.41428571428571\n",
      "0.9999620928307383\n",
      "Iteration: 48::Train accuracy: 15.971428571428573::Val accuracy: 15.871428571428572::Train Acc BP::47.08571428571429 Val Acc BP::46.84285714285714\n",
      "0.9999614504288715\n",
      "Iteration: 49::Train accuracy: 16.06190476190476::Val accuracy: 15.928571428571429::Train Acc BP::47.50793650793651 Val Acc BP::47.385714285714286\n",
      "0.9999608156836131\n",
      "Iteration: 50::Train accuracy: 16.157142857142855::Val accuracy: 16.057142857142857::Train Acc BP::47.923809523809524 Val Acc BP::47.699999999999996\n",
      "0.9999601876799649\n",
      "Iteration: 51::Train accuracy: 16.255555555555556::Val accuracy: 16.114285714285714::Train Acc BP::48.32857142857143 Val Acc BP::48.142857142857146\n",
      "0.9999595663700823\n",
      "Iteration: 52::Train accuracy: 16.347619047619048::Val accuracy: 16.214285714285715::Train Acc BP::48.67936507936508 Val Acc BP::48.528571428571425\n",
      "0.9999589509619153\n",
      "Iteration: 53::Train accuracy: 16.466666666666665::Val accuracy: 16.242857142857144::Train Acc BP::49.047619047619044 Val Acc BP::48.91428571428572\n",
      "0.9999583424616668\n",
      "Iteration: 54::Train accuracy: 16.553968253968254::Val accuracy: 16.314285714285713::Train Acc BP::49.42857142857143 Val Acc BP::49.371428571428574\n",
      "0.9999577407988471\n",
      "Iteration: 55::Train accuracy: 16.63174603174603::Val accuracy: 16.37142857142857::Train Acc BP::49.834920634920636 Val Acc BP::49.72857142857143\n",
      "0.9999571454566911\n",
      "Iteration: 56::Train accuracy: 16.71904761904762::Val accuracy: 16.428571428571427::Train Acc BP::50.14761904761905 Val Acc BP::50.18571428571429\n",
      "0.9999565562271636\n",
      "Iteration: 57::Train accuracy: 16.825396825396826::Val accuracy: 16.52857142857143::Train Acc BP::50.4984126984127 Val Acc BP::50.6\n",
      "0.9999559728266323\n",
      "Iteration: 58::Train accuracy: 16.914285714285715::Val accuracy: 16.685714285714287::Train Acc BP::50.828571428571436 Val Acc BP::50.957142857142856\n",
      "0.9999553948606779\n",
      "Iteration: 59::Train accuracy: 17.02063492063492::Val accuracy: 16.8::Train Acc BP::51.16666666666667 Val Acc BP::51.37142857142857\n",
      "0.9999548227235879\n",
      "Iteration: 60::Train accuracy: 17.125396825396823::Val accuracy: 16.957142857142856::Train Acc BP::51.487301587301594 Val Acc BP::51.67142857142857\n",
      "0.9999542562479857\n",
      "Iteration: 61::Train accuracy: 17.201587301587303::Val accuracy: 17.057142857142857::Train Acc BP::51.82222222222222 Val Acc BP::51.97142857142857\n",
      "0.9999536955407411\n",
      "Iteration: 62::Train accuracy: 17.287301587301588::Val accuracy: 17.12857142857143::Train Acc BP::52.15396825396825 Val Acc BP::52.300000000000004\n",
      "0.9999531413684388\n",
      "Iteration: 63::Train accuracy: 17.377777777777776::Val accuracy: 17.15714285714286::Train Acc BP::52.46349206349207 Val Acc BP::52.55714285714286\n",
      "0.9999525924454208\n",
      "Iteration: 64::Train accuracy: 17.44761904761905::Val accuracy: 17.228571428571428::Train Acc BP::52.77460317460317 Val Acc BP::52.900000000000006\n",
      "0.9999520489978656\n",
      "Iteration: 65::Train accuracy: 17.51904761904762::Val accuracy: 17.285714285714285::Train Acc BP::53.076190476190476 Val Acc BP::53.25714285714286\n",
      "0.999951511083581\n",
      "Iteration: 66::Train accuracy: 17.584126984126986::Val accuracy: 17.299999999999997::Train Acc BP::53.41428571428571 Val Acc BP::53.614285714285714\n",
      "0.9999509780341992\n",
      "Iteration: 67::Train accuracy: 17.679365079365077::Val accuracy: 17.457142857142856::Train Acc BP::53.71111111111111 Val Acc BP::53.957142857142856\n",
      "0.9999504498641594\n",
      "Iteration: 68::Train accuracy: 17.744444444444444::Val accuracy: 17.5::Train Acc BP::53.98412698412698 Val Acc BP::54.22857142857143\n",
      "0.9999499262870707\n",
      "Iteration: 69::Train accuracy: 17.814285714285713::Val accuracy: 17.57142857142857::Train Acc BP::54.27301587301587 Val Acc BP::54.45714285714286\n",
      "0.999949408067556\n",
      "Iteration: 70::Train accuracy: 17.90793650793651::Val accuracy: 17.757142857142856::Train Acc BP::54.4984126984127 Val Acc BP::54.74285714285714\n",
      "0.9999488943777787\n",
      "Iteration: 71::Train accuracy: 17.995238095238093::Val accuracy: 17.87142857142857::Train Acc BP::54.72857142857143 Val Acc BP::55.08571428571428\n",
      "0.9999483853001079\n",
      "Iteration: 72::Train accuracy: 18.073015873015873::Val accuracy: 17.92857142857143::Train Acc BP::54.96825396825397 Val Acc BP::55.35714285714286\n",
      "0.9999478810652096\n",
      "Iteration: 73::Train accuracy: 18.152380952380952::Val accuracy: 18.0::Train Acc BP::55.20952380952381 Val Acc BP::55.614285714285714\n",
      "0.9999473813761124\n",
      "Iteration: 74::Train accuracy: 18.226984126984128::Val accuracy: 18.085714285714285::Train Acc BP::55.46984126984127 Val Acc BP::55.92857142857143\n",
      "0.9999468858518142\n",
      "Iteration: 75::Train accuracy: 18.29206349206349::Val accuracy: 18.142857142857142::Train Acc BP::55.76190476190476 Val Acc BP::56.24285714285714\n",
      "0.9999463948937064\n",
      "Iteration: 76::Train accuracy: 18.376190476190477::Val accuracy: 18.242857142857144::Train Acc BP::55.980952380952374 Val Acc BP::56.51428571428572\n",
      "0.9999459085023812\n",
      "Iteration: 77::Train accuracy: 18.455555555555556::Val accuracy: 18.357142857142858::Train Acc BP::56.27619047619048 Val Acc BP::56.72857142857143\n",
      "0.999945425648125\n",
      "Iteration: 78::Train accuracy: 18.530158730158732::Val accuracy: 18.385714285714286::Train Acc BP::56.4968253968254 Val Acc BP::56.99999999999999\n",
      "0.9999449471112105\n",
      "Iteration: 79::Train accuracy: 18.604761904761904::Val accuracy: 18.414285714285715::Train Acc BP::56.73809523809524 Val Acc BP::57.32857142857143\n",
      "0.9999444722440929\n",
      "Iteration: 80::Train accuracy: 18.66190476190476::Val accuracy: 18.45714285714286::Train Acc BP::56.938095238095244 Val Acc BP::57.61428571428572\n",
      "0.999944001361166\n",
      "Iteration: 81::Train accuracy: 18.73333333333333::Val accuracy: 18.52857142857143::Train Acc BP::57.16031746031746 Val Acc BP::57.8\n",
      "0.9999435347080066\n",
      "Iteration: 82::Train accuracy: 18.788888888888888::Val accuracy: 18.6::Train Acc BP::57.38412698412698 Val Acc BP::58.099999999999994\n",
      "0.9999430719304538\n",
      "Iteration: 83::Train accuracy: 18.86031746031746::Val accuracy: 18.628571428571426::Train Acc BP::57.5968253968254 Val Acc BP::58.32857142857143\n",
      "0.9999426125479574\n",
      "Iteration: 84::Train accuracy: 18.93015873015873::Val accuracy: 18.7::Train Acc BP::57.784126984126985 Val Acc BP::58.471428571428575\n",
      "0.9999421570189035\n",
      "Iteration: 85::Train accuracy: 19.001587301587303::Val accuracy: 18.771428571428572::Train Acc BP::58.025396825396825 Val Acc BP::58.81428571428572\n",
      "0.9999417050961348\n",
      "Iteration: 86::Train accuracy: 19.076190476190476::Val accuracy: 18.871428571428574::Train Acc BP::58.219047619047615 Val Acc BP::58.98571428571429\n",
      "0.9999412567016195\n",
      "Iteration: 87::Train accuracy: 19.14920634920635::Val accuracy: 18.95714285714286::Train Acc BP::58.439682539682536 Val Acc BP::59.15714285714285\n",
      "0.9999408117134039\n",
      "Iteration: 88::Train accuracy: 19.233333333333334::Val accuracy: 19.0::Train Acc BP::58.6031746031746 Val Acc BP::59.357142857142854\n",
      "0.999940369872262\n",
      "Iteration: 89::Train accuracy: 19.303174603174604::Val accuracy: 19.02857142857143::Train Acc BP::58.7952380952381 Val Acc BP::59.55714285714285\n",
      "0.9999399308941685\n",
      "Iteration: 90::Train accuracy: 19.37936507936508::Val accuracy: 19.085714285714285::Train Acc BP::58.99682539682539 Val Acc BP::59.74285714285714\n",
      "0.9999394949886351\n",
      "Iteration: 91::Train accuracy: 19.446031746031746::Val accuracy: 19.157142857142855::Train Acc BP::59.16825396825397 Val Acc BP::59.785714285714285\n",
      "0.9999390622922886\n",
      "Iteration: 92::Train accuracy: 19.523809523809526::Val accuracy: 19.257142857142856::Train Acc BP::59.357142857142854 Val Acc BP::60.014285714285705\n",
      "0.999938632335762\n",
      "Iteration: 93::Train accuracy: 19.58888888888889::Val accuracy: 19.357142857142858::Train Acc BP::59.56825396825397 Val Acc BP::60.21428571428571\n",
      "0.999938205310331\n",
      "Iteration: 94::Train accuracy: 19.63968253968254::Val accuracy: 19.371428571428574::Train Acc BP::59.738095238095234 Val Acc BP::60.385714285714286\n",
      "0.9999377804614717\n",
      "Iteration: 95::Train accuracy: 19.70952380952381::Val accuracy: 19.442857142857143::Train Acc BP::59.91111111111111 Val Acc BP::60.542857142857144\n",
      "0.9999373585399819\n",
      "Iteration: 96::Train accuracy: 19.8015873015873::Val accuracy: 19.485714285714288::Train Acc BP::60.098412698412695 Val Acc BP::60.699999999999996\n",
      "0.9999369390548026\n",
      "Iteration: 97::Train accuracy: 19.877777777777776::Val accuracy: 19.57142857142857::Train Acc BP::60.25873015873016 Val Acc BP::60.871428571428574\n",
      "0.9999365226654353\n",
      "Iteration: 98::Train accuracy: 19.942857142857143::Val accuracy: 19.685714285714287::Train Acc BP::60.42857142857143 Val Acc BP::61.1\n",
      "0.9999361085592147\n",
      "Iteration: 99::Train accuracy: 20.017460317460316::Val accuracy: 19.757142857142856::Train Acc BP::60.592063492063495 Val Acc BP::61.24285714285714\n",
      "0.9999356970216483\n",
      "Iteration: 100::Train accuracy: 20.06984126984127::Val accuracy: 19.8::Train Acc BP::60.73492063492063 Val Acc BP::61.357142857142854\n"
     ]
    }
   ],
   "source": [
    "w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _ = batchGDComp(x_train,y_train,100, 0.0001, 0.000005, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAHyCAYAAAAQi/NkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABp9ElEQVR4nO3dd3zV1f3H8dcJBMJGcVEVUatGARkCylAZIqLgFhxFtLXOWu1wtrZWsaK2P+uk1WrVOuuqrXWjiIoKAcIKmyQkIcyQvW5yz++Pc2/WvVmQe7+5yfv5e+RB7veOfG7yS31z+JzPMdZaRERERESkfnFeFyAiIiIi0topNIuIiIiINEKhWURERESkEQrNIiIiIiKNUGgWEREREWmEQrOIiIiISCMUmkUkphljUo0x1hjzQ69rkegzxlwV+Pl397oWEWnbFJpFJGYZY0YB/QM3L/OwFBERaeMUmkUkll0GFAHf04pCszGmgzGmk9d1iIhIy1FoFpGYZIzpAEwH/gM8DxxvjBkc5nGnGWO+MMYUGmPyjDHzjTFDa9x/hDHmNWPMLmNMsTFmhTHm8sB94wL/9D+wzmvON8a8VeP2C8aYJGPM+caY1UApcLIxpq8x5nljzGZjTIkxZr0xZnbdQG2M6WKMedgYk26MKQu0nDwYuO/hwPNNnedcZYwpN8YcGOY9dzPGFBljbgpz32JjzMuBz3sbY/5ujNlqjCk1xmwxxjzbhO/9eYH3W2qM2RaoMb7G/fcGvp9jjDFLA49LNsaMrfM6HQKP3RJ436uD3/s6j2vwZxhwpDHm08D7XmuMubDOa4w1xnxljMkPfCQbYy5p7L2KiAQpNItIrBoPHAy8DrwF+Kiz2myMGQfMC9w3C5gBfAUcGrj/IOBbYATwa2Aa8Bxw+F7U0x94GHgQmAKkAgcAOcAvgbOAR4CrgSdq1GiA94AbgKeAs4HfB54L7i8ERwKn1/l6VwP/tdburFuItbYIeB/3l4oqxpijgOG47xnA/wFjgV8Ak4G7AdvQmzTGTAfeARYB5wJ/AK4NvO+augIvA38FLgFygQ+NMYfUeMx9wG+AZwKv9Q3wijGm6ufY2M+whldxf4G6ANgAvG6MOSzwGj0D34/NwEXAxcA/gd4NvVcRkVqstfrQhz70EXMfuHC7B+gUuP0+kAaYGo/5Fkiqea3OazyIa+/oW8/943AhcmCd6/OBt2rcfiHwuCGN1NwRuBy3Eh2se3Lguec28LyvgRdr3D4K8ANTG3jOBUAl8IMa1+7Chfj4wO1VwM3N+J4bIB34R53rPwZKgD6B2/cG3tPlNR7TPfC15wRu7x/43v++zmt9AKxrxs/wqsDX+nGNa32ACuD6wO3hgcf08Pr/b/WhD33E7odWmkUk5gTaGy4E3rXWlgcuvw4cAYwKPKYbcDIubNa3ejoB+Mham90CZWVZa5Pr1GmMMbcaY1KMMSW41dJXgM5Avxo15Fhr/9PAaz8HXFRjQsRVwHbgowae8yFQiFvlDZqB+575AreTgduMMTcaY45t5P0BHBuo+1/GmI7BD+BzIAEYWOfx7wY/sdYWAp8CIwOXBuJWo9+s85w3gGONMQc28WcY9EmNr7Ub2AEcFri0Cfe9eDXQWtK7Ce9VRKQWhWYRiUVTcP+0/kGgL7c3bvW3jOoWjf1wK6MNBeI+jdzfHNvDXLsV+BMuPJ6HC4zBPuOEZtTwL9zK8vRAO8cs4CVrbUV9T7DWluLaPmYAGGOOAwZT3ZoB8DPg38DvgHXGmA3GmEsbqCPYMvIB7i8AwY/UwPWabS2F1tqSOs/fAfQNfB78s+73LXh7f5r2MwzKrXO7nMD32Fq7B5gExOO+lzuNMf8LtKuIiDSJQrOIxKJgMH4T16KxB8jAreBeYtwmwT24oNk37Cs4uxu5vzTwZ91JGPuFeWy4ldBLcG0cv7HWfmKtXYxrSWhODVjXo/w6boV5Am619x8NPSfgDeAUY0w/XHjeiVsVDr5urrX259baQ3CB+ntcT/EJ9bxeTuDPa3F94HU/Pqzx2O7GmC51nn8Q1QE4u8a1mg6u8bWa8jNsEmvtd9bas3B/2boQt2r+6r6+roi0HwrNIhJTAv9kPw14DbcZsObHL3Gha0IgaH4PXFl38kQN84DJxpiD67k/M/Dn8TW+/uFAYhPL7YJb/a7pijA17G+MmdrIaz0HnIrrF/7OWru2CV//E9wK7HRcaH7LWlsZ7oHW2hXAbbj/LtT3/tYBWUB/a21SmI/ddR5/QfCTQGvJJNwGQnD91MXUbh8hUOt6a+3OJv4Mm8VaW2Kt/S9ug2V9fzkQEQnR0esCRESa6TxcL+xj1trva95hjPkGN43hMlz/7J3AZ7ipDc/gVnlHAUnW2veBR4Erga+MMQ/gVquPB7pZax+21mYaY5KA+40xxbhAeTfVK66N+RT4uTHme1xf7RVA3ZMLPwU+xvXb3gcsxa2snmatvS74IGvt98aNsxsLXEcTWGt9xph3cH+Z6AvcWPN+Y8zXuNaRVbiV8p/ivkeLCMNa6zfG/Ar4Z2AixYe4NoijgPOBi621xYGHlwAPBMLyVtx0kk7AY4HXyjHG/AX4rTGmArfZ70Lc9JCaU1Aa+xk2yhhzDm6z4r+BLbjJG9dRY9VdRKQxCs0iEmsuAzbUDcxQFRL/BVxujLnBWrvAGDMJuB83/qwcWIYLT1hrdxpjxuBGxf0F196xgdrj0y4D/h54fiZwO25EW1PcBxwIzA7cfgf4OfDfGjVbY8wFgRpvDTx+K+FbB/6NC6ivh7mvPq8DPwm85ld17vsW1/LRHzdpYxkwxVqbST2stW8YY/Jxf3n4ceB5m3HTS8prPLQY9xeSJ3B/EVkLnF1n0+XvcFMubsD9C8FG4EfW2qr319jPsIk24v5S8EdcO8jOQL13N+M1RKSdM41vSBYRkdbAGLMIN45tpte1NMQYcy/wM2vtAY09VkQkVmilWUSklTPGDMdtABxB9fQNERGJIoVmEZHWbzFuQ99dgQkcIiISZWrPEBERERFphEbOiYiIiIg0QqFZRERERKQRMdHTfMABB9j+/ft7XYaIiIiItGFLlizZZa09MNx9MRGa+/fvT1JSktdliIiIiEgbZoxJr+8+tWeIiIiIiDRCoVlEREREpBEKzSIiIiIijYiJnuZwfD4fmZmZlJaWel2KRFFCQgKHHXYY8fHxXpciIiIi7UjMhubMzEx69OhB//79McZ4XY5EgbWW3bt3k5mZyZFHHul1OSIiItKOxGx7RmlpKX369FFgbkeMMfTp00f/uiAiIiJRF7OhGVBgbof0MxcREREvxHRo9lqHDh0YMmQIgwcPZtiwYSxcuHCvXueqq67irbfeauHqou+FF15g69atVbevueYaUlJSPKxIREREpGXEbE9za9ClSxeSk5MB+Pjjj7nrrrv48ssvPaunoqKCjh29+5G+8MILDBw4kB/84AcA/P3vf/esFhEREZGW1L5Wmr/9Fh580P3ZwvLz89lvv/0AKCwsZOLEiQwbNoxBgwbx3nvvVT3upZde4sQTT2Tw4MHMnDkz5HXuuecerrrqKiorK2tdHzduHLfccgtDhgxh4MCBLFq0CIB7772XmTNnMmbMGGbOnElaWhoTJkzgxBNPZOLEiWzZsgVwq9nXX389w4cP59hjj+X9998HIC0tjVNPPZVhw4bVWi33+/3ceOONJCYmMmnSJM4+++yq1fD77ruPESNGMHDgQK699lqstbz11lskJSVxxRVXMGTIEEpKShg3blzVSY6vvfYagwYNYuDAgdxxxx1V76t79+785je/YfDgwZxyyils3769RX4eIiIiIi3KWtvqP0466SRbV0pKSsi1Bi1caG2XLtZ26OD+XLiwec8PIy4uzg4ePNged9xxtmfPnjYpKclaa63P57N5eXnWWmt37txpjz76aOv3++2qVavsMcccY3fu3GmttXb37t3WWmtnzZpl33zzTfvrX//aXnfdddbv94d8rdNPP91ec8011lprv/zySztgwABrrbW///3v7bBhw2xxcbG11tqpU6faF154wVpr7XPPPWfPO++8qq8xefJkW1lZadevX28PPfRQW1JSYouKimxJSYm11tr169fb4Pf6zTfftFOmTLGVlZU2Ozvb9u7d27755pu16rbW2h/96Ef2P//5T1WNixcvrlXz4sWLbVZWlj388MPtjh07rM/ns+PHj7fvvvuutdZaoOr5t912m73//vsb/b43+2cvIiIi0gRAkq0nj7afleb586G8HCor3Z/z5+/zSwbbM9auXctHH33ElVdeWfWNvfvuuznxxBM544wzyMrKYvv27Xz++edccsklHHDAAQDsv//+Va91//33k5eXx1//+td6N7tddtllAJx22mnk5+eTm5sLwLnnnkuXLl0A+Pbbb7n88ssBmDlzJl9//XXV86dPn05cXBzHHHMMRx11FGvXrsXn8/HTn/6UQYMGcckll1T1IH/99ddccsklxMXFccghhzB+/Piq1/niiy84+eSTGTRoEJ9//jmrV69u8Pu0ePFixo0bx4EHHkjHjh254oorWLBgAQCdOnVi6tSpAJx00kmkpaU16XsvIiIiEk3tp6d53Djo1MkF5k6d3O0WNGrUKHbt2sXOnTv54IMP2LlzJ0uWLCE+Pp7+/fs3OiZtxIgRLFmyhJycnFphuqa6YTp4u1u3bk2qMdzzH330UQ4++GCWL1+O3+8nISGhwdcoLS3lxhtvJCkpicMPP5x77713n0bAxcfHV9XVoUMHKioq9vq1RERERCKl/aw0jxoF8+bB/fe7P0eNatGXX7t2LZWVlfTp04e8vDwOOugg4uPj+eKLL0hPTwdgwoQJvPnmm+zevRuAnJycquefddZZ3HnnnZxzzjkUFBSE/RpvvPEG4FaBe/XqRa9evUIeM3r0aF5//XUAXnnlFU499dSq+9588038fj+bNm1i8+bNHHfcceTl5dG3b1/i4uL45z//WdVLPWbMGN5++238fj/bt29nfmBlPhiQDzjgAAoLC2tN/ejRo0fY2keOHMmXX37Jrl27qKys5LXXXuP0009v2jdWREREpBVoPyvN4IJyC4blkpIShgwZArje8BdffJEOHTpwxRVXMG3aNAYNGsTw4cNJTEwEYMCAAfzmN7/h9NNPp0OHDgwdOpQXXnih6vUuueQSCgoKOPfcc/nggw+qWi6CEhISGDp0KD6fj+effz5sTU888QRXX301jzzyCAceeCD/+Mc/qu7r168fI0eOJD8/n7/+9a8kJCRw4403ctFFF/HSSy9x1llnVa1aX3TRRcybN48TTjiBww8/nGHDhtGrVy969+7NT3/6UwYOHMghhxzCiBEjql4/uNmwS5cufFtjs2Xfvn2ZM2cO48ePx1rLOeecw3nnnbdP33sRERFpw6yFkhLo0gVayRkNxvU8t27Dhw+3wSkMQWvWrOH444/3qKLoGzduHH/6058YPnz4Xj3/qquuYurUqVx88cVNfk5hYSHdu3dn9+7djBw5km+++YZDDjlkr75+S2pvP3sREZE2raIC8vOhoADy8tyf+fnu+sSJ0LVr1Eoxxiyx1oYNW+1rpVmaZerUqeTm5lJeXs4999zTKgKziIiIxDCfD3Jzqz/y86G4uPZj1q6FlSth0CAYMSKqobkhCs0xYv4+Tvuo2QYSra8pIiIi7VhlpVs53rOnOiTXDMg1w3GglZW1a+G3v3WrzB07Qr9+MGOGF9WHUGgWERERkX1jLRQWumC8Z4/7KChw15sSjmfPdvetXOmu+f3uzwULFJpFREREJAZZ61aMg6vHeXnuY9WqvQ/HK1e664MGuccFH7+Xe7kiIaKh2RjTG/g7MBCwwI+BdcAbQH8gDZhurd0TyTpEREREZC9VVrqV45wc95Gb63qTa9rXcDxiBPTtC8cdB8cfD0uWwKRJMHq0J285nEivND8GfGStvdgY0wnoCtwNzLPWzjHG3AncCdwR4TpEREREpCnKykJDcnDaWrhWC2h6OB40CHr0cIH46KMhOdl9XuNcCY49FgKnBbcmEQvNxphewGnAVQDW2nKg3BhzHjAu8LAXgfnEaGg2xvDLX/6SP//5zwD86U9/orCwkHvvvbfJr9G9e3cKCwub/Pj+/fvTo0cPjDEccsghvPTSS02eapGbm8urr77KjTfe2OSvF9TckXfJycls3bqVs88+G4D//Oc/pKSkcOeddzb7a4uIiEiE+P3Vm/WCHyUlzetDhvDhGGDoUPjrX2H1ajjjDPfRMRA/hwyBCy+M+lveW5FcaT4S2An8wxgzGFgC3AIcbK3NDjxmG3BwuCcbY64FrgV3KEdr1LlzZ9555x3uuusuDjjggKh93S+++IIDDjiAu+++mz/+8Y88/vjjjT6noqKC3Nxcnn766WaH5uApgc2RnJxMUlJSVWg+99xzOffcc5v9OiIiItKCioqqp1ns2QPffw8rVuxbHzK4Px98EDZsgNNOg/HjYb/9oHNnz95qS4tkaO4IDANuttZ+b4x5DNeKUcVaa40xYU9XsdY+AzwD7nCTer/Kf//bYgXXa9q0sJc7duzItddey6OPPsoDDzxQ6760tDR+/OMfs2vXrqqT+fr160dqaiqXX345hYWFIafiPfLII/zrX/+irKyMCy64gD/84Q8NlnXaaafx+OOPU1lZyZ133sn8+fMpKyvjpptu4rrrrmP+/Pncc8897Lfffqxdu5Zhw4axadMmhgwZwqRJkzjnnHP405/+xPvvvw/Az372M4YPH85VV11F//79mTFjBp9++im33347AP/85z+55pprqKio4Pnnn2fkyJEsWrSIW265hdLSUrp06cI//vEPjjzySH73u99RUlLC119/zV133UVJSQlJSUk8+eST9X5vrrrqKnr27ElSUhLbtm3j4YcfbtZhLCIiIlKDtW6Cxc6dsGuXC8k1e5H3tQ957Fg46ijo3dt9TJ3aak7vi4S4CL52JpBprf0+cPstXIjebozpCxD4c0cEa4i4m266iVdeeYW8vLxa12+++WZmzZrFihUruOKKK/j5z38OwC233MINN9zAypUr6du3b9XjP/nkEzZs2MCiRYtITk5myZIlLFiwoMGv/f777zNo0CCee+45evXqxeLFi1m8eDHPPvssqampACxdupTHHnuM9evXM2fOHI4++miSk5N55JFHGn1vffr0YenSpVx66aUAFBcXk5yczNNPP82Pf/xjABITE/nqq69YtmwZ9913H3fffTedOnXivvvuY8aMGSQnJzOjzqiY+r43ANnZ2Xz99de8//77auUQERFprpIS2LIFli6FTz6BL7+ElBQ3uu3VV11QDgoXjqE6HMfFVbdaJCS40/lee80F7S++gGuvhQED4NBDoVu3Nh2YIYIrzdbabcaYDGPMcdbadcBEICXwMQuYE/jzvUjVEA09e/bkyiuv5PHHH6dLly5V17/99lveeecdAGbOnFm1WvvNN9/w9ttvV12/4w7Xzv3JJ5/wySefMHToUMAdYb1hwwZOO+20kK85fvx4OnTowIknnsjs2bO55pprWLFiBW+99RYAeXl5bNiwgU6dOjFy5EiOPPLIvXpvdcPuZZddBrgV7vz8fHJzcykoKGDWrFls2LABYwy+urtpw6jvewNw/vnnExcXxwknnMD27dv3qm4REZF2o7AQdu92G/Z274Zly5rei1xfH/Lxx8Ojj7rnnXGG26gXzDjDhsVUH3JLivT0jJuBVwKTMzYDV+NWt/9ljPkJkA5Mj3ANEXfrrbcybNgwrr766iY93oT5m5i1lrvuuovrrruu0ecHe5prPveJJ55g8uTJtR43f/58unXrVu/rdOzYEb/fX3W7tLS01v11n1u3bmMM99xzD+PHj+fdd98lLS2NcePGNVp/QzrX6H2ytv6uHBERkXan5gl7wckW5eXV9ze33SIx0T1mzRoYMwZOP931IffuDR06ePY2W6tItmdgrU221g631p5orT3fWrvHWrvbWjvRWnuMtfYMa21OJGuIhv3335/p06fz3HPPVV0bPXo0r7/+OgCvvPIKpwZGqYwZM6bW9aDJkyfz/PPPV03SyMrKYseOpnWuTJ48mblz51at8q5fv56ioqKQx/Xo0YOCgoKq20cccQQpKSmUlZWRm5vLvHnzGvw6b7zxBgBff/01vXr1olevXuTl5XHooYcCtY/qrvu1aqrveyMiIiI1FBVBRoYLuQsWwIcfwjffuHaL+fPhlVf2rt1izBg37u2kk+BnP4Pnn4ef/AR++EPo00eBuR6xfyJgPZv0ou1Xv/oVTz75ZNXtJ554gquvvppHHnmkarMbwGOPPcbll1/OQw89VGsj4JlnnsmaNWsYNWoU4EbRvfzyyxx00EGNfu1rrrmGtLQ0hg0bhrWWAw88kH//+98hj+vTpw9jxoxh4MCBTJkyhUceeYTp06czcOBAjjzyyKrWkPokJCQwdOhQfD4fzz//PAC33347s2bNYvbs2ZxzzjlVjx0/fjxz5sxhyJAh3HXXXbVep77vjYiISLtWUuI27O3a5Votmjv6LVy7RVycm4H88svuxL4zz3ShWZrNxMI/gQ8fPtwmJSXVurZmzRqOP/54jyoSL+lnLyIibUJxcXU/8q5d7nZN9YXjN990q8x+vwvFV1wBl1zinrNuHaxf70a+TZoE++/vHiNNYoxZYq0NeyhF7K80i4iIiLR21rpNe8ENezk5biUZ9v2UvbFj3Sl6++8PU6ZUHx4iLUrfVREREZGWVlzsDhAJfuTlufaIfT1lr0sXmDDBtVusWAFnnaV2iyhRaBYRERHZV6Wl7hCR4EfNqRawd6fsHX88PP64e+7EiW78W0KCu++kk6pbMiQqYjo0W2vDjm+TtisWevBFRKQd8Ptdi8XOnbBjB+TnV98Xrt2iqa0Wo0ZVT7HYf3+1WrQiMfuTSEhIYPfu3fTp00fBuZ2w1rJ7924Sgn/LFhERiZbKyup+5N27XctFSsq+TbYwxoXkF15wrzNlipt0Ia1SzIbmww47jMzMTHbu3Ol1KRJFCQkJHHbYYV6XISIi7UFeHmzf7laSc3PdZr6gvTlI5MEHYcMGd4jIGWe4g0SCM5EDp+5K6xWzoTk+Pn6vj4cWERERCeH3u1XkbdvcR/Ck3H1ptzjzTBg61J2yN3WqW12WmBSzoVlERERkn/j9bgU5eCT17t0u7NbUnHaLHj3gnHNgwABYtsxNuQgcWiaxT6FZRERE2oeafck5ObBnjwvOQc1ZUU5MhIcego0b3WSLKVOqJ1uccAKcfXb0359ElEKziIiItE3WuqkWwTFwOTnhN+9B01eUJ0yAwYPddItp07x7bxJ1Cs0iIiLSdpSVuY174eYlN3SQSLgV5eOPd9MsXn3VHUxy5plqt2jHFJpFREQkdgX7knfscB95ee56c1otoPaKcnw8XHwxTJ7sPge46KKovzVpXRSaRUREJLaUlFQfKrJgASQn7/2s5E6dXKvFgAGu7SIpCcaP14qyhFBoFhERkdYtuIEvuJpcWOiu782s5Jqb9848E7p1q/46Rx3lromEodAsIiIirUtlpZtssWtXw6fvNXVW8qmnuokWBx6ozXuy1xSaRURExHuFhe5AkR07wo+Ca2q7BcCIEfCPf8Dq1W7025gx3rwnaVMUmkVERCT6rHUtF9u3u7BcVOSuN3dW8uzZLhyfdpo7mvrAA6FLF+/el7RZCs0iIiISHaWl1aPgduwAn6/2/c1ZUe7VCw46yK0i9+4NcXGevCVpPxSaRUREJDL8freaHAzJ+fnV9zV3Rfnhh90GvjPOgLPOgs6dvXlP0m4pNIuIiEjLys2FzEz3sXJlaDjW6XsSgxSaRUREZN+VlkJWFmRkQEGBu9bckXCjR8PLL7se5cmTNStZWhWFZhEREdk7BQXVR1bv2uU299XUlJFw8fFw4YUuJHfq5J53ySXRfy8ijVBoFhERkaYpL6/eyLdzp1tdhvD9yRB+A1+PHnDOOe4EvmXLXAuGVpQlBhhb92+FrdDw4cNtUlKS12WIiIi0P36/GwuXkeGOrF6xomn9yUHr10NqavUGvoQEb96HSBMYY5ZYa4eHu08rzSIiIhJqzx4XlLdudaPhmtufvP/+cPjhMGWKe7xIjNP/F4uIiIiTn+9C8tat1YeNBDX1yOqzz3YtF926efMeRCJEoVlERKQ9y8tzITk7u+FT+cL1J8fFwdixbuLFqlVuM9/o0d69F5EIUmgWERFpT/x+13qxbZv7KC6ufX99bRjBI6vXrHEryZMmufnJwdYLTbyQNk6hWUREpK3z+dy0i23bah9f3dRT+QYMgL594ZRT4IADwBjv3ouIRxSaRURE2qLiYjf1Yvt2+Prrpk+9qNuGceGFcOaZ2swn7Z5+A0RERNoCa11/8rZtLijn57vrzZ16MWYMvPqqO5Vv0iTNUBYJUGgWERGJVcG2ix073Mfy5U1rtwi3onz++W5FuXNn97yLLvLsbYm0RgrNIiIisaSgwK0k79gBOTnVR1c3td1i0CDo0AHGj4d//cu1bZxxhlaURRqh0CwiItKaVVTUXk0uLW36Br5wUy/OPNNt5ouLg+HD4bzzvH1/IjFCoVlERKS1KS2FrCy3olxzNRmat6LcqxccfDCcdpr7XET2mkKziIhIa1BR4TbxZWa6leVwq8nQ8IryAw/A5s2u3eLssyEhwbv3I9LGKDSLiIh4xVrYtcsF5exsqKx01+tbTYbQFeXhw6F/fzjoIBeUO3Tw7O2ItGUKzSIiItFkrTuRLyvLBeXmTLwAOOEEeOIJWL8ezjrLrSqLSMQpNIuIiERDXp4Lylu3QkmJu9bU/uThw+GII9xq8gEH6KAREQ/ot05ERCRSyspc60VGBixe3LyJF3/6E6SmupYLrSaLeE6hWUREpCX5/W7qRUaGGxFnbdNXlIcOdSvKhx0G06Z5/U5EpAaFZhERkZZQXOxWhjMy3El9NTU28SI1FSZPhqlT3fxkEWl1FJpFRET2xc6dLvRu3+5uhxsVV3dF+cQTXX/yD37gNvPFx3tXv4g0iUKziIhIc1VUuF7l1FQoLKy+Xl8bRvBUvk2b3Il8U6dCp07e1S8izabQLCIi0lS7d7v2i61bYfXqpm3sGzrU9ShPmADdunlbv4jsNYVmERGRhpSUVE/AKCpy15qysS8+Hi67DCZOBGO8fQ8iss8UmkVEROoKTsDYssVNwKirvo19Q4fCyy/DunUuLI8aFf3aRSQiFJpFRESCioogPd2tLJeVuWtN2dg3diyMGAEHH6xVZZE2SqFZRETat8pKd5z1li2uZ7mmhjb2zZkDaWlunrIOHxFp8xSaRUSkfcrPd0E5M9OtJNddTYbwbRinngqHH+5O6uvQwbv6RSSqFJpFRKT9qKiArCwXlnNz3bX6VpMhdGPfj38Mp5ziWfki4h2FZhERafvy810rxaefwvLljY+JS0x0J/NNmADvvgvJyTB+vDb2ibRjCs0iItI2+f2wbZs7gCQnp2lj4jp2hJEjYcAAN1s5eADJ2Wd7+15ExHMKzSIi0raUlroJGOnp1RMwoP4V5cRE+OMf3Ur01KkwebJnpYtI66XQLCIibUNOjltVzs6GNWsaHxM3aBD06gVHHAFnneV6lkVE6qHQLCIisauy0m3sS011fcvQ8Ji42bPd8ddnnOFGxfXq5W39IhIzFJpFRCT2BA8h2bIFfL7a99XXhtGjB1xyCfziFy5Mi4g0g/5XQ0REYkNwY196Ouza5a415bS+8eNh9Gjo08e72kUk5ik0i4hI61ZSUr2qXHNjX0NtGA895B5/7rkwbpxnpYtI26HQLCIirdOuXa5Xefv28Bv7wrVhjBoFRx4J55zj5iyLiLQQhWYREWk9KivdsdapqVBQ4K41Zb5yfDzMnAljx3pbv4i0WQrNIiLivb3Z2HfSSfDKKy5UT5yo0/pEJKIUmkVExBvWwo4d7lCRHTvcNW3sE5FWSqFZRESiq7zcrSinp0NxcfX1hjb2Pfxw9ca+00/3rnYRabcUmkVEJDoKCmDzZteznJLStI19o0drY5+ItAoKzSIiElk7d7qwXLMFoykb+668EsaM8bZ2EZGAiIZmY0waUABUAhXW2uHGmP2BN4D+QBow3Vq7J5J1iIhIlAWPt968uXoKRlB9G/uGDYOXX3ah+owztLFPRFqVaKw0j7fW7qpx+05gnrV2jjHmzsDtO6JQh4iIRFpenutVzsqCVatCWzAgdGPf6afDySfDgQeCMd7VLiLSAC/aM84DxgU+fxGYj0KziEjs8vlcSN6yxYVmqL8FA9yfc+a4cH3eeW4ahohIKxfp0GyBT4wxFvibtfYZ4GBrbXbg/m3AwRGuQUREIqG8HDZtgg8+gOXLG9/Ul5gIXbvCUUfBlCkuTIuIxIhI/y/WWGttljHmIOBTY8zamndaa20gUIcwxlwLXAvQr1+/CJcpIiJN5vO5XuXNm10LRmOb+jp2dBv6RoyAgw9WC4aIxKSIhmZrbVbgzx3GmHeBkcB2Y0xfa222MaYvsKOe5z4DPAMwfPjwsMFaRESiqKLCHW+9aVP1qX31rSgnJsKDD7qDS6ZNg0mTPC1dRGRfRSw0G2O6AXHW2oLA52cC9wH/AWYBcwJ/vhepGkREpAVUVLjwu2mTa8moqe6K8qBB0Ls3HHGEWjBEpE2J5P+aHQy8a9w/w3UEXrXWfmSMWQz8yxjzEyAdmB7BGkREZG+Vl7uV5dRUt7Ic7ojrxETXkrFqlVtNPvdc6NnT27pFRCIgYqHZWrsZGBzm+m5gYqS+roiI7KOyMreqnJbm5i1D/dMw4uLgrLPg5pshIcHTskVEIkn/biYiIk5xsQvLH38MK1Y0PA1j1So480w49ljo0sXbukVEokChWUSkvcvLc2F561ZYs6ZpR1zPmgWDQ/4xUUSkzVJoFhFpr3btgo0bYefO6mv1TcM44QR49lnX33zmmTriWkTaHYVmEZH2xO+H7Gy3svz996Eb++pOwzjxRDjySDj6aLVhiEi7ptAsItIe+Hzu2OrUVCgtrX9jX3AaxurVMHkyXHghdO7sdfUiIp5TaBYRacuKitzJfRkZ1ZMwoP42jM6d4YIL4NZbXe+yiIgACs0iIm1TcTGsXw+ZmW5zX2NtGCNGuFaMww93Y+RERKQWhWYRkbakpMSF5YwMsLbxNowNG9wx11OngjuMSkREwlBoFhFpC0pL3SSM9HTXchEUrg3j+OPhkENgzBjYf3/vahYRiSEKzSIisay83IXl1FRISWm8DWPyZJgwAbp29bZuEZEYo9AsIhKLKircBr9Nm9znDbVhzJkDWVlw3nlw6qleVy4iEpMUmkVEYonfD2lprhe5vLz6erg2jEGD4KijYMoUF6RFRGSv6X9FRURigbVuc9+//w1JSbVbMCC0DWPKFJg4UWPjRERaiEKziEhrt22bGxuXlBS+BQPcnw884EbMXXghnHaatzWLiLQxCs0iIq3V7t0uLO/Z427XdyCJMXDEETBpEiQkeFuziEgbpdAsItLaFBa6Y6x37Kh9vW4LxqBBcNhhcNxxmoYhIhJhCs0iIq2F3++mYaxfH358XPBAkpUr4fTT4ZJLoGdPb2sWEWknFJpFRFqD/HxIToa8vPrHxwGMHg0/+YkOJRERiTKFZhERL/n9bmV540Y3IQPC9y6PHOlO8jvoIG/rFRFppxSaRUS8kpMDy5e7Huaa6vYuX3yxm4ZhjDd1ioiIQrOISNSVlrqe5aws14oRrnc5OD7u4oth7Fhv6xUREYVmEZGoCW7027ABKivr713u0QOuuQZ69/a6YhERCVBoFhGJhm3b3Bi54uLqa3V7l1etgnPPhWOOgbg472oVEZEQCs0iIpFUVOTC8IIFoW0YdXuXZ850M5dFRKTVUWgWEYmEigrXhrF5s+tfDteGkZgIc+a4VejzznPj5EREpFVSaBYRaWlZWS4ol5a62+FGyB1/PBx5JJx1FsTHe1uviIg0SqFZRKSl5Oe7QJyTU/t63TaMsWPdiX49enhTp4iINJtCs4jIvqqshHXrXCvGmjX1H3+9di2cfz5Mm+ZpuSIi0nwKzSIi+2LHDlixAkpK6h8hFxfnpmL88pfQoYPXFYuIyF5QaBYR2RulpW6E3Nat1dfC9S6PGwcDBkDXrp6VKiIi+06hWUSkOayFLVtcG4bPV/u+ur3Ll14KI0Z4U6eIiLQohWYRkaYqKHCtGDk5DR9/nZ0NF14IY8Z4W6+IiLQYhWYRkcb4/W7m8saN7vP6epcPOAAmTIBu3byuWEREWphCs4hIQ3bvhuXL3cl+QXV7l1NS4LLL4LDDvKtTREQiSqFZRCQcn8+F4S1bQu+r2bscHw9XX63ALCLSxik0i4jUtXUrrFoFZWX19y7/6U+wfTuccw6MGuVtvSIiEnEKzSIiQaWlbqPf9u3udrje5eOPhx/+0IXluDhv6xURkahRaBYRsRbS090YuYqK6ut1e5c3boTrr9fx1yIi7ZBCs4i0b/n5Lhzn5ITeV7d3edYsBWYRkXZKoVlE2qfyctd+ETyopG7fMrjPn34asrJg0iT1LouItGMKzSLSvvj9kJYG69e7CRn1zVzu3NmF6L59va5YRERaAYVmEWk/tm+H1asbnrm8ciWceSaccIJryRAREUGhWUTag/JyNxXjiy9C2zBq9i137AiXXw6DB3tbr4iItDoKzSLStm3b5gLz8uXh2zASE+HBByE7G84/H8aM8bpiERFphRSaRaRtqqhwrRjBE/3CtWEkJsIRR7h2jM6dva1XRERaNYVmEWl7du+GZcugpKT6Wt02jLFj4fTToWdP7+oUEZGYodAsIm2H3+/Gx33wQfijr2fPhpQUuOACOPdcb2sVEZGYotAsIm1DXp5bXV68OHzvMsBpp8HNN0NCgre1iohIzFFoFpHY5ve7463Xr3fHYYfrXR44EAYMgH79vK5WRERilEKziMSuwkK3upybW32tbu/ymDGud7lrV8/KFBGR2KfQLCKxx1pITXX9y35/7fuCvcurVsHUqXDxxWCMN3WKiEibodAsIrGltBSSk2HnTncEdt0NfwAjR8L110OPHp6VKSIibYtCs4jEjuxsd0iJz+cCc90Nf8cfDz/8IRx7LMTFeV2tiIi0IQrNItL6VVS4douMjOprdTf8rVsH11wD++3nXZ0iItJmKTSLSOuWk+M2+xUX175ec8NffDxcdZUCs4iIRIxCs4i0Tn6/GyO3caPb8BfusJKHH4YdO9yGv1GjvK1XRETaNIVmEWl9iopg6VI3Si5c73JiIvTtC5MnQ6dOXlcrIiLtgEKziLQuW7a4/uXKSne7bu/y6tVw6aVw+OHe1ikiIu2KQrOItA7l5bBihZuQUVPdw0pmzVJgFhGRqFNoFhHv7d7t2jGSk8P3Lj/wgAvTF10Eo0d7WqqIiLRPCs0i4h1r3Ua/devcZr9wvctdu8JPfqLJGCIi4imFZhHxRnm5W13eudPdrtu7vHIlTJoEAwe6EC0iIuIh/ZdIRKIvJweWLHFHYgfV7V2++GIYMsSzEkVERGpSaBaR6LEWNm1yY+SsrX1fYqJrydi0yU3HGDfOkxJFRETCUWgWkegoLnYb/b75JnSzX9DUqe6aMZ6UKCIiUh+FZhGJvPR0SElx85fDbfbr1AmGDoWDDvK6UhERkbAUmkUkckpL3epyQ5v9Ro+GYcOgSxdPSxUREWmIQrOIREZmpltZ9vmqr9Xd7DdlCowaBXFx3tUpIiLSBE0KzcYYA7wL3GWtXRPZkkQkpvl87mS/zz8Pf1DJ7NluI+DFF7vQLCIiEgOautJ8JjACuAb4VXO+gDGmA5AEZFlrpxpjjgReB/oAS4CZ1try5rymiLRSublulNzSpeF7lwHOPBN+8QvNXhYRkZjS1H8T/QkuME8zxjT3v3S3ADVXpx8CHrXW/hDYE3htEYl1qaluMkZxcfje5c6dYeRIGDxYgVlERGJOo6HZGHMAMMBa+yHwGXB+U1/cGHMYcA7w98BtA0wA3go85MXmvJ6ItEI+Hyxe7PqX/X53Ldi7HBfn/pwwAcaPh4MP9rZWERGRvdSU5Z6ZwGuBz/8B3E916G3MX4DbgR6B232AXGttReB2JnBoE19LRFqbPXtcO0ZJSe3rwd7l1avhwgvh3HO9qU9ERKSFNCU0/xg4C8Bau9gY09cYc7i1NqOhJxljpgI7rLVLjDHjmluYMeZa4FqAfv36NffpIhJpGRluw19KSvjDSk4+GW68Ebp1865GERGRFtJgaDbG9AaetNZm1bj8a+AAoMHQDIwBzjXGnA0kAD2Bx4DexpiOgdXmw4CscE+21j4DPAMwfPhwG+4xIuIBa11Q3rzZTcEIt+HvyCPhhBM0Sk5ERNqMBv+LZq3Ntdb+rc61T621yxp7YWvtXdbaw6y1/YFLgc+ttVcAXwAXBx42C3hvryoXkejz+eD7711ghtANfykpMHw4DByowCwiIm1Ks/6rZoxZ2gJf8w7gl8aYjbge5+da4DVFJNIKC+Grr6pP94PQDX9XXgl9+3pXo4iISIQ0d+6T2ZsvYq2dD8wPfL4ZGLk3ryMiHtmxw234q6iofT244W/LFpgxA8aO9aY+ERGRCGtuaP5fRKoQkdbJWti40fUur10bfsPfhRfC0Ud7V6OIiEgUNCs0W2t/G6lCRKSVqaiAZctg27bwG/4GDoSTToKDDvK6UhERkYjTTh0RCRXsX962zd2uu+Fv3To49VQFZhERaTd0lq2I1LZtm1thrtm/HNzwV1EB8fEwaxZ07+5djSIiIlHWrNBsjDka6GqtXRmhekTEK9a6FeQNG0LvC27427oVLrkERo+Ofn0iIiIeanJoNsbcDfwQ8BtjOltrZ0auLBGJqrr9y3U3/HXsCD/6kcbJiYhIu1VvaDbG/Bx4ylpbGbg02Fo7I3DfimgUJyJRUFQEixdDQUH4DX8nnQQjRkCPHl5XKiIi4pmGNgLuBj4yxpwbuP2JMeYjY8wnwMeRL01EIm7XLrfhr6DA3a674S8tzW34U2AWEZF2rt6VZmvtK8aYt4FfG2OuAX4HvAbEW2vzolWgiERIaiqsXu16mYPqbvi74gr3p4iISDvXWE/z0cC/gL8D9weu3QMoNIvEKr/frShv2RJ6X2Ii/PGPsHs3TJsGo0ZFvz4REZFWqKGe5hcAH9AVyLLW/tQYMxR41hiz2Fp7X5RqFJGWUl4OSUkuFIfb8Ne1K/z0p9Czp7d1ioiItDINrTQPtdYOBjDGLAOw1i4DphljzotGcSLSggoLYdEit/Ev3Ia/MWNg+HDo1MnrSkVERFqdhkLzh8aYj4F44NWad1hr34toVSLSsnbtcivMPp+7XXfDX2YmnHIKxOmQUBERkXAa2gh4pzGmJ+C31hZGsSYRaUnp6S4kN7Th79JLFZhFREQa0OBGQGttfrQKEZEWZi2kpMDmzaH3JSbCnDmut/mcc7ThT0REpBHNOkZbRGJEaSksXVr/hr9u3dyGv+7dva1TREQkRig0i7Q1u3a5wFxWpg1/IiIiLaTRJkZjzCXGmB6Bz39rjHnHGDMs8qWJSLNYC+vXw7ffusAMoRv+tmxxG/4UmEVERJqlKTt/7rHWFhhjxgJnAM8BcyNblog0S1kZfP89rFtX+3pww19cnNvwd/nl2vAnIiKyF5rSnlEZ+PMc4Blr7f+MMbMjWJOINMeePfDSS7BkSe2+ZXCfP/SQa9nQhj8REZG91pTQnGWM+RswCXjIGNOZpq1Qi0ikbd8OL78Md99du285GJz79IFJkyAhwds6RUREYlxTwu904GNgsrU2F9gfuC2SRYlIE2RmwuLFsHx57b7llSvd/cce61aWFZhFRET2WaMrzdbaYuAdY8xBxph+gctrI1uWiDRo0yY3gxlqH1TSsSMMHeo2+x14oLc1ioiItCGNhmZjzLnAn4EfADuAfrjQPCCypYlIWCkpLjQHJSa6loyVK11Y/vGPtbosIiLSwprS03w/cArwmbV2qDFmPPCjyJYlIiH8flixAjIyQu9LTISxY2HECLfaLCIiIi2qKT3NPmvtbiDOGBNnrf0CGB7hukSkJr8fkpLg00/hzTfdoSU19e0LJ5+swCwiIhIhTfkvbK4xpjuwAHjFGLMDKIpsWSJSpaLCbfj7+uvQ0/0SE6F/fxg4EIzxulIREZE2qykrzecBxcAvgI+ATcC0SBYlIgE+nzu0ZNeu0NP9Vq6E445zGwEVmEVERCKqKdMzgqvKfuDFyJYjIlXKy+G77yAvz92uOyXj/PPdWDkRERGJODVAirRGpaUuMBcUVF8LTslYtQouugjOPde7+kRERNoZhWaR1qakBL79ForCbB044QSYORMOOST6dYmIiLRjjfY0G2OmGWN0bLZINBQVuQ1/S5aETsno0AFGjlRgFhER8UBTVppnAH8xxrwNPG+t1WmAIpGQn+9aMpYvD52SMXCgGym3//5eVykiItIuNbqCbK39ETAUNzXjBWPMt8aYa40xPSJenUh7kZsLCxdCWVnolIyUFBg9WoFZRETEQ01qu7DW5gNvAa8DfYELgKXGmJsjWJtI+5CT43qYfT53OzglIy7O/XnFFdCrl7c1ioiItHONtmcYY84FrgZ+CLwEjLTW7jDGdAVSgCciW6JIG7Zzpzu4pLKy+lpwSsbatS4wT5jgXX0iIiICNK2n+SLgUWvtgpoXrbXFxpifRKYskXZg+3Z3NLbfH3rfSSfBz34GXbpEvy4REREJ0ZTQfC+QHbxhjOkCHGytTbPWzotUYSJtWlYWLFsGa9a4HuZBg9wKM0CPHnDKKZCQ4G2NIiIiUqUpoflNYHSN25WBayMiUpFIW5eW5oLy2rWhUzJOPtkF5k6dvK5SREREamjKRsCO1try4I3A5/ovusje2LDBBWYInZKxaROMGqXALCIi0go1JTTvDGwGBMAYcx6wK3IlibRRKSm1DysJNyUjPt67+kRERKReTWnPuB54xRjzJGCADODKiFYl0pZYCytWwJYtta8Hp2SkpcHll8OYMZ6UJyIiIo1rNDRbazcBpxhjugduF0a8KpG2wu+HpUvhiy9CN/wBTJoEgweDMd7VKCIiIo1qykozxphzgAFAggn8x91ae18E6xKJfda6CRlffBG64S8xEY46Ck44QYFZREQkBjTa02yM+SswA7gZ155xCXBEhOsSiW3WwvLlsHVr6Ia/lSvhuONgwAAFZhERkRjRlI2Ao621VwJ7rLV/AEYBx0a2LJEYt2oVZGS4z+tu+DvvPDhWv0IiIiKxpCntGaWBP4uNMT8AdgN9I1eSSIxbs8Zt7gsKbvhbuRIuuMB9iIiISExpSmj+rzGmN/AIsBSwwLORLEokZq1fDxs3hl5PTISLL4Yjj4x+TSIiIrLPGgzNxpg4YJ61Nhd42xjzPpBgrc2LRnEiMWXzZli3Lvx9xx+vwCwiIhLDGuxpttb6gadq3C5TYBYJIzUVVq92h5e8+WbtQ0yOOQZ++EPvahMREZF91pT2jHnGmIuAd6y1NtIFicScTZuqT/urO1ru7LNrz2UWERGRmNSU6RnXAW8CZcaYfGNMgTEmP8J1icSGDRtcYIbQ0XIZGW6snIiIiMS8ppwI2CMahYjEnHXr3Ma/oOBouYoKiI+HSy/1rjYRERFpUY2GZmPMaeGuW2sXtHw5IjFizZrQKRnB0XJbtsBll8Ho0d7UJiIiIi2uKT3Nt9X4PAEYCSwBJkSkIpHWbvVqNykjnDPPhBNP1El/IiIibUxT2jOm1bxtjDkc+EukChJp1YKBee1a18M8aFD1Rr/+/WHgQAVmERGRNqgpK811ZQLHt3QhIq3emjXVgTnclAxt+hMREWmzmtLT/ATuFEBw0zaG4E4GFGk/1q2r7mGuOyVj61YFZhERkTauKSvNSTU+rwBes9Z+E6F6RFqfjRsbnpIxfbp3tYmIiEhUNCU0vwWUWmsrAYwxHYwxXa21xZEtTaQV2LzZtWXUFJySsXWrC8yjRnlTm4iIiERNk04EBM4ACgO3uwCfAJqnJW1berrb+BeOephFRETalaacCJhgrQ0GZgKfd41cSSKtQGYm/Otf8OabbuNfTUccocAsIiLSzjRlpbnIGDPMWrsUwBhzElAS2bJEPLRtG7z+euiEjMREOPxw19MsIiIi7UpTQvOtwJvGmK2AAQ4BZkSyKBHP7N4NS5bAihW1J2SsXAkTJ8LgwZrDLCIi0g415XCTxcaYROC4wKV11lpfZMsS8UBeHixa5IJyzQkZHTvCuHEwZIgCs4iISDvVlDnNNwGvWGtXBW7vZ4y5zFr7dMSrE4mWoiL4/nsXkqF6QsbKlXDqqXDVVRDXlC0AIiIi0hY1JQX81FqbG7xhrd0D/DRiFYlEW2kpfPcdlJXVvp6YCD/9KVx9tQKziIhIO9eUJNDBmOp/kzbGdAA6Ra4kkSjy+VxgLg4zdrx7dzj5ZNeeISIiIu1aU9LAR8Abxpi/BW5fF7gmEtsqK10P8+LFrg1j0CC3ugzQpYs7tKST/n4oIiIiTQvNdwDXAjcEbn8KPNvYk4wxCcACoHPg67xlrf29MeZI4HWgD7AEmGmtLd+L2kX2nrVuSsbChaGj5U48EU45BRISvK5SREREWolG2zOstX5r7V+ttRdbay8GUoAnmvDaZcAEa+1gYAhwljHmFOAh4FFr7Q+BPcBP9rp6kb21YgVs3+5WmGuOllu92rVkdO/udYUiIiLSijRpd5MxZqgx5mFjTBpwH7C2kadgneBJgvGBDwtMAN4KXH8ROL+ZNYvsm3XrYMsW93lwtFxcnPtzxgzo3dvT8kRERKT1qbc9wxhzLHBZ4GMX8AZgrLXjm/rigU2DS4AfAk8Bm4Bca21grheZwKH1PPdaXFsI/fr1a+qXFGlYWhqsX199u+ZouYsvhilTPCtNREREWq+GeprXAl8BU621GwGMMb9ozotbayuBIcaY3sC7QGIznvsM8AzA8OHDbXO+rkhY2dkuHNeVmOgC85FHRr8mERERiQkNtWdcCGQDXxhjnjXGTMQdo91sgTnPXwCjgN7GmGBYPwzI2pvXFGmW3bth6dLw9x1zjAKziIiINKje0Gyt/be19lLc6vAXwK3AQcaYucaYMxt7YWPMgYEVZowxXYBJwJrAa10ceNgs4L19eQMijQoej52SAm++CWtrtOQffnj1mDkRERGRejQ6cs5aWwS8CrxqjNkPuAQ3hu6TRp7aF3gx0NccB/zLWvu+MSYFeN0YMxtYBjy3L29ApEEFBfDtt7BqVehoudNPh8GDva5QREREYkCzjjoLHKFd1WvcyGNXAEPDXN8MjGzO1xXZK0VFLjD7fKGj5TZuhF/+EsxedRyJiIhIO9OkkXMiMaekxAXmsjJ3u+5ouSuugA4dvK1RREREYkazVppFYkJpqTvpr6Sk+lpwtNzatTBzJpx2mnf1iYiISMxRaJa2pbzcrTAXF4feN3Qo3HQTdO0a/bpEREQkpqk9Q9oOn88F5qSk0CkZnTvDqFEKzCIiIrJXtNIsbUNlpRsrt2hR6JSMQYNcYO7WzesqRUREJEZppVlin7Xu4JKcnNApGatXu8Dco4fXVYqIiEgMU2iW2Ld8OWzb5j6vOyXjssugVy9v6xMREZGYp/YMiW0pKZCRUX07OCVj1SqYMQMmT/auNhEREWkzFJoldm3cCJs2hV5PTITLL4dDD41+TSIiItImqT1DYtOWLbBmTfj7Bg1SYBYREZEWpdAssWf7dlixwo2Uqzta7rjjoH9/z0oTERGRtkntGRJb8vPdpIw1a0JHy02ZAsce63WFIiIi0gZppVliR1mZm8NcURE6Wi49HQYM8LpCERERaaMUmiU2VFbC4sVQUuJu1xwtFx/vRssZ422NIiIi0mapPUNiw/LlsGdP9e3gaLn16+HKK2HMGO9qExERkTZPoVlav/XrISsr9PqJJ8JNN0HXrtGvSURERNoVtWdI65aVBevWhV6Pi4MRIxSYRUREJCoUmqX1ysmB5OTwo+UGD4b99/esNBEREWlf1J4hrVNeHnz/vTsmu+5ouWnT4LDDvK5QRERE2hGtNEvrU1AA334bfrRcWpo7wEREREQkihSapXUpKnKB2edzt2uOluvYUaPlRERExBNqz5DWo6TEBeaysuprwdFy69a50XJjx3pXn4iIiLRbCs3SOpSWwsKF1YeX1DRsGNx8MyQkRL8uEREREdSeIa1BeblbYS4uDr2vSxcYNUqBWURERDyl0Cze8vth0SJISgodK9e5swvMmsUsIiIiHlN7hnhr+XK3ylx3rNygQS4wd+vmdYUiIiIiWmkWD23aBJmZoWPlVq92gblHD68rFBEREQEUmsUrO3a4g0sgdKzcpZdCr17e1iciIiJSg9ozJPoKC2HJkurbwbFyK1fCBRfAWWd5V5uIiIhIGArNEl0+n9v4V1FR+3piIpx9NgwY4E1dIiIiIg1Qe4ZEj7VuhbmoKPS+Aw+EE06Ifk0iIiIiTaDQLNGzejV89VXoaLlu3eCkk3Q8toiIiLRaas+Q6EhLgw8/DD9abuRIiI/3ukIRERGRemmlWSJv505YtSp0tNyqVW6FuXt3rysUERERaZBCs0RWQYE77c/a0NFy06a5XmYRERGRVk7tGRI55eW1J2XUHC03aRJcfLG39YmIiIg0kUKzRIbfD4sXQ3Fx7euJiTB2LJx8sjd1iYiIiOwFtWdIZCQnQ05O6PXu3WH4cNeiISIiIhIjlFyk5a1fD/PmhY6W69RJkzJEREQkJqk9Q1rW1q3w3nuho+VOOMGtMHfr5nWFIiIiIs2mlWZpOXv2wLJloaPlVq6EE0+EPn28rlBERERkryg0S8soLnYb//z+0NFyU6bA4Yd7XaGIiIjIXlN7huy7igo3Wq6szN2uOVpu3Di49FJPyxMRERHZVwrNsm+shSVL3CEmNSUmwimnwOjRYIw3tYmIiIi0ELVnyL5ZvRp27Ai9npAAI0ZAhw7Rr0lERESkhSk0y95LTXUfdXXo4A4vSUiIfk0iIiIiEaDQLHtn5063yrx2beg85pNOgp49vatNREREpIWpp1mar7jY9TGvWRM6j/mii+Dgg72uUERERKRFaaVZmic4KcPnC53HnJEBRx3ldYUiIiIiLU6hWZonObl6UkbdecwzZnhamoiIiEikqD1Dmm7DBsjOrr4dnMe8di3MmgVjxnhXm4iIiEgEKTRL02zfXnuzX9CAAXDdddr4JyIiIm2a2jOkcYWFsHRp+PuGDFFgFhERkTZPoVka5vO5jX+rVoWOljvmGPjBD7yrTURERCRK1J4h9Qsekb1kSehoudNOg+OO87pCERERkajQSrPUb/Vqd4hJ3dFy69bBsGFgjNcVioiIiESFQrOEl55efUR23dFyl10G8fHe1iciIiISRWrPkFC7drnV5aDgaLlVq2D6dJg40bvaRERERDyg0Cy1FRVBUpLrZ64pMdEdka0T/0RERKQdUnuGVAtOyvD5Qu/r10+BWURERNothWZxrHWzmAsLQ+/bf3/X1ywiIiLSTik0i7NmDezY4eYw15zH3LUrjBjhNgGKiIiItFPqaRbIzIRNm1xQrjmP+cEH4ac/hU6dvK5QRERExFNaPmzvcnNh+XL3ed15zLt2QY8enpYnIiIi0hooNLdnZWWweLELyVB7HnN8PEyb5m19IiIiIq2E2jPaK7/fjZYrLa2+FpzHnJ4OV1wBo0Z5V5+IiIhIK6LQ3F6tXAk5OaHXTz4ZfvlL6NAh+jWJiIiItFJqz2iP0tJgy5bQ6507u0kZCswiIiIitUQsNBtjDjfGfGGMSTHGrDbG3BK4vr8x5lNjzIbAn/tFqgYJY/dudxx23dFyxsDw4dCli7f1iYiIiADWWvzW73UZVSLZnlEB/Mpau9QY0wNYYoz5FLgKmGetnWOMuRO4E7gjgnVIUHGx62Nes6b2aLnZs2H6dHeIiYiIiEgLs9ZSVllGWUUZZZVllFaUkpGXwS0f3cJDZzxEz8498fl9+Cp9bCvcxuyvZnPb6NuYeOREDu15qNflAxEMzdbabCA78HmBMWYNcChwHjAu8LAXgfkoNEdeRYWblFFeHjpaLiMDjjjC6wpFREQkhlT6K6sCcFlFGRl5Gdz04U08fMbD9Ojcg/LKcsory8kuyOaBrx7g9jG3s19CdYPB3KS5LM1eyv99+39cP/z6qusvLn+R1TtW88aqNzi136levLWwotLTbIzpDwwFvgcODgRqgG3AwdGooV2zFpYtg/x8d7vmaLmOHWHGDG/rExERkVYlIy+Dsc+PZdWOVWzJ28K6XetI3pbMf9f9l8F/HcwrK1/hgw0fMG/zPL7Z8g1JW5O498t7WbJ1CX9a+CeyC7LZXbybgrIC/rnin6TsTOGNVW9UvX5OSQ6fbf4Mi+WzzZ+xp3RP2OtbC7Z69S0IEfHQbIzpDrwN3Gqtza95n7XWArae511rjEkyxiTt3Lkz0mW2bevXw7Zt1beDo+Wuugo++QTGjPGsNBEREYksX6WPTTmbGPP8GFbtWMXWgq2k56azMWcjC9IWcNIzJ/Hhhg/5Kv0rPtv8Gf9b/z+ue/86FmYs5I5P72D5tuWs372ejLwM5ibNZeX2lby64tVaX6OpITh4/Y3Vb2ADEdCPvypQ173+5KIno/VtalREQ7MxJh4XmF+x1r4TuLzdGNM3cH9fYEe451prn7HWDrfWDj/wwAMjWWbblp3tQnNdAwbAo4/C6adHvyYRERHZJ1tytzD2+bGs3rGarPwsNuVsYvWO1Xy88WOG/m0o76x5h083fcr/1v+PjzZ+xM0f3sy3Gd9yx6d3sGTrElZsX8GanWuY880clmUv48lFT5JbmkuJr4RdxbuaFYCh6SH4jVVvVL1Ohb8CgAp/BZ9t/ozUPakh199d+y7bCmss/HkoktMzDPAcsMZa+3817voPMCvw+SzgvUjV0O7l57u2jHCGDoWePaNbj4iIiNSrvLKcgrICdhbtJDM/k405G5mfOp+TnjmJ/63/HwvSF1QF4ev/dz0LMxZy+6e3szR7KSk7U9i8ZzOPff8Yy7ct57mlz1FaUYrf+iO2Chy83pwQ/Nnmz2q9TpDF8nTS0yHfE4vl/i/vb+lv9V6J5ErzGGAmMMEYkxz4OBuYA0wyxmwAzgjclpZWVgaLFkFlZeh9xx4LfftGvyYREZF2xm/9FJUXsbt4N0u3LmXksyNZkL6A5G3JfJ/5PQvSF/D6qtcZ+PRAXl/1OvPT5vNd5ncsy17Gmp1reHjhwyzLXsbTi58mrzSP0orSZq8ER2oV+LPNn1HsK+bdte82OQRjYHHW4qrXCfL5faTnpePz+2pdL68sZ2Hmwn35EbSYSE7P+Bow9dw9MVJfV3Ab/5YscavMK1e6jX+Jie6+Qw5xoVlERET2mq/SR3puOlf++0qeOvspeiX0qhqnlpWfxd2f382dY++ke3z3qufMTZpL0tYkHvr6oVrTIl5a/lLVRrma1+uG4BkDZ7Bfwn5hw+71w68Pe336gOlhw+7koyeHvV5aURoSgP34eWLREyHfA4tlQfoC0vPSmxWCD+t1GDtvj739ajpGuy1auxa++SZ0FvPIka4tw9T3dxkREZH2KyMvg8vevoxnpz1L74TeVXOFM/Mzue3T2/j96b+nW6dulFWU4bd+5ibN5bvM7/jt57+tFXaDm+VeXv5y1fX6AnB91yH8SnBLheDHFz0e8v4tluXblocE4Ap/BZkFmSEB2Of3sTBzIcuuq6cVtI1RaG5rtm2DjRtDZzGnpMBNN7kALSIi0k6UV5ZXrQAHZwnf/NHNzJk4hx6de9S676nFT7EwYyG//uTXISE4eVsyzyx5Zq9DcHNWh68ffn29bRItFYKzCrLChuDDex/Orut27eN3vW1SgmpLioogOdl9HpzFHFxpnj4dunb1tDwREZF94bf+WiG3rLKs6lS5OWfMoVfnXpRXluPz+xo8UGPJ1iUhB2pEMgQ3d3X4uuHX8f7698P2CSsEe0ehua2orHRHZPsCvzDBWcwrV8K0aTBlirf1iYiI1BE8Ua5mEN6St4VffPwL/jjxj+5o5Uof5ZXlbC/czh+//mO9p8o9+u2jtUJwzQM1GlsdhvCtEC0Vgn1+X9gA/FTSU6HfFAMfb/yYjXs2hu0TVgj2jkJzW7FqVfWJf0GJiW4O84gR3tQkIiLtkq/SR2lFKaUVpaTnpXPj/27kkUmP0KNzj6ojl7MLshsMwY9991itEPzyypebHIKbuzpcXytES22WW5a9LGwA3pK3pd5pEe2lTziWKDS3BVu2uI+6unbVxj8REWkxmXmZXPr2pTwz7Rl6de5VFYwz8jK4c96d/ObU39A1vit+6696TnBixMPfPBy1ENyc1eEZA2eEnRvckpvlYnVahNSm0Bzr8vNdC0ZdcXEwfDjEx0e/JhERiQnZBdlMf2s6/zjvH25aREUZpRWlZOZn8stPfsn94++ne6fulFeWU15ZzpOLnmRhxkJu++S2kI1yK7av4IXkF1ptn7AN/F9NFsuXaV+SmZ+pPmFplEJzLKuocH3MKSmh85gHDYJevbytT0REPGGtxef3VbVCBFskHp70MN07da9aIX7020f5Zss33PLhLSEheFn2Mp5a9FSLj0xrkRCc+pl7ny1woMbSbUtZccOKff2WSzug0BzLUlLcISZ15zFPmgT9+nldnYiIRIDf+kndk8rMd2fy1NlP0bNzT0oqStwKcV4mv5v/O24fczu9OlcvnARbJB755pFaIfjTzZ9GfWRac0Pw3KS5Yb8Pi7IWtYsDNaT1UGiOVdu3Q3p66Dzm9evhl7/0ujoREdkLFf6KWqPUrn3/Wh6Z9AjdO3WnxFdCsa+Y0orSeg/VeGbpM6zasYrXVr7W6ApxJFshLh90OW+vebvpK8HUH4LT8tIUgqVVUGiOReXlsHy5+7zuPObLLoMOHbytT0REqlT6Kyn2FZOWm8Y1/72Gv0z+C70SelUdupFdkM3v5v+OO8bcQc/OPaueNzdpLouzFoccudwSfcKRPlVuYcZCMvIztBIsbYpCcyxavhzKytznNecxn38+TJjgaWkiIu2BtZYteVu44p0reGbaM+yXsJ8LwZVlZOVn8atPfsU9p99Dl45d8FW6gBgMwbMXzA67OvzqylejNk+4vhD82PePtciBGhqZJm2RQnOsychwR2XXlJgIp50GJ5/sTU0iIm2AtbYq+AZbJMory8nMy+QXn/yiapJE8L6nFz9d7ySJ5G3J/H3J3z2fJ1zhrwgbgpO3JYcNwVsLt+pADZF6KDTHkuJid4hJXfHxMHhw9OsREYkR1lrKKstI3ZPK1e9dzWNnPeb6hIMb6PIzmb1gdshBG9AykyQiPU/4icWhh2pgYGn20rAhuF/vfuy+bneLfG9F2guF5lhhLSxb5nqX6xo8GBISol+TiIjHtuZvZcbbM3j+3OfpldCr1pzhX33yK35/+u/pGt+VssoyrHWTGBZlLeK+L++rtTr80vKXQg7aAG820dV35PL81Pn1zhPOzA89VEN9wiItS6E5VmzaBDk5odcPOwz69o1+PSIiUVLhr2BTziZm/XsWfznrL3SL70aRr4hiXzF/XvhnvtnyDbd+dGvYFolnljzT4nOGm72JrjK0f9hieXLxk6Fv1lDvkcvLti/TPGERDyk0x4L8fFi3DtaurX2ISZcuMHCg19WJiOyTSn8lJRUllPhKSMtN48YPbuSBCQ/QpWMXin3FlFeWV60Q3//l/Z7PGW7uJInl2aGb6Hx+Hxn5GVodFokhCs2tnd/v2jJSUkIPMfnxj3VMtoi0epX+yqqV4WJfMUXlRWzJ28Ldn9/NHWPuoEenHlWPnZs0lyVbl/DYd495Omf4+uHX8/7691tkkoQ20Ym0DQrNrd2GDW6lue4hJllZ0KeP19WJSDtX6a8kLTet3tPp7pl/D7eNvi3s5rqV21fyyopX9ioct8ScYYvlqcVPhb4pAx9t/IiNezZqkoSIVFFobs1yc11ohtBDTC65xNPSRKRtC45fCwbgEl8JW/K28IuPf8F94++jW3w3yirL8FX6GjydbvWO1fu0uS6Sc4Z9fh9b8reEbZHQnGERqUuhubUKtmXYwP/4Bw8xWbUKfvQjGDPG2/pEJOZZaymtKKWwvJAiXxGF5YUUlheyJW8L9315X8j4teDmurmL50Ztc119K8SaMywi0abQ3FqtXQuFhbWvJSbCuefCccd5U5OIxBS/9VPiKyF1Tyo/+e9P+POZf3aziX0llFSUkJWfxUPfPBQSjl9IfiFk/FqkN9fVN2atvhVizRkWkWhTaG6NcnLciLm6evWCY46Jfj0i0ioFp04U+4op8ZWQnpfOrR/dyr3j7qVLxy6UVpQC1cc3P/jVg7XaJF5b9do+hePm9hXfMOIG/rfhfyHvw2LrHbOmFWIRaS0UmlubykpITg69HhcHQ4a4P0Wkzduav5Xpb03n2WnP0iuhV9XqcEZeBnfOu5M7x95J9/jutZ4TbJ/4W9LfWvTkuub2Fdd3Ot2HGz5kQ86GsBMmNGZNRFo7hebWZs0aKCoKvX7ccdCzZ/TrEZEWFTzOORiC03PTufnDm/njxD/Ss3NPSitKKaso47HvH2NhxkJ+/cmvQw7tWLl9JS8vfzlqJ9cBzRq9Vt/pdNpcJyKxTKG5Ndm5E1JTQw8x2W8/OPpor6sTkSbIzMvk0rcv5a9T/0rPzj2rZhNn5GVwzxdu/FrvhN5Vj5+bNJel2UubNJc4kifXzUudR6eOnULfkIFFWYvUOiEi7Z5Cc2tRXu6mZaxdW/sQkwcegOuvB2O8rlBEAnyVPlL3pHLlv6/k0cmP0jW+K0W+IorKi3j0u0dZmLGQOz69o9ZK8N+W/I1VO1bx+qrXo7a5rr7JE08nPR3yniyWr9O/1gl1IiL1UGhuLZKToaws9BCT7duhe/dGny4iLaesoqxqhTgtN42ff/Rz7h9/P13ju1LiK6HCX1F1rPPsBbOj0j9c3+a6WUNm8d6695o1eSI9L13hWESkmRSaW4O0NBeOIfQQkwsu8LQ0kbaovLKcEl9g6kSgr/hXn/yK3572WxI6JlDpr6x67NykuSzLXsZTi56KyrHO9W2ue3zR4yHvw2L5Mu1LUnNT1T4hIhJhCs1eKyiA1aurbwcPMUlJgVmzYPRo72oTiUHWWjbv2czMd2fyxJQnqvqKg3OJ7/3y3pBjnYNTJ55b+lyLn1xX3wpxpa1s1qEdWQVZYadOaHOdiEh0KDR7qbISlixxrRg1JSbCzJlwyCHe1CXSilX6KymrLKO0opT03HSue/865pwxx/UVlxdRUlHC04uf5rvM7/jdF7+rFYKfW/ZcyLHOkTy5zo+fJxaFH7+2ZOsSHdohIhJDFJq9tGaNW2muq39/BWZpd6y1pO5J5Ufv/oi558ytGr9WWlFKZn4mv/3it9wx5g56dOpR9Zy5SXNJ2prEnxf+uUX7ivdmc119K8SZBeHHr6l/WEQktig0e2X7djderq4ePeCEE6Jfj0iE1T3S+ZFJj9Q60rm0orRqhfjueXeHnU38yopXotJXfPmgy3l37bs61llERKooNHuhrMxNy6g7jzkuDoYNgw4dvK5QZK/4Kn1VUydS96Ry80c384dxf6BLxy6UVJRgra060vmhrx9qUv9wJPuKIfyhHQszFpKel67NdSIiUkWh2QurVsGKFbXnMc+eDRdfrFP/pNVKz03n8ncu529T/0avzr3cyXWVZWTmZXLnvDu5Y+wdtY51Dk6dmLt47l73D7fU5IknFz8Z+oYaOLRDm+tERKQuheZo27EDtm4NncecmgpHHul1ddKOVfgrKPYVszlnM9e+fy0PT3qYrvFd3eQJXwlPLHqCbzO+DTm0Y27SXFZsX1HrWOeW6h+ub4XY5/c1q684Iz9DfcUiIrJPFJqjqbLShWUIncc8Y4a3tUm74Kv0UeQrorC8kNQ9qdzy0S38/vTfk9AxgfLKcqB6c90j3zwSlUM7ZgycUevxQfVNnrBYlmUvU1+xiIhElUJzNG3YAMXF7vPgPOaVK+Gii+D0072tTdqMCn8FReVFVcc6p+Wmcdunt4Vtn0jelswzS57x7NAOi2V+6nwy8zObPHnC5/dphVhERKJOoTlaCgth06ba1xITYcwYHWAizRKcQlFSUVK16S49N507PrsjJBjDvrdPNHdzXYW/ollTJ5ZtX8aKG1ZE5HslIiLSUhSao2XlytBDTOLi4MQTvalHWrX03HQue/synjr7Kbp36k6xr5giXxEZeRncv+B+bh9ze8iJdnWDMbRMOG7usc5Ls5dq6oSIiLQ5Cs3RkJkJu8KEhaOPhu7dQ69Lu+C3fjbu3siV/76S/5v8f3Tp2KWq3/jx7x/nu8zv+O3nv60Vgl9c/iIpO1Mic6JdZfMO7ajvWGe1ToiISFuk0BxpPh+sXh16vWtXOOaY6NcjUWWtpbyynNQ9qVz13lU8POlhunTsQkF5AcW+Yp5e/DSLshbxwIIHonaiXYWtp30iW4d2iIiI1EehOdLWrIHy8tDrgwbpEJM2wFpLSUUJReVFFPuKSctN4xcf/4J7x91Lt/hulFaU4rd+5ibNZVHWoloHekRy093VQ6/mP+v+EzYcL92q9gkREZHmUmiOpD17ID099OS/H/wADjrI6+qkicoqyqo23ZX43J8ZeRnc9fld3Db6Nnp17lX12OBEir8l/S3ym+5SPyPOxIXUa7F8kfoFm3M3hw3Hap8QERFpPoXmSPH7YflyF5hrnvz34IMwaZLX1Uk9isqL2FO6hz0le9iYs5Hfzf8dt42+rdamO3DheOX2lby28rUWDcfNOtEO+C7zu7B9xTrRTkREpGUpNEfK5s1QUBB68l92NiQkeF1du1fpr6TIV0RBWQGb92zm5x/9nNvH3F5rXNuzS59l9Y7VtTbdQeTCsU60ExERab0UmiOhqAjWrXOf1z3574ILvK2tHSr2FZNbmsuG3Ru49eNbuXvs3SR0rP6Ly9ykuSzftrxJc4yhmRMpmhGOtelORESk9VJojoSaM5mDJ/+tWgU/+pEOMomw0opS1u1axzX/vYZ7T7+XjnEdax0PvXzbcl5IfmGvN+PVN5HCb/3NOtBD4VhERCS2KDS3tKws2Fnnn8sTE+Hss2HAAG9qaqM252zminev4OEzHqZTh07kleVRVlHG3KS5LNm6hKcXP92i/cY3jbyJDzZ8EFqIgaStSZpIISIi0oYpNLek8nK3olxXly5w3HHRr6cNKasoI7c0t+ojryyPv3z3F77P/J45X89p0XBcd8UYAAP/W/8/1uesV2+xiIhIO6TQ3JJSUuqfydxR3+qmsNZSWF5IQXkB+WX55Jfls3nPZmYvmF3r6OiWCMc+vy9sS8XirMVhg7EmUoiIiLRfSnItZfduyMgIvf6DH8DBB0e/nhhR7CsmpySHdbvW8ctPfhky9xjgpeUvhRwd3ZxwXFZZFjYcL8tepjnGIiIi0iQKzS0hOJO5rvh49THX4Ld+Nu7eyMx/z+T+8fdjMJRWlALh5x5D+BVla23zJlVkh59UoXAsIiIiTaXQ3BI2bHBj5uqe/Hf88e12JnNmXibT35rOY2c9VrVJr6CsgKcWP8XirMU89t1jez3ezQb+r6aGxrhpM56IiIjsK4XmfVVUBBs3hp789/jjMHWq19VFjd/62VOyh13Fu9hVvIs/fvVHvsv8jnvn39ui493mpc6jX69+GuMmIiIiUaXQvK+CM5nDnfxnjNfVRVRheSHLty3nZx/+jF+N+lVVL3JOSQ6fbv50nzbp3TDiBj7c+GHI17RYJh01ifU3r4/eGxUREZF2T6F5X2zdWj2TuebJf/HxMHmyt7VFSEFZAdmF2Wwt2EpBWUHVgSE1e5GbdWJeZWgfMgY+3PAh63avq3eKhYiIiEg0KTTvrYoKWL26+nbw5L+1a+Hqq2HUKO9qa0HWWvLL8lmxfUXVinJDY9+au0lveXboiXka7yYiIiKtjULz3lq3DkpLa19LTIQrr4z5EXO+Sh+7inexvWg7O4p2VJ2yt3L7ykbHvmmTnoiIiLRFCs17Iz8fUlNDrx9ySMwG5oKyAnYU7WD1ztXcPe/uRg8SqW9F+bCeh2mTnoiIiLQ5Cs3NZS2sWOH+rKlDh5iayVzpr6y1mlziKwHgr0l/bdJBIuFWlDFw1g/P4qlznorqexERERGJtDivC4g5GRmwZ0/o9WOPha5do19PMxSVF5G6J5XvMr/j1ZWvMvGliSRvS64KzHVXlPeU7gk79u2z1M/YmLOx3l5kERERkbZGK83NUV4OKSmh17t3h6OOin49jcjKz+KSNy/hwYkP4rd+CssLq+57bdVre7+iDBr7JiIiIu2KVpqbY80aN4/5zTfdlIygQYMgzvtvZbDlYt2udXyb8S3X/vdavsv8joe/ebhWYG7qivK81Hls3rNZK8oiIiLS7mmlual27IBPPql96t/s2XDGGXDAAVEvx1pLYXkheWV5bNy9kVs/vpVfjfoVvRN6A/UfMALNm3ox8ciJrP3ZWkRERETaM++XR2OBzwfLl4ee+peSAiecEPEvX+GvYE/JHhZnLWb4M8N5b+17fLjxQ+anzWdZ9jIeXvgwK7av4PVVr1c9J1wwBsKuKH+e+jnpuelaURYRERGph1aam2LVKjeTueapfx07wrRp0Llzi32ZSn8lheWFbMrZxHX/u477xt1HfIf4qo16c5PmsjR7Kc8seaaqD7k54+BmDJzBu2vfDfm6fvyM6z+O1TetDrlPRERERBSaG5edDZmZ7vPgqX8rV8Kpp8KFF+7VS/oqfRSUF7ApZxM3/O8G/jDuD3Tu2LlWOF6ydQlPLnqywXC8X8J+zWq1WLhlIel56TqaWkRERKSZFJobUlbmZjLXlJjoVpxPP73Rp/sqfRT5itzK8fvXcd/4+4iPi6e0wp0kGFw5fnrx03sVjqcPmN6sA0a+3/q9jqYWERER2QsRC83GmOeBqcAOa+3AwLX9gTeA/kAaMN1aG2bocSuxYoUbM1fXwIHQpQvgNuQV+4opLC9k857N/OzDn/H7039PQscEyirKgOpw/NSip1o0HJdXluuAEREREZEoiORGwBeAs+pcuxOYZ609BpgXuN06ZWbCtm1VNytsJbkVhSxJyGXEhxfw0caP+CL1Cz7Y8AGfp37OoqxF3L/gfpZlL+NvSX+rCszhxrtB+I16YQ8S2fwZLy1/KWy7RfK2ZG3eExEREYmCiK00W2sXGGP617l8HjAu8PmLwHzgjkjVsDd8lT4K8ney6bt3uGHz4/zhB5fT2cRT4i+H+I7MLfyWJVuX8MT3T1StGkPLrByXVpSGDcfLty0P225xWK/D2Hn7zgh/R0REREQk2j3NB1trswOfbwMOjvLXb9SmPZvYsPC/zF3zHEsrN/F06r+4vv8lAOT07c1nCz5v8uzjesNxZfPC8eG9D2fXdbsi/dZFREREpB6ebQS01lpjTOj5zAHGmGuBawH69esXtbq678glZ3USn5Wvw3aEz8rXMSNzA/sNPoU3Mj8OCcbXD7++3raKesNxtsKxiIiISCyJdmjebozpa63NNsb0BXbU90Br7TPAMwDDhw+vN1y3qIoKemzO4o09X2MT3CW/gTdyv2b6wWfy2ZLws4/fTHlT4VhERESkDYt2aP4PMAuYE/jzvSh//YZ17EjBkOP5bMl2glG3ogN81nU7pateqXf2cVZBlsKxiIiISBsWyZFzr+E2/R1gjMkEfo8Ly/8yxvwESAemR+rr760/Ln8SG2fAX33Nxpl6+401+1hERESk7Yvk9IzL6rlrYqS+Zkv4NvNbrRqLiIiISC06EbAOrRqLiIiISF2RPNxERERERKRNUGgWEREREWmEQrOIiIiISCMUmkVEREREGqHQLCIiIiLSCIVmEREREZFGKDSLiIiIiDRCoVlEREREpBEKzSIiIiIijVBoFhERERFphEKziIiIiEgjFJpFRERERBqh0CwiIiIi0giFZhERERGRRig0i4iIiIg0wlhrva6hUcaYnUC6B1/6AGCXB19Xok8/6/ZDP+v2Qz/r9kM/6/Yj0j/rI6y1B4a7IyZCs1eMMUnW2uFe1yGRp591+6Gfdfuhn3X7oZ91++Hlz1rtGSIiIiIijVBoFhERERFphEJzw57xugCJGv2s2w/9rNsP/azbD/2s2w/PftbqaRYRERERaYRWmkVEREREGqHQXA9jzFnGmHXGmI3GmDu9rkdajjHmcGPMF8aYFGPMamPMLYHr+xtjPjXGbAj8uZ/Xtcq+M8Z0MMYsM8a8H7h9pDHm+8Dv9hvGmE5e1ygtwxjT2xjzljFmrTFmjTFmlH6v2yZjzC8C//u9yhjzmjEmQb/bbYMx5nljzA5jzKoa18L+Hhvn8cDPfIUxZlgka1NoDsMY0wF4CpgCnABcZow5wduqpAVVAL+y1p4AnALcFPj53gnMs9YeA8wL3JbYdwuwpsbth4BHrbU/BPYAP/GkKomEx4CPrLWJwGDcz12/122MMeZQ4OfAcGvtQKADcCn63W4rXgDOqnOtvt/jKcAxgY9rgbmRLEyhObyRwEZr7WZrbTnwOnCexzVJC7HWZltrlwY+L8D9h/VQ3M/4xcDDXgTO96RAaTHGmMOAc4C/B24bYALwVuAh+jm3EcaYXsBpwHMA1tpya20u+r1uqzoCXYwxHYGuQDb63W4TrLULgJw6l+v7PT4PeMk63wG9jTF9I1WbQnN4hwIZNW5nBq5JG2OM6Q8MBb4HDrbWZgfu2gYc7FVd0mL+AtwO+AO3+wC51tqKwG39brcdRwI7gX8E2nH+bozphn6v2xxrbRbwJ2ALLiznAUvQ73ZbVt/vcVTzmkKztFvGmO7A28Ct1tr8mvdZN1ZGo2VimDFmKrDDWrvE61okKjoCw4C51tqhQBF1WjH0e902BPpZz8P9RekHQDdC/zlf2igvf48VmsPLAg6vcfuwwDVpI4wx8bjA/Iq19p3A5e3Bf9YJ/LnDq/qkRYwBzjXGpOFarCbgel57B/5JF/S73ZZkApnW2u8Dt9/ChWj9Xrc9ZwCp1tqd1lof8A7u912/221Xfb/HUc1rCs3hLQaOCezE7YTbYPAfj2uSFhLoa30OWGOt/b8ad/0HmBX4fBbwXrRrk5Zjrb3LWnuYtbY/7nf4c2vtFcAXwMWBh+nn3EZYa7cBGcaY4wKXJgIp6Pe6LdoCnGKM6Rr43/Pgz1q/221Xfb/H/wGuDEzROAXIq9HG0eJ0uEk9jDFn4/ohOwDPW2sf8LYiaSnGmLHAV8BKqntd78b1Nf8L6AekA9OttXU3I0gMMsaMA35trZ1qjDkKt/K8P7AM+JG1tszD8qSFGGOG4DZ9dgI2A1fjFof0e93GGGP+AMzATUNaBlyD62XV73aMM8a8BowDDgC2A78H/k2Y3+PAX5qexLXnFANXW2uTIlabQrOIiIiISMPUniEiIiIi0giFZhERERGRRig0i4iIiIg0QqFZRERERKQRCs0iIiIiIo1QaBYR8YAxptIYk1zj487Gn9Xk1+5vjFnVUq8nIiLu2FEREYm+EmvtEK+LaIwxZj9r7R6v6xAR8ZpWmkVEWhFjTJox5mFjzEpjzCJjzA8D1/sbYz43xqwwxswzxvQLXD/YGPOuMWZ54GN04KU6GGOeNcasNsZ8YozpEnj8z40xKYHXeb0JJd0WqOM6Y0zPyLxrEZHWT6FZRMQbXeq0Z8yocV+etXYQ7qSrvwSuPQG8aK09EXgFeDxw/XHgS2vtYGAYsDpw/RjgKWvtACAXuChw/U5gaOB1rm+sSGvt3cBM4ChgqTHmH4FTNUVE2hWdCCgi4gFjTKG1tnuY62nABGvtZmNMPLDNWtvHGLML6Gut9QWuZ1trDzDG7AQOq3lcsDGmP/CptfaYwO07gHhr7WxjzEdAIe5Y2n9bawubUXMH4DLgKVyA//nevXsRkdijlWYRkdbH1vN5c5TV+LyS6j0s5+BC7zBgsTGm1t6WwEpysjHmgxrXjDFmAvAi8Dvc6vaf97IuEZGYpNAsItL6zKjx57eBzxcClwY+vwL4KvD5POAGcCvBxphe9b2oMSYOONxa+wVwB9ALqLXaba292lo7xFp7duA5VwBrgZuAV4HjrbX3WGvT9+0tiojEFk3PEBHxRhdjTHKN2x9Za4Nj5/YzxqzArRZfFrh2M/APY8xtwE7g6sD1W4BnjDE/wa0o3wBk1/M1OwAvB4K1AR631uY2Umc6MNZau7PJ70xEpA1ST7OISCsS6Gkebq3d5XUtIiJSTe0ZIiIiIiKN0EqziIiIiEgjtNIsIiIiItIIhWYRERERkUYoNIuIiIiINEKhWURERESkEQrNIiIiIiKNUGgWEREREWnE/wORNL27TIghzAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainBP = [i[0] for i in trainAccBoth]\n",
    "trainNP = [i[1] for i in trainAccBoth]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainBP, 'r.')\n",
    "plt.plot(trainBP, 'r', linewidth=5,alpha=0.3)\n",
    "plt.plot(trainNP, 'g^')\n",
    "plt.plot(trainNP, 'g', linewidth=5,alpha=0.3)\n",
    "plt.legend([\"Back propagation\", \"Node Perturbation\"])\n",
    "plt.title(\"Accuracy vs epochs\", size=15)\n",
    "plt.xlabel(\"Epochs ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cosine Similiarity of the NP and BP updates')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(w1Sim)\n",
    "plt.plot(b1Sim)\n",
    "plt.plot(w2Sim)\n",
    "plt.plot(b2Sim)\n",
    "plt.plot(w3Sim)\n",
    "plt.plot(b3Sim)\n",
    "plt.legend([\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"])\n",
    "plt.title(\"Cosine similarity of the weights of NP and BP with the epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cosine Similiarity of the NP and BP updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance of BP and NP with the full variability play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "  #print(\"b3 shape = \",b3.shape, \" A2fp shape = \", A2.shape,\" and W3 shape = \", W3.shape)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDCompOC(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "  W1comp = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init(2)\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      W1varocnp = weightTransformWithVariability(W1np, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocnp = weightTransformWithVariability(b1np, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocnp = weightTransformWithVariability(W2np, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocnp = weightTransformWithVariability(b2np, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocnp = weightTransformWithVariability(W3np, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocnp = weightTransformWithVariability(b3np, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      W1comp.append([W1np[0, 0], W1varocnp[0, 0]])\n",
    "      \n",
    "\n",
    "      #print(W3varocnp.shape)\n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocnp, b1varocnp, W2varocnp,b2varocnp, W3varocnp, b3varocnp) \n",
    "      print(f\"NP Iter {i} -> sub iter {j} : {round(accuracy(predictions(A3), Y1), 3)}\", end = \"\\r\", flush = True)\n",
    "      #lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "      lossBeforePert = crossEntropy(one_hot_encoding(Y1), A3)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varocnp, W2varocnp, W3varocnp,b1varocnp, b2varocnp, b3varocnp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      dW1varocnp = weightTransformWithVariability(dW1np, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varocnp = weightTransformWithVariability(db1np.reshape(db1np.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varocnp = weightTransformWithVariability(dW2np, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varocnp = weightTransformWithVariability(db2np.reshape(db2np.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varocnp = weightTransformWithVariability(dW3np, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varocnp = weightTransformWithVariability(db3np.reshape(db3np.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1varocnp, b1varocnp, W2varocnp,b2varocnp, W3varocnp, b3varocnp, dW1varocnp, db1varocnp, dW2varocnp, db2varocnp, dW3varocnp, db3varocnp, lr = lrNP)\n",
    "      #print(W1)\n",
    "      ###print(f\"Max W1={np.max(np.abs(W1np))},b1={np.max(np.abs(b1np))},W2={np.max(np.abs(W2np))},b2={np.max(np.abs(b2np))},W3={np.max(np.abs(W3np))},b3={np.max(np.abs(b3np))}\", end= 'r', flush=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "\n",
    "      W1varocbp = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocbp = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocbp = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocbp = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocbp = weightTransformWithVariability(W3bp, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocbp = weightTransformWithVariability(b3bp, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocbp, b1varocbp, W2varocbp,b2varocbp, W3varocbp, b3varocbp) \n",
    "      print(f\"                                                                        BP Iter {i} -> sub iter {j} : {round(accuracy(predictions(A3), Y1),3)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1varocbp, W2varocbp, W3varocbp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      dW1varocbp = weightTransformWithVariability(dW1bp, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varocbp = weightTransformWithVariability(db1bp.reshape(db1bp.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varocbp = weightTransformWithVariability(dW2bp, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varocbp = weightTransformWithVariability(db2bp.reshape(db2bp.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varocbp = weightTransformWithVariability(dW3bp, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varocbp = weightTransformWithVariability(db3bp.reshape(db3bp.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1varocbp, b1varocbp, W2varocbp, b2varocbp, W3varocbp, b3varocbp, dW1varocbp, db1varocbp, dW2varocbp, db2varocbp, dW3varocbp, db3varocbp, lr = lrBP)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    #lrNP = lrNP*np.exp(-0.1)\n",
    "    #lrBP = lrBP*np.exp(-0.1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {round(train_score, 3)}::Val accuracy: {round(val_score, 3)}:#########################:Train Acc BP::{round(accuracy(predictions(A3_train_bp), Y), 3)} Val Acc BP::{round(accuracy(predictions(A3_val_bp), y_val), 3)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_acc, val_acc, train_loss, val_loss, sum_weights, W1comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDCompFPOC(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "  W1comp = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init(2)\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    #w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    #b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    #w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    #b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    #w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    #b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    plt.figure()\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      W1varocnp = weightTransformWithVariability(W1np, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocnp = weightTransformWithVariability(b1np, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocnp = weightTransformWithVariability(W2np, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocnp = weightTransformWithVariability(b2np, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocnp = weightTransformWithVariability(W3np, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocnp = weightTransformWithVariability(b3np, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      #W1comp.append([W1np[0, 0], W1varocnp[0, 0]])\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocnp, b1varocnp, W2varocnp,b2varocnp, W3varocnp, b3varocnp) \n",
    "      #print(W2np[3], \" #####\")\n",
    "      #Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1np, b1np, W2np,b2np, W3varocnp, b3np) \n",
    "      print(f\"NP Iter {i+1} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "      lossBeforePert = crossEntropy(one_hot_encoding(Y1), A3)\n",
    "      \n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n",
    "      #NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      \n",
    "      #print(dW1np, W3np)\n",
    "\n",
    "      #dW1varocnp = weightTransformWithVariability(dW1np, dW1Currents, precision, step, discreteSteps)\n",
    "      #db1varocnp = weightTransformWithVariability(db1np.reshape(db1np.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      #dW2varocnp = weightTransformWithVariability(dW2np, dW2Currents, precision, step, discreteSteps)\n",
    "      #db2varocnp = weightTransformWithVariability(db2np.reshape(db2np.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      #dW3varocnp = weightTransformWithVariability(dW3np, dW3Currents, precision, step, discreteSteps)\n",
    "      #db3varocnp = weightTransformWithVariability(db3np.reshape(db3np.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1np, b1np, W2np,b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr = lrNP)\n",
    "      #print(W1)\n",
    "      ###print(f\"Max W1={np.max(np.abs(W1np))},b1={np.max(np.abs(b1np))},W2={np.max(np.abs(W2np))},b2={np.max(np.abs(b2np))},W3={np.max(np.abs(W3np))},b3={np.max(np.abs(b3np))}\", end= 'r', flush=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "\n",
    "      W1varocbp = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocbp = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocbp = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocbp = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocbp = weightTransformWithVariability(W3bp, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocbp = weightTransformWithVariability(b3bp, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocbp, b1varocbp, W2varocbp,b2varocbp, W3varocbp, b3varocbp) \n",
    "      print(f\"                                                                        BP Iter {i} -> sub iter {j} : {round(accuracy(predictions(A3), Y1),3)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      \n",
    "      dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1bp, W2bp, W3bp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      #dW1varocbp = weightTransformWithVariability(dW1bp, dW1Currents, precision, step, discreteSteps)\n",
    "      #db1varocbp = weightTransformWithVariability(db1bp.reshape(db1bp.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      #dW2varocbp = weightTransformWithVariability(dW2bp, dW2Currents, precision, step, discreteSteps)\n",
    "      #db2varocbp = weightTransformWithVariability(db2bp.reshape(db2bp.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      #dW3varocbp = weightTransformWithVariability(dW3bp, dW3Currents, precision, step, discreteSteps)\n",
    "      #db3varocbp = weightTransformWithVariability(db3bp.reshape(db3bp.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      #plt.plot(b3np, b3varocnp, 'r.')\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1bp, b1bp, W2bp, b2bp, W3bp, b3bp, dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp, lr = lrBP)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    #lrNP = lrNP*np.exp(-0.1)\n",
    "    #lrBP = lrBP*np.exp(-0.1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {round(train_score, 3)}::Val accuracy: {round(val_score, 3)}:#########################:Train Acc BP::{round(accuracy(predictions(A3_train_bp), Y), 3)} Val Acc BP::{round(accuracy(predictions(A3_val_bp), y_val), 3)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_acc, val_acc, train_loss, val_loss, sum_weights, W1comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter=1\n",
    "lrBP=0.01\n",
    "lrNP=0.01\n",
    "pert=0.01\n",
    "mu = 0.7\n",
    "sigma = 0.01\n",
    "vDD = 5\n",
    "precision = 10#setting the precision value of the calculations\n",
    "wRange = 10\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization\n",
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "0.9999999999999996\n",
      "NP Iter 1 -> sub iter 1 : 10.0                                          BP Iter 0 -> sub iter 0 : 10.317\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20272\\791946858.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  loss=-np.sum(np.multiply(y, np.log(y_pre)), axis = 0)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20272\\791946858.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss=-np.sum(np.multiply(y, np.log(y_pre)), axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP Iter 1 -> sub iter 4 : 10.0                                          BP Iter 0 -> sub iter 3 : 14.444\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=0'>1</a>\u001b[0m batchGDCompFPOC(x_train,y_train,\u001b[39miter\u001b[39;49m, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 47\u001b[0m in \u001b[0;36mbatchGDCompFPOC\u001b[1;34m(X, Y, iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=86'>87</a>\u001b[0m lossBeforePert \u001b[39m=\u001b[39m crossEntropy(one_hot_encoding(Y1), A3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=89'>90</a>\u001b[0m \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=90'>91</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=91'>92</a>\u001b[0m dW1np, db1np, dW2np, db2np, dW3np, db3np \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=92'>93</a>\u001b[0m \u001b[39m#NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=93'>94</a>\u001b[0m \u001b[39m#print(f\"iter in iter{j}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=94'>95</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=101'>102</a>\u001b[0m \u001b[39m#dW3varocnp = weightTransformWithVariability(dW3np, dW3Currents, precision, step, discreteSteps)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=102'>103</a>\u001b[0m \u001b[39m#db3varocnp = weightTransformWithVariability(db3np.reshape(db3np.shape[0],1), db3Currents, precision, step, discreteSteps)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=106'>107</a>\u001b[0m W1np, b1np, W2np, b2np, W3np, b3np \u001b[39m=\u001b[39m param_update(W1np, b1np, W2np,b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr \u001b[39m=\u001b[39m lrNP)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 47\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=51'>52</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=53'>54</a>\u001b[0m Z3pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W3,A2pert) \u001b[39m+\u001b[39m b3\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=54'>55</a>\u001b[0m A3pert \u001b[39m=\u001b[39m softmax(Z3pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=56'>57</a>\u001b[0m \u001b[39m#lossArrayAfterPertZ1[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=57'>58</a>\u001b[0m lossArrayAfterPertZ1[i, :] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m crossEntropy(one_hot_encoding(Y1), A3pert)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 47\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(Z)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(Z):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=52'>53</a>\u001b[0m   \u001b[39m#return np.exp(Z) / np.sum(np.exp(Z),0)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=53'>54</a>\u001b[0m   Z \u001b[39m=\u001b[39m Z\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39;49mmax(Z, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000045?line=54'>55</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mexp(Z) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(Z),\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2679\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2680\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2791\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2793\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[0;32m   2794\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batchGDCompFPOC(x_train,y_train,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, step, discreteSteps, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "0.9999999999999996\n",
      "Iteration: 1::Train accuracy: 41.408::Val accuracy: 40.943::Train Acc BP::41.373 Val Acc BP::40.87141.276\n",
      "0.9999280895856614\n",
      "Iteration: 2::Train accuracy: 50.224::Val accuracy: 49.543::Train Acc BP::49.789 Val Acc BP::48.8: 50.476\n",
      "0.9996815593434452\n",
      "Iteration: 3::Train accuracy: 51.716::Val accuracy: 51.6::Train Acc BP::51.565 Val Acc BP::51.571: 53.333\n",
      "0.9993184439240635\n",
      "Iteration: 4::Train accuracy: 53.787::Val accuracy: 53.629::Train Acc BP::53.662 Val Acc BP::53.75754.762\n",
      "0.9991144973890551\n",
      "Iteration: 5::Train accuracy: 53.116::Val accuracy: 52.4::Train Acc BP::54.244 Val Acc BP::54.257: 55.873\n",
      "0.9989618359287288\n",
      "Iteration: 6::Train accuracy: 53.225::Val accuracy: 52.743::Train Acc BP::53.743 Val Acc BP::53.71453.817\n",
      "0.998836525315705\n",
      "                                                                        BP Iter 6 -> sub iter 16 : 50.476\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 49\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=0'>1</a>\u001b[0m w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_accwithVarg, val_accwithVarg, train_losswithVarg, val_losswithVarg, sum_weights, compMeComp \u001b[39m=\u001b[39m batchGDCompOC(x_train,y_train,\u001b[39miter\u001b[39;49m, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 49\u001b[0m in \u001b[0;36mbatchGDCompOC\u001b[1;34m(X, Y, iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=64'>65</a>\u001b[0m X1, Y1 \u001b[39m=\u001b[39m shuffle(X[:, j\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m: (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m]\u001b[39m.\u001b[39mT,Y[j\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m: (j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m630\u001b[39m]) \u001b[39m#shuffle each batch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=65'>66</a>\u001b[0m X1 \u001b[39m=\u001b[39m X1\u001b[39m.\u001b[39mT \u001b[39m#take transpose to match the sizes\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=67'>68</a>\u001b[0m W1varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(W1np, W1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=68'>69</a>\u001b[0m b1varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(b1np, b1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=69'>70</a>\u001b[0m W2varocnp \u001b[39m=\u001b[39m weightTransformWithVariability(W2np, W2Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 49\u001b[0m in \u001b[0;36mweightTransformWithVariability\u001b[1;34m(weightArray, currents, precision, step, discreteSteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=14'>15</a>\u001b[0m analogWeightArray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(weightArray, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m bitLevel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(precision):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=17'>18</a>\u001b[0m   analogWeightArray \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmultiply(np\u001b[39m.\u001b[39msign(weightArray),  np\u001b[39m.\u001b[39mwhere(np\u001b[39m.\u001b[39mbitwise_and(clippedWeightIndexArray, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbitLevel)\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbitLevel))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=21'>22</a>\u001b[0m weightWithVariability \u001b[39m=\u001b[39m (analogWeightArray\u001b[39m/\u001b[39miOnNominal)\u001b[39m*\u001b[39mstep\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000047?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m weightWithVariability\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_accwithVarg, val_accwithVarg, train_losswithVarg, val_losswithVarg, sum_weights, compMeComp = batchGDCompOC(x_train,y_train,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainBPAcc= [i[0] for i in train_accwithVarg]\n",
    "trainNPAcc = [i[1] for i in train_accwithVarg]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainNPAcc, 'r*')\n",
    "plt.plot(trainBPAcc, 'g^')\n",
    "plt.plot(trainNPAcc, 'r', linewidth=3, alpha = 0.3)\n",
    "plt.plot(trainBPAcc, 'g', linewidth=3, alpha = 0.3)\n",
    "plt.legend([\"Node perturbation\", \" Back propagation\"])\n",
    "plt.title(\"Accuracy vs epochs for the  different perturbation\", size=20)\n",
    "plt.xlabel(\"Epochs ->\", size=15)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
