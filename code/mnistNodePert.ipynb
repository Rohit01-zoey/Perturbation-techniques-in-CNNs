{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  print(\"BP \\n \", dZ3)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z1\")\n",
    "    #print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "def batchGDNP(X,Y,iter, lr, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "      #print(W1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsToTrain = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertList = [1,0.1, 0.01, 0.001, 0.0001]\n",
    "trainAccPertList = []\n",
    "valAccPertList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 69.841269841269834\n",
      "Iteration: 1\n",
      "Train accuracy: 70.33809523809524\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 1 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 2\n",
      "Train accuracy: 79.0079365079365\n",
      "Val accuracy: 78.08571428571427\n",
      "Iter 2 -> sub iter 99 : 82.69841269841271\n",
      "Iteration: 3\n",
      "Train accuracy: 82.1079365079365\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 3 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 4\n",
      "Train accuracy: 84.05238095238096\n",
      "Val accuracy: 83.52857142857142\n",
      "Iter 4 -> sub iter 99 : 85.23809523809524\n",
      "Iteration: 5\n",
      "Train accuracy: 85.35396825396825\n",
      "Val accuracy: 84.88571428571429\n",
      "Iter 5 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 6\n",
      "Train accuracy: 86.41904761904762\n",
      "Val accuracy: 85.97142857142858\n",
      "Iter 6 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 7\n",
      "Train accuracy: 87.17777777777778\n",
      "Val accuracy: 86.62857142857143\n",
      "Iter 7 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 8\n",
      "Train accuracy: 87.86825396825397\n",
      "Val accuracy: 87.41428571428571\n",
      "Iter 8 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 9\n",
      "Train accuracy: 88.39047619047619\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 9 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 10\n",
      "Train accuracy: 88.84761904761905\n",
      "Val accuracy: 88.25714285714285\n",
      "Iter 10 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 11\n",
      "Train accuracy: 89.2095238095238\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 11 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 12\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 12 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 13\n",
      "Train accuracy: 89.89047619047619\n",
      "Val accuracy: 89.4\n",
      "Iter 13 -> sub iter 99 : 90.01111111111111\n",
      "Iteration: 14\n",
      "Train accuracy: 90.15555555555555\n",
      "Val accuracy: 89.64285714285715\n",
      "Iter 14 -> sub iter 99 : 90.08730158730158\n",
      "Iteration: 15\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.84285714285714\n",
      "Iter 15 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 16\n",
      "Train accuracy: 90.62857142857142\n",
      "Val accuracy: 90.17142857142856\n",
      "Iter 16 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 17\n",
      "Train accuracy: 90.86349206349207\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 17 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 18\n",
      "Train accuracy: 91.09047619047618\n",
      "Val accuracy: 90.64285714285715\n",
      "Iter 18 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 19\n",
      "Train accuracy: 91.25079365079365\n",
      "Val accuracy: 90.8\n",
      "Iter 19 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 20\n",
      "Train accuracy: 91.42857142857143\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 20 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 21\n",
      "Train accuracy: 91.58730158730158\n",
      "Val accuracy: 91.01428571428572\n",
      "Iter 21 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 22\n",
      "Train accuracy: 91.73174603174603\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 22 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 23\n",
      "Train accuracy: 91.87777777777778\n",
      "Val accuracy: 91.42857142857143\n",
      "Iter 23 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 24\n",
      "Train accuracy: 91.99841269841271\n",
      "Val accuracy: 91.41428571428571\n",
      "Iter 24 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 25\n",
      "Train accuracy: 92.0984126984127\n",
      "Val accuracy: 91.54285714285714\n",
      "Iter 25 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 26\n",
      "Train accuracy: 92.2031746031746\n",
      "Val accuracy: 91.60000000000001\n",
      "Iter 26 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 27\n",
      "Train accuracy: 92.3079365079365\n",
      "Val accuracy: 91.75714285714285\n",
      "Iter 27 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 28\n",
      "Train accuracy: 92.41428571428571\n",
      "Val accuracy: 91.88571428571429\n",
      "Iter 28 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 29\n",
      "Train accuracy: 92.5063492063492\n",
      "Val accuracy: 92.01428571428572\n",
      "Iter 29 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 30\n",
      "Train accuracy: 92.57619047619048\n",
      "Val accuracy: 92.02857142857142\n",
      "Iter 30 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 31\n",
      "Train accuracy: 92.67301587301587\n",
      "Val accuracy: 92.12857142857143\n",
      "Iter 31 -> sub iter 99 : 91.90476190476191\n",
      "Iteration: 32\n",
      "Train accuracy: 92.74761904761904\n",
      "Val accuracy: 92.14285714285714\n",
      "Iter 32 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 33\n",
      "Train accuracy: 92.83809523809524\n",
      "Val accuracy: 92.21428571428572\n",
      "Iter 33 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 34\n",
      "Train accuracy: 92.93174603174603\n",
      "Val accuracy: 92.34285714285714\n",
      "Iter 34 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 35\n",
      "Train accuracy: 92.9968253968254\n",
      "Val accuracy: 92.4\n",
      "Iter 35 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 36\n",
      "Train accuracy: 93.06349206349206\n",
      "Val accuracy: 92.5\n",
      "Iter 36 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 37\n",
      "Train accuracy: 93.12222222222222\n",
      "Val accuracy: 92.51428571428572\n",
      "Iter 37 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 38\n",
      "Train accuracy: 93.16349206349206\n",
      "Val accuracy: 92.58571428571429\n",
      "Iter 38 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 39\n",
      "Train accuracy: 93.2047619047619\n",
      "Val accuracy: 92.62857142857143\n",
      "Iter 39 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 40\n",
      "Train accuracy: 93.25873015873016\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 40 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 41\n",
      "Train accuracy: 93.32222222222222\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 41 -> sub iter 99 : 92.69841269841273\n",
      "Iteration: 42\n",
      "Train accuracy: 93.38253968253967\n",
      "Val accuracy: 92.77142857142857\n",
      "Iter 42 -> sub iter 99 : 93.01587301587301\n",
      "Iteration: 43\n",
      "Train accuracy: 93.42857142857143\n",
      "Val accuracy: 92.85714285714286\n",
      "Iter 43 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 44\n",
      "Train accuracy: 93.4888888888889\n",
      "Val accuracy: 92.95714285714286\n",
      "Iter 44 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 45\n",
      "Train accuracy: 93.55079365079365\n",
      "Val accuracy: 93.05714285714286\n",
      "Iter 45 -> sub iter 99 : 93.49206349206356\n",
      "Iteration: 46\n",
      "Train accuracy: 93.6015873015873\n",
      "Val accuracy: 93.12857142857143\n",
      "Iter 46 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 47\n",
      "Train accuracy: 93.66507936507936\n",
      "Val accuracy: 93.18571428571428\n",
      "Iter 47 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 48\n",
      "Train accuracy: 93.72222222222221\n",
      "Val accuracy: 93.22857142857143\n",
      "Iter 48 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 49\n",
      "Train accuracy: 93.76984126984127\n",
      "Val accuracy: 93.27142857142857\n",
      "Iter 49 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 50\n",
      "Train accuracy: 93.83968253968254\n",
      "Val accuracy: 93.31428571428572\n",
      "Iter 50 -> sub iter 99 : 94.12698412698413\n",
      "Iteration: 51\n",
      "Train accuracy: 93.88571428571429\n",
      "Val accuracy: 93.32857142857142\n",
      "Iter 51 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 93.92380952380952\n",
      "Val accuracy: 93.34285714285714\n",
      "Iter 52 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 53\n",
      "Train accuracy: 93.97777777777779\n",
      "Val accuracy: 93.38571428571429\n",
      "Iter 53 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 54\n",
      "Train accuracy: 94.01428571428572\n",
      "Val accuracy: 93.41428571428571\n",
      "Iter 54 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 55\n",
      "Train accuracy: 94.07142857142857\n",
      "Val accuracy: 93.4\n",
      "Iter 55 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 56\n",
      "Train accuracy: 94.12063492063491\n",
      "Val accuracy: 93.45714285714286\n",
      "Iter 56 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 57\n",
      "Train accuracy: 94.13650793650794\n",
      "Val accuracy: 93.47142857142858\n",
      "Iter 57 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 58\n",
      "Train accuracy: 94.16349206349206\n",
      "Val accuracy: 93.54285714285714\n",
      "Iter 58 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 59\n",
      "Train accuracy: 94.22222222222221\n",
      "Val accuracy: 93.58571428571429\n",
      "Iter 59 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 60\n",
      "Train accuracy: 94.24285714285713\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 60 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 61\n",
      "Train accuracy: 94.28412698412698\n",
      "Val accuracy: 93.57142857142857\n",
      "Iter 61 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 62\n",
      "Train accuracy: 94.33174603174604\n",
      "Val accuracy: 93.61428571428571\n",
      "Iter 62 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 63\n",
      "Train accuracy: 94.4031746031746\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 63 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 64\n",
      "Train accuracy: 94.43492063492064\n",
      "Val accuracy: 93.62857142857143\n",
      "Iter 64 -> sub iter 99 : 94.60317460317463\n",
      "Iteration: 65\n",
      "Train accuracy: 94.46190476190476\n",
      "Val accuracy: 93.64285714285714\n",
      "Iter 65 -> sub iter 99 : 94.60317460317467\n",
      "Iteration: 66\n",
      "Train accuracy: 94.4888888888889\n",
      "Val accuracy: 93.68571428571428\n",
      "Iter 66 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 94.51587301587303\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 67 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 68\n",
      "Train accuracy: 94.54444444444444\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 68 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 69\n",
      "Train accuracy: 94.57301587301588\n",
      "Val accuracy: 93.8\n",
      "Iter 69 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 70\n",
      "Train accuracy: 94.61587301587302\n",
      "Val accuracy: 93.8\n",
      "Iter 70 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 71\n",
      "Train accuracy: 94.63492063492063\n",
      "Val accuracy: 93.84285714285714\n",
      "Iter 71 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 72\n",
      "Train accuracy: 94.65396825396826\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 72 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 73\n",
      "Train accuracy: 94.67301587301587\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 73 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 74\n",
      "Train accuracy: 94.71269841269842\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 74 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 75\n",
      "Train accuracy: 94.74126984126984\n",
      "Val accuracy: 93.91428571428571\n",
      "Iter 75 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 76\n",
      "Train accuracy: 94.77777777777779\n",
      "Val accuracy: 93.92857142857143\n",
      "Iter 76 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 77\n",
      "Train accuracy: 94.80952380952381\n",
      "Val accuracy: 93.95714285714286\n",
      "Iter 77 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 78\n",
      "Train accuracy: 94.82698412698413\n",
      "Val accuracy: 93.98571428571428\n",
      "Iter 78 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 79\n",
      "Train accuracy: 94.83968253968254\n",
      "Val accuracy: 94.01428571428572\n",
      "Iter 79 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 80\n",
      "Train accuracy: 94.86825396825397\n",
      "Val accuracy: 94.04285714285714\n",
      "Iter 80 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 81\n",
      "Train accuracy: 94.89999999999999\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 81 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 82\n",
      "Train accuracy: 94.92063492063491\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 82 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 83\n",
      "Train accuracy: 94.94920634920635\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 83 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 84\n",
      "Train accuracy: 94.98253968253968\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 84 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 85\n",
      "Train accuracy: 95.0031746031746\n",
      "Val accuracy: 94.1\n",
      "Iter 85 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 86\n",
      "Train accuracy: 95.03174603174604\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 86 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 87\n",
      "Train accuracy: 95.06190476190476\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 87 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 88\n",
      "Train accuracy: 95.09206349206349\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 88 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 89\n",
      "Train accuracy: 95.11904761904762\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 89 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 90\n",
      "Train accuracy: 95.13174603174603\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 90 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 91\n",
      "Train accuracy: 95.18095238095238\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 91 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 92\n",
      "Train accuracy: 95.21111111111111\n",
      "Val accuracy: 94.15714285714286\n",
      "Iter 92 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 93\n",
      "Train accuracy: 95.24603174603175\n",
      "Val accuracy: 94.18571428571428\n",
      "Iter 93 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 94\n",
      "Train accuracy: 95.25396825396825\n",
      "Val accuracy: 94.21428571428572\n",
      "Iter 94 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 95\n",
      "Train accuracy: 95.3\n",
      "Val accuracy: 94.28571428571428\n",
      "Iter 95 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 96\n",
      "Train accuracy: 95.31904761904761\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 96 -> sub iter 99 : 95.39682539682549\n",
      "Iteration: 97\n",
      "Train accuracy: 95.33015873015873\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 97 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 98\n",
      "Train accuracy: 95.34444444444445\n",
      "Val accuracy: 94.22857142857143\n",
      "Iter 98 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 99\n",
      "Train accuracy: 95.36666666666666\n",
      "Val accuracy: 94.3\n",
      "Iter 99 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 100\n",
      "Train accuracy: 95.3920634920635\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 100 -> sub iter 99 : 95.71428571428572\n",
      "Iteration: 101\n",
      "Train accuracy: 95.41428571428571\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 101 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 102\n",
      "Train accuracy: 95.42222222222222\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 102 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 103\n",
      "Train accuracy: 95.43333333333334\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 103 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 104\n",
      "Train accuracy: 95.46507936507936\n",
      "Val accuracy: 94.38571428571429\n",
      "Iter 104 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 105\n",
      "Train accuracy: 95.47936507936508\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 105 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 106\n",
      "Train accuracy: 95.5079365079365\n",
      "Val accuracy: 94.39999999999999\n",
      "Iter 106 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 107\n",
      "Train accuracy: 95.51587301587303\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 107 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 108\n",
      "Train accuracy: 95.52857142857142\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 108 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 109\n",
      "Train accuracy: 95.54126984126984\n",
      "Val accuracy: 94.42857142857143\n",
      "Iter 109 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 110\n",
      "Train accuracy: 95.56031746031746\n",
      "Val accuracy: 94.47142857142858\n",
      "Iter 110 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 111\n",
      "Train accuracy: 95.55873015873016\n",
      "Val accuracy: 94.48571428571428\n",
      "Iter 111 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 112\n",
      "Train accuracy: 95.57460317460318\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 112 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 113\n",
      "Train accuracy: 95.6047619047619\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 113 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 114\n",
      "Train accuracy: 95.62380952380953\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 114 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 115\n",
      "Train accuracy: 95.63968253968254\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 115 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 116\n",
      "Train accuracy: 95.65079365079366\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 116 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 117\n",
      "Train accuracy: 95.67142857142858\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 117 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 118\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 118 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 119\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 119 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 120\n",
      "Train accuracy: 95.71111111111111\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 120 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 121\n",
      "Train accuracy: 95.72063492063492\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 121 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 122\n",
      "Train accuracy: 95.72539682539683\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 122 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 123\n",
      "Train accuracy: 95.73174603174604\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 123 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 124\n",
      "Train accuracy: 95.74920634920635\n",
      "Val accuracy: 94.6\n",
      "Iter 124 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 125\n",
      "Train accuracy: 95.74603174603175\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 125 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 126\n",
      "Train accuracy: 95.75396825396825\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 126 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 127\n",
      "Train accuracy: 95.75873015873016\n",
      "Val accuracy: 94.6\n",
      "Iter 127 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 128\n",
      "Train accuracy: 95.78571428571429\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 128 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 129\n",
      "Train accuracy: 95.80634920634921\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 129 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 130\n",
      "Train accuracy: 95.82380952380952\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 130 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 131\n",
      "Train accuracy: 95.83650793650794\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 131 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 132\n",
      "Train accuracy: 95.85555555555555\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 132 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 133\n",
      "Train accuracy: 95.86031746031746\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 133 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 134\n",
      "Train accuracy: 95.87142857142858\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 134 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 135\n",
      "Train accuracy: 95.87301587301587\n",
      "Val accuracy: 94.65714285714286\n",
      "Iter 135 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 136\n",
      "Train accuracy: 95.8873015873016\n",
      "Val accuracy: 94.67142857142858\n",
      "Iter 136 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 137\n",
      "Train accuracy: 95.91428571428573\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 137 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 138\n",
      "Train accuracy: 95.92222222222222\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 138 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 139\n",
      "Train accuracy: 95.94126984126984\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 139 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 140\n",
      "Train accuracy: 95.95714285714286\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 140 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 141\n",
      "Train accuracy: 95.97777777777777\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 141 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 142\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.72857142857143\n",
      "Iter 142 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 143\n",
      "Train accuracy: 96.02857142857142\n",
      "Val accuracy: 94.77142857142857\n",
      "Iter 143 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 144\n",
      "Train accuracy: 96.04285714285714\n",
      "Val accuracy: 94.8\n",
      "Iter 144 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 145\n",
      "Train accuracy: 96.06507936507937\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 145 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 146\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 146 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 147\n",
      "Train accuracy: 96.0936507936508\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 147 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 148\n",
      "Train accuracy: 96.10634920634921\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 148 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 149\n",
      "Train accuracy: 96.13174603174603\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 149 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 150\n",
      "Train accuracy: 96.15079365079366\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 150 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 151\n",
      "Train accuracy: 96.16666666666667\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 151 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 152\n",
      "Train accuracy: 96.18253968253968\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 152 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 153\n",
      "Train accuracy: 96.18571428571428\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 153 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 154\n",
      "Train accuracy: 96.2\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 154 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 155\n",
      "Train accuracy: 96.20793650793651\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 155 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 156\n",
      "Train accuracy: 96.21428571428572\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 156 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 157\n",
      "Train accuracy: 96.22539682539683\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 157 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 158\n",
      "Train accuracy: 96.22857142857143\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 158 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 159\n",
      "Train accuracy: 96.24126984126984\n",
      "Val accuracy: 94.91428571428571\n",
      "Iter 159 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 160\n",
      "Train accuracy: 96.24444444444444\n",
      "Val accuracy: 94.94285714285714\n",
      "Iter 160 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 161\n",
      "Train accuracy: 96.25396825396825\n",
      "Val accuracy: 94.97142857142858\n",
      "Iter 161 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 162\n",
      "Train accuracy: 96.27142857142857\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 162 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 163\n",
      "Train accuracy: 96.27777777777777\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 163 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 164\n",
      "Train accuracy: 96.3047619047619\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 164 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 165\n",
      "Train accuracy: 96.32063492063492\n",
      "Val accuracy: 95.0\n",
      "Iter 165 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 166\n",
      "Train accuracy: 96.33809523809524\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 166 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 167\n",
      "Train accuracy: 96.34126984126983\n",
      "Val accuracy: 95.04285714285714\n",
      "Iter 167 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 168\n",
      "Train accuracy: 96.34761904761905\n",
      "Val accuracy: 95.08571428571429\n",
      "Iter 168 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 169\n",
      "Train accuracy: 96.35873015873015\n",
      "Val accuracy: 95.12857142857143\n",
      "Iter 169 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 170\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 170 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 171\n",
      "Train accuracy: 96.3873015873016\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 171 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 172\n",
      "Train accuracy: 96.3984126984127\n",
      "Val accuracy: 95.15714285714286\n",
      "Iter 172 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 173\n",
      "Train accuracy: 96.41428571428573\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 173 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 174\n",
      "Train accuracy: 96.42857142857143\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 174 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 175\n",
      "Train accuracy: 96.44285714285714\n",
      "Val accuracy: 95.21428571428572\n",
      "Iter 175 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 176\n",
      "Train accuracy: 96.45396825396826\n",
      "Val accuracy: 95.22857142857143\n",
      "Iter 176 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 177\n",
      "Train accuracy: 96.46031746031746\n",
      "Val accuracy: 95.27142857142857\n",
      "Iter 177 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 178\n",
      "Train accuracy: 96.46666666666667\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 178 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 179\n",
      "Train accuracy: 96.46825396825398\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 179 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 180\n",
      "Train accuracy: 96.48095238095237\n",
      "Val accuracy: 95.3\n",
      "Iter 180 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 181\n",
      "Train accuracy: 96.49682539682539\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 181 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 182\n",
      "Train accuracy: 96.50634920634921\n",
      "Val accuracy: 95.3\n",
      "Iter 182 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 183\n",
      "Train accuracy: 96.52222222222223\n",
      "Val accuracy: 95.3\n",
      "Iter 183 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 184\n",
      "Train accuracy: 96.53015873015873\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 184 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 185\n",
      "Train accuracy: 96.54444444444444\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 185 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 186\n",
      "Train accuracy: 96.54761904761905\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 186 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 187\n",
      "Train accuracy: 96.56349206349206\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 187 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 188\n",
      "Train accuracy: 96.56031746031746\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 188 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 189\n",
      "Train accuracy: 96.56825396825397\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 189 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 190\n",
      "Train accuracy: 96.58571428571429\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 190 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 191\n",
      "Train accuracy: 96.5952380952381\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 191 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 192\n",
      "Train accuracy: 96.60317460317461\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 192 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 193\n",
      "Train accuracy: 96.61111111111111\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 193 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 194\n",
      "Train accuracy: 96.62380952380953\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 194 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 195\n",
      "Train accuracy: 96.62857142857143\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 195 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 196\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 196 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 197\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 197 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 198\n",
      "Train accuracy: 96.63492063492065\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 198 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 199\n",
      "Train accuracy: 96.63809523809523\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 199 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 200\n",
      "Train accuracy: 96.65238095238095\n",
      "Val accuracy: 95.35714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 23.333333333333332\n",
      "Iteration: 1\n",
      "Train accuracy: 21.192063492063493\n",
      "Val accuracy: 21.185714285714287\n",
      "Iter 1 -> sub iter 99 : 32.380952380952387\n",
      "Iteration: 2\n",
      "Train accuracy: 30.338095238095235\n",
      "Val accuracy: 30.185714285714287\n",
      "Iter 2 -> sub iter 99 : 39.365079365079374\n",
      "Iteration: 3\n",
      "Train accuracy: 36.630158730158726\n",
      "Val accuracy: 35.885714285714286\n",
      "Iter 3 -> sub iter 99 : 43.333333333333336\n",
      "Iteration: 4\n",
      "Train accuracy: 40.7015873015873\n",
      "Val accuracy: 39.957142857142856\n",
      "Iter 4 -> sub iter 99 : 45.873015873015874\n",
      "Iteration: 5\n",
      "Train accuracy: 43.32222222222222\n",
      "Val accuracy: 43.042857142857144\n",
      "Iter 5 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 6\n",
      "Train accuracy: 45.303174603174604\n",
      "Val accuracy: 44.94285714285714\n",
      "Iter 6 -> sub iter 99 : 49.365079365079376\n",
      "Iteration: 7\n",
      "Train accuracy: 48.33174603174603\n",
      "Val accuracy: 47.3\n",
      "Iter 7 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 8\n",
      "Train accuracy: 53.77301587301587\n",
      "Val accuracy: 52.67142857142857\n",
      "Iter 8 -> sub iter 99 : 58.888888888888896\n",
      "Iteration: 9\n",
      "Train accuracy: 57.51904761904761\n",
      "Val accuracy: 56.471428571428575\n",
      "Iter 9 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 10\n",
      "Train accuracy: 60.73968253968254\n",
      "Val accuracy: 59.68571428571428\n",
      "Iter 10 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 11\n",
      "Train accuracy: 63.834920634920636\n",
      "Val accuracy: 62.97142857142857\n",
      "Iter 11 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 12\n",
      "Train accuracy: 66.26349206349207\n",
      "Val accuracy: 65.65714285714286\n",
      "Iter 12 -> sub iter 99 : 66.349206349206345\n",
      "Iteration: 13\n",
      "Train accuracy: 68.01904761904763\n",
      "Val accuracy: 67.37142857142857\n",
      "Iter 13 -> sub iter 99 : 67.619047619047625\n",
      "Iteration: 14\n",
      "Train accuracy: 69.34920634920636\n",
      "Val accuracy: 68.45714285714286\n",
      "Iter 14 -> sub iter 99 : 68.57142857142857\n",
      "Iteration: 15\n",
      "Train accuracy: 70.35714285714286\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 15 -> sub iter 99 : 69.68253968253968\n",
      "Iteration: 16\n",
      "Train accuracy: 71.15873015873015\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 16 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 17\n",
      "Train accuracy: 71.80952380952381\n",
      "Val accuracy: 70.6\n",
      "Iter 17 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 18\n",
      "Train accuracy: 72.47777777777777\n",
      "Val accuracy: 71.31428571428572\n",
      "Iter 18 -> sub iter 99 : 72.22222222222221\n",
      "Iteration: 19\n",
      "Train accuracy: 73.23015873015873\n",
      "Val accuracy: 72.17142857142858\n",
      "Iter 19 -> sub iter 99 : 73.49206349206354\n",
      "Iteration: 20\n",
      "Train accuracy: 74.12222222222222\n",
      "Val accuracy: 73.08571428571429\n",
      "Iter 20 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 21\n",
      "Train accuracy: 75.04603174603174\n",
      "Val accuracy: 73.75714285714285\n",
      "Iter 21 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 22\n",
      "Train accuracy: 75.86190476190477\n",
      "Val accuracy: 74.5142857142857\n",
      "Iter 22 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 23\n",
      "Train accuracy: 76.63809523809523\n",
      "Val accuracy: 74.9857142857143\n",
      "Iter 23 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 24\n",
      "Train accuracy: 77.25555555555556\n",
      "Val accuracy: 75.74285714285715\n",
      "Iter 24 -> sub iter 99 : 77.30158730158733\n",
      "Iteration: 25\n",
      "Train accuracy: 77.86031746031746\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 25 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 26\n",
      "Train accuracy: 78.41269841269842\n",
      "Val accuracy: 76.91428571428571\n",
      "Iter 26 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 27\n",
      "Train accuracy: 78.94285714285715\n",
      "Val accuracy: 77.58571428571429\n",
      "Iter 27 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 28\n",
      "Train accuracy: 79.43650793650794\n",
      "Val accuracy: 78.01428571428572\n",
      "Iter 28 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 29\n",
      "Train accuracy: 79.9\n",
      "Val accuracy: 78.65714285714286\n",
      "Iter 29 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 80.32857142857142\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 30 -> sub iter 99 : 80.07460317460318\n",
      "Iteration: 31\n",
      "Train accuracy: 80.68095238095238\n",
      "Val accuracy: 79.31428571428572\n",
      "Iter 31 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 32\n",
      "Train accuracy: 81.01428571428572\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 32 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 33\n",
      "Train accuracy: 81.36984126984127\n",
      "Val accuracy: 79.94285714285714\n",
      "Iter 33 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 34\n",
      "Train accuracy: 81.67619047619048\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 34 -> sub iter 99 : 81.90476190476196\n",
      "Iteration: 35\n",
      "Train accuracy: 81.95396825396826\n",
      "Val accuracy: 80.4\n",
      "Iter 35 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 36\n",
      "Train accuracy: 82.28095238095237\n",
      "Val accuracy: 80.7\n",
      "Iter 36 -> sub iter 99 : 81.90476190476198\n",
      "Iteration: 37\n",
      "Train accuracy: 82.53809523809524\n",
      "Val accuracy: 81.10000000000001\n",
      "Iter 37 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 38\n",
      "Train accuracy: 82.73492063492064\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 38 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 39\n",
      "Train accuracy: 82.9968253968254\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 39 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 40\n",
      "Train accuracy: 83.2\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 40 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 41\n",
      "Train accuracy: 83.46507936507936\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 41 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 42\n",
      "Train accuracy: 83.66349206349206\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 42 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 43\n",
      "Train accuracy: 83.87142857142858\n",
      "Val accuracy: 82.47142857142858\n",
      "Iter 43 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 44\n",
      "Train accuracy: 84.04761904761905\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 44 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 45\n",
      "Train accuracy: 84.21111111111111\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 45 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 46\n",
      "Train accuracy: 84.38571428571429\n",
      "Val accuracy: 83.05714285714285\n",
      "Iter 46 -> sub iter 99 : 85.07936507936508\n",
      "Iteration: 47\n",
      "Train accuracy: 84.57301587301588\n",
      "Val accuracy: 83.12857142857143\n",
      "Iter 47 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 48\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 83.34285714285714\n",
      "Iter 48 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 49\n",
      "Train accuracy: 84.91746031746031\n",
      "Val accuracy: 83.5\n",
      "Iter 49 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 50\n",
      "Train accuracy: 85.06984126984128\n",
      "Val accuracy: 83.61428571428571\n",
      "Iter 50 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 51\n",
      "Train accuracy: 85.22222222222223\n",
      "Val accuracy: 83.78571428571429\n",
      "Iter 51 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 52\n",
      "Train accuracy: 85.39999999999999\n",
      "Val accuracy: 83.84285714285714\n",
      "Iter 52 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 53\n",
      "Train accuracy: 85.54603174603174\n",
      "Val accuracy: 83.92857142857143\n",
      "Iter 53 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 54\n",
      "Train accuracy: 85.66984126984127\n",
      "Val accuracy: 84.12857142857143\n",
      "Iter 54 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 55\n",
      "Train accuracy: 85.8\n",
      "Val accuracy: 84.2\n",
      "Iter 55 -> sub iter 99 : 86.03174603174604\n",
      "Iteration: 56\n",
      "Train accuracy: 85.91587301587302\n",
      "Val accuracy: 84.31428571428572\n",
      "Iter 56 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 57\n",
      "Train accuracy: 85.99365079365079\n",
      "Val accuracy: 84.41428571428573\n",
      "Iter 57 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 58\n",
      "Train accuracy: 86.1\n",
      "Val accuracy: 84.52857142857142\n",
      "Iter 58 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 59\n",
      "Train accuracy: 86.23015873015873\n",
      "Val accuracy: 84.61428571428571\n",
      "Iter 59 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 60\n",
      "Train accuracy: 86.34444444444445\n",
      "Val accuracy: 84.68571428571428\n",
      "Iter 60 -> sub iter 99 : 86.50793650793652\n",
      "Iteration: 61\n",
      "Train accuracy: 86.42857142857143\n",
      "Val accuracy: 84.72857142857143\n",
      "Iter 61 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 62\n",
      "Train accuracy: 86.53968253968254\n",
      "Val accuracy: 84.85714285714285\n",
      "Iter 62 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 63\n",
      "Train accuracy: 86.65555555555555\n",
      "Val accuracy: 85.02857142857142\n",
      "Iter 63 -> sub iter 99 : 86.98412698412699\n",
      "Iteration: 64\n",
      "Train accuracy: 86.76984126984128\n",
      "Val accuracy: 85.08571428571429\n",
      "Iter 64 -> sub iter 99 : 87.14285714285714\n",
      "Iteration: 65\n",
      "Train accuracy: 86.89206349206349\n",
      "Val accuracy: 85.21428571428571\n",
      "Iter 65 -> sub iter 99 : 87.30158730158735\n",
      "Iteration: 66\n",
      "Train accuracy: 86.9920634920635\n",
      "Val accuracy: 85.3\n",
      "Iter 66 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 67\n",
      "Train accuracy: 87.06349206349206\n",
      "Val accuracy: 85.45714285714286\n",
      "Iter 67 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 68\n",
      "Train accuracy: 87.14603174603175\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 68 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 69\n",
      "Train accuracy: 87.1984126984127\n",
      "Val accuracy: 85.58571428571429\n",
      "Iter 69 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 70\n",
      "Train accuracy: 87.27936507936508\n",
      "Val accuracy: 85.8\n",
      "Iter 70 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 71\n",
      "Train accuracy: 87.34603174603176\n",
      "Val accuracy: 85.91428571428571\n",
      "Iter 71 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 72\n",
      "Train accuracy: 87.44761904761906\n",
      "Val accuracy: 86.02857142857144\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 87.55238095238094\n",
      "Val accuracy: 86.11428571428571\n",
      "Iter 73 -> sub iter 99 : 87.77777777777777\n",
      "Iteration: 74\n",
      "Train accuracy: 87.61904761904762\n",
      "Val accuracy: 86.18571428571428\n",
      "Iter 74 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 75\n",
      "Train accuracy: 87.69206349206348\n",
      "Val accuracy: 86.25714285714285\n",
      "Iter 75 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 76\n",
      "Train accuracy: 87.7968253968254\n",
      "Val accuracy: 86.34285714285714\n",
      "Iter 76 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 77\n",
      "Train accuracy: 87.87460317460317\n",
      "Val accuracy: 86.37142857142858\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.96666666666667\n",
      "Val accuracy: 86.5142857142857\n",
      "Iter 78 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 79\n",
      "Train accuracy: 88.00634920634921\n",
      "Val accuracy: 86.61428571428571\n",
      "Iter 79 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 80\n",
      "Train accuracy: 88.07301587301588\n",
      "Val accuracy: 86.71428571428571\n",
      "Iter 80 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 81\n",
      "Train accuracy: 88.16190476190476\n",
      "Val accuracy: 86.81428571428572\n",
      "Iter 81 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 82\n",
      "Train accuracy: 88.22539682539683\n",
      "Val accuracy: 86.88571428571429\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.27301587301586\n",
      "Val accuracy: 87.02857142857144\n",
      "Iter 83 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 84\n",
      "Train accuracy: 88.33333333333333\n",
      "Val accuracy: 87.08571428571429\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.38571428571429\n",
      "Val accuracy: 87.1\n",
      "Iter 85 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 86\n",
      "Train accuracy: 88.43174603174603\n",
      "Val accuracy: 87.14285714285714\n",
      "Iter 86 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 87\n",
      "Train accuracy: 88.47301587301587\n",
      "Val accuracy: 87.25714285714285\n",
      "Iter 87 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 88\n",
      "Train accuracy: 88.50634920634921\n",
      "Val accuracy: 87.37142857142857\n",
      "Iter 88 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 89\n",
      "Train accuracy: 88.55079365079365\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 89 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 90\n",
      "Train accuracy: 88.60317460317461\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 90 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 91\n",
      "Train accuracy: 88.66349206349207\n",
      "Val accuracy: 87.42857142857143\n",
      "Iter 91 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 92\n",
      "Train accuracy: 88.70476190476191\n",
      "Val accuracy: 87.44285714285715\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 88.75714285714285\n",
      "Val accuracy: 87.5142857142857\n",
      "Iter 93 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 94\n",
      "Train accuracy: 88.81269841269841\n",
      "Val accuracy: 87.55714285714285\n",
      "Iter 94 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 95\n",
      "Train accuracy: 88.87142857142857\n",
      "Val accuracy: 87.6\n",
      "Iter 95 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 96\n",
      "Train accuracy: 88.93015873015872\n",
      "Val accuracy: 87.64285714285714\n",
      "Iter 96 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 97\n",
      "Train accuracy: 89.0047619047619\n",
      "Val accuracy: 87.71428571428571\n",
      "Iter 97 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 98\n",
      "Train accuracy: 89.04761904761904\n",
      "Val accuracy: 87.78571428571429\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 87.81428571428572\n",
      "Iter 99 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 100\n",
      "Train accuracy: 89.13650793650794\n",
      "Val accuracy: 87.84285714285714\n",
      "Iter 100 -> sub iter 99 : 90.02222222222223\n",
      "Iteration: 101\n",
      "Train accuracy: 89.17142857142856\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 101 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 102\n",
      "Train accuracy: 89.23968253968255\n",
      "Val accuracy: 87.97142857142856\n",
      "Iter 102 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 103\n",
      "Train accuracy: 89.28888888888889\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 103 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 104\n",
      "Train accuracy: 89.33174603174604\n",
      "Val accuracy: 88.12857142857143\n",
      "Iter 104 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 105\n",
      "Train accuracy: 89.36666666666667\n",
      "Val accuracy: 88.2\n",
      "Iter 105 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 106\n",
      "Train accuracy: 89.4015873015873\n",
      "Val accuracy: 88.27142857142857\n",
      "Iter 106 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 107\n",
      "Train accuracy: 89.43968253968254\n",
      "Val accuracy: 88.34285714285714\n",
      "Iter 107 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 108\n",
      "Train accuracy: 89.46666666666667\n",
      "Val accuracy: 88.35714285714286\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 89.5047619047619\n",
      "Val accuracy: 88.38571428571429\n",
      "Iter 109 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 110\n",
      "Train accuracy: 89.55396825396825\n",
      "Val accuracy: 88.44285714285715\n",
      "Iter 110 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 111\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 88.54285714285714\n",
      "Iter 111 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 112\n",
      "Train accuracy: 89.64444444444445\n",
      "Val accuracy: 88.6\n",
      "Iter 112 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 113\n",
      "Train accuracy: 89.67936507936508\n",
      "Val accuracy: 88.62857142857142\n",
      "Iter 113 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 114\n",
      "Train accuracy: 89.72698412698412\n",
      "Val accuracy: 88.67142857142856\n",
      "Iter 114 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 115\n",
      "Train accuracy: 89.78095238095239\n",
      "Val accuracy: 88.68571428571428\n",
      "Iter 115 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 116\n",
      "Train accuracy: 89.8079365079365\n",
      "Val accuracy: 88.77142857142857\n",
      "Iter 116 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 117\n",
      "Train accuracy: 89.83968253968254\n",
      "Val accuracy: 88.81428571428572\n",
      "Iter 117 -> sub iter 99 : 90.79365079365088\n",
      "Iteration: 118\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 88.87142857142857\n",
      "Iter 118 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 119\n",
      "Train accuracy: 89.91428571428571\n",
      "Val accuracy: 88.88571428571429\n",
      "Iter 119 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 120\n",
      "Train accuracy: 89.95714285714286\n",
      "Val accuracy: 88.91428571428571\n",
      "Iter 120 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 121\n",
      "Train accuracy: 89.97777777777777\n",
      "Val accuracy: 88.97142857142856\n",
      "Iter 121 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 122\n",
      "Train accuracy: 90.0079365079365\n",
      "Val accuracy: 89.07142857142857\n",
      "Iter 122 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 123\n",
      "Train accuracy: 90.04126984126984\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 123 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 124\n",
      "Train accuracy: 90.07619047619048\n",
      "Val accuracy: 89.22857142857143\n",
      "Iter 124 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 125\n",
      "Train accuracy: 90.11587301587302\n",
      "Val accuracy: 89.24285714285715\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 90.14761904761905\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 90.16507936507936\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 90.1968253968254\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 90.23492063492064\n",
      "Val accuracy: 89.31428571428572\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 90.26190476190476\n",
      "Val accuracy: 89.37142857142857\n",
      "Iter 130 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 131\n",
      "Train accuracy: 90.27936507936508\n",
      "Val accuracy: 89.34285714285714\n",
      "Iter 131 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 132\n",
      "Train accuracy: 90.32063492063493\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 132 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 133\n",
      "Train accuracy: 90.34920634920634\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 133 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 134\n",
      "Train accuracy: 90.36825396825397\n",
      "Val accuracy: 89.45714285714286\n",
      "Iter 134 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 135\n",
      "Train accuracy: 90.4\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 135 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 136\n",
      "Train accuracy: 90.42857142857143\n",
      "Val accuracy: 89.60000000000001\n",
      "Iter 136 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 137\n",
      "Train accuracy: 90.44761904761904\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 137 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 138\n",
      "Train accuracy: 90.46825396825396\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 138 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 139\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 139 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 140\n",
      "Train accuracy: 90.4952380952381\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 140 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 141\n",
      "Train accuracy: 90.51269841269841\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 141 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 142\n",
      "Train accuracy: 90.52222222222223\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 142 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 143\n",
      "Train accuracy: 90.54603174603174\n",
      "Val accuracy: 89.6857142857143\n",
      "Iter 143 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 144\n",
      "Train accuracy: 90.57777777777778\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 144 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 145\n",
      "Train accuracy: 90.6079365079365\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 145 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 146\n",
      "Train accuracy: 90.62222222222222\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 146 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 147\n",
      "Train accuracy: 90.63015873015873\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 147 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 148\n",
      "Train accuracy: 90.65396825396826\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 148 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 149\n",
      "Train accuracy: 90.67936507936508\n",
      "Val accuracy: 89.8\n",
      "Iter 149 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 150\n",
      "Train accuracy: 90.6920634920635\n",
      "Val accuracy: 89.81428571428572\n",
      "Training for 0.1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 13.650793650793653\n",
      "Iteration: 1\n",
      "Train accuracy: 13.998412698412698\n",
      "Val accuracy: 14.32857142857143\n",
      "Iter 1 -> sub iter 99 : 19.365079365079367\n",
      "Iteration: 2\n",
      "Train accuracy: 20.185714285714283\n",
      "Val accuracy: 20.314285714285717\n",
      "Iter 2 -> sub iter 99 : 22.380952380952383\n",
      "Iteration: 3\n",
      "Train accuracy: 23.184126984126983\n",
      "Val accuracy: 23.32857142857143\n",
      "Iter 3 -> sub iter 99 : 25.555555555555554\n",
      "Iteration: 4\n",
      "Train accuracy: 26.05396825396825\n",
      "Val accuracy: 26.285714285714285\n",
      "Iter 4 -> sub iter 99 : 29.682539682539684\n",
      "Iteration: 5\n",
      "Train accuracy: 29.965079365079365\n",
      "Val accuracy: 30.542857142857144\n",
      "Iter 5 -> sub iter 99 : 39.841269841269845\n",
      "Iteration: 6\n",
      "Train accuracy: 39.268253968253966\n",
      "Val accuracy: 39.628571428571426\n",
      "Iter 6 -> sub iter 99 : 45.714285714285715\n",
      "Iteration: 7\n",
      "Train accuracy: 44.371428571428574\n",
      "Val accuracy: 44.800000000000004\n",
      "Iter 7 -> sub iter 99 : 48.888888888888886\n",
      "Iteration: 8\n",
      "Train accuracy: 47.52222222222222\n",
      "Val accuracy: 47.92857142857142\n",
      "Iter 8 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 9\n",
      "Train accuracy: 49.804761904761904\n",
      "Val accuracy: 50.2\n",
      "Iter 9 -> sub iter 99 : 53.174603174603185\n",
      "Iteration: 10\n",
      "Train accuracy: 51.74126984126984\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 10 -> sub iter 99 : 54.444444444444444\n",
      "Iteration: 11\n",
      "Train accuracy: 53.179365079365084\n",
      "Val accuracy: 53.214285714285715\n",
      "Iter 11 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 12\n",
      "Train accuracy: 54.36984126984127\n",
      "Val accuracy: 54.38571428571428\n",
      "Iter 12 -> sub iter 99 : 57.301587301587396\n",
      "Iteration: 13\n",
      "Train accuracy: 55.74761904761905\n",
      "Val accuracy: 55.51428571428572\n",
      "Iter 13 -> sub iter 99 : 60.793650793650794\n",
      "Iteration: 14\n",
      "Train accuracy: 58.53174603174603\n",
      "Val accuracy: 58.08571428571428\n",
      "Iter 14 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 15\n",
      "Train accuracy: 61.68730158730159\n",
      "Val accuracy: 61.01428571428571\n",
      "Iter 15 -> sub iter 99 : 66.507936507936544\n",
      "Iteration: 16\n",
      "Train accuracy: 63.53809523809524\n",
      "Val accuracy: 63.15714285714286\n",
      "Iter 16 -> sub iter 99 : 67.619047619047626\n",
      "Iteration: 17\n",
      "Train accuracy: 64.57619047619048\n",
      "Val accuracy: 64.31428571428572\n",
      "Iter 17 -> sub iter 99 : 68.730158730158735\n",
      "Iteration: 18\n",
      "Train accuracy: 65.37777777777778\n",
      "Val accuracy: 65.11428571428571\n",
      "Iter 18 -> sub iter 99 : 68.888888888888895\n",
      "Iteration: 19\n",
      "Train accuracy: 66.02698412698412\n",
      "Val accuracy: 65.75714285714285\n",
      "Iter 19 -> sub iter 99 : 69.523809523809526\n",
      "Iteration: 20\n",
      "Train accuracy: 66.6126984126984\n",
      "Val accuracy: 66.4\n",
      "Iter 20 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 21\n",
      "Train accuracy: 67.1952380952381\n",
      "Val accuracy: 66.81428571428572\n",
      "Iter 21 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 22\n",
      "Train accuracy: 68.56507936507936\n",
      "Val accuracy: 68.37142857142857\n",
      "Iter 22 -> sub iter 99 : 73.33333333333333\n",
      "Iteration: 23\n",
      "Train accuracy: 70.21904761904761\n",
      "Val accuracy: 70.35714285714286\n",
      "Iter 23 -> sub iter 99 : 73.80952380952381\n",
      "Iteration: 24\n",
      "Train accuracy: 71.43174603174603\n",
      "Val accuracy: 71.8\n",
      "Iter 24 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 25\n",
      "Train accuracy: 72.41587301587302\n",
      "Val accuracy: 73.2\n",
      "Iter 25 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 26\n",
      "Train accuracy: 73.25714285714285\n",
      "Val accuracy: 73.87142857142858\n",
      "Iter 26 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 27\n",
      "Train accuracy: 73.98095238095237\n",
      "Val accuracy: 74.58571428571429\n",
      "Iter 27 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 28\n",
      "Train accuracy: 74.55555555555556\n",
      "Val accuracy: 75.17142857142856\n",
      "Iter 28 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 29\n",
      "Train accuracy: 75.05873015873016\n",
      "Val accuracy: 75.72857142857143\n",
      "Iter 29 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 30\n",
      "Train accuracy: 75.51904761904763\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 30 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 31\n",
      "Train accuracy: 75.84444444444445\n",
      "Val accuracy: 76.5142857142857\n",
      "Iter 31 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 32\n",
      "Train accuracy: 76.22698412698414\n",
      "Val accuracy: 76.84285714285714\n",
      "Iter 32 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 33\n",
      "Train accuracy: 76.47460317460317\n",
      "Val accuracy: 77.14285714285715\n",
      "Iter 33 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 34\n",
      "Train accuracy: 76.77777777777777\n",
      "Val accuracy: 77.4\n",
      "Iter 34 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 35\n",
      "Train accuracy: 77.0015873015873\n",
      "Val accuracy: 77.65714285714286\n",
      "Iter 35 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 36\n",
      "Train accuracy: 77.21904761904761\n",
      "Val accuracy: 77.81428571428572\n",
      "Iter 36 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 37\n",
      "Train accuracy: 77.44920634920635\n",
      "Val accuracy: 78.02857142857142\n",
      "Iter 37 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 38\n",
      "Train accuracy: 77.64603174603174\n",
      "Val accuracy: 78.18571428571428\n",
      "Iter 38 -> sub iter 99 : 79.52380952380952\n",
      "Iteration: 39\n",
      "Train accuracy: 77.83333333333333\n",
      "Val accuracy: 78.3\n",
      "Iter 39 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 40\n",
      "Train accuracy: 78.01904761904763\n",
      "Val accuracy: 78.54285714285714\n",
      "Iter 40 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 41\n",
      "Train accuracy: 78.1984126984127\n",
      "Val accuracy: 78.74285714285715\n",
      "Iter 41 -> sub iter 99 : 80.00634920634929\n",
      "Iteration: 42\n",
      "Train accuracy: 78.37936507936509\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 42 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 43\n",
      "Train accuracy: 78.54603174603174\n",
      "Val accuracy: 78.98571428571428\n",
      "Iter 43 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 44\n",
      "Train accuracy: 78.71904761904761\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 44 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 45\n",
      "Train accuracy: 78.84761904761905\n",
      "Val accuracy: 79.27142857142857\n",
      "Iter 45 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 46\n",
      "Train accuracy: 78.98730158730159\n",
      "Val accuracy: 79.47142857142858\n",
      "Iter 46 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 47\n",
      "Train accuracy: 79.11587301587302\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 47 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 48\n",
      "Train accuracy: 79.23492063492064\n",
      "Val accuracy: 79.74285714285713\n",
      "Iter 48 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 49\n",
      "Train accuracy: 79.34126984126985\n",
      "Val accuracy: 79.87142857142857\n",
      "Iter 49 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 50\n",
      "Train accuracy: 79.43015873015872\n",
      "Val accuracy: 79.88571428571429\n",
      "Iter 50 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 51\n",
      "Train accuracy: 79.53492063492064\n",
      "Val accuracy: 80.01428571428572\n",
      "Iter 51 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 52\n",
      "Train accuracy: 79.62857142857143\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 52 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 53\n",
      "Train accuracy: 79.74285714285713\n",
      "Val accuracy: 80.27142857142857\n",
      "Iter 53 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 54\n",
      "Train accuracy: 79.84126984126985\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 54 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 55\n",
      "Train accuracy: 79.94920634920635\n",
      "Val accuracy: 80.37142857142857\n",
      "Iter 55 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 80.04603174603174\n",
      "Val accuracy: 80.44285714285714\n",
      "Iter 56 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 57\n",
      "Train accuracy: 80.12698412698413\n",
      "Val accuracy: 80.52857142857142\n",
      "Iter 57 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 58\n",
      "Train accuracy: 80.22857142857143\n",
      "Val accuracy: 80.57142857142857\n",
      "Iter 58 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 59\n",
      "Train accuracy: 80.32380952380952\n",
      "Val accuracy: 80.64285714285714\n",
      "Iter 59 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 60\n",
      "Train accuracy: 80.4047619047619\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 60 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 61\n",
      "Train accuracy: 80.51428571428572\n",
      "Val accuracy: 80.85714285714286\n",
      "Iter 61 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 62\n",
      "Train accuracy: 80.61111111111111\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 62 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 63\n",
      "Train accuracy: 80.72857142857143\n",
      "Val accuracy: 80.94285714285714\n",
      "Iter 63 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 64\n",
      "Train accuracy: 80.81428571428572\n",
      "Val accuracy: 81.01428571428572\n",
      "Iter 64 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 65\n",
      "Train accuracy: 80.95079365079366\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 65 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 66\n",
      "Train accuracy: 81.2047619047619\n",
      "Val accuracy: 81.21428571428572\n",
      "Iter 66 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 81.55238095238096\n",
      "Val accuracy: 81.55714285714286\n",
      "Iter 67 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 68\n",
      "Train accuracy: 82.17142857142858\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 68 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 69\n",
      "Train accuracy: 82.97301587301588\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 69 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 70\n",
      "Train accuracy: 83.9015873015873\n",
      "Val accuracy: 83.6\n",
      "Iter 70 -> sub iter 99 : 87.30158730158731\n",
      "Iteration: 71\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 84.48571428571428\n",
      "Iter 71 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 72\n",
      "Train accuracy: 85.50952380952381\n",
      "Val accuracy: 85.28571428571429\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 86.03650793650793\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 73 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 74\n",
      "Train accuracy: 86.52063492063492\n",
      "Val accuracy: 85.94285714285715\n",
      "Iter 74 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 75\n",
      "Train accuracy: 86.90952380952382\n",
      "Val accuracy: 86.52857142857144\n",
      "Iter 75 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 76\n",
      "Train accuracy: 87.25714285714285\n",
      "Val accuracy: 86.92857142857143\n",
      "Iter 76 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 77\n",
      "Train accuracy: 87.59523809523809\n",
      "Val accuracy: 87.22857142857143\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.83492063492064\n",
      "Val accuracy: 87.4\n",
      "Iter 78 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 79\n",
      "Train accuracy: 88.07460317460317\n",
      "Val accuracy: 87.62857142857143\n",
      "Iter 79 -> sub iter 99 : 88.41269841269849\n",
      "Iteration: 80\n",
      "Train accuracy: 88.23968253968253\n",
      "Val accuracy: 87.8\n",
      "Iter 80 -> sub iter 99 : 88.41269841269842\n",
      "Iteration: 81\n",
      "Train accuracy: 88.4063492063492\n",
      "Val accuracy: 87.94285714285715\n",
      "Iter 81 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 82\n",
      "Train accuracy: 88.58095238095238\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.73174603174603\n",
      "Val accuracy: 88.15714285714286\n",
      "Iter 83 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 84\n",
      "Train accuracy: 88.86349206349206\n",
      "Val accuracy: 88.3\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.97142857142856\n",
      "Val accuracy: 88.37142857142857\n",
      "Iter 85 -> sub iter 99 : 88.41269841269845\n",
      "Iteration: 86\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 88.45714285714286\n",
      "Iter 86 -> sub iter 99 : 88.41269841269844\n",
      "Iteration: 87\n",
      "Train accuracy: 89.19365079365079\n",
      "Val accuracy: 88.52857142857142\n",
      "Iter 87 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 88\n",
      "Train accuracy: 89.29206349206349\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 88 -> sub iter 99 : 88.41269841269848\n",
      "Iteration: 89\n",
      "Train accuracy: 89.40476190476191\n",
      "Val accuracy: 88.8\n",
      "Iter 89 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 90\n",
      "Train accuracy: 89.51269841269841\n",
      "Val accuracy: 88.9857142857143\n",
      "Iter 90 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 91\n",
      "Train accuracy: 89.63333333333333\n",
      "Val accuracy: 89.1\n",
      "Iter 91 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 92\n",
      "Train accuracy: 89.71746031746032\n",
      "Val accuracy: 89.17142857142856\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 89.80317460317461\n",
      "Val accuracy: 89.2\n",
      "Iter 93 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 94\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 94 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 95\n",
      "Train accuracy: 89.94920634920635\n",
      "Val accuracy: 89.35714285714286\n",
      "Iter 95 -> sub iter 99 : 89.52380952380953\n",
      "Iteration: 96\n",
      "Train accuracy: 90.01904761904763\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 96 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 97\n",
      "Train accuracy: 90.09206349206349\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 97 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 98\n",
      "Train accuracy: 90.14126984126985\n",
      "Val accuracy: 89.47142857142858\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 90.2095238095238\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 99 -> sub iter 99 : 90.02857142857143\n",
      "Iteration: 100\n",
      "Train accuracy: 90.2936507936508\n",
      "Val accuracy: 89.58571428571429\n",
      "Iter 100 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 101\n",
      "Train accuracy: 90.36031746031746\n",
      "Val accuracy: 89.67142857142856\n",
      "Iter 101 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 102\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.75714285714285\n",
      "Iter 102 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 103\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.81428571428572\n",
      "Iter 103 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 104\n",
      "Train accuracy: 90.54920634920634\n",
      "Val accuracy: 89.87142857142857\n",
      "Iter 104 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 105\n",
      "Train accuracy: 90.60952380952381\n",
      "Val accuracy: 89.9\n",
      "Iter 105 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 106\n",
      "Train accuracy: 90.65873015873017\n",
      "Val accuracy: 89.97142857142858\n",
      "Iter 106 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 107\n",
      "Train accuracy: 90.7031746031746\n",
      "Val accuracy: 90.04285714285714\n",
      "Iter 107 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 108\n",
      "Train accuracy: 90.73174603174603\n",
      "Val accuracy: 90.08571428571429\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 90.78412698412698\n",
      "Val accuracy: 90.12857142857142\n",
      "Iter 109 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 110\n",
      "Train accuracy: 90.84603174603174\n",
      "Val accuracy: 90.15714285714286\n",
      "Iter 110 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 111\n",
      "Train accuracy: 90.89365079365079\n",
      "Val accuracy: 90.2\n",
      "Iter 111 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 112\n",
      "Train accuracy: 90.94920634920635\n",
      "Val accuracy: 90.24285714285715\n",
      "Iter 112 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 113\n",
      "Train accuracy: 91.0079365079365\n",
      "Val accuracy: 90.25714285714285\n",
      "Iter 113 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 114\n",
      "Train accuracy: 91.06349206349206\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 114 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 115\n",
      "Train accuracy: 91.10000000000001\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 115 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 116\n",
      "Train accuracy: 91.14603174603174\n",
      "Val accuracy: 90.31428571428572\n",
      "Iter 116 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 117\n",
      "Train accuracy: 91.1952380952381\n",
      "Val accuracy: 90.34285714285714\n",
      "Iter 117 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 118\n",
      "Train accuracy: 91.23968253968255\n",
      "Val accuracy: 90.4\n",
      "Iter 118 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 119\n",
      "Train accuracy: 91.28571428571428\n",
      "Val accuracy: 90.4\n",
      "Iter 119 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 120\n",
      "Train accuracy: 91.33015873015873\n",
      "Val accuracy: 90.42857142857143\n",
      "Iter 120 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 121\n",
      "Train accuracy: 91.35555555555555\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 121 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 122\n",
      "Train accuracy: 91.41269841269842\n",
      "Val accuracy: 90.52857142857142\n",
      "Iter 122 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 123\n",
      "Train accuracy: 91.43968253968254\n",
      "Val accuracy: 90.58571428571427\n",
      "Iter 123 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 124\n",
      "Train accuracy: 91.48412698412697\n",
      "Val accuracy: 90.60000000000001\n",
      "Iter 124 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 125\n",
      "Train accuracy: 91.53333333333333\n",
      "Val accuracy: 90.62857142857142\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 91.57142857142857\n",
      "Val accuracy: 90.65714285714286\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 91.60634920634921\n",
      "Val accuracy: 90.68571428571428\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 91.63650793650794\n",
      "Val accuracy: 90.71428571428571\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 91.66349206349206\n",
      "Val accuracy: 90.74285714285715\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 91.70158730158731\n",
      "Val accuracy: 90.8\n",
      "Iter 130 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 131\n",
      "Train accuracy: 91.72063492063492\n",
      "Val accuracy: 90.82857142857142\n",
      "Iter 131 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 132\n",
      "Train accuracy: 91.74285714285715\n",
      "Val accuracy: 90.84285714285714\n",
      "Iter 132 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 133\n",
      "Train accuracy: 91.76507936507936\n",
      "Val accuracy: 90.87142857142857\n",
      "Iter 133 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 134\n",
      "Train accuracy: 91.8015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 134 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 135\n",
      "Train accuracy: 91.83015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 135 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 136\n",
      "Train accuracy: 91.86666666666666\n",
      "Val accuracy: 90.9\n",
      "Iter 136 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 137\n",
      "Train accuracy: 91.91746031746032\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 137 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 138\n",
      "Train accuracy: 91.94761904761904\n",
      "Val accuracy: 90.97142857142858\n",
      "Iter 138 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 139\n",
      "Train accuracy: 91.98888888888888\n",
      "Val accuracy: 91.05714285714286\n",
      "Iter 139 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 140\n",
      "Train accuracy: 92.02698412698412\n",
      "Val accuracy: 91.11428571428571\n",
      "Iter 140 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 141\n",
      "Train accuracy: 92.05238095238096\n",
      "Val accuracy: 91.12857142857142\n",
      "Iter 141 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 142\n",
      "Train accuracy: 92.08571428571429\n",
      "Val accuracy: 91.15714285714286\n",
      "Iter 142 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 143\n",
      "Train accuracy: 92.1079365079365\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 143 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 144\n",
      "Train accuracy: 92.13809523809525\n",
      "Val accuracy: 91.21428571428571\n",
      "Iter 144 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 145\n",
      "Train accuracy: 92.17142857142858\n",
      "Val accuracy: 91.24285714285715\n",
      "Iter 145 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 146\n",
      "Train accuracy: 92.1968253968254\n",
      "Val accuracy: 91.25714285714285\n",
      "Iter 146 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 147\n",
      "Train accuracy: 92.23174603174603\n",
      "Val accuracy: 91.27142857142857\n",
      "Iter 147 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 148\n",
      "Train accuracy: 92.25873015873016\n",
      "Val accuracy: 91.3\n",
      "Iter 148 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 149\n",
      "Train accuracy: 92.28253968253968\n",
      "Val accuracy: 91.3\n",
      "Iter 149 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 150\n",
      "Train accuracy: 92.3015873015873\n",
      "Val accuracy: 91.3\n",
      "Training for 0.01\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 24.285714285714285\n",
      "Iteration: 1\n",
      "Train accuracy: 21.285714285714285\n",
      "Val accuracy: 20.42857142857143\n",
      "Iter 1 -> sub iter 99 : 26.666666666666668\n",
      "Iteration: 2\n",
      "Train accuracy: 24.23968253968254\n",
      "Val accuracy: 23.414285714285715\n",
      "Iter 2 -> sub iter 99 : 29.523809523809526\n",
      "Iteration: 3\n",
      "Train accuracy: 26.51904761904762\n",
      "Val accuracy: 25.785714285714285\n",
      "Iter 3 -> sub iter 99 : 34.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 30.831746031746032\n",
      "Val accuracy: 30.242857142857144\n",
      "Iter 4 -> sub iter 99 : 37.301587301587304\n",
      "Iteration: 5\n",
      "Train accuracy: 33.77460317460317\n",
      "Val accuracy: 33.15714285714286\n",
      "Iter 5 -> sub iter 99 : 39.047619047619054\n",
      "Iteration: 6\n",
      "Train accuracy: 35.75238095238095\n",
      "Val accuracy: 35.25714285714286\n",
      "Iter 6 -> sub iter 99 : 39.523809523809526\n",
      "Iteration: 7\n",
      "Train accuracy: 37.7\n",
      "Val accuracy: 37.25714285714285\n",
      "Iter 7 -> sub iter 99 : 42.698412698412696\n",
      "Iteration: 8\n",
      "Train accuracy: 41.15555555555556\n",
      "Val accuracy: 40.34285714285714\n",
      "Iter 8 -> sub iter 99 : 46.190476190476195\n",
      "Iteration: 9\n",
      "Train accuracy: 44.84444444444444\n",
      "Val accuracy: 43.67142857142857\n",
      "Iter 9 -> sub iter 99 : 51.904761904761916\n",
      "Iteration: 10\n",
      "Train accuracy: 52.49206349206349\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 10 -> sub iter 99 : 55.238095238095246\n",
      "Iteration: 11\n",
      "Train accuracy: 56.10793650793651\n",
      "Val accuracy: 55.442857142857136\n",
      "Iter 11 -> sub iter 99 : 58.412698412698425\n",
      "Iteration: 12\n",
      "Train accuracy: 58.439682539682536\n",
      "Val accuracy: 57.72857142857143\n",
      "Iter 12 -> sub iter 99 : 60.317460317460316\n",
      "Iteration: 13\n",
      "Train accuracy: 60.098412698412695\n",
      "Val accuracy: 59.61428571428572\n",
      "Iter 13 -> sub iter 99 : 62.698412698412696\n",
      "Iteration: 14\n",
      "Train accuracy: 61.66031746031746\n",
      "Val accuracy: 61.42857142857143\n",
      "Iter 14 -> sub iter 99 : 65.238095238095245\n",
      "Iteration: 15\n",
      "Train accuracy: 63.32698412698413\n",
      "Val accuracy: 62.857142857142854\n",
      "Iter 15 -> sub iter 99 : 66.190476190476196\n",
      "Iteration: 16\n",
      "Train accuracy: 64.60317460317461\n",
      "Val accuracy: 64.14285714285714\n",
      "Iter 16 -> sub iter 99 : 66.507936507936545\n",
      "Iteration: 17\n",
      "Train accuracy: 65.4952380952381\n",
      "Val accuracy: 65.17142857142856\n",
      "Iter 17 -> sub iter 99 : 67.777777777777796\n",
      "Iteration: 18\n",
      "Train accuracy: 66.21904761904761\n",
      "Val accuracy: 65.9\n",
      "Iter 18 -> sub iter 99 : 68.730158730158736\n",
      "Iteration: 19\n",
      "Train accuracy: 66.91111111111111\n",
      "Val accuracy: 66.60000000000001\n",
      "Iter 19 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 20\n",
      "Train accuracy: 68.81111111111112\n",
      "Val accuracy: 68.54285714285714\n",
      "Iter 20 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 21\n",
      "Train accuracy: 70.85396825396826\n",
      "Val accuracy: 70.61428571428571\n",
      "Iter 21 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 22\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 72.0\n",
      "Iter 22 -> sub iter 99 : 73.49206349206358\n",
      "Iteration: 23\n",
      "Train accuracy: 73.44444444444444\n",
      "Val accuracy: 73.17142857142858\n",
      "Iter 23 -> sub iter 99 : 74.44444444444444\n",
      "Iteration: 24\n",
      "Train accuracy: 74.31428571428572\n",
      "Val accuracy: 74.11428571428571\n",
      "Iter 24 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 25\n",
      "Train accuracy: 74.92857142857143\n",
      "Val accuracy: 74.67142857142856\n",
      "Iter 25 -> sub iter 99 : 75.23809523809524\n",
      "Iteration: 26\n",
      "Train accuracy: 75.4968253968254\n",
      "Val accuracy: 75.0\n",
      "Iter 26 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 27\n",
      "Train accuracy: 75.91904761904762\n",
      "Val accuracy: 75.44285714285715\n",
      "Iter 27 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 28\n",
      "Train accuracy: 76.29047619047618\n",
      "Val accuracy: 75.68571428571428\n",
      "Iter 28 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 29\n",
      "Train accuracy: 76.65238095238095\n",
      "Val accuracy: 76.0\n",
      "Iter 29 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 30\n",
      "Train accuracy: 76.95714285714286\n",
      "Val accuracy: 76.31428571428572\n",
      "Iter 30 -> sub iter 99 : 76.34920634920634\n",
      "Iteration: 31\n",
      "Train accuracy: 77.22063492063492\n",
      "Val accuracy: 76.6\n",
      "Iter 31 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 32\n",
      "Train accuracy: 77.4920634920635\n",
      "Val accuracy: 76.92857142857143\n",
      "Iter 32 -> sub iter 99 : 77.14285714285715\n",
      "Iteration: 33\n",
      "Train accuracy: 77.72222222222223\n",
      "Val accuracy: 77.08571428571429\n",
      "Iter 33 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 34\n",
      "Train accuracy: 77.96190476190476\n",
      "Val accuracy: 77.21428571428571\n",
      "Iter 34 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 35\n",
      "Train accuracy: 78.16984126984127\n",
      "Val accuracy: 77.47142857142858\n",
      "Iter 35 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 36\n",
      "Train accuracy: 78.37460317460318\n",
      "Val accuracy: 77.8\n",
      "Iter 36 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 37\n",
      "Train accuracy: 78.57936507936508\n",
      "Val accuracy: 77.91428571428571\n",
      "Iter 37 -> sub iter 99 : 78.09523809523812\n",
      "Iteration: 38\n",
      "Train accuracy: 78.75555555555556\n",
      "Val accuracy: 78.04285714285714\n",
      "Iter 38 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 39\n",
      "Train accuracy: 78.92063492063492\n",
      "Val accuracy: 78.22857142857143\n",
      "Iter 39 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 40\n",
      "Train accuracy: 79.0936507936508\n",
      "Val accuracy: 78.37142857142857\n",
      "Iter 40 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 41\n",
      "Train accuracy: 79.26666666666667\n",
      "Val accuracy: 78.51428571428572\n",
      "Iter 41 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 79.4\n",
      "Val accuracy: 78.58571428571427\n",
      "Iter 42 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 43\n",
      "Train accuracy: 79.52539682539683\n",
      "Val accuracy: 78.77142857142857\n",
      "Iter 43 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 44\n",
      "Train accuracy: 79.62539682539682\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 44 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 45\n",
      "Train accuracy: 79.71587301587302\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 45 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 46\n",
      "Train accuracy: 79.83492063492064\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 46 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 47\n",
      "Train accuracy: 79.95396825396826\n",
      "Val accuracy: 79.37142857142857\n",
      "Iter 47 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 48\n",
      "Train accuracy: 80.04444444444444\n",
      "Val accuracy: 79.45714285714286\n",
      "Iter 48 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 49\n",
      "Train accuracy: 80.13333333333334\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 49 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 50\n",
      "Train accuracy: 80.21111111111111\n",
      "Val accuracy: 79.7\n",
      "Iter 50 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 51\n",
      "Train accuracy: 80.2984126984127\n",
      "Val accuracy: 79.84285714285714\n",
      "Iter 51 -> sub iter 99 : 79.20634920634924\n",
      "Iteration: 52\n",
      "Train accuracy: 80.38253968253967\n",
      "Val accuracy: 79.9\n",
      "Iter 52 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 53\n",
      "Train accuracy: 80.48095238095239\n",
      "Val accuracy: 79.91428571428571\n",
      "Iter 53 -> sub iter 99 : 80.15873015873017\n",
      "Iteration: 54\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 80.05714285714286\n",
      "Iter 54 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 55\n",
      "Train accuracy: 80.64126984126983\n",
      "Val accuracy: 80.12857142857143\n",
      "Iter 55 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 56\n",
      "Train accuracy: 80.70793650793651\n",
      "Val accuracy: 80.21428571428572\n",
      "Iter 56 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 57\n",
      "Train accuracy: 80.78888888888889\n",
      "Val accuracy: 80.32857142857142\n",
      "Iter 57 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 58\n",
      "Train accuracy: 80.85873015873017\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 58 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 59\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.48571428571428\n",
      "Iter 59 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 60\n",
      "Train accuracy: 80.99047619047619\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 60 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 61\n",
      "Train accuracy: 81.06507936507936\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 61 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 62\n",
      "Train accuracy: 81.12857142857143\n",
      "Val accuracy: 80.60000000000001\n",
      "Iter 62 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 63\n",
      "Train accuracy: 81.1920634920635\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 63 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 64\n",
      "Train accuracy: 81.25555555555556\n",
      "Val accuracy: 80.65714285714286\n",
      "Iter 64 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 65\n",
      "Train accuracy: 81.32857142857142\n",
      "Val accuracy: 80.7\n",
      "Iter 65 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 66\n",
      "Train accuracy: 81.37936507936509\n",
      "Val accuracy: 80.78571428571428\n",
      "Iter 66 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 67\n",
      "Train accuracy: 81.43333333333334\n",
      "Val accuracy: 80.80000000000001\n",
      "Iter 67 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 68\n",
      "Train accuracy: 81.48253968253968\n",
      "Val accuracy: 80.84285714285714\n",
      "Iter 68 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 69\n",
      "Train accuracy: 81.52539682539683\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 69 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 70\n",
      "Train accuracy: 81.5904761904762\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 70 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 71\n",
      "Train accuracy: 81.65238095238095\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 71 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 72\n",
      "Train accuracy: 81.70793650793651\n",
      "Val accuracy: 80.95714285714286\n",
      "Iter 72 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 73\n",
      "Train accuracy: 81.76825396825397\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 73 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 74\n",
      "Train accuracy: 81.81904761904762\n",
      "Val accuracy: 81.08571428571429\n",
      "Iter 74 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 75\n",
      "Train accuracy: 81.85238095238095\n",
      "Val accuracy: 81.12857142857143\n",
      "Iter 75 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 76\n",
      "Train accuracy: 81.8968253968254\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 76 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 77\n",
      "Train accuracy: 81.93015873015874\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 77 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 78\n",
      "Train accuracy: 82.0\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 78 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 79\n",
      "Train accuracy: 82.04444444444444\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 79 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 80\n",
      "Train accuracy: 82.1\n",
      "Val accuracy: 81.3\n",
      "Iter 80 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 81\n",
      "Train accuracy: 82.13333333333334\n",
      "Val accuracy: 81.34285714285714\n",
      "Iter 81 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 82\n",
      "Train accuracy: 82.15396825396826\n",
      "Val accuracy: 81.32857142857142\n",
      "Iter 82 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 83\n",
      "Train accuracy: 82.2015873015873\n",
      "Val accuracy: 81.37142857142857\n",
      "Iter 83 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 84\n",
      "Train accuracy: 82.24761904761905\n",
      "Val accuracy: 81.38571428571429\n",
      "Iter 84 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 85\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.41428571428571\n",
      "Iter 85 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 86\n",
      "Train accuracy: 82.33015873015873\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 86 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 87\n",
      "Train accuracy: 82.37301587301587\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 87 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 88\n",
      "Train accuracy: 82.43809523809524\n",
      "Val accuracy: 81.54285714285714\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 82.46507936507936\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 89 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 90\n",
      "Train accuracy: 82.5047619047619\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 90 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 91\n",
      "Train accuracy: 82.53650793650795\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 91 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 92\n",
      "Train accuracy: 82.57142857142857\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 92 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 93\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.74285714285713\n",
      "Iter 93 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 94\n",
      "Train accuracy: 82.62857142857143\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 94 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 95\n",
      "Train accuracy: 82.65714285714286\n",
      "Val accuracy: 81.81428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 82.6920634920635\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 96 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 97\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 97 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 98\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.94285714285714\n",
      "Iter 98 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 99\n",
      "Train accuracy: 82.78095238095237\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 99 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 100\n",
      "Train accuracy: 82.81746031746032\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.84603174603174\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.87777777777777\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 102 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 103\n",
      "Train accuracy: 82.89047619047619\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 104\n",
      "Train accuracy: 82.92380952380952\n",
      "Val accuracy: 82.01428571428572\n",
      "Iter 104 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 105\n",
      "Train accuracy: 82.95555555555556\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 105 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 106\n",
      "Train accuracy: 82.97777777777777\n",
      "Val accuracy: 82.05714285714286\n",
      "Iter 106 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 107\n",
      "Train accuracy: 83.0\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 107 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 108\n",
      "Train accuracy: 83.02539682539683\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 108 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 109\n",
      "Train accuracy: 83.05079365079365\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 109 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 110\n",
      "Train accuracy: 83.07460317460318\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 110 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 111\n",
      "Train accuracy: 83.1\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 111 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 112\n",
      "Train accuracy: 83.13015873015873\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 112 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 113\n",
      "Train accuracy: 83.16190476190476\n",
      "Val accuracy: 82.24285714285713\n",
      "Iter 113 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 114\n",
      "Train accuracy: 83.17936507936507\n",
      "Val accuracy: 82.27142857142857\n",
      "Iter 114 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 115\n",
      "Train accuracy: 83.20476190476191\n",
      "Val accuracy: 82.28571428571428\n",
      "Iter 115 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 116\n",
      "Train accuracy: 83.22698412698412\n",
      "Val accuracy: 82.32857142857142\n",
      "Iter 116 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 117\n",
      "Train accuracy: 83.25396825396825\n",
      "Val accuracy: 82.37142857142857\n",
      "Iter 117 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 118\n",
      "Train accuracy: 83.28253968253968\n",
      "Val accuracy: 82.38571428571429\n",
      "Iter 118 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 119\n",
      "Train accuracy: 83.30634920634921\n",
      "Val accuracy: 82.39999999999999\n",
      "Iter 119 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 120\n",
      "Train accuracy: 83.31904761904761\n",
      "Val accuracy: 82.42857142857143\n",
      "Iter 120 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 121\n",
      "Train accuracy: 83.34603174603174\n",
      "Val accuracy: 82.45714285714286\n",
      "Iter 121 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 122\n",
      "Train accuracy: 83.37460317460318\n",
      "Val accuracy: 82.48571428571428\n",
      "Iter 122 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 123\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.51428571428572\n",
      "Iter 123 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 124\n",
      "Train accuracy: 83.4095238095238\n",
      "Val accuracy: 82.55714285714286\n",
      "Iter 124 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 125\n",
      "Train accuracy: 83.44444444444444\n",
      "Val accuracy: 82.6\n",
      "Iter 125 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 126\n",
      "Train accuracy: 83.46190476190476\n",
      "Val accuracy: 82.62857142857143\n",
      "Iter 126 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 127\n",
      "Train accuracy: 83.49047619047619\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 127 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 128\n",
      "Train accuracy: 83.5079365079365\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 128 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 129\n",
      "Train accuracy: 83.53333333333333\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 129 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 130\n",
      "Train accuracy: 83.54920634920634\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 130 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 131\n",
      "Train accuracy: 83.57460317460318\n",
      "Val accuracy: 82.72857142857143\n",
      "Iter 131 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 132\n",
      "Train accuracy: 83.6015873015873\n",
      "Val accuracy: 82.78571428571428\n",
      "Iter 132 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 133\n",
      "Train accuracy: 83.62857142857143\n",
      "Val accuracy: 82.8\n",
      "Iter 133 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 134\n",
      "Train accuracy: 83.65714285714286\n",
      "Val accuracy: 82.81428571428572\n",
      "Iter 134 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 135\n",
      "Train accuracy: 83.67460317460318\n",
      "Val accuracy: 82.84285714285714\n",
      "Iter 135 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 136\n",
      "Train accuracy: 83.69682539682539\n",
      "Val accuracy: 82.85714285714286\n",
      "Iter 136 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 137\n",
      "Train accuracy: 83.72539682539683\n",
      "Val accuracy: 82.87142857142857\n",
      "Iter 137 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 138\n",
      "Train accuracy: 83.74444444444444\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 139\n",
      "Train accuracy: 83.75873015873016\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 139 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 140\n",
      "Train accuracy: 83.77619047619046\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 140 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 141\n",
      "Train accuracy: 83.7968253968254\n",
      "Val accuracy: 82.94285714285714\n",
      "Iter 141 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 142\n",
      "Train accuracy: 83.82063492063492\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 142 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 143\n",
      "Train accuracy: 83.83809523809524\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 143 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 144\n",
      "Train accuracy: 83.86190476190475\n",
      "Val accuracy: 82.97142857142858\n",
      "Iter 144 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 145\n",
      "Train accuracy: 83.88571428571429\n",
      "Val accuracy: 82.98571428571428\n",
      "Iter 145 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 146\n",
      "Train accuracy: 83.9031746031746\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 146 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 147\n",
      "Train accuracy: 83.91904761904762\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 147 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 148\n",
      "Train accuracy: 83.95238095238096\n",
      "Val accuracy: 83.0\n",
      "Iter 148 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 149\n",
      "Train accuracy: 83.97301587301588\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 149 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 150\n",
      "Train accuracy: 83.97460317460317\n",
      "Val accuracy: 83.04285714285714\n",
      "Training for 0.001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 20.158730158730158\n",
      "Iteration: 1\n",
      "Train accuracy: 16.414285714285715\n",
      "Val accuracy: 15.814285714285713\n",
      "Iter 1 -> sub iter 99 : 30.158730158730158\n",
      "Iteration: 2\n",
      "Train accuracy: 27.836507936507935\n",
      "Val accuracy: 27.15714285714286\n",
      "Iter 2 -> sub iter 99 : 37.936507936507946\n",
      "Iteration: 3\n",
      "Train accuracy: 36.93015873015873\n",
      "Val accuracy: 36.15714285714286\n",
      "Iter 3 -> sub iter 99 : 44.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 42.15238095238095\n",
      "Val accuracy: 41.27142857142857\n",
      "Iter 4 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 5\n",
      "Train accuracy: 46.84285714285714\n",
      "Val accuracy: 45.92857142857143\n",
      "Iter 5 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 6\n",
      "Train accuracy: 51.34603174603175\n",
      "Val accuracy: 50.55714285714286\n",
      "Iter 6 -> sub iter 99 : 54.603174603174605\n",
      "Iteration: 7\n",
      "Train accuracy: 54.85079365079365\n",
      "Val accuracy: 53.51428571428571\n",
      "Iter 7 -> sub iter 99 : 56.825396825396824\n",
      "Iteration: 8\n",
      "Train accuracy: 57.369841269841274\n",
      "Val accuracy: 56.07142857142857\n",
      "Iter 8 -> sub iter 99 : 58.253968253968264\n",
      "Iteration: 9\n",
      "Train accuracy: 59.23492063492064\n",
      "Val accuracy: 57.8\n",
      "Iter 9 -> sub iter 99 : 59.841269841269844\n",
      "Iteration: 10\n",
      "Train accuracy: 60.663492063492065\n",
      "Val accuracy: 59.5\n",
      "Iter 10 -> sub iter 99 : 60.476190476190474\n",
      "Iteration: 11\n",
      "Train accuracy: 61.850793650793655\n",
      "Val accuracy: 60.785714285714285\n",
      "Iter 11 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 12\n",
      "Train accuracy: 62.790476190476184\n",
      "Val accuracy: 62.02857142857143\n",
      "Iter 12 -> sub iter 99 : 62.539682539682545\n",
      "Iteration: 13\n",
      "Train accuracy: 63.549206349206344\n",
      "Val accuracy: 62.94285714285714\n",
      "Iter 13 -> sub iter 99 : 63.492063492063495\n",
      "Iteration: 14\n",
      "Train accuracy: 64.18253968253968\n",
      "Val accuracy: 63.67142857142857\n",
      "Iter 14 -> sub iter 99 : 63.809523809523835\n",
      "Iteration: 15\n",
      "Train accuracy: 64.75555555555556\n",
      "Val accuracy: 64.24285714285715\n",
      "Iter 15 -> sub iter 99 : 64.603174603174615\n",
      "Iteration: 16\n",
      "Train accuracy: 65.28412698412698\n",
      "Val accuracy: 64.85714285714286\n",
      "Iter 16 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 17\n",
      "Train accuracy: 65.6952380952381\n",
      "Val accuracy: 65.18571428571428\n",
      "Iter 17 -> sub iter 99 : 66.031746031746024\n",
      "Iteration: 18\n",
      "Train accuracy: 66.10317460317461\n",
      "Val accuracy: 65.84285714285714\n",
      "Iter 18 -> sub iter 99 : 66.190476190476195\n",
      "Iteration: 19\n",
      "Train accuracy: 66.45555555555556\n",
      "Val accuracy: 66.17142857142856\n",
      "Iter 19 -> sub iter 99 : 66.507936507936514\n",
      "Iteration: 20\n",
      "Train accuracy: 66.79047619047618\n",
      "Val accuracy: 66.57142857142857\n",
      "Iter 20 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 21\n",
      "Train accuracy: 67.14126984126985\n",
      "Val accuracy: 66.8\n",
      "Iter 21 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 22\n",
      "Train accuracy: 67.43492063492063\n",
      "Val accuracy: 67.0\n",
      "Iter 22 -> sub iter 99 : 67.142857142857146\n",
      "Iteration: 23\n",
      "Train accuracy: 67.76507936507936\n",
      "Val accuracy: 67.31428571428572\n",
      "Iter 23 -> sub iter 99 : 67.460317460317474\n",
      "Iteration: 24\n",
      "Train accuracy: 68.03968253968254\n",
      "Val accuracy: 67.52857142857142\n",
      "Iter 24 -> sub iter 99 : 68.09523809523813\n",
      "Iteration: 25\n",
      "Train accuracy: 68.27619047619048\n",
      "Val accuracy: 67.92857142857143\n",
      "Iter 25 -> sub iter 99 : 68.25396825396825\n",
      "Iteration: 26\n",
      "Train accuracy: 68.53174603174604\n",
      "Val accuracy: 68.14285714285714\n",
      "Iter 26 -> sub iter 99 : 68.73015873015873\n",
      "Iteration: 27\n",
      "Train accuracy: 68.77301587301588\n",
      "Val accuracy: 68.27142857142857\n",
      "Iter 27 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 28\n",
      "Train accuracy: 68.98412698412699\n",
      "Val accuracy: 68.51428571428572\n",
      "Iter 28 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 29\n",
      "Train accuracy: 69.19047619047619\n",
      "Val accuracy: 68.77142857142857\n",
      "Iter 29 -> sub iter 99 : 69.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 69.39047619047619\n",
      "Val accuracy: 68.94285714285714\n",
      "Iter 30 -> sub iter 99 : 69.52380952380952\n",
      "Iteration: 31\n",
      "Train accuracy: 69.55238095238096\n",
      "Val accuracy: 69.07142857142857\n",
      "Iter 31 -> sub iter 99 : 70.15873015873015\n",
      "Iteration: 32\n",
      "Train accuracy: 69.72857142857143\n",
      "Val accuracy: 69.22857142857143\n",
      "Iter 32 -> sub iter 99 : 70.31746031746032\n",
      "Iteration: 33\n",
      "Train accuracy: 69.92063492063491\n",
      "Val accuracy: 69.32857142857142\n",
      "Iter 33 -> sub iter 99 : 70.47619047619048\n",
      "Iteration: 34\n",
      "Train accuracy: 70.0904761904762\n",
      "Val accuracy: 69.44285714285714\n",
      "Iter 34 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 35\n",
      "Train accuracy: 70.22380952380952\n",
      "Val accuracy: 69.69999999999999\n",
      "Iter 35 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 36\n",
      "Train accuracy: 70.36984126984127\n",
      "Val accuracy: 69.88571428571429\n",
      "Iter 36 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 37\n",
      "Train accuracy: 70.5\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 37 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 38\n",
      "Train accuracy: 70.63650793650794\n",
      "Val accuracy: 70.02857142857142\n",
      "Iter 38 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 39\n",
      "Train accuracy: 70.74126984126984\n",
      "Val accuracy: 70.11428571428571\n",
      "Iter 39 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 40\n",
      "Train accuracy: 70.84920634920636\n",
      "Val accuracy: 70.15714285714286\n",
      "Iter 40 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 41\n",
      "Train accuracy: 70.92539682539683\n",
      "Val accuracy: 70.3\n",
      "Iter 41 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 42\n",
      "Train accuracy: 71.02539682539683\n",
      "Val accuracy: 70.34285714285714\n",
      "Iter 42 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 43\n",
      "Train accuracy: 71.13650793650793\n",
      "Val accuracy: 70.39999999999999\n",
      "Iter 43 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 44\n",
      "Train accuracy: 71.22222222222221\n",
      "Val accuracy: 70.48571428571428\n",
      "Iter 44 -> sub iter 99 : 71.26984126984127\n",
      "Iteration: 45\n",
      "Train accuracy: 71.28730158730158\n",
      "Val accuracy: 70.55714285714285\n",
      "Iter 45 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 46\n",
      "Train accuracy: 71.36984126984127\n",
      "Val accuracy: 70.71428571428572\n",
      "Iter 46 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 47\n",
      "Train accuracy: 71.44603174603175\n",
      "Val accuracy: 70.84285714285714\n",
      "Iter 47 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 48\n",
      "Train accuracy: 71.5047619047619\n",
      "Val accuracy: 70.94285714285714\n",
      "Iter 48 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 49\n",
      "Train accuracy: 71.56825396825397\n",
      "Val accuracy: 70.98571428571428\n",
      "Iter 49 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 50\n",
      "Train accuracy: 71.615873015873\n",
      "Val accuracy: 71.02857142857142\n",
      "Iter 50 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 51\n",
      "Train accuracy: 71.67301587301587\n",
      "Val accuracy: 71.11428571428571\n",
      "Iter 51 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 52\n",
      "Train accuracy: 71.73650793650793\n",
      "Val accuracy: 71.21428571428572\n",
      "Iter 52 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 53\n",
      "Train accuracy: 71.8047619047619\n",
      "Val accuracy: 71.25714285714285\n",
      "Iter 53 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 54\n",
      "Train accuracy: 71.88253968253969\n",
      "Val accuracy: 71.28571428571429\n",
      "Iter 54 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 55\n",
      "Train accuracy: 71.96984126984127\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 55 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 72.04126984126984\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 56 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 57\n",
      "Train accuracy: 72.0920634920635\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 57 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 58\n",
      "Train accuracy: 72.16984126984127\n",
      "Val accuracy: 71.41428571428573\n",
      "Iter 58 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 59\n",
      "Train accuracy: 72.22698412698414\n",
      "Val accuracy: 71.45714285714286\n",
      "Iter 59 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 60\n",
      "Train accuracy: 72.27936507936508\n",
      "Val accuracy: 71.55714285714285\n",
      "Iter 60 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 61\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 71.64285714285714\n",
      "Iter 61 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 62\n",
      "Train accuracy: 72.38888888888889\n",
      "Val accuracy: 71.68571428571428\n",
      "Iter 62 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 63\n",
      "Train accuracy: 72.45238095238096\n",
      "Val accuracy: 71.7\n",
      "Iter 63 -> sub iter 99 : 72.69841269841276\n",
      "Iteration: 64\n",
      "Train accuracy: 72.52222222222223\n",
      "Val accuracy: 71.75714285714285\n",
      "Iter 64 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 65\n",
      "Train accuracy: 72.59523809523809\n",
      "Val accuracy: 71.77142857142857\n",
      "Iter 65 -> sub iter 99 : 72.69841269841278\n",
      "Iteration: 66\n",
      "Train accuracy: 72.65079365079366\n",
      "Val accuracy: 71.82857142857144\n",
      "Iter 66 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 67\n",
      "Train accuracy: 72.73650793650793\n",
      "Val accuracy: 71.97142857142858\n",
      "Iter 67 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 68\n",
      "Train accuracy: 73.05396825396825\n",
      "Val accuracy: 72.27142857142857\n",
      "Iter 68 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 69\n",
      "Train accuracy: 74.32857142857144\n",
      "Val accuracy: 73.97142857142858\n",
      "Iter 69 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 70\n",
      "Train accuracy: 76.29206349206349\n",
      "Val accuracy: 75.61428571428571\n",
      "Iter 70 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 71\n",
      "Train accuracy: 77.46507936507938\n",
      "Val accuracy: 76.81428571428572\n",
      "Iter 71 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 72\n",
      "Train accuracy: 78.24126984126984\n",
      "Val accuracy: 77.54285714285714\n",
      "Iter 72 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 73\n",
      "Train accuracy: 78.8\n",
      "Val accuracy: 77.95714285714286\n",
      "Iter 73 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 74\n",
      "Train accuracy: 79.25555555555556\n",
      "Val accuracy: 78.38571428571429\n",
      "Iter 74 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 75\n",
      "Train accuracy: 79.56031746031746\n",
      "Val accuracy: 78.7\n",
      "Iter 75 -> sub iter 99 : 80.95238095238095\n",
      "Iteration: 76\n",
      "Train accuracy: 79.81428571428572\n",
      "Val accuracy: 78.95714285714286\n",
      "Iter 76 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 77\n",
      "Train accuracy: 79.98571428571428\n",
      "Val accuracy: 79.11428571428571\n",
      "Iter 77 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 78\n",
      "Train accuracy: 80.18571428571428\n",
      "Val accuracy: 79.34285714285714\n",
      "Iter 78 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 79\n",
      "Train accuracy: 80.33015873015873\n",
      "Val accuracy: 79.48571428571428\n",
      "Iter 79 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 80\n",
      "Train accuracy: 80.43492063492064\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 80 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 81\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 79.72857142857143\n",
      "Iter 81 -> sub iter 99 : 81.58730158730158\n",
      "Iteration: 82\n",
      "Train accuracy: 80.71428571428572\n",
      "Val accuracy: 79.82857142857142\n",
      "Iter 82 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 83\n",
      "Train accuracy: 80.80952380952381\n",
      "Val accuracy: 79.97142857142858\n",
      "Iter 83 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 84\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.07142857142857\n",
      "Iter 84 -> sub iter 99 : 81.90476190476199\n",
      "Iteration: 85\n",
      "Train accuracy: 81.0047619047619\n",
      "Val accuracy: 80.17142857142858\n",
      "Iter 85 -> sub iter 99 : 81.90476190476194\n",
      "Iteration: 86\n",
      "Train accuracy: 81.10952380952381\n",
      "Val accuracy: 80.30000000000001\n",
      "Iter 86 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 87\n",
      "Train accuracy: 81.17460317460318\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 87 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 88\n",
      "Train accuracy: 81.26031746031745\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 81.33968253968254\n",
      "Val accuracy: 80.47142857142858\n",
      "Iter 89 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 90\n",
      "Train accuracy: 81.41428571428571\n",
      "Val accuracy: 80.51428571428572\n",
      "Iter 90 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 91\n",
      "Train accuracy: 81.48571428571428\n",
      "Val accuracy: 80.54285714285714\n",
      "Iter 91 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 92\n",
      "Train accuracy: 81.54444444444444\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 92 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 93\n",
      "Train accuracy: 81.6047619047619\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 93 -> sub iter 99 : 82.69841269841272\n",
      "Iteration: 94\n",
      "Train accuracy: 81.66984126984127\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 94 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 95\n",
      "Train accuracy: 81.72539682539683\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 81.78730158730158\n",
      "Val accuracy: 80.77142857142857\n",
      "Iter 96 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 97\n",
      "Train accuracy: 81.83809523809525\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 97 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 98\n",
      "Train accuracy: 81.87619047619049\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 98 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 99\n",
      "Train accuracy: 81.93333333333334\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 99 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 100\n",
      "Train accuracy: 81.99047619047619\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.02063492063492\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.05873015873016\n",
      "Val accuracy: 81.0\n",
      "Iter 102 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 103\n",
      "Train accuracy: 82.11746031746033\n",
      "Val accuracy: 81.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 104\n",
      "Train accuracy: 82.15714285714286\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 104 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 105\n",
      "Train accuracy: 82.1968253968254\n",
      "Val accuracy: 81.11428571428571\n",
      "Iter 105 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 106\n",
      "Train accuracy: 82.24285714285713\n",
      "Val accuracy: 81.15714285714286\n",
      "Iter 106 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 107\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 107 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 108\n",
      "Train accuracy: 82.32857142857142\n",
      "Val accuracy: 81.17142857142858\n",
      "Iter 108 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 109\n",
      "Train accuracy: 82.37619047619049\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 109 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 110\n",
      "Train accuracy: 82.41746031746032\n",
      "Val accuracy: 81.25714285714287\n",
      "Iter 110 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 111\n",
      "Train accuracy: 82.46190476190476\n",
      "Val accuracy: 81.28571428571428\n",
      "Iter 111 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 112\n",
      "Train accuracy: 82.4952380952381\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 112 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 113\n",
      "Train accuracy: 82.53015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 113 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 114\n",
      "Train accuracy: 82.55396825396826\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 114 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 115\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 115 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 116\n",
      "Train accuracy: 82.63174603174603\n",
      "Val accuracy: 81.39999999999999\n",
      "Iter 116 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 117\n",
      "Train accuracy: 82.67142857142858\n",
      "Val accuracy: 81.42857142857143\n",
      "Iter 117 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 118\n",
      "Train accuracy: 82.7031746031746\n",
      "Val accuracy: 81.47142857142858\n",
      "Iter 118 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 119\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 119 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 120\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 120 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 121\n",
      "Train accuracy: 82.77460317460317\n",
      "Val accuracy: 81.57142857142857\n",
      "Iter 121 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 122\n",
      "Train accuracy: 82.79841269841269\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 122 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 123\n",
      "Train accuracy: 82.82380952380952\n",
      "Val accuracy: 81.6\n",
      "Iter 123 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 124\n",
      "Train accuracy: 82.86825396825397\n",
      "Val accuracy: 81.61428571428571\n",
      "Iter 124 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 125\n",
      "Train accuracy: 82.88412698412698\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 125 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 126\n",
      "Train accuracy: 82.91111111111111\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 126 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 127\n",
      "Train accuracy: 82.93174603174603\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 127 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 128\n",
      "Train accuracy: 82.94920634920635\n",
      "Val accuracy: 81.65714285714286\n",
      "Iter 128 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 129\n",
      "Train accuracy: 82.98253968253968\n",
      "Val accuracy: 81.68571428571428\n",
      "Iter 129 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 130\n",
      "Train accuracy: 83.01746031746032\n",
      "Val accuracy: 81.67142857142858\n",
      "Iter 130 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 131\n",
      "Train accuracy: 83.03968253968253\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 131 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 132\n",
      "Train accuracy: 83.06825396825397\n",
      "Val accuracy: 81.72857142857143\n",
      "Iter 132 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 133\n",
      "Train accuracy: 83.0952380952381\n",
      "Val accuracy: 81.75714285714287\n",
      "Iter 133 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 134\n",
      "Train accuracy: 83.12698412698413\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 134 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 135\n",
      "Train accuracy: 83.15714285714286\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 135 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 136\n",
      "Train accuracy: 83.18412698412698\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 136 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 137\n",
      "Train accuracy: 83.1984126984127\n",
      "Val accuracy: 81.82857142857142\n",
      "Iter 137 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 138\n",
      "Train accuracy: 83.21904761904761\n",
      "Val accuracy: 81.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 139\n",
      "Train accuracy: 83.23968253968253\n",
      "Val accuracy: 81.92857142857143\n",
      "Iter 139 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 140\n",
      "Train accuracy: 83.27460317460319\n",
      "Val accuracy: 81.97142857142858\n",
      "Iter 140 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 141\n",
      "Train accuracy: 83.3015873015873\n",
      "Val accuracy: 82.04285714285714\n",
      "Iter 141 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 142\n",
      "Train accuracy: 83.32698412698413\n",
      "Val accuracy: 82.07142857142857\n",
      "Iter 142 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 143\n",
      "Train accuracy: 83.34444444444445\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 143 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 144\n",
      "Train accuracy: 83.36031746031746\n",
      "Val accuracy: 82.17142857142858\n",
      "Iter 144 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 145\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 145 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 146\n",
      "Train accuracy: 83.40793650793651\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 146 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 147\n",
      "Train accuracy: 83.43015873015874\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 147 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 148\n",
      "Train accuracy: 83.45555555555556\n",
      "Val accuracy: 82.22857142857143\n",
      "Iter 148 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 149\n",
      "Train accuracy: 83.47777777777777\n",
      "Val accuracy: 82.25714285714287\n",
      "Iter 149 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 150\n",
      "Train accuracy: 83.4920634920635\n",
      "Val accuracy: 82.27142857142857\n",
      "Training for 0.0001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 21.904761904761905\n",
      "Iteration: 1\n",
      "Train accuracy: 22.957142857142856\n",
      "Val accuracy: 24.22857142857143\n",
      "Iter 1 -> sub iter 99 : 27.777777777777785\n",
      "Iteration: 2\n",
      "Train accuracy: 29.887301587301586\n",
      "Val accuracy: 31.157142857142855\n",
      "Iter 2 -> sub iter 99 : 31.904761904761975\n",
      "Iteration: 3\n",
      "Train accuracy: 34.33968253968254\n",
      "Val accuracy: 35.65714285714286\n",
      "Iter 3 -> sub iter 99 : 38.571428571428584\n",
      "Iteration: 4\n",
      "Train accuracy: 39.353968253968254\n",
      "Val accuracy: 40.87142857142857\n",
      "Iter 4 -> sub iter 99 : 41.111111111111116\n",
      "Iteration: 5\n",
      "Train accuracy: 42.35714285714286\n",
      "Val accuracy: 43.9\n",
      "Iter 5 -> sub iter 99 : 42.539682539682546\n",
      "Iteration: 6\n",
      "Train accuracy: 44.34603174603174\n",
      "Val accuracy: 46.1\n",
      "Iter 6 -> sub iter 99 : 44.603174603174614\n",
      "Iteration: 7\n",
      "Train accuracy: 45.76190476190476\n",
      "Val accuracy: 47.599999999999994\n",
      "Iter 7 -> sub iter 99 : 46.031746031746034\n",
      "Iteration: 8\n",
      "Train accuracy: 46.68571428571428\n",
      "Val accuracy: 48.22857142857143\n",
      "Iter 8 -> sub iter 99 : 46.825396825396824\n",
      "Iteration: 9\n",
      "Train accuracy: 47.41746031746032\n",
      "Val accuracy: 48.871428571428574\n",
      "Iter 9 -> sub iter 99 : 48.095238095238095\n",
      "Iteration: 10\n",
      "Train accuracy: 48.03809523809524\n",
      "Val accuracy: 49.542857142857144\n",
      "Iter 10 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 11\n",
      "Train accuracy: 48.53174603174603\n",
      "Val accuracy: 49.81428571428572\n",
      "Iter 11 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 12\n",
      "Train accuracy: 48.94761904761904\n",
      "Val accuracy: 50.24285714285715\n",
      "Iter 12 -> sub iter 99 : 48.571428571428575\n",
      "Iteration: 13\n",
      "Train accuracy: 49.32857142857143\n",
      "Val accuracy: 50.5\n",
      "Iter 13 -> sub iter 99 : 49.206349206349296\n",
      "Iteration: 14\n",
      "Train accuracy: 49.71111111111111\n",
      "Val accuracy: 50.91428571428571\n",
      "Iter 14 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 15\n",
      "Train accuracy: 50.00476190476191\n",
      "Val accuracy: 51.28571428571429\n",
      "Iter 15 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 16\n",
      "Train accuracy: 50.3111111111111\n",
      "Val accuracy: 51.4\n",
      "Iter 16 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 17\n",
      "Train accuracy: 50.56349206349206\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 17 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 18\n",
      "Train accuracy: 50.7984126984127\n",
      "Val accuracy: 52.028571428571425\n",
      "Iter 18 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 19\n",
      "Train accuracy: 51.01269841269841\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 19 -> sub iter 99 : 50.158730158730165\n",
      "Iteration: 20\n",
      "Train accuracy: 51.217460317460315\n",
      "Val accuracy: 52.51428571428571\n",
      "Iter 20 -> sub iter 99 : 50.158730158730164\n",
      "Iteration: 21\n",
      "Train accuracy: 51.42539682539683\n",
      "Val accuracy: 52.65714285714286\n",
      "Iter 21 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 22\n",
      "Train accuracy: 51.65238095238095\n",
      "Val accuracy: 52.87142857142857\n",
      "Iter 22 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 23\n",
      "Train accuracy: 51.866666666666674\n",
      "Val accuracy: 53.128571428571426\n",
      "Iter 23 -> sub iter 99 : 50.952380952380956\n",
      "Iteration: 24\n",
      "Train accuracy: 52.05714285714286\n",
      "Val accuracy: 53.38571428571428\n",
      "Iter 24 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 25\n",
      "Train accuracy: 52.23174603174603\n",
      "Val accuracy: 53.57142857142857\n",
      "Iter 25 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 26\n",
      "Train accuracy: 52.38095238095239\n",
      "Val accuracy: 53.7\n",
      "Iter 26 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 27\n",
      "Train accuracy: 52.549206349206344\n",
      "Val accuracy: 53.800000000000004\n",
      "Iter 27 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 28\n",
      "Train accuracy: 52.7079365079365\n",
      "Val accuracy: 54.0\n",
      "Iter 28 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 29\n",
      "Train accuracy: 52.84920634920635\n",
      "Val accuracy: 54.142857142857146\n",
      "Iter 29 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 30\n",
      "Train accuracy: 52.957142857142856\n",
      "Val accuracy: 54.27142857142857\n",
      "Iter 30 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 31\n",
      "Train accuracy: 53.07777777777778\n",
      "Val accuracy: 54.371428571428574\n",
      "Iter 31 -> sub iter 99 : 52.222222222222234\n",
      "Iteration: 32\n",
      "Train accuracy: 53.20952380952381\n",
      "Val accuracy: 54.51428571428571\n",
      "Iter 32 -> sub iter 99 : 52.222222222222235\n",
      "Iteration: 33\n",
      "Train accuracy: 53.31428571428572\n",
      "Val accuracy: 54.65714285714286\n",
      "Iter 33 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 34\n",
      "Train accuracy: 53.42857142857142\n",
      "Val accuracy: 54.75714285714286\n",
      "Iter 34 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 35\n",
      "Train accuracy: 53.51904761904762\n",
      "Val accuracy: 54.95714285714286\n",
      "Iter 35 -> sub iter 99 : 52.698412698412785\n",
      "Iteration: 36\n",
      "Train accuracy: 53.642857142857146\n",
      "Val accuracy: 55.01428571428571\n",
      "Iter 36 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 37\n",
      "Train accuracy: 53.73015873015873\n",
      "Val accuracy: 55.08571428571428\n",
      "Iter 37 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 38\n",
      "Train accuracy: 53.78571428571428\n",
      "Val accuracy: 55.22857142857143\n",
      "Iter 38 -> sub iter 99 : 52.857142857142865\n",
      "Iteration: 39\n",
      "Train accuracy: 53.86825396825397\n",
      "Val accuracy: 55.371428571428574\n",
      "Iter 39 -> sub iter 99 : 52.857142857142866\n",
      "Iteration: 40\n",
      "Train accuracy: 53.955555555555556\n",
      "Val accuracy: 55.385714285714286\n",
      "Iter 40 -> sub iter 99 : 53.174603174603184\n",
      "Iteration: 41\n",
      "Train accuracy: 54.03174603174603\n",
      "Val accuracy: 55.471428571428575\n",
      "Iter 41 -> sub iter 99 : 53.174603174603186\n",
      "Iteration: 42\n",
      "Train accuracy: 54.1\n",
      "Val accuracy: 55.58571428571428\n",
      "Iter 42 -> sub iter 99 : 53.492063492063494\n",
      "Iteration: 43\n",
      "Train accuracy: 54.179365079365084\n",
      "Val accuracy: 55.68571428571428\n",
      "Iter 43 -> sub iter 99 : 53.650793650793654\n",
      "Iteration: 44\n",
      "Train accuracy: 54.250793650793646\n",
      "Val accuracy: 55.74285714285714\n",
      "Iter 44 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 45\n",
      "Train accuracy: 54.31587301587302\n",
      "Val accuracy: 55.84285714285714\n",
      "Iter 45 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 46\n",
      "Train accuracy: 54.37777777777778\n",
      "Val accuracy: 55.871428571428574\n",
      "Iter 46 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 47\n",
      "Train accuracy: 54.44126984126984\n",
      "Val accuracy: 55.91428571428572\n",
      "Iter 47 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 48\n",
      "Train accuracy: 54.50000000000001\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 48 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 49\n",
      "Train accuracy: 54.541269841269845\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 49 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 50\n",
      "Train accuracy: 54.6063492063492\n",
      "Val accuracy: 56.128571428571426\n",
      "Iter 50 -> sub iter 99 : 53.968253968253976\n",
      "Iteration: 51\n",
      "Train accuracy: 54.646031746031746\n",
      "Val accuracy: 56.15714285714286\n",
      "Iter 51 -> sub iter 99 : 54.126984126984136\n",
      "Iteration: 52\n",
      "Train accuracy: 54.68730158730158\n",
      "Val accuracy: 56.214285714285715\n",
      "Iter 52 -> sub iter 99 : 54.285714285714285\n",
      "Iteration: 53\n",
      "Train accuracy: 54.72380952380952\n",
      "Val accuracy: 56.27142857142857\n",
      "Iter 53 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 54\n",
      "Train accuracy: 54.75714285714286\n",
      "Val accuracy: 56.34285714285714\n",
      "Iter 54 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 55\n",
      "Train accuracy: 54.801587301587304\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 55 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 56\n",
      "Train accuracy: 54.82857142857143\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 56 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 57\n",
      "Train accuracy: 54.87301587301587\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 57 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 58\n",
      "Train accuracy: 54.919047619047625\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 58 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 59\n",
      "Train accuracy: 54.96507936507936\n",
      "Val accuracy: 56.442857142857136\n",
      "Iter 59 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 60\n",
      "Train accuracy: 55.00158730158731\n",
      "Val accuracy: 56.49999999999999\n",
      "Iter 60 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 61\n",
      "Train accuracy: 55.05238095238095\n",
      "Val accuracy: 56.51428571428572\n",
      "Iter 61 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 62\n",
      "Train accuracy: 55.093650793650795\n",
      "Val accuracy: 56.528571428571425\n",
      "Iter 62 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 63\n",
      "Train accuracy: 55.141269841269846\n",
      "Val accuracy: 56.557142857142864\n",
      "Iter 63 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 64\n",
      "Train accuracy: 55.18253968253968\n",
      "Val accuracy: 56.58571428571428\n",
      "Iter 64 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 65\n",
      "Train accuracy: 55.21904761904762\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 65 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 66\n",
      "Train accuracy: 55.24444444444444\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 66 -> sub iter 99 : 54.920634920634924\n",
      "Iteration: 67\n",
      "Train accuracy: 55.269841269841265\n",
      "Val accuracy: 56.67142857142857\n",
      "Iter 67 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 68\n",
      "Train accuracy: 55.304761904761904\n",
      "Val accuracy: 56.714285714285715\n",
      "Iter 68 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 69\n",
      "Train accuracy: 55.33968253968254\n",
      "Val accuracy: 56.74285714285714\n",
      "Iter 69 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 70\n",
      "Train accuracy: 55.37460317460317\n",
      "Val accuracy: 56.81428571428572\n",
      "Iter 70 -> sub iter 99 : 55.238095238095244\n",
      "Iteration: 71\n",
      "Train accuracy: 55.4031746031746\n",
      "Val accuracy: 56.84285714285714\n",
      "Iter 71 -> sub iter 99 : 55.396825396825435\n",
      "Iteration: 72\n",
      "Train accuracy: 55.43492063492064\n",
      "Val accuracy: 56.871428571428574\n",
      "Iter 72 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 73\n",
      "Train accuracy: 55.474603174603175\n",
      "Val accuracy: 56.89999999999999\n",
      "Iter 73 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 74\n",
      "Train accuracy: 55.4952380952381\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 74 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 75\n",
      "Train accuracy: 55.52222222222222\n",
      "Val accuracy: 56.92857142857143\n",
      "Iter 75 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 76\n",
      "Train accuracy: 55.55238095238095\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 76 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 77\n",
      "Train accuracy: 55.574603174603176\n",
      "Val accuracy: 56.98571428571428\n",
      "Iter 77 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 78\n",
      "Train accuracy: 55.6063492063492\n",
      "Val accuracy: 56.99999999999999\n",
      "Iter 78 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 79\n",
      "Train accuracy: 55.62222222222222\n",
      "Val accuracy: 57.028571428571425\n",
      "Iter 79 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 80\n",
      "Train accuracy: 55.63809523809524\n",
      "Val accuracy: 57.04285714285714\n",
      "Iter 80 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 81\n",
      "Train accuracy: 55.65714285714286\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 81 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 82\n",
      "Train accuracy: 55.68730158730158\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 82 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 83\n",
      "Train accuracy: 55.70952380952381\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 83 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 84\n",
      "Train accuracy: 55.72380952380952\n",
      "Val accuracy: 57.15714285714286\n",
      "Iter 84 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 85\n",
      "Train accuracy: 55.73968253968255\n",
      "Val accuracy: 57.17142857142857\n",
      "Iter 85 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 86\n",
      "Train accuracy: 55.76190476190476\n",
      "Val accuracy: 57.18571428571428\n",
      "Iter 86 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 87\n",
      "Train accuracy: 55.78412698412698\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 87 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 88\n",
      "Train accuracy: 55.7952380952381\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 88 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 89\n",
      "Train accuracy: 55.80952380952381\n",
      "Val accuracy: 57.24285714285714\n",
      "Iter 89 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 90\n",
      "Train accuracy: 55.822222222222216\n",
      "Val accuracy: 57.25714285714286\n",
      "Iter 90 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 91\n",
      "Train accuracy: 55.83968253968254\n",
      "Val accuracy: 57.27142857142857\n",
      "Iter 91 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 92\n",
      "Train accuracy: 55.85555555555556\n",
      "Val accuracy: 57.285714285714285\n",
      "Iter 92 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 93\n",
      "Train accuracy: 55.87777777777778\n",
      "Val accuracy: 57.3\n",
      "Iter 93 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 94\n",
      "Train accuracy: 55.888888888888886\n",
      "Val accuracy: 57.3\n",
      "Iter 94 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 95\n",
      "Train accuracy: 55.907936507936505\n",
      "Val accuracy: 57.32857142857143\n",
      "Iter 95 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 96\n",
      "Train accuracy: 55.919047619047625\n",
      "Val accuracy: 57.371428571428574\n",
      "Iter 96 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 97\n",
      "Train accuracy: 55.93015873015873\n",
      "Val accuracy: 57.385714285714286\n",
      "Iter 97 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 98\n",
      "Train accuracy: 55.941269841269836\n",
      "Val accuracy: 57.44285714285714\n",
      "Iter 98 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 99\n",
      "Train accuracy: 55.94920634920635\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 99 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 100\n",
      "Train accuracy: 55.96666666666666\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 100 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 101\n",
      "Train accuracy: 55.98888888888889\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 101 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 102\n",
      "Train accuracy: 56.007936507936506\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 102 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 103\n",
      "Train accuracy: 56.019047619047626\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 103 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 104\n",
      "Train accuracy: 56.03174603174603\n",
      "Val accuracy: 57.49999999999999\n",
      "Iter 104 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 105\n",
      "Train accuracy: 56.046031746031744\n",
      "Val accuracy: 57.51428571428572\n",
      "Iter 105 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 106\n",
      "Train accuracy: 56.06666666666666\n",
      "Val accuracy: 57.54285714285714\n",
      "Iter 106 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 107\n",
      "Train accuracy: 56.092063492063495\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 107 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 108\n",
      "Train accuracy: 56.1031746031746\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 108 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 109\n",
      "Train accuracy: 56.12380952380952\n",
      "Val accuracy: 57.57142857142858\n",
      "Iter 109 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 110\n",
      "Train accuracy: 56.14285714285714\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 110 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 111\n",
      "Train accuracy: 56.15555555555556\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 111 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 112\n",
      "Train accuracy: 56.18253968253968\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 112 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 113\n",
      "Train accuracy: 56.19047619047619\n",
      "Val accuracy: 57.61428571428572\n",
      "Iter 113 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 114\n",
      "Train accuracy: 56.2047619047619\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 114 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 115\n",
      "Train accuracy: 56.21587301587302\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 115 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 116\n",
      "Train accuracy: 56.23650793650794\n",
      "Val accuracy: 57.657142857142865\n",
      "Iter 116 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 117\n",
      "Train accuracy: 56.25238095238095\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 117 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 118\n",
      "Train accuracy: 56.268253968253966\n",
      "Val accuracy: 57.68571428571428\n",
      "Iter 118 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 119\n",
      "Train accuracy: 56.29206349206349\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 119 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 120\n",
      "Train accuracy: 56.3031746031746\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 120 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 121\n",
      "Train accuracy: 56.31587301587302\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 121 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 122\n",
      "Train accuracy: 56.33015873015873\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 122 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 123\n",
      "Train accuracy: 56.34603174603174\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 123 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 124\n",
      "Train accuracy: 56.353968253968254\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 124 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 125\n",
      "Train accuracy: 56.35714285714286\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 125 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 126\n",
      "Train accuracy: 56.36190476190476\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 126 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 127\n",
      "Train accuracy: 56.371428571428574\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 127 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 128\n",
      "Train accuracy: 56.38253968253968\n",
      "Val accuracy: 57.74285714285714\n",
      "Iter 128 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 129\n",
      "Train accuracy: 56.3968253968254\n",
      "Val accuracy: 57.77142857142857\n",
      "Iter 129 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 130\n",
      "Train accuracy: 56.4015873015873\n",
      "Val accuracy: 57.785714285714285\n",
      "Iter 130 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 131\n",
      "Train accuracy: 56.41111111111111\n",
      "Val accuracy: 57.8\n",
      "Iter 131 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 132\n",
      "Train accuracy: 56.423809523809524\n",
      "Val accuracy: 57.8\n",
      "Iter 132 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 133\n",
      "Train accuracy: 56.426984126984124\n",
      "Val accuracy: 57.8\n",
      "Iter 133 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 134\n",
      "Train accuracy: 56.442857142857136\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 134 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 135\n",
      "Train accuracy: 56.44603174603174\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 135 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 136\n",
      "Train accuracy: 56.45873015873016\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 136 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 137\n",
      "Train accuracy: 56.461904761904755\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 137 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 138\n",
      "Train accuracy: 56.46984126984127\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 138 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 139\n",
      "Train accuracy: 56.48253968253968\n",
      "Val accuracy: 57.84285714285714\n",
      "Iter 139 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 140\n",
      "Train accuracy: 56.48412698412698\n",
      "Val accuracy: 57.871428571428574\n",
      "Iter 140 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 141\n",
      "Train accuracy: 56.48888888888889\n",
      "Val accuracy: 57.885714285714286\n",
      "Iter 141 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 142\n",
      "Train accuracy: 56.5031746031746\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 142 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 143\n",
      "Train accuracy: 56.50952380952381\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 143 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 144\n",
      "Train accuracy: 56.51587301587302\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 144 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 145\n",
      "Train accuracy: 56.52063492063492\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 145 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 146\n",
      "Train accuracy: 56.526984126984125\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 146 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 147\n",
      "Train accuracy: 56.528571428571425\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 147 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 148\n",
      "Train accuracy: 56.53650793650794\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 148 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 149\n",
      "Train accuracy: 56.54920634920635\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 149 -> sub iter 99 : 55.873015873015875\n",
      "Iteration: 150\n",
      "Train accuracy: 56.56031746031746\n",
      "Val accuracy: 57.92857142857143\n"
     ]
    }
   ],
   "source": [
    "for pert in pertList:\n",
    "    print(f\"Training for {pert}\")\n",
    "    W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.01, pert, 1)\n",
    "    trainAccPertList.append(train_acc)\n",
    "    valAccPertList.append(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 81.73650793650793\n",
      "Val accuracy: 81.02857142857142\n",
      "Iteration: 2\n",
      "Train accuracy: 86.43174603174603\n",
      "Val accuracy: 85.91428571428571\n",
      "Iteration: 3\n",
      "Train accuracy: 88.5984126984127\n",
      "Val accuracy: 88.18571428571428\n",
      "Iteration: 4\n",
      "Train accuracy: 89.78571428571429\n",
      "Val accuracy: 89.44285714285715\n",
      "Iteration: 5\n",
      "Train accuracy: 90.70158730158731\n",
      "Val accuracy: 90.21428571428571\n",
      "Iteration: 6\n",
      "Train accuracy: 91.39206349206349\n",
      "Val accuracy: 90.72857142857143\n",
      "Iteration: 7\n",
      "Train accuracy: 91.88730158730158\n",
      "Val accuracy: 91.15714285714286\n",
      "Iteration: 8\n",
      "Train accuracy: 92.33650793650794\n",
      "Val accuracy: 91.45714285714286\n",
      "Iteration: 9\n",
      "Train accuracy: 92.70793650793651\n",
      "Val accuracy: 91.75714285714285\n",
      "Iteration: 10\n",
      "Train accuracy: 93.04126984126984\n",
      "Val accuracy: 92.08571428571429\n",
      "Iteration: 11\n",
      "Train accuracy: 93.31746031746032\n",
      "Val accuracy: 92.41428571428571\n",
      "Iteration: 12\n",
      "Train accuracy: 93.57460317460318\n",
      "Val accuracy: 92.55714285714286\n",
      "Iteration: 13\n",
      "Train accuracy: 93.7984126984127\n",
      "Val accuracy: 92.84285714285714\n",
      "Iteration: 14\n",
      "Train accuracy: 94.04285714285714\n",
      "Val accuracy: 93.02857142857142\n",
      "Iteration: 15\n",
      "Train accuracy: 94.20952380952382\n",
      "Val accuracy: 93.10000000000001\n",
      "Iteration: 16\n",
      "Train accuracy: 94.3984126984127\n",
      "Val accuracy: 93.30000000000001\n",
      "Iteration: 17\n",
      "Train accuracy: 94.57619047619048\n",
      "Val accuracy: 93.5\n",
      "Iteration: 18\n",
      "Train accuracy: 94.70952380952382\n",
      "Val accuracy: 93.61428571428571\n",
      "Iteration: 19\n",
      "Train accuracy: 94.86507936507937\n",
      "Val accuracy: 93.71428571428572\n",
      "Iteration: 20\n",
      "Train accuracy: 95.01428571428572\n",
      "Val accuracy: 93.75714285714287\n",
      "Iteration: 21\n",
      "Train accuracy: 95.12063492063491\n",
      "Val accuracy: 93.92857142857143\n",
      "Iteration: 22\n",
      "Train accuracy: 95.23809523809523\n",
      "Val accuracy: 94.04285714285714\n",
      "Iteration: 23\n",
      "Train accuracy: 95.36190476190475\n",
      "Val accuracy: 94.18571428571428\n",
      "Iteration: 24\n",
      "Train accuracy: 95.46825396825398\n",
      "Val accuracy: 94.25714285714287\n",
      "Iteration: 25\n",
      "Train accuracy: 95.5936507936508\n",
      "Val accuracy: 94.32857142857142\n",
      "Iteration: 26\n",
      "Train accuracy: 95.72222222222221\n",
      "Val accuracy: 94.38571428571429\n",
      "Iteration: 27\n",
      "Train accuracy: 95.83015873015873\n",
      "Val accuracy: 94.45714285714286\n",
      "Iteration: 28\n",
      "Train accuracy: 95.93015873015874\n",
      "Val accuracy: 94.58571428571429\n",
      "Iteration: 29\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 30\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 31\n",
      "Train accuracy: 96.16031746031746\n",
      "Val accuracy: 94.65714285714286\n",
      "Iteration: 32\n",
      "Train accuracy: 96.23968253968253\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 33\n",
      "Train accuracy: 96.3031746031746\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 34\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 94.77142857142857\n",
      "Iteration: 35\n",
      "Train accuracy: 96.42380952380952\n",
      "Val accuracy: 94.8\n",
      "Iteration: 36\n",
      "Train accuracy: 96.48412698412699\n",
      "Val accuracy: 94.87142857142857\n",
      "Iteration: 37\n",
      "Train accuracy: 96.58412698412698\n",
      "Val accuracy: 94.85714285714286\n",
      "Iteration: 38\n",
      "Train accuracy: 96.65396825396826\n",
      "Val accuracy: 94.95714285714286\n",
      "Iteration: 39\n",
      "Train accuracy: 96.72222222222221\n",
      "Val accuracy: 94.94285714285714\n",
      "Iteration: 40\n",
      "Train accuracy: 96.75555555555555\n",
      "Val accuracy: 95.01428571428572\n",
      "Iteration: 41\n",
      "Train accuracy: 96.81904761904761\n",
      "Val accuracy: 95.07142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 96.88412698412698\n",
      "Val accuracy: 95.1\n",
      "Iteration: 43\n",
      "Train accuracy: 96.93650793650794\n",
      "Val accuracy: 95.11428571428571\n",
      "Iteration: 44\n",
      "Train accuracy: 97.0079365079365\n",
      "Val accuracy: 95.17142857142858\n",
      "Iteration: 45\n",
      "Train accuracy: 97.06190476190476\n",
      "Val accuracy: 95.15714285714286\n",
      "Iteration: 46\n",
      "Train accuracy: 97.12380952380953\n",
      "Val accuracy: 95.21428571428572\n",
      "Iteration: 47\n",
      "Train accuracy: 97.17619047619047\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 48\n",
      "Train accuracy: 97.22380952380952\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 49\n",
      "Train accuracy: 97.27460317460317\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 50\n",
      "Train accuracy: 97.31269841269842\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 51\n",
      "Train accuracy: 97.35079365079365\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 97.38095238095238\n",
      "Val accuracy: 95.3\n",
      "Iteration: 53\n",
      "Train accuracy: 97.42698412698412\n",
      "Val accuracy: 95.32857142857142\n",
      "Iteration: 54\n",
      "Train accuracy: 97.47142857142858\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 55\n",
      "Train accuracy: 97.5047619047619\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 56\n",
      "Train accuracy: 97.55079365079365\n",
      "Val accuracy: 95.31428571428572\n",
      "Iteration: 57\n",
      "Train accuracy: 97.6\n",
      "Val accuracy: 95.37142857142857\n",
      "Iteration: 58\n",
      "Train accuracy: 97.64761904761905\n",
      "Val accuracy: 95.38571428571429\n",
      "Iteration: 59\n",
      "Train accuracy: 97.68730158730159\n",
      "Val accuracy: 95.41428571428571\n",
      "Iteration: 60\n",
      "Train accuracy: 97.73174603174604\n",
      "Val accuracy: 95.42857142857143\n",
      "Iteration: 61\n",
      "Train accuracy: 97.76984126984128\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 62\n",
      "Train accuracy: 97.78730158730158\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 63\n",
      "Train accuracy: 97.82222222222222\n",
      "Val accuracy: 95.5\n",
      "Iteration: 64\n",
      "Train accuracy: 97.85238095238095\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 65\n",
      "Train accuracy: 97.87936507936507\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 66\n",
      "Train accuracy: 97.93015873015874\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 67\n",
      "Train accuracy: 97.96349206349207\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 68\n",
      "Train accuracy: 98.00634920634921\n",
      "Val accuracy: 95.48571428571428\n",
      "Iteration: 69\n",
      "Train accuracy: 98.03809523809524\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 70\n",
      "Train accuracy: 98.08571428571429\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 71\n",
      "Train accuracy: 98.12539682539682\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 72\n",
      "Train accuracy: 98.17142857142858\n",
      "Val accuracy: 95.67142857142858\n",
      "Iteration: 73\n",
      "Train accuracy: 98.1920634920635\n",
      "Val accuracy: 95.71428571428572\n",
      "Iteration: 74\n",
      "Train accuracy: 98.21587301587302\n",
      "Val accuracy: 95.7\n",
      "Iteration: 75\n",
      "Train accuracy: 98.25079365079365\n",
      "Val accuracy: 95.7\n",
      "Iteration: 76\n",
      "Train accuracy: 98.27619047619048\n",
      "Val accuracy: 95.7\n",
      "Iteration: 77\n",
      "Train accuracy: 98.30793650793652\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 78\n",
      "Train accuracy: 98.33333333333333\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 79\n",
      "Train accuracy: 98.35873015873015\n",
      "Val accuracy: 95.74285714285715\n",
      "Iteration: 80\n",
      "Train accuracy: 98.38571428571429\n",
      "Val accuracy: 95.75714285714285\n",
      "Iteration: 81\n",
      "Train accuracy: 98.40476190476191\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 82\n",
      "Train accuracy: 98.41904761904762\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 83\n",
      "Train accuracy: 98.43968253968254\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 84\n",
      "Train accuracy: 98.47142857142858\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 85\n",
      "Train accuracy: 98.4984126984127\n",
      "Val accuracy: 95.8\n",
      "Iteration: 86\n",
      "Train accuracy: 98.52380952380952\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 87\n",
      "Train accuracy: 98.55238095238094\n",
      "Val accuracy: 95.8\n",
      "Iteration: 88\n",
      "Train accuracy: 98.56666666666666\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 89\n",
      "Train accuracy: 98.58888888888889\n",
      "Val accuracy: 95.8\n",
      "Iteration: 90\n",
      "Train accuracy: 98.62222222222222\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 91\n",
      "Train accuracy: 98.64761904761905\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 92\n",
      "Train accuracy: 98.66666666666667\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 93\n",
      "Train accuracy: 98.67936507936508\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 94\n",
      "Train accuracy: 98.70476190476191\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 95\n",
      "Train accuracy: 98.72222222222223\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 96\n",
      "Train accuracy: 98.73333333333333\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 97\n",
      "Train accuracy: 98.74920634920635\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 98\n",
      "Train accuracy: 98.78412698412698\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 99\n",
      "Train accuracy: 98.79523809523809\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 100\n",
      "Train accuracy: 98.80793650793652\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 101\n",
      "Train accuracy: 98.84126984126983\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 102\n",
      "Train accuracy: 98.86984126984127\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 103\n",
      "Train accuracy: 98.88888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 104\n",
      "Train accuracy: 98.89682539682539\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 105\n",
      "Train accuracy: 98.91428571428571\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 106\n",
      "Train accuracy: 98.92380952380952\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 107\n",
      "Train accuracy: 98.94603174603175\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 108\n",
      "Train accuracy: 98.95873015873016\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 109\n",
      "Train accuracy: 98.97142857142858\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 110\n",
      "Train accuracy: 98.98095238095237\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 111\n",
      "Train accuracy: 99.0031746031746\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 112\n",
      "Train accuracy: 99.01269841269841\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 113\n",
      "Train accuracy: 99.02539682539683\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 114\n",
      "Train accuracy: 99.05079365079365\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 115\n",
      "Train accuracy: 99.06031746031746\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 116\n",
      "Train accuracy: 99.06666666666666\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 117\n",
      "Train accuracy: 99.08888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 118\n",
      "Train accuracy: 99.12222222222222\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 119\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 120\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 121\n",
      "Train accuracy: 99.16507936507936\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 122\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 123\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 124\n",
      "Train accuracy: 99.21904761904761\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 125\n",
      "Train accuracy: 99.21111111111112\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 126\n",
      "Train accuracy: 99.22857142857143\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 127\n",
      "Train accuracy: 99.25079365079365\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 128\n",
      "Train accuracy: 99.26031746031747\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 129\n",
      "Train accuracy: 99.27936507936508\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 130\n",
      "Train accuracy: 99.28095238095239\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 131\n",
      "Train accuracy: 99.30952380952381\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 132\n",
      "Train accuracy: 99.32857142857144\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 133\n",
      "Train accuracy: 99.33809523809524\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 134\n",
      "Train accuracy: 99.35238095238094\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 135\n",
      "Train accuracy: 99.35079365079366\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 136\n",
      "Train accuracy: 99.36507936507937\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 137\n",
      "Train accuracy: 99.37777777777778\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 138\n",
      "Train accuracy: 99.38412698412698\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 139\n",
      "Train accuracy: 99.39365079365079\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 140\n",
      "Train accuracy: 99.41587301587302\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 141\n",
      "Train accuracy: 99.42857142857143\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 142\n",
      "Train accuracy: 99.44603174603175\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 143\n",
      "Train accuracy: 99.46031746031746\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 144\n",
      "Train accuracy: 99.46825396825398\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 145\n",
      "Train accuracy: 99.47619047619047\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 146\n",
      "Train accuracy: 99.4904761904762\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 147\n",
      "Train accuracy: 99.4968253968254\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 148\n",
      "Train accuracy: 99.50952380952381\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 149\n",
      "Train accuracy: 99.51746031746032\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 150\n",
      "Train accuracy: 99.52857142857144\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 151\n",
      "Train accuracy: 99.53809523809524\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 152\n",
      "Train accuracy: 99.54126984126984\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 153\n",
      "Train accuracy: 99.55079365079365\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 154\n",
      "Train accuracy: 99.55238095238094\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 155\n",
      "Train accuracy: 99.56825396825397\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 156\n",
      "Train accuracy: 99.57777777777778\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 157\n",
      "Train accuracy: 99.58253968253969\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 158\n",
      "Train accuracy: 99.58571428571429\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 159\n",
      "Train accuracy: 99.6\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 160\n",
      "Train accuracy: 99.60793650793651\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 161\n",
      "Train accuracy: 99.615873015873\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 162\n",
      "Train accuracy: 99.62063492063493\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 163\n",
      "Train accuracy: 99.63492063492063\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 164\n",
      "Train accuracy: 99.63650793650794\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 165\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 166\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 167\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 168\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 169\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 170\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 171\n",
      "Train accuracy: 99.68095238095238\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 172\n",
      "Train accuracy: 99.68253968253968\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 173\n",
      "Train accuracy: 99.69047619047619\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 174\n",
      "Train accuracy: 99.69365079365079\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 175\n",
      "Train accuracy: 99.6984126984127\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 176\n",
      "Train accuracy: 99.70476190476191\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 177\n",
      "Train accuracy: 99.71269841269842\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 178\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 179\n",
      "Train accuracy: 99.72222222222223\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 180\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 181\n",
      "Train accuracy: 99.73174603174603\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 182\n",
      "Train accuracy: 99.73968253968253\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 183\n",
      "Train accuracy: 99.74603174603175\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 184\n",
      "Train accuracy: 99.74444444444444\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 185\n",
      "Train accuracy: 99.75238095238095\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 186\n",
      "Train accuracy: 99.76031746031747\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 187\n",
      "Train accuracy: 99.77460317460317\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 188\n",
      "Train accuracy: 99.77301587301586\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 189\n",
      "Train accuracy: 99.78730158730158\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 190\n",
      "Train accuracy: 99.79206349206349\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 191\n",
      "Train accuracy: 99.7984126984127\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 192\n",
      "Train accuracy: 99.80317460317461\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 193\n",
      "Train accuracy: 99.80952380952381\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 194\n",
      "Train accuracy: 99.81269841269841\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 195\n",
      "Train accuracy: 99.81587301587301\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 196\n",
      "Train accuracy: 99.82857142857144\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 197\n",
      "Train accuracy: 99.82698412698413\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 198\n",
      "Train accuracy: 99.83015873015873\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 199\n",
      "Train accuracy: 99.84126984126985\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 200\n",
      "Train accuracy: 99.84603174603176\n",
      "Val accuracy: 95.95714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_bp, val_acc_bp, train_loss_bp, val_loss_bp, sum_weights_bp = batch_grad_descent(x_train,y_train,epochsToTrain, 0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22a64942500>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP\", \"BP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imposing variability and seeing the effect of variability on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 0.3\n",
    "vDD = 5\n",
    "precision = 10#setting the precision value of the calculations\n",
    "wRange = 3\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization\n",
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVth(mu, sigma, shape):\n",
    "  #last dimension represents the binary rep for each weight\n",
    "  return np.random.normal(loc=mu, scale=sigma, size=shape) #each bit is represented by an sram so we need those many vth values for each mosfet in this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initMosParam(shape, mu, sigma, vDD, precision):\n",
    "    dim1, dim2 = shape\n",
    "    sizeI = (dim1, dim2, precision)\n",
    "\n",
    "    Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "\n",
    "    iOn = ((vDD - Vth)**2)*1e-06#scaling the current according to Ioff values arbitraryfor now!!\n",
    "\n",
    "\n",
    "    iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "\n",
    "\n",
    "    iOff = np.random.uniform(low=0, high=1e-10, size = sizeI)#no negative value\n",
    "    return (iOn, iOnNominal, iOff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, currents, precision, step, discreteSteps):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  \n",
    "  iOn, iOnNominal, iOff = currents\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray += np.sign(weightArray) * np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel)\n",
    "\n",
    "\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descentFPOCBP(X,Y,iter, lr, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    print(\"Z1\")\n",
    "    print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_grad_descentFPOCNP(X,Y,iter, lr, pert, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      \n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train loss {1/63000*np.sum((A3_train-one_hot_encoding(Y))**2)}::Val Loss {1/63000*np.sum((A3_val-one_hot_encoding(y_val))**2)}\")\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/Y.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentOCBP(X,Y,iter, lr, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, X1, Y1)\n",
    "\n",
    "      dW1varoc = weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varoc = weightTransformWithVariability(db1.reshape(db1.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varoc = weightTransformWithVariability(dW2, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varoc = weightTransformWithVariability(db2.reshape(db2.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varoc = weightTransformWithVariability(dW3, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varoc = weightTransformWithVariability(db3.reshape(db3.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1varoc, db1varoc, dW2varoc, db2varoc, dW3varoc, db3varoc, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descentOCNP(X,Y,iter, lr, pert, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "  \n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      #startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      #endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "      #print(np.min(np.abs(W1)), np.min(np.abs(b1)), np.min(np.abs(W2)), np.min(np.abs(b2)), np.min(np.abs(W3)), np.min(np.abs(b3)))\n",
    "\n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n",
    "      #print(db1.shape)\n",
    "\n",
    "      dW1varoc = weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varoc = weightTransformWithVariability(db1.reshape(db1.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varoc = weightTransformWithVariability(dW2, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varoc = weightTransformWithVariability(db2.reshape(db2.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varoc = weightTransformWithVariability(dW3, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varoc = weightTransformWithVariability(db3.reshape(db3.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "      #print(dW3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      #W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1varoc, db1varoc, dW2varoc, db2varoc, dW3varoc, db3varoc, lr = lr)\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train loss {1/63000*np.sum((A3_train-one_hot_encoding(Y))**2)}::Val Loss {1/63000*np.sum((A3_val-one_hot_encoding(y_val))**2)}\")\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the effect of variability on the final accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTrainAcc = []\n",
    "finalValAcc = []\n",
    "sigmaList = [0.1, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 12.15079365079365\n",
      "Val accuracy: 11.928571428571429\n",
      "Iteration: 2\n",
      "Train accuracy: 12.376190476190477\n",
      "Val accuracy: 12.828571428571427\n",
      "Iteration: 3\n",
      "Train accuracy: 12.617460317460317\n",
      "Val accuracy: 12.9\n",
      "Iteration: 4\n",
      "Train accuracy: 12.536507936507938\n",
      "Val accuracy: 12.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m sigma \u001b[39min\u001b[39;00m sigmaList:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=1'>2</a>\u001b[0m     W1, b1, W2, b2, W3, b3, train_acc_bpVar, val_acc_bpVar, train_loss_bpVar, val_loss_bpVar, sum_weights_bpVar \u001b[39m=\u001b[39m batch_grad_descentOCBP(x_train,y_train,\u001b[39m100\u001b[39;49m , \u001b[39m0.0005\u001b[39;49m, mu,sigma, vDD, precision, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=2'>3</a>\u001b[0m     finalTrainAcc\u001b[39m.\u001b[39mappend(train_acc_bpVar[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=3'>4</a>\u001b[0m     finalValAcc\u001b[39m.\u001b[39mappend(val_acc_bpVar[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 18'\u001b[0m in \u001b[0;36mbatch_grad_descentOCBP\u001b[1;34m(X, Y, iter, lr, mu, sigma, vDD, precision, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=40'>41</a>\u001b[0m X1 \u001b[39m=\u001b[39m X1\u001b[39m.\u001b[39mT \u001b[39m#take transpose to match the sizes \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=42'>43</a>\u001b[0m \u001b[39m#startin = time.time()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=43'>44</a>\u001b[0m W1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=44'>45</a>\u001b[0m b1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000017?line=45'>46</a>\u001b[0m W2varoc \u001b[39m=\u001b[39m weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 16'\u001b[0m in \u001b[0;36mweightTransformWithVariability\u001b[1;34m(weightArray, currents, precision, step, discreteSteps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=14'>15</a>\u001b[0m analogWeightArray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(weightArray, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m bitLevel \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(precision):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=17'>18</a>\u001b[0m   analogWeightArray \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(weightArray) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mwhere(np\u001b[39m.\u001b[39;49mbitwise_and(clippedWeightIndexArray, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbitLevel)\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbitLevel)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=21'>22</a>\u001b[0m weightWithVariability \u001b[39m=\u001b[39m (analogWeightArray\u001b[39m/\u001b[39miOnNominal)\u001b[39m*\u001b[39mstep\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000015?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m weightWithVariability\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sigma in sigmaList:\n",
    "    W1, b1, W2, b2, W3, b3, train_acc_bpVar, val_acc_bpVar, train_loss_bpVar, val_loss_bpVar, sum_weights_bpVar = batch_grad_descentOCBP(x_train,y_train,100 , 0.0005, mu,sigma, vDD, precision, print_op=1)\n",
    "    finalTrainAcc.append(train_acc_bpVar[-1])\n",
    "    finalValAcc.append(val_acc_bpVar[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(finalTrainAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 48.65238095238095::Val accuracy: 47.65714285714286::Train loss 1.0125491483283615::Val Loss 0.11469425632050993\n",
      "Iter 1 -> sub iter 9 : 47.777777777777785\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=0'>1</a>\u001b[0m W1, b1, W2, b2, W3, b3, train_acc_npVarAll, val_acc_npVarAll, train_loss_npVarAll, val_loss_npVarAll, sum_weights_npVarAll \u001b[39m=\u001b[39m batch_grad_descentOCNP(X\u001b[39m=\u001b[39;49mx_train,Y\u001b[39m=\u001b[39;49my_train,\u001b[39miter\u001b[39;49m \u001b[39m=\u001b[39;49m epochsToTrain, lr\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, pert\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m, mu\u001b[39m=\u001b[39;49mmu, sigma\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, vDD \u001b[39m=\u001b[39;49m vDD, precision \u001b[39m=\u001b[39;49m precision, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mbatch_grad_descentOCNP\u001b[1;34m(X, Y, iter, lr, pert, mu, sigma, vDD, precision, print_op)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=60'>61</a>\u001b[0m lossBeforePert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((A3\u001b[39m-\u001b[39mone_hot_encoding(Y1))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=62'>63</a>\u001b[0m \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=63'>64</a>\u001b[0m \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=64'>65</a>\u001b[0m dW1, db1, dW2, db2, dW3, db3 \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varoc, W2varoc, W3varoc, b1varoc, b2varoc, b3varoc, X1, Y1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=65'>66</a>\u001b[0m \u001b[39m#print(db1.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=67'>68</a>\u001b[0m dW1varoc \u001b[39m=\u001b[39m weightTransformWithVariability(dW1, dW1Currents, precision, step, discreteSteps)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=53'>54</a>\u001b[0m A1pert \u001b[39m=\u001b[39m relu(Z1pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=55'>56</a>\u001b[0m Z2pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2,A1pert) \u001b[39m+\u001b[39m b2 \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=56'>57</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=58'>59</a>\u001b[0m Z3pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W3,A2pert) \u001b[39m+\u001b[39m b3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=59'>60</a>\u001b[0m A3pert \u001b[39m=\u001b[39m softmax(Z3pert)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 24\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=31'>32</a>\u001b[0m   \u001b[39m#Z4 = np.matmul(W4,A3) + b4\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=32'>33</a>\u001b[0m   \u001b[39m#A4 = relu(Z4)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=33'>34</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=41'>42</a>\u001b[0m   \u001b[39m#A2 is 10*m, final predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=42'>43</a>\u001b[0m   \u001b[39m# print(\"Fp Done\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=44'>45</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m Z1, A1, Z2, A2, Z3, A3\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=48'>49</a>\u001b[0m    \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmaximum(x,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(Z):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000023?line=52'>53</a>\u001b[0m   \u001b[39m#return np.exp(Z) / np.sum(np.exp(Z),0)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_npVarAll, val_acc_npVarAll, train_loss_npVarAll, val_loss_npVarAll, sum_weights_npVarAll = batch_grad_descentOCNP(X=x_train,Y=y_train,iter = epochsToTrain, lr=0.5, pert=0.0001, mu=mu, sigma=0.1, vDD = vDD, precision = precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1::Train accuracy: 13.200000000000001::Val accuracy: 13.314285714285715::Train loss 1.566857139916198::Val Loss 0.17345653254052462\n",
      "Iteration: 2::Train accuracy: 21.284126984126985::Val accuracy: 21.37142857142857::Train loss 1.4363407935332517::Val Loss 0.1594937005890703\n",
      "Iteration: 3::Train accuracy: 25.63015873015873::Val accuracy: 25.142857142857146::Train loss 1.3699294923959326::Val Loss 0.15311371716078398\n",
      "Iteration: 4::Train accuracy: 28.40793650793651::Val accuracy: 27.885714285714286::Train loss 1.3242668092887602::Val Loss 0.1484728716058764\n",
      "Iteration: 5::Train accuracy: 30.631746031746033::Val accuracy: 30.028571428571425::Train loss 1.2878223717344564::Val Loss 0.1446851014398137\n",
      "Iteration: 6::Train accuracy: 32.05396825396825::Val accuracy: 31.2::Train loss 1.263716840929155::Val Loss 0.1423615132114519\n",
      "Iteration: 7::Train accuracy: 33.46825396825397::Val accuracy: 32.471428571428575::Train loss 1.2398404015421536::Val Loss 0.13983673487543052\n",
      "Iteration: 8::Train accuracy: 34.4968253968254::Val accuracy: 33.614285714285714::Train loss 1.2220524340238694::Val Loss 0.13791989162570473\n",
      "Iteration: 9::Train accuracy: 35.20634920634921::Val accuracy: 34.385714285714286::Train loss 1.207064330270723::Val Loss 0.13606595934210522\n",
      "Iteration: 10::Train accuracy: 36.7968253968254::Val accuracy: 35.542857142857144::Train loss 1.1673545220101735::Val Loss 0.1325893568270379\n",
      "Iteration: 11::Train accuracy: 38.58412698412698::Val accuracy: 36.957142857142856::Train loss 1.127328240093724::Val Loss 0.12889188314924008\n",
      "Iteration: 12::Train accuracy: 40.10793650793651::Val accuracy: 38.08571428571428::Train loss 1.0979546352344713::Val Loss 0.12574812070687272\n",
      "Iteration: 13::Train accuracy: 41.31269841269841::Val accuracy: 39.55714285714286::Train loss 1.0744097788738356::Val Loss 0.12298202407682617\n",
      "Iteration: 14::Train accuracy: 42.12698412698413::Val accuracy: 40.52857142857143::Train loss 1.0559987977357943::Val Loss 0.12082054188286893\n",
      "Iteration: 15::Train accuracy: 42.87619047619048::Val accuracy: 41.214285714285715::Train loss 1.0409342705764992::Val Loss 0.11908680334024636\n",
      "Iteration: 16::Train accuracy: 43.40952380952381::Val accuracy: 41.77142857142857::Train loss 1.0280037255057215::Val Loss 0.11762459499191995\n",
      "Iteration: 17::Train accuracy: 43.99523809523809::Val accuracy: 42.44285714285714::Train loss 1.008443673565804::Val Loss 0.11548025444037016\n",
      "Iteration: 18::Train accuracy: 46.08253968253968::Val accuracy: 44.628571428571426::Train loss 0.9476629664236439::Val Loss 0.1082331064889093\n",
      "Iteration: 19::Train accuracy: 48.86825396825397::Val accuracy: 47.08571428571429::Train loss 0.8924231179696367::Val Loss 0.10237569442578502\n",
      "Iteration: 20::Train accuracy: 51.36190476190477::Val accuracy: 49.7::Train loss 0.8484831582996054::Val Loss 0.0971278127885064\n",
      "Iteration: 21::Train accuracy: 53.353968253968254::Val accuracy: 51.82857142857142::Train loss 0.8151814632890194::Val Loss 0.09304330935881679\n",
      "Iteration: 22::Train accuracy: 54.85079365079365::Val accuracy: 53.7::Train loss 0.7901488132958926::Val Loss 0.09035121809806243\n",
      "Iteration: 23::Train accuracy: 56.01746031746032::Val accuracy: 54.91428571428572::Train loss 0.7726854876540288::Val Loss 0.0885385309859091\n",
      "Iteration: 24::Train accuracy: 56.941269841269836::Val accuracy: 55.7::Train loss 0.7592634175166658::Val Loss 0.08703825972935011\n",
      "Iteration: 25::Train accuracy: 57.66031746031746::Val accuracy: 56.48571428571428::Train loss 0.7486318367256443::Val Loss 0.0858822341144995\n",
      "Iteration: 26::Train accuracy: 58.20952380952381::Val accuracy: 56.92857142857143::Train loss 0.7400621705266568::Val Loss 0.08495708822996367\n",
      "Iteration: 27::Train accuracy: 58.66825396825397::Val accuracy: 57.15714285714286::Train loss 0.7327846887543837::Val Loss 0.08423344310401153\n",
      "Iteration: 28::Train accuracy: 59.02857142857143::Val accuracy: 57.61428571428572::Train loss 0.7262426895187616::Val Loss 0.08359608727630793\n",
      "Iteration: 29::Train accuracy: 59.34603174603175::Val accuracy: 57.74285714285714::Train loss 0.7210538013914125::Val Loss 0.0831341752165388\n",
      "Iteration: 30::Train accuracy: 59.63650793650793::Val accuracy: 57.94285714285714::Train loss 0.7159276283956159::Val Loss 0.08263832455559401\n",
      "Iteration: 31::Train accuracy: 59.871428571428574::Val accuracy: 58.199999999999996::Train loss 0.7112034275209694::Val Loss 0.08218186965566306\n",
      "Iteration: 32::Train accuracy: 60.109523809523814::Val accuracy: 58.542857142857144::Train loss 0.7068308798673936::Val Loss 0.08175110523584024\n",
      "Iteration: 33::Train accuracy: 60.303174603174604::Val accuracy: 58.9::Train loss 0.7025330249648811::Val Loss 0.08134857028133265\n",
      "Iteration: 34::Train accuracy: 60.52222222222222::Val accuracy: 59.042857142857144::Train loss 0.6982700487345431::Val Loss 0.08100981276000896\n",
      "Iteration: 35::Train accuracy: 60.68730158730159::Val accuracy: 59.07142857142858::Train loss 0.6942341406417082::Val Loss 0.08067336818930004\n",
      "Iteration: 36::Train accuracy: 60.87619047619047::Val accuracy: 59.15714285714285::Train loss 0.6902779816163724::Val Loss 0.08028398806431553\n",
      "Iteration: 37::Train accuracy: 61.06349206349206::Val accuracy: 59.3::Train loss 0.6861637202575336::Val Loss 0.07981914929236583\n",
      "Iteration: 38::Train accuracy: 61.31111111111112::Val accuracy: 59.71428571428572::Train loss 0.6818478344841853::Val Loss 0.07925897747850831\n",
      "Iteration: 39::Train accuracy: 61.60634920634921::Val accuracy: 59.971428571428575::Train loss 0.6759474864697054::Val Loss 0.07860031272778223\n",
      "Iteration: 40::Train accuracy: 61.93333333333333::Val accuracy: 60.34285714285714::Train loss 0.6694505649192561::Val Loss 0.07786240869673679\n",
      "Iteration: 41::Train accuracy: 62.25555555555555::Val accuracy: 60.61428571428571::Train loss 0.6624677723564442::Val Loss 0.07705512375113802\n",
      "Iteration: 42::Train accuracy: 62.56507936507937::Val accuracy: 60.94285714285714::Train loss 0.6560501841817448::Val Loss 0.07619817115595598\n",
      "Iteration: 43::Train accuracy: 63.05555555555556::Val accuracy: 61.27142857142858::Train loss 0.6473395098219097::Val Loss 0.07520433829909705\n",
      "Iteration: 44::Train accuracy: 63.48412698412699::Val accuracy: 61.74285714285715::Train loss 0.6389574081105465::Val Loss 0.07430330453825759\n",
      "Iteration: 45::Train accuracy: 63.993650793650794::Val accuracy: 62.28571428571429::Train loss 0.6304632062725702::Val Loss 0.0733821225251423\n",
      "Iteration: 46::Train accuracy: 64.40476190476191::Val accuracy: 62.771428571428565::Train loss 0.6212307151427998::Val Loss 0.07238706124263183\n",
      "Iteration: 47::Train accuracy: 64.9031746031746::Val accuracy: 63.37142857142857::Train loss 0.6118972230571534::Val Loss 0.07133069847968727\n",
      "Iteration: 48::Train accuracy: 65.37460317460318::Val accuracy: 63.82857142857142::Train loss 0.602534636254433::Val Loss 0.0703437798880992\n",
      "Iteration: 49::Train accuracy: 65.86507936507935::Val accuracy: 64.18571428571428::Train loss 0.5924183853812914::Val Loss 0.0693357245971397\n",
      "Iteration: 50::Train accuracy: 66.33333333333333::Val accuracy: 64.61428571428571::Train loss 0.5832160986345554::Val Loss 0.06843089820634923\n",
      "Iteration: 51::Train accuracy: 66.7::Val accuracy: 64.91428571428571::Train loss 0.5743728419503267::Val Loss 0.06750626935496203\n",
      "Iteration: 52::Train accuracy: 67.05873015873016::Val accuracy: 65.28571428571428::Train loss 0.5658968898188959::Val Loss 0.06654167669859441\n",
      "Iteration: 53::Train accuracy: 67.5079365079365::Val accuracy: 65.60000000000001::Train loss 0.5557929883868066::Val Loss 0.06547782624049356\n",
      "Iteration: 54::Train accuracy: 68.10000000000001::Val accuracy: 66.28571428571428::Train loss 0.5405740099324423::Val Loss 0.06372877661198328\n",
      "Iteration: 55::Train accuracy: 68.96349206349205::Val accuracy: 67.08571428571429::Train loss 0.5230621874696225::Val Loss 0.061585221413256376\n",
      "Iteration: 56::Train accuracy: 69.77936507936508::Val accuracy: 68.01428571428572::Train loss 0.5067061800288143::Val Loss 0.05939179810132212\n",
      "Iteration: 57::Train accuracy: 70.6936507936508::Val accuracy: 69.32857142857142::Train loss 0.489210367574694::Val Loss 0.05724441181936453\n",
      "Iteration: 58::Train accuracy: 71.52539682539683::Val accuracy: 70.0::Train loss 0.4744003446947302::Val Loss 0.05544247607969578\n",
      "Iteration: 59::Train accuracy: 72.17936507936507::Val accuracy: 71.1::Train loss 0.4623977945309487::Val Loss 0.05392138917870933\n",
      "Iteration: 60::Train accuracy: 72.75555555555555::Val accuracy: 71.61428571428571::Train loss 0.4522700737808248::Val Loss 0.052731181590657816\n",
      "Iteration: 61::Train accuracy: 73.23015873015873::Val accuracy: 72.11428571428571::Train loss 0.44308804971709287::Val Loss 0.051632713808782794\n",
      "Iteration: 62::Train accuracy: 73.63809523809523::Val accuracy: 72.57142857142857::Train loss 0.4348761643848828::Val Loss 0.050673730365417904\n",
      "Iteration: 63::Train accuracy: 74.07460317460317::Val accuracy: 73.0::Train loss 0.4272682491363391::Val Loss 0.04974865868531299\n",
      "Iteration: 64::Train accuracy: 74.3873015873016::Val accuracy: 73.34285714285714::Train loss 0.4213311027409983::Val Loss 0.04900178174948363\n",
      "Iteration: 65::Train accuracy: 74.71746031746032::Val accuracy: 73.61428571428571::Train loss 0.4158759552442644::Val Loss 0.04841171393602029\n",
      "Iteration: 66::Train accuracy: 74.98253968253968::Val accuracy: 74.0142857142857::Train loss 0.41123739271511395::Val Loss 0.047847580511822856\n",
      "Iteration: 67::Train accuracy: 75.14444444444445::Val accuracy: 74.17142857142856::Train loss 0.40750603546743264::Val Loss 0.0473729781176053\n",
      "Iteration: 68::Train accuracy: 75.20476190476191::Val accuracy: 74.32857142857144::Train loss 0.4044411712381762::Val Loss 0.04701874269508109\n",
      "Iteration: 69::Train accuracy: 75.35079365079365::Val accuracy: 74.52857142857144::Train loss 0.40080949912923075::Val Loss 0.04662500559570586\n",
      "Iteration: 70::Train accuracy: 75.67301587301587::Val accuracy: 74.67142857142856::Train loss 0.39606091452170117::Val Loss 0.04612529895336065\n",
      "Iteration: 71::Train accuracy: 75.80634920634921::Val accuracy: 74.8::Train loss 0.3930276834723899::Val Loss 0.045768092346855783\n",
      "Iteration: 72::Train accuracy: 75.93968253968254::Val accuracy: 74.88571428571429::Train loss 0.38975023840666617::Val Loss 0.04536594516067845\n",
      "Iteration: 73::Train accuracy: 76.05238095238094::Val accuracy: 75.07142857142857::Train loss 0.387058850964811::Val Loss 0.045067949489097474\n",
      "Iteration: 74::Train accuracy: 76.16825396825396::Val accuracy: 75.08571428571429::Train loss 0.38443934984364236::Val Loss 0.04478501293285217\n",
      "Iteration: 75::Train accuracy: 76.23174603174603::Val accuracy: 75.0::Train loss 0.38248895885768064::Val Loss 0.044630863254052806\n",
      "Iteration: 76::Train accuracy: 76.3015873015873::Val accuracy: 74.94285714285715::Train loss 0.3803032954024225::Val Loss 0.04441068170828668\n",
      "Iteration: 77::Train accuracy: 76.25396825396825::Val accuracy: 75.0::Train loss 0.37955029563788056::Val Loss 0.044375198855480434\n",
      "Iteration: 78::Train accuracy: 76.23968253968255::Val accuracy: 74.84285714285714::Train loss 0.3786147323187425::Val Loss 0.044212330638647865\n",
      "Iteration: 79::Train accuracy: 76.19206349206348::Val accuracy: 74.9857142857143::Train loss 0.37756385385093755::Val Loss 0.04403500870330437\n",
      "Iteration: 80::Train accuracy: 76.21904761904761::Val accuracy: 75.1::Train loss 0.3761446652932145::Val Loss 0.043805003571116226\n",
      "Iteration: 81::Train accuracy: 76.13968253968254::Val accuracy: 75.08571428571429::Train loss 0.3755324485036911::Val Loss 0.043685519491015935\n",
      "Iteration: 82::Train accuracy: 76.0::Val accuracy: 74.9857142857143::Train loss 0.37603545454433085::Val Loss 0.04366540601855181\n",
      "Iteration: 83::Train accuracy: 75.86349206349206::Val accuracy: 74.9::Train loss 0.3759558638146004::Val Loss 0.043555407449570926\n",
      "Iteration: 84::Train accuracy: 75.72380952380952::Val accuracy: 74.75714285714285::Train loss 0.3764619718391046::Val Loss 0.04354496903240951\n",
      "Iteration: 85::Train accuracy: 75.45396825396826::Val accuracy: 74.37142857142857::Train loss 0.3774222717049735::Val Loss 0.04363045513779789\n",
      "Iteration: 86::Train accuracy: 75.28730158730158::Val accuracy: 74.14285714285714::Train loss 0.37806966257343666::Val Loss 0.04367095285227896\n",
      "Iteration: 87::Train accuracy: 75.24444444444444::Val accuracy: 74.27142857142857::Train loss 0.37679004982275616::Val Loss 0.04346263023202824\n",
      "Iteration: 88::Train accuracy: 75.36190476190477::Val accuracy: 74.44285714285715::Train loss 0.37431357331506143::Val Loss 0.043159011203238035\n",
      "Iteration: 89::Train accuracy: 75.37936507936507::Val accuracy: 74.4::Train loss 0.372442377330091::Val Loss 0.04294066130588854\n",
      "Iteration: 90::Train accuracy: 75.4095238095238::Val accuracy: 74.37142857142857::Train loss 0.3703551843358487::Val Loss 0.0426511653072721\n",
      "Iteration: 91::Train accuracy: 75.38095238095238::Val accuracy: 74.44285714285715::Train loss 0.37001127005786494::Val Loss 0.04253241198347452\n",
      "Iteration: 92::Train accuracy: 75.33492063492064::Val accuracy: 74.52857142857144::Train loss 0.36979248763980616::Val Loss 0.0424579559239301\n",
      "Iteration: 93::Train accuracy: 75.46031746031746::Val accuracy: 74.74285714285715::Train loss 0.36694780598842563::Val Loss 0.042105558351730035\n",
      "Iteration: 94::Train accuracy: 75.61746031746031::Val accuracy: 75.02857142857144::Train loss 0.3646148428776769::Val Loss 0.041790042860175146\n",
      "Iteration: 95::Train accuracy: 75.59047619047618::Val accuracy: 75.05714285714285::Train loss 0.3648352164631686::Val Loss 0.041721667197602764\n",
      "Iteration: 96::Train accuracy: 75.75238095238095::Val accuracy: 75.34285714285714::Train loss 0.3626116211144255::Val Loss 0.041392339687590145\n",
      "Iteration: 97::Train accuracy: 75.95079365079364::Val accuracy: 75.4857142857143::Train loss 0.36068142544299947::Val Loss 0.04116651226151273\n",
      "Iteration: 98::Train accuracy: 76.15079365079364::Val accuracy: 75.52857142857144::Train loss 0.3587569371636391::Val Loss 0.040938139728600546\n",
      "Iteration: 99::Train accuracy: 76.17936507936508::Val accuracy: 75.67142857142856::Train loss 0.35855645099895916::Val Loss 0.040898363110898193\n",
      "Iteration: 100::Train accuracy: 76.14126984126985::Val accuracy: 75.7::Train loss 0.3603090228730898::Val Loss 0.041091918678338925\n",
      "Iteration: 101::Train accuracy: 76.05873015873016::Val accuracy: 75.67142857142856::Train loss 0.3633811688095412::Val Loss 0.04139103917980882\n",
      "Iteration: 102::Train accuracy: 75.77460317460317::Val accuracy: 75.41428571428571::Train loss 0.3697073784403775::Val Loss 0.04203141035073541\n",
      "Iteration: 103::Train accuracy: 75.59047619047618::Val accuracy: 75.37142857142857::Train loss 0.37553841262084997::Val Loss 0.042614337691663305\n",
      "Iteration: 104::Train accuracy: 74.65555555555555::Val accuracy: 74.5142857142857::Train loss 0.3925318499161501::Val Loss 0.04453789074917866\n",
      "Iteration: 105::Train accuracy: 74.03492063492064::Val accuracy: 73.52857142857144::Train loss 0.4062282198761424::Val Loss 0.04606551445734629\n",
      "Iteration: 106::Train accuracy: 73.50952380952381::Val accuracy: 73.15714285714286::Train loss 0.4167716790389977::Val Loss 0.047250658743916846\n",
      "Iteration: 107::Train accuracy: 72.92063492063492::Val accuracy: 71.98571428571428::Train loss 0.4285144158104274::Val Loss 0.04861148137111578\n",
      "Iteration: 108::Train accuracy: 72.52539682539683::Val accuracy: 71.62857142857143::Train loss 0.4360526724749092::Val Loss 0.049494780643807565\n",
      "Iteration: 109::Train accuracy: 72.03015873015873::Val accuracy: 70.98571428571428::Train loss 0.44381307483809895::Val Loss 0.050411663839491076\n",
      "Iteration: 110::Train accuracy: 71.44126984126984::Val accuracy: 70.12857142857143::Train loss 0.45285981429068334::Val Loss 0.05145296990772659\n",
      "Iteration: 111::Train accuracy: 71.21269841269842::Val accuracy: 69.89999999999999::Train loss 0.45768231069119786::Val Loss 0.05197435486593842\n",
      "Iteration: 112::Train accuracy: 71.4015873015873::Val accuracy: 70.31428571428572::Train loss 0.4555815011716835::Val Loss 0.051729342107397505\n",
      "Iteration: 113::Train accuracy: 71.58095238095238::Val accuracy: 70.67142857142858::Train loss 0.4518406348077735::Val Loss 0.05122947627832551\n",
      "Iteration: 114::Train accuracy: 71.63968253968254::Val accuracy: 70.75714285714285::Train loss 0.44970297789928254::Val Loss 0.051001722800778686\n",
      "Iteration: 115::Train accuracy: 71.63174603174603::Val accuracy: 70.8::Train loss 0.44879977180704705::Val Loss 0.05087280964761152\n",
      "Iteration: 116::Train accuracy: 71.82063492063492::Val accuracy: 70.85714285714285::Train loss 0.4448732883339239::Val Loss 0.05042981349158813\n",
      "Iteration: 117::Train accuracy: 72.11746031746031::Val accuracy: 71.11428571428571::Train loss 0.43914083614618565::Val Loss 0.049753420922208226\n",
      "Iteration: 118::Train accuracy: 72.51587301587301::Val accuracy: 71.67142857142858::Train loss 0.4313158543088888::Val Loss 0.04886907058081938\n",
      "Iteration: 119::Train accuracy: 72.63968253968254::Val accuracy: 71.72857142857143::Train loss 0.4264904555005334::Val Loss 0.04833908911282878\n",
      "Iteration: 120::Train accuracy: 72.53968253968253::Val accuracy: 71.65714285714286::Train loss 0.42508536502553856::Val Loss 0.04815584908342543\n",
      "Iteration: 121::Train accuracy: 72.46190476190476::Val accuracy: 71.6::Train loss 0.42569663546838377::Val Loss 0.048192561405076814\n",
      "Iteration: 122::Train accuracy: 72.45238095238096::Val accuracy: 71.61428571428571::Train loss 0.424480185093886::Val Loss 0.04800480277468351\n",
      "Iteration: 123::Train accuracy: 72.52380952380952::Val accuracy: 71.75714285714285::Train loss 0.42212436637617207::Val Loss 0.047714394392754746\n",
      "Iteration: 124::Train accuracy: 72.93174603174603::Val accuracy: 72.25714285714285::Train loss 0.41469491666851543::Val Loss 0.04688845646547452\n",
      "Iteration: 125::Train accuracy: 73.54603174603174::Val accuracy: 73.14285714285714::Train loss 0.40487448253409136::Val Loss 0.04580463974639918\n",
      "Iteration: 126::Train accuracy: 74.07777777777778::Val accuracy: 73.57142857142858::Train loss 0.39510412135947726::Val Loss 0.04471319419426244\n",
      "Iteration: 127::Train accuracy: 74.65238095238095::Val accuracy: 74.04285714285714::Train loss 0.3853637250331926::Val Loss 0.04365159928588187\n",
      "Iteration: 128::Train accuracy: 75.2031746031746::Val accuracy: 74.67142857142856::Train loss 0.3757434750632165::Val Loss 0.0425925081682317\n",
      "Iteration: 129::Train accuracy: 75.57460317460317::Val accuracy: 75.08571428571429::Train loss 0.36784123346279224::Val Loss 0.0416964991065021\n",
      "Iteration: 130::Train accuracy: 76.07142857142857::Val accuracy: 75.57142857142857::Train loss 0.3598113027001118::Val Loss 0.040767845470462416\n",
      "Iteration: 131::Train accuracy: 76.36984126984126::Val accuracy: 75.95714285714286::Train loss 0.3537747876799183::Val Loss 0.04008755263143746\n",
      "Iteration: 132::Train accuracy: 76.66349206349207::Val accuracy: 76.25714285714285::Train loss 0.34922555188499865::Val Loss 0.039578890011053344\n",
      "Iteration: 133::Train accuracy: 76.7936507936508::Val accuracy: 76.41428571428571::Train loss 0.34673503298157565::Val Loss 0.03930649675222413\n",
      "Iteration: 134::Train accuracy: 76.78253968253969::Val accuracy: 76.72857142857143::Train loss 0.3459664410310996::Val Loss 0.03920809186034574\n",
      "Iteration: 135::Train accuracy: 76.92063492063492::Val accuracy: 76.52857142857142::Train loss 0.34436680543312437::Val Loss 0.03901479362611946\n",
      "Iteration: 136::Train accuracy: 77.10952380952381::Val accuracy: 76.72857142857143::Train loss 0.3415524614446109::Val Loss 0.038651525677897515\n",
      "Iteration: 137::Train accuracy: 77.36825396825397::Val accuracy: 77.21428571428571::Train loss 0.33734795861190325::Val Loss 0.038155595535838374\n",
      "Iteration: 138::Train accuracy: 77.53015873015873::Val accuracy: 77.15714285714286::Train loss 0.33505144723164754::Val Loss 0.03794106830924203\n",
      "Iteration: 139::Train accuracy: 77.45396825396826::Val accuracy: 77.31428571428572::Train loss 0.33630277638174577::Val Loss 0.038031622766569385\n",
      "Iteration: 140::Train accuracy: 77.54761904761904::Val accuracy: 77.42857142857143::Train loss 0.33528901383066934::Val Loss 0.037870626064066125\n",
      "Iteration: 141::Train accuracy: 77.81428571428572::Val accuracy: 77.68571428571428::Train loss 0.33150462171464873::Val Loss 0.03740308288645637\n",
      "Iteration: 142::Train accuracy: 78.07936507936508::Val accuracy: 77.94285714285715::Train loss 0.32871417032806655::Val Loss 0.037064373185121624\n",
      "Iteration: 143::Train accuracy: 78.21904761904761::Val accuracy: 78.15714285714286::Train loss 0.3261954556600593::Val Loss 0.03677323974413577\n",
      "Iteration: 144::Train accuracy: 78.33809523809524::Val accuracy: 78.31428571428572::Train loss 0.3242875501261326::Val Loss 0.036519788594353085\n",
      "Iteration: 145::Train accuracy: 78.37619047619047::Val accuracy: 78.38571428571429::Train loss 0.3239869610894462::Val Loss 0.036417311865783904\n",
      "Iteration: 146::Train accuracy: 78.36190476190477::Val accuracy: 78.5::Train loss 0.324959401773729::Val Loss 0.036505361445708304\n",
      "Iteration: 147::Train accuracy: 78.34761904761905::Val accuracy: 78.54285714285714::Train loss 0.3254484462736096::Val Loss 0.03655231390633435\n",
      "Iteration: 148::Train accuracy: 78.37142857142857::Val accuracy: 78.54285714285714::Train loss 0.3256102718955764::Val Loss 0.036533326113977624\n",
      "Iteration: 149::Train accuracy: 78.45238095238095::Val accuracy: 78.65714285714286::Train loss 0.3251646934241628::Val Loss 0.036422419681394126\n",
      "Iteration: 150::Train accuracy: 78.5936507936508::Val accuracy: 78.72857142857143::Train loss 0.32370404433659367::Val Loss 0.03621963779096084\n",
      "Iteration: 151::Train accuracy: 78.58253968253969::Val accuracy: 78.85714285714286::Train loss 0.3240790397305::Val Loss 0.036224410761862245\n",
      "Iteration: 152::Train accuracy: 78.64920634920635::Val accuracy: 78.97142857142858::Train loss 0.32330584704680626::Val Loss 0.0361126041143741\n",
      "Iteration: 153::Train accuracy: 78.89047619047619::Val accuracy: 79.14285714285715::Train loss 0.3204246674001256::Val Loss 0.03574646816727893\n",
      "Iteration: 154::Train accuracy: 78.74761904761904::Val accuracy: 79.2::Train loss 0.3220850491178458::Val Loss 0.03589316612966283\n",
      "Iteration: 155::Train accuracy: 78.62698412698413::Val accuracy: 78.97142857142858::Train loss 0.3250543649492729::Val Loss 0.03620275354140952\n",
      "Iteration: 156::Train accuracy: 78.52539682539683::Val accuracy: 78.87142857142857::Train loss 0.32852065861238544::Val Loss 0.036581647934342224\n",
      "Iteration: 157::Train accuracy: 78.45238095238095::Val accuracy: 78.64285714285715::Train loss 0.3318121158214347::Val Loss 0.03695303111799949\n",
      "Iteration: 158::Train accuracy: 78.38730158730158::Val accuracy: 78.62857142857142::Train loss 0.3346306283671121::Val Loss 0.03726064014163791\n",
      "Iteration: 159::Train accuracy: 78.32857142857142::Val accuracy: 78.54285714285714::Train loss 0.33655155103070145::Val Loss 0.037436661178767094\n",
      "Iteration: 160::Train accuracy: 78.16349206349207::Val accuracy: 78.41428571428571::Train loss 0.33886458197589714::Val Loss 0.03766531181859757\n",
      "Iteration: 161::Train accuracy: 78.13968253968254::Val accuracy: 78.4::Train loss 0.3407794255564295::Val Loss 0.03786916876837811\n",
      "Iteration: 162::Train accuracy: 77.98412698412699::Val accuracy: 78.25714285714285::Train loss 0.34345365552643736::Val Loss 0.03815579489451571\n",
      "Iteration: 163::Train accuracy: 77.83809523809524::Val accuracy: 78.11428571428571::Train loss 0.34602319912082796::Val Loss 0.03842016905505417\n",
      "Iteration: 164::Train accuracy: 77.77777777777779::Val accuracy: 78.04285714285714::Train loss 0.34726307782610605::Val Loss 0.038534055245543875\n",
      "Iteration: 165::Train accuracy: 77.76984126984127::Val accuracy: 78.10000000000001::Train loss 0.34628173877473833::Val Loss 0.03842883998671159\n",
      "Iteration: 166::Train accuracy: 77.91746031746032::Val accuracy: 78.37142857142857::Train loss 0.34413120918183876::Val Loss 0.03819404277077673\n",
      "Iteration: 167::Train accuracy: 77.94285714285715::Val accuracy: 78.45714285714286::Train loss 0.34314926698357623::Val Loss 0.038081409818027157\n",
      "Iteration: 168::Train accuracy: 78.04444444444445::Val accuracy: 78.58571428571427::Train loss 0.34123772318660534::Val Loss 0.03787181293365782\n",
      "Iteration: 169::Train accuracy: 78.12222222222222::Val accuracy: 78.57142857142857::Train loss 0.33894717898953663::Val Loss 0.03761846890296244\n",
      "Iteration: 170::Train accuracy: 78.17301587301587::Val accuracy: 78.68571428571428::Train loss 0.3377868220684938::Val Loss 0.03748346525812862\n",
      "Iteration: 171::Train accuracy: 78.25873015873016::Val accuracy: 78.72857142857143::Train loss 0.336310023576885::Val Loss 0.037317685113656886\n",
      "Iteration: 172::Train accuracy: 78.4095238095238::Val accuracy: 78.94285714285715::Train loss 0.33328583024875486::Val Loss 0.03699458590116936\n",
      "Iteration: 173::Train accuracy: 78.5111111111111::Val accuracy: 79.01428571428572::Train loss 0.33089000785910594::Val Loss 0.03673326558427378\n",
      "Iteration: 174::Train accuracy: 78.53809523809524::Val accuracy: 79.02857142857142::Train loss 0.32980022848665386::Val Loss 0.036610338643806736\n",
      "Iteration: 175::Train accuracy: 78.54285714285714::Val accuracy: 79.10000000000001::Train loss 0.3295164433700145::Val Loss 0.03657247949066407\n",
      "Iteration: 176::Train accuracy: 78.38888888888889::Val accuracy: 79.01428571428572::Train loss 0.33197709907562056::Val Loss 0.03684312594536459\n",
      "Iteration: 177::Train accuracy: 78.29206349206349::Val accuracy: 78.92857142857143::Train loss 0.33434511142684253::Val Loss 0.03709414356653327\n",
      "Iteration: 178::Train accuracy: 78.21746031746032::Val accuracy: 78.77142857142857::Train loss 0.33581264962256324::Val Loss 0.03726263784901043\n",
      "Iteration: 179::Train accuracy: 78.27936507936508::Val accuracy: 78.9::Train loss 0.3341776367978907::Val Loss 0.03709022848702856\n",
      "Iteration: 180::Train accuracy: 78.31269841269841::Val accuracy: 78.92857142857143::Train loss 0.3330168555892107::Val Loss 0.036975167605223916\n",
      "Iteration: 181::Train accuracy: 78.40158730158731::Val accuracy: 79.04285714285714::Train loss 0.33084605089704655::Val Loss 0.03674369046078428\n",
      "Iteration: 182::Train accuracy: 78.45079365079364::Val accuracy: 79.04285714285714::Train loss 0.33000111945201394::Val Loss 0.03665620677420437\n",
      "Iteration: 183::Train accuracy: 78.44444444444446::Val accuracy: 79.01428571428572::Train loss 0.3299851497575845::Val Loss 0.036658530639535974\n",
      "Iteration: 184::Train accuracy: 78.52380952380953::Val accuracy: 79.07142857142857::Train loss 0.32887066082956407::Val Loss 0.036536264208544\n",
      "Iteration: 185::Train accuracy: 78.72380952380954::Val accuracy: 79.17142857142856::Train loss 0.3255459727355709::Val Loss 0.036176031131740394\n",
      "Iteration: 186::Train accuracy: 78.91746031746032::Val accuracy: 79.24285714285715::Train loss 0.32218005726354465::Val Loss 0.035809655896149976\n",
      "Iteration: 187::Train accuracy: 79.05714285714286::Val accuracy: 79.4::Train loss 0.31863161791168565::Val Loss 0.035416493538705976\n",
      "Iteration: 188::Train accuracy: 79.24603174603175::Val accuracy: 79.60000000000001::Train loss 0.3148895627857311::Val Loss 0.03500817729271245\n",
      "Iteration: 189::Train accuracy: 79.45238095238095::Val accuracy: 79.84285714285714::Train loss 0.3113584861719522::Val Loss 0.03462124728093009\n",
      "Iteration: 190::Train accuracy: 79.6031746031746::Val accuracy: 79.87142857142857::Train loss 0.3084282786433664::Val Loss 0.03429779423033516\n",
      "Iteration: 191::Train accuracy: 79.76190476190477::Val accuracy: 79.98571428571428::Train loss 0.30600708401886934::Val Loss 0.03403197561652687\n",
      "Iteration: 192::Train accuracy: 79.87936507936509::Val accuracy: 79.97142857142858::Train loss 0.3043254949741518::Val Loss 0.0338513075143452\n",
      "Iteration: 193::Train accuracy: 79.97301587301587::Val accuracy: 80.05714285714286::Train loss 0.3027831966961689::Val Loss 0.03368597072694213\n",
      "Iteration: 194::Train accuracy: 80.04761904761905::Val accuracy: 79.98571428571428::Train loss 0.3018773511778084::Val Loss 0.033600222099888104\n",
      "Iteration: 195::Train accuracy: 80.10000000000001::Val accuracy: 80.02857142857142::Train loss 0.300726502523443::Val Loss 0.033481602841493166\n",
      "Iteration: 196::Train accuracy: 80.13650793650794::Val accuracy: 80.10000000000001::Train loss 0.3008678712281815::Val Loss 0.0334981984007354\n",
      "Iteration: 197::Train accuracy: 80.30000000000001::Val accuracy: 80.32857142857142::Train loss 0.29818347838176096::Val Loss 0.033199616393923104\n",
      "Iteration: 198::Train accuracy: 80.4063492063492::Val accuracy: 80.38571428571429::Train loss 0.2963257647810298::Val Loss 0.03299437540784002\n",
      "Iteration: 199::Train accuracy: 80.51904761904763::Val accuracy: 80.5::Train loss 0.2943972000115268::Val Loss 0.0327829922011792\n",
      "Iteration: 200::Train accuracy: 80.5968253968254::Val accuracy: 80.65714285714286::Train loss 0.2928825318239694::Val Loss 0.03261730544325987\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_npVar, val_acc_npVar, train_loss_npVar, val_loss_npVar, sum_weights_npVar = batch_grad_descentFPOCNP(X=x_train,Y=y_train,iter = epochsToTrain, lr=0.005, pert=0.5, mu=mu, sigma=sigma, vDD = vDD, precision = precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22aa7a7dbd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.plot(train_acc_bpVar, \"c^\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP(software)\", \"BP(software)\", \"BP(with variability)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.5873015873016"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc_bpVar[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the software NP and BP algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  #print(Z3.shape)\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i, :] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    lossArrayAfterPertZ3[i, :] += np.sum(np.square(A3pert-one_hot_encoding(Y1)), axis=0)\n",
    "\n",
    "\n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "\n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    lossArrayAfterPertZ2[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "    A1pert = relu(Z1pert)\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    \n",
    "    lossArrayAfterPertZ1[i,:] += np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "    \n",
    "  #print(lossArrayAfterPertZ1)\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDComp(X,Y,iter, lrBP, lrNP, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "\n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1np, b1np, W2np,b2np, W3np, b3np) \n",
    "      print(f\"NP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1np, W2np, W3np, b1np, b2np, b3np, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1np, b1np, W2np, b2np, W3np, b3np, dW1np, db1np, dW2np, db2np, dW3np, db3np, lr = lrNP)\n",
    "      #print(W1)\n",
    "\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1bp, b1bp, W2bp,b2bp, W3bp, b3bp) \n",
    "      print(f\"                                                                        BP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1bp, W2bp, W3bp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1bp, b1bp, W2bp, b2bp, W3bp, b3bp, dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp, lr = lrBP)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    lrNP = lrNP*np.exp(-0.01)\n",
    "    lrBP = lrBP*np.exp(-0.01)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {train_score}::Val accuracy: {val_score}::Train Acc BP::{accuracy(predictions(A3_train_bp), Y)} Val Acc BP::{accuracy(predictions(A3_val_bp), y_val)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "0.9999999999999999\n",
      "Iteration: 1::Train accuracy: 83.86825396825397::Val accuracy: 84.01428571428572::Train Acc BP::78.87777777777778 Val Acc BP::79.05714285714286\n",
      "0.9995836635382563\n",
      "Iteration: 2::Train accuracy: 88.56825396825397::Val accuracy: 88.3::Train Acc BP::84.95238095238096 Val Acc BP::84.95714285714286\n",
      "0.9990861305443578\n",
      "Iteration: 3::Train accuracy: 90.18571428571428::Val accuracy: 89.78571428571429::Train Acc BP::87.54444444444445 Val Acc BP::87.58571428571429\n",
      "0.9986414556035522\n",
      "Iteration: 4::Train accuracy: 91.25238095238095::Val accuracy: 90.8::Train Acc BP::89.02857142857142 Val Acc BP::89.0\n",
      "0.9982375986346534\n",
      "Iteration: 5::Train accuracy: 91.9968253968254::Val accuracy: 91.48571428571428::Train Acc BP::90.14285714285715 Val Acc BP::89.97142857142858\n",
      "0.9978611626408245\n",
      "Iteration: 6::Train accuracy: 92.57936507936509::Val accuracy: 91.74285714285715::Train Acc BP::90.94761904761904 Val Acc BP::90.55714285714286\n",
      "0.9975004704632473\n",
      "Iteration: 7::Train accuracy: 93.05238095238096::Val accuracy: 92.10000000000001::Train Acc BP::91.5952380952381 Val Acc BP::90.97142857142858\n",
      "0.9971449589438852\n",
      "Iteration: 8::Train accuracy: 93.42539682539682::Val accuracy: 92.44285714285714::Train Acc BP::92.13174603174603 Val Acc BP::91.44285714285715\n",
      "0.9968038316752404\n",
      "Iteration: 9::Train accuracy: 93.74444444444444::Val accuracy: 92.52857142857142::Train Acc BP::92.55238095238096 Val Acc BP::91.87142857142857\n",
      "0.9964711128849748\n",
      "Iteration: 10::Train accuracy: 94.01587301587301::Val accuracy: 92.82857142857142::Train Acc BP::92.86825396825397 Val Acc BP::92.25714285714287\n",
      "0.9961471830489816\n",
      "Iteration: 11::Train accuracy: 94.24920634920635::Val accuracy: 93.05714285714286::Train Acc BP::93.16825396825396 Val Acc BP::92.44285714285714\n",
      "0.9958263133213214\n",
      "Iteration: 12::Train accuracy: 94.43015873015874::Val accuracy: 93.10000000000001::Train Acc BP::93.41904761904762 Val Acc BP::92.64285714285714\n",
      "0.9955045360824365\n",
      "Iteration: 13::Train accuracy: 94.55714285714286::Val accuracy: 93.30000000000001::Train Acc BP::93.67301587301587 Val Acc BP::92.77142857142857\n",
      "0.9951864331936199\n",
      "Iteration: 14::Train accuracy: 94.73968253968253::Val accuracy: 93.51428571428572::Train Acc BP::93.83968253968254 Val Acc BP::92.98571428571428\n",
      "0.9948702929249279\n",
      "Iteration: 15::Train accuracy: 94.89047619047619::Val accuracy: 93.54285714285714::Train Acc BP::94.02539682539683 Val Acc BP::93.12857142857143\n",
      "0.9945576380193666\n",
      "Iteration: 16::Train accuracy: 94.9968253968254::Val accuracy: 93.57142857142857::Train Acc BP::94.16190476190476 Val Acc BP::93.37142857142857\n",
      "0.9942534100040661\n",
      "Iteration: 17::Train accuracy: 95.0936507936508::Val accuracy: 93.68571428571428::Train Acc BP::94.34285714285714 Val Acc BP::93.55714285714286\n",
      "0.9939581755478033\n",
      "Iteration: 18::Train accuracy: 95.19047619047619::Val accuracy: 93.7::Train Acc BP::94.51587301587303 Val Acc BP::93.71428571428572\n",
      "0.9936686353520052\n",
      "Iteration: 19::Train accuracy: 95.2904761904762::Val accuracy: 93.84285714285714::Train Acc BP::94.64285714285714 Val Acc BP::93.74285714285713\n",
      "0.9933871795440747\n",
      "Iteration: 20::Train accuracy: 95.37301587301587::Val accuracy: 94.0::Train Acc BP::94.78412698412698 Val Acc BP::93.84285714285714\n"
     ]
    }
   ],
   "source": [
    "w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _ = batchGDComp(x_train,y_train,20, 0.1, 0.5, 1, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "1.0\n",
      "Iteration: 1::Train accuracy: 25.349206349206348::Val accuracy: 26.071428571428573::Train Acc BP::80.46031746031747 Val Acc BP::80.07142857142857\n",
      "0.9994119482902641\n",
      "Iteration: 2::Train accuracy: 29.06190476190476::Val accuracy: 29.85714285714286::Train Acc BP::86.41111111111111 Val Acc BP::85.8\n",
      "0.998970464425642\n",
      "Iteration: 3::Train accuracy: 31.482539682539684::Val accuracy: 32.4::Train Acc BP::88.71587301587302 Val Acc BP::88.1\n",
      "0.9985511396217523\n",
      "Iteration: 4::Train accuracy: 32.90793650793651::Val accuracy: 33.52857142857143::Train Acc BP::90.0936507936508 Val Acc BP::89.44285714285715\n",
      "0.9981614170185603\n",
      "Iteration: 5::Train accuracy: 32.695238095238096::Val accuracy: 33.91428571428571::Train Acc BP::91.02222222222223 Val Acc BP::90.02857142857142\n",
      "0.9978258245114084\n",
      "Iteration: 6::Train accuracy: 36.75873015873016::Val accuracy: 37.885714285714286::Train Acc BP::91.68571428571428 Val Acc BP::90.68571428571428\n",
      "0.9974571636282409\n",
      "Iteration: 7::Train accuracy: 38.7968253968254::Val accuracy: 40.08571428571429::Train Acc BP::92.25873015873016 Val Acc BP::91.21428571428571\n",
      "0.9971635432890604\n",
      "Iteration: 8::Train accuracy: 40.46825396825397::Val accuracy: 41.58571428571429::Train Acc BP::92.72063492063492 Val Acc BP::91.60000000000001\n",
      "0.9968722813408111\n",
      "Iteration: 9::Train accuracy: 41.05555555555556::Val accuracy: 41.67142857142857::Train Acc BP::93.1047619047619 Val Acc BP::91.87142857142857\n",
      "0.9965443261045565\n",
      "Iteration: 10::Train accuracy: 44.333333333333336::Val accuracy: 45.714285714285715::Train Acc BP::93.3952380952381 Val Acc BP::92.24285714285713\n",
      "0.9962831135180967\n",
      "Iteration: 11::Train accuracy: 46.67301587301587::Val accuracy: 47.98571428571429::Train Acc BP::93.68253968253968 Val Acc BP::92.4\n",
      "0.9960141329624136\n",
      "Iteration: 12::Train accuracy: 47.196825396825396::Val accuracy: 48.34285714285714::Train Acc BP::93.94761904761904 Val Acc BP::92.55714285714286\n",
      "0.9957161937472889\n",
      "Iteration: 13::Train accuracy: 47.890476190476186::Val accuracy: 48.55714285714286::Train Acc BP::94.1968253968254 Val Acc BP::92.68571428571428\n",
      "0.9954046578926007\n",
      "Iteration: 14::Train accuracy: 48.31428571428572::Val accuracy: 49.628571428571426::Train Acc BP::94.3968253968254 Val Acc BP::92.91428571428571\n",
      "0.9951690416826062\n",
      "Iteration: 15::Train accuracy: 47.74444444444445::Val accuracy: 49.24285714285715::Train Acc BP::94.56825396825397 Val Acc BP::93.08571428571429\n",
      "0.9948712122665715\n",
      "Iteration: 16::Train accuracy: 50.21904761904762::Val accuracy: 51.05714285714286::Train Acc BP::94.71111111111111 Val Acc BP::93.32857142857142\n",
      "0.9946920095053556\n",
      "Iteration: 17::Train accuracy: 52.38253968253969::Val accuracy: 53.142857142857146::Train Acc BP::94.86349206349206 Val Acc BP::93.35714285714286\n",
      "0.9944910109123659\n",
      "Iteration: 18::Train accuracy: 51.67777777777778::Val accuracy: 52.32857142857142::Train Acc BP::94.9888888888889 Val Acc BP::93.41428571428571\n",
      "0.9942545967336793\n",
      "Iteration: 19::Train accuracy: 53.98730158730159::Val accuracy: 54.51428571428571::Train Acc BP::95.08412698412698 Val Acc BP::93.55714285714286\n",
      "0.9940382290126166\n",
      "Iteration: 20::Train accuracy: 52.67460317460318::Val accuracy: 53.15714285714286::Train Acc BP::95.21587301587302 Val Acc BP::93.65714285714286\n",
      "Params Initialised\n",
      "1.0\n",
      "Iteration: 1::Train accuracy: 49.42539682539683::Val accuracy: 49.91428571428572::Train Acc BP::78.43492063492063 Val Acc BP::78.67142857142856\n",
      "0.999672215481469\n",
      "Iteration: 2::Train accuracy: 55.45873015873016::Val accuracy: 56.10000000000001::Train Acc BP::84.82857142857144 Val Acc BP::85.04285714285714\n",
      "0.9995379064666058\n",
      "Iteration: 3::Train accuracy: 60.439682539682536::Val accuracy: 61.385714285714286::Train Acc BP::87.33174603174602 Val Acc BP::87.27142857142857\n",
      "0.9994288867036104\n",
      "Iteration: 4::Train accuracy: 61.48571428571429::Val accuracy: 62.25714285714285::Train Acc BP::88.87460317460317 Val Acc BP::88.75714285714285\n",
      "0.9993256619154804\n",
      "Iteration: 5::Train accuracy: 66.08730158730158::Val accuracy: 67.10000000000001::Train Acc BP::89.85873015873015 Val Acc BP::89.64285714285715\n",
      "0.9992357224056005\n",
      "Iteration: 6::Train accuracy: 73.84285714285714::Val accuracy: 74.8::Train Acc BP::90.6952380952381 Val Acc BP::90.27142857142857\n",
      "0.9991720961506195\n",
      "Iteration: 7::Train accuracy: 77.68412698412699::Val accuracy: 78.2::Train Acc BP::91.31746031746032 Val Acc BP::90.8\n",
      "0.9991084719930232\n",
      "Iteration: 8::Train accuracy: 79.04920634920634::Val accuracy: 79.72857142857143::Train Acc BP::91.88253968253969 Val Acc BP::91.4\n",
      "0.9990375712775517\n",
      "Iteration: 9::Train accuracy: 79.8984126984127::Val accuracy: 80.41428571428571::Train Acc BP::92.34285714285714 Val Acc BP::91.84285714285714\n",
      "0.9989667589376805\n",
      "Iteration: 10::Train accuracy: 80.53650793650795::Val accuracy: 81.14285714285714::Train Acc BP::92.73650793650793 Val Acc BP::92.05714285714286\n",
      "0.9988971743539226\n",
      "Iteration: 11::Train accuracy: 81.09206349206349::Val accuracy: 81.62857142857143::Train Acc BP::93.06825396825397 Val Acc BP::92.32857142857142\n",
      "0.9988296709252887\n",
      "Iteration: 12::Train accuracy: 81.52539682539683::Val accuracy: 81.91428571428571::Train Acc BP::93.34761904761905 Val Acc BP::92.58571428571429\n",
      "0.998764783968695\n",
      "Iteration: 13::Train accuracy: 81.8047619047619::Val accuracy: 82.45714285714286::Train Acc BP::93.63333333333334 Val Acc BP::92.71428571428572\n",
      "0.9987021704956484\n",
      "Iteration: 14::Train accuracy: 82.11111111111111::Val accuracy: 82.8::Train Acc BP::93.84761904761905 Val Acc BP::92.95714285714286\n",
      "0.9986410097849677\n",
      "Iteration: 15::Train accuracy: 82.35873015873015::Val accuracy: 82.95714285714286::Train Acc BP::94.05873015873016 Val Acc BP::93.12857142857143\n",
      "0.9985821742108926\n",
      "Iteration: 16::Train accuracy: 82.61111111111111::Val accuracy: 83.18571428571428::Train Acc BP::94.28571428571428 Val Acc BP::93.32857142857142\n",
      "0.9985251422041582\n",
      "Iteration: 17::Train accuracy: 82.84126984126983::Val accuracy: 83.48571428571428::Train Acc BP::94.45238095238095 Val Acc BP::93.5\n",
      "0.9984699606034008\n",
      "Iteration: 18::Train accuracy: 83.01904761904761::Val accuracy: 83.6::Train Acc BP::94.5968253968254 Val Acc BP::93.54285714285714\n",
      "0.9984163317220767\n",
      "Iteration: 19::Train accuracy: 83.20476190476191::Val accuracy: 83.7::Train Acc BP::94.76507936507936 Val Acc BP::93.58571428571429\n",
      "0.9983643573480832\n",
      "Iteration: 20::Train accuracy: 83.37460317460318::Val accuracy: 83.78571428571429::Train Acc BP::94.91428571428571 Val Acc BP::93.71428571428572\n",
      "Params Initialised\n",
      "0.9999999999999998\n",
      "Iteration: 1::Train accuracy: 52.195238095238096::Val accuracy: 51.714285714285715::Train Acc BP::81.01587301587301 Val Acc BP::80.98571428571428\n",
      "0.9997402418661159\n",
      "Iteration: 2::Train accuracy: 66.42698412698412::Val accuracy: 66.07142857142857::Train Acc BP::86.21746031746031 Val Acc BP::85.9\n",
      "0.9996537899727624\n",
      "Iteration: 3::Train accuracy: 74.9968253968254::Val accuracy: 74.22857142857143::Train Acc BP::88.46190476190476 Val Acc BP::88.15714285714286\n",
      "0.9995803541527069\n",
      "Iteration: 4::Train accuracy: 77.22857142857143::Val accuracy: 76.38571428571429::Train Acc BP::89.8015873015873 Val Acc BP::89.61428571428571\n",
      "0.9995080228005806\n",
      "Iteration: 5::Train accuracy: 78.41746031746032::Val accuracy: 77.81428571428572::Train Acc BP::90.78253968253969 Val Acc BP::90.27142857142857\n",
      "0.9994403762086118\n",
      "Iteration: 6::Train accuracy: 79.2::Val accuracy: 78.77142857142857::Train Acc BP::91.5063492063492 Val Acc BP::90.77142857142857\n",
      "0.9993787949620859\n",
      "Iteration: 7::Train accuracy: 79.81269841269841::Val accuracy: 79.25714285714285::Train Acc BP::92.02222222222223 Val Acc BP::91.28571428571428\n",
      "0.999321056709429\n",
      "Iteration: 8::Train accuracy: 86.42698412698412::Val accuracy: 86.15714285714286::Train Acc BP::92.42857142857143 Val Acc BP::91.75714285714285\n",
      "0.9992866704882359\n",
      "Iteration: 9::Train accuracy: 88.73015873015872::Val accuracy: 88.38571428571429::Train Acc BP::92.8047619047619 Val Acc BP::92.12857142857143\n",
      "0.9992486141596315\n",
      "Iteration: 10::Train accuracy: 89.73015873015872::Val accuracy: 89.08571428571429::Train Acc BP::93.16031746031747 Val Acc BP::92.48571428571428\n",
      "0.9992045284282678\n",
      "Iteration: 11::Train accuracy: 90.35873015873015::Val accuracy: 89.67142857142856::Train Acc BP::93.44285714285714 Val Acc BP::92.7\n",
      "0.9991598579974093\n",
      "Iteration: 12::Train accuracy: 90.78571428571428::Val accuracy: 89.97142857142858::Train Acc BP::93.68730158730159 Val Acc BP::92.85714285714286\n",
      "0.9991154028147511\n",
      "Iteration: 13::Train accuracy: 91.1079365079365::Val accuracy: 90.3::Train Acc BP::93.8984126984127 Val Acc BP::92.91428571428571\n",
      "0.9990718015552692\n",
      "Iteration: 14::Train accuracy: 91.43968253968254::Val accuracy: 90.52857142857142::Train Acc BP::94.07460317460318 Val Acc BP::93.07142857142857\n",
      "0.9990296097201864\n",
      "Iteration: 15::Train accuracy: 91.7095238095238::Val accuracy: 90.78571428571428::Train Acc BP::94.26190476190476 Val Acc BP::93.24285714285713\n",
      "0.9989882118445018\n",
      "Iteration: 16::Train accuracy: 91.95873015873016::Val accuracy: 91.0::Train Acc BP::94.43809523809524 Val Acc BP::93.35714285714286\n",
      "0.9989473570069396\n",
      "Iteration: 17::Train accuracy: 92.17619047619048::Val accuracy: 91.17142857142856::Train Acc BP::94.57460317460318 Val Acc BP::93.47142857142858\n",
      "0.9989071633001937\n",
      "Iteration: 18::Train accuracy: 92.38412698412698::Val accuracy: 91.31428571428572::Train Acc BP::94.73174603174603 Val Acc BP::93.61428571428571\n",
      "0.9988681105671597\n",
      "Iteration: 19::Train accuracy: 92.56825396825397::Val accuracy: 91.38571428571429::Train Acc BP::94.85238095238095 Val Acc BP::93.74285714285713\n",
      "0.9988299696105643\n",
      "Iteration: 20::Train accuracy: 92.75555555555556::Val accuracy: 91.67142857142856::Train Acc BP::94.96190476190476 Val Acc BP::93.81428571428572\n",
      "Params Initialised\n",
      "1.0000000000000004\n",
      "Iteration: 1::Train accuracy: 66.64761904761905::Val accuracy: 65.81428571428572::Train Acc BP::82.13809523809525 Val Acc BP::81.54285714285714\n",
      "0.9998524504568801\n",
      "Iteration: 2::Train accuracy: 77.9095238095238::Val accuracy: 76.9::Train Acc BP::86.9888888888889 Val Acc BP::86.11\n",
      "0.9997717779739823\n",
      "Iteration: 3::Train accuracy: 81.72539682539683::Val accuracy: 80.51428571428572::Train Acc BP::89.08253968253969 Val Acc BP::87.9\n",
      "0.9996709447974755\n",
      "Iteration: 4::Train accuracy: 83.97142857142858::Val accuracy: 83.18571428571428::Train Acc BP::90.2968253968254 Val Acc BP::89.0142857142857\n",
      "0.9995650261931688\n",
      "Iteration: 5::Train accuracy: 85.46349206349207::Val accuracy: 84.72857142857143::Train Acc BP::91.14444444444445 Val Acc BP::89.81428571428572\n",
      "0.9994608426098263\n",
      "Iteration: 6::Train accuracy: 86.58571428571429::Val accuracy: 85.71428571428571::Train Acc BP::91.73968253968255 Val Acc BP::90.42857142857143\n",
      "0.9993593707697663\n",
      "Iteration: 7::Train accuracy: 87.33492063492064::Val accuracy: 86.82857142857144::Train Acc BP::92.23809523809524 Val Acc BP::91.07142857142857\n",
      "0.999261650108695\n",
      "Iteration: 8::Train accuracy: 87.98412698412699::Val accuracy: 87.61428571428571::Train Acc BP::92.65555555555555 Val Acc BP::91.45714285714286\n",
      "0.9991682898698578\n",
      "Iteration: 9::Train accuracy: 88.5968253968254::Val accuracy: 88.12857142857143::Train Acc BP::93.03174603174604 Val Acc BP::91.74285714285715\n",
      "0.9990785019745377\n",
      "Iteration: 10::Train accuracy: 89.06825396825397::Val accuracy: 88.61428571428571::Train Acc BP::93.31111111111112 Val Acc BP::91.95714285714286\n",
      "0.9989923654337676\n",
      "Iteration: 11::Train accuracy: 89.37777777777778::Val accuracy: 88.9857142857143::Train Acc BP::93.56666666666666 Val Acc BP::92.25714285714287\n",
      "0.9989098259456818\n",
      "Iteration: 12::Train accuracy: 89.68888888888888::Val accuracy: 89.34285714285714::Train Acc BP::93.81904761904762 Val Acc BP::92.41428571428571\n",
      "0.9988290305661676\n",
      "Iteration: 13::Train accuracy: 89.97301587301587::Val accuracy: 89.58571428571429::Train Acc BP::94.0047619047619 Val Acc BP::92.61428571428571\n",
      "0.998750245047302\n",
      "Iteration: 14::Train accuracy: 90.26984126984127::Val accuracy: 89.72857142857143::Train Acc BP::94.19047619047619 Val Acc BP::92.74285714285713\n",
      "0.9986735021825085\n",
      "Iteration: 15::Train accuracy: 90.4984126984127::Val accuracy: 90.0::Train Acc BP::94.36031746031746 Val Acc BP::92.85714285714286\n",
      "0.9985980930896188\n",
      "Iteration: 16::Train accuracy: 90.76190476190476::Val accuracy: 90.25714285714285::Train Acc BP::94.51746031746032 Val Acc BP::93.02857142857142\n",
      "0.9985246162449434\n",
      "Iteration: 17::Train accuracy: 90.96507936507936::Val accuracy: 90.4::Train Acc BP::94.64126984126983 Val Acc BP::93.18571428571428\n",
      "0.9984528841156125\n",
      "Iteration: 18::Train accuracy: 91.15079365079364::Val accuracy: 90.52857142857142::Train Acc BP::94.76507936507936 Val Acc BP::93.32857142857142\n",
      "0.9983830505201434\n",
      "Iteration: 19::Train accuracy: 91.3::Val accuracy: 90.8::Train Acc BP::94.91904761904762 Val Acc BP::93.41428571428571\n",
      "0.9983144251847127\n",
      "Iteration: 20::Train accuracy: 91.45555555555556::Val accuracy: 90.97142857142858::Train Acc BP::95.03492063492064 Val Acc BP::93.58571428571429\n"
     ]
    }
   ],
   "source": [
    "perturbation = [0.0000000000000001, 0.000001, 0.01, 1]\n",
    "trainBPList = []\n",
    "trainNPList = []\n",
    "for pert in perturbation:\n",
    "    w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _ = batchGDComp(x_train,y_train,20, 0.1, 0.1,pert, print_op=1)\n",
    "    trainBPTemp= [i[0] for i in trainAccBoth]\n",
    "    trainNPTemp = [i[1] for i in trainAccBoth]\n",
    "    trainBPList.append(trainBPTemp)\n",
    "    trainNPList.append(trainNPTemp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "format = ['bx', 'g.', 'r^', 'kd']\n",
    "col = ['b', 'g', 'r', 'k']\n",
    "leg = []\n",
    "plt.figure(figsize=(12,8))\n",
    "for _ in range(len(perturbation)):\n",
    "    plt.plot(trainNPList[_], col[_], linewidth=3,alpha=0.4)\n",
    "plt.plot(trainBPList[0], 'm', linewidth=3,alpha=0.4)\n",
    "for _ in range(len(perturbation)):\n",
    "    leg.append(f\"Node pertubation with pert = {perturbation[_]}\")\n",
    "    plt.plot(trainNPList[_], format[_])\n",
    "    #plt.plot(trainNPList[_], col[_], linewidth=5,alpha=0.3)\n",
    "plt.plot(trainBPList[0], 'm*')\n",
    "leg.append(\"Back propagation\")\n",
    "plt.legend(leg)\n",
    "\n",
    "plt.title(\"Accuracy vs epochs for the  different perturbation\", size=15)\n",
    "plt.xlabel(\"Epochs ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAHyCAYAAAAgB+JFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAB2GklEQVR4nO3deXzcVb3/8dcne5o03feylK2spZS0gohSdgFBZSmICHoBcUX9yaaXC4JeoeB1QURRFvWilFW5WhDKjiA0hVJLKW3pAk23tNn3ZOb8/jgz6WRmkkySWbK8n4/HPDJzvt/vfM9MJu07J5/vOeacQ0RERERE+i8r0x0QERERERkqFK5FRERERJJE4VpEREREJEkUrkVEREREkkThWkREREQkSRSuRURERESSROFapB/MbIOZOTPbL9N9kfQzs0tC3//iFDz3QWb2spk1hM6xdwrOcbKZfStO+/1mVpbs82WSmU00sxuT/T6GnnNnkp7rPDO7JE77C2b2SDLOkU5mNs3MHjezOjPbaWa/NLMRCRyXb2Y/MbMdoc//36O/b2ZWGvqcvmdmQTO7P1WvQ6S3FK5F+sjMjgb2Dj28IINdkaHpNmA0cCZwNLA1Bec4GfhWCp53IJoI3MDun9mB6DzgkjjtXwWuS29X+sfMcoF/AHsB5wNXAucCdydw+C/w78N3gXOA8cAzZlYQsc8xwMeApcC2pHVcJAlyMt0BkUHsAqABWBm6f3Nmu+OZWTaQ7ZxrzXRfpF8OBJ5wzj3bnycxMwPynXPNyenW4BMVypL1nLlAMNnPG49zblU6zpNk5wAHAfs55zYAmFkb8KCZ/cA5tzbeQWY2HfgP4EvOuT+E2lYAG4DPA78L7XqHc+7noe1D6q8sMvhp5FqkD0IB9jzgCeBe4CAzOzzOfh83s+fNrN7MakJ/3j0iYvteZvbn0J9MG81shZl9LrTtuFA5wKFRz9npT8ThP+Gb2afN7B2gGfiImU0xs3vNbL2ZNZnZGjP7oZnlRT1foZktNLNNZtYSKnX5cWjbwtDxFnXMJWbWamYT4rzmotCfcr8WZ9tSM/vf0P3RZvY7M9tiZs1m9oGZ/TaB9/6s0OttNrNtoT7mRmy/MfR+HmNmb4b2W25mH4t6nuzQvh+EXvc74fc+ar9uv4chM8zsmdDrXm1mn416jo+ZL/GoDd2Wm9m5Xby+vc3MAfsC3w59Bl6I2P51M1sb6vM6M/t21PHh1/8xM1uK/zzEnMvMbgT+H7BX6Bwu+k/rZnZS6DPZYGavmNkhUduzzOzaUD9aQp+xi+O9ru5EfNZPNrO/hc73gZldEWffY83sxdDPyy4z+62ZjYzYHi7VmRf6XjUBVwH/Du3yfPj1Ru1fHHWejWZ2e8TjF8zsETO73Mzex7+vUyO29/R5+0LoPaw0s6rQZ6o0Yvv9wNnAJyK+HzdGnjvq+Y43s9dD59tuZr+KfA0R7+lxZvZw6PO73sy+muC3pb8+CSwNB+uQvwCtwKndHHdy6Otj4QbnXDnwSug5w21p+cVGpC8UrkX6Zj4wCXgQeARoI6o0xMyOA54NbbsYWAC8DEwLbZ8IvAbMxf/581PAPcAefejP3sBC4Mf4/4A24P+UWgl8B/+f2W3AF4E7IvpowF+BrwB3Aqfh/3Q+PrTLvcAM4BNR5/si8H/OuYrojjjnGoC/4X/56GBm+wCl+PcM4H/wf9b9NnAK8D3Adfcizew8/H+6b+DLJX4AXB563ZFGAP8L/BofLKuBJ81scsQ+NwHfx/+Z+kzgn8ADZtbxfezpexjhT/hftD4DrMWPzk0PPUdJ6P1Yjw9P5wB/xJd8xLMVXwayLfS8R+PLAjCzy/Dfvyfwn5eHgZ+Y2bVxXv/v8aN8p4ber2i/Cz3/ttA5jqbzX1/2xH9mfoT/bE8EFoU+M2F3AP+Jfw9PBx4H7jWzM7p4bT25B1gBfBZYDNwV+VxmdgywJNTnc/AlLacB98V5rj8D/xfa/jRwYaj9axGvt7eOwf+sXIN//2tC7Yl83vYG/hDa/jngQ+Dl0M8F+Pf+eeCtiP79jjhCv+Q8BezEf6ZuCD1nvLrs3wJv4z+bLwB3mtm87l5k6JemnB5u2d09B/4vL6sjG0J/TXs/tK274zY75+qj2t/t4TiRgcM5p5tuuvXyhg8BVUBe6PHfgI2ARezzGlAW2Rb1HD/Gl5VM6WL7cfiweWhU+wvAIxGP7w/tN7uHPufg/wNujuj3KaFjz+zmuFeA30c83gf/5/AzujnmM0AAmBrRdh0+7OeGHq8EvtGL99yATcB9Ue1fApqAcaHHN4Ze0+ci9ikOnfuW0OOxoff+hqjnWgy814vv4SWhc30pom0c0A5cEXpcGtpnZC8/YxuB2yMeZwHlcV7/r/AhryDq9Z+VwDluBzbGab8/9Br2j2j7dOh5Dww93i/0Obg46tg/4Ecse/Naw5/1u6PanwH+FfH4ZeD5qH2OJ+LnJOJ7cmXUfoeG2o/r4ntY3MP7/0LoczYpar8eP29xXm8W/udxNfBfEe2PAC/E2f8FOv/MP4j/JS47ou28UD+OjnpPb4rYJxeo6KpfUd9/18Mtpp9Rz7EW+Fmc9leAP3Vz3G+B5XHafwhs6eKYMuD+3nzmdNMtlTeNXIv0kvmyis8Cj7vddc0P4i/cOTq0TxHwEXwo7Wo09njgKedcMi5UK3fOLY/qp5nZt8xsVehP423AA0A+flQy3IdK59wT3Tz3PcDZEX9yvgTYjh8568qTQD2dyxEW4N+zttDj5cBVZvZVMzugh9cHcECo3w9FjqABzwEF+PAU6fHwHedHwZ4BwiN2h+JHGx+OOmYRcICZTUjwexj2dMS5dgE7gOmhpvfx78WfzJe0jE7gtcYzHV+GEK/PJcBhEW0O/z3oj42uc11suO43/LpOwIfrx6O+H88CsxMY2Yzn8ajHjwFHmi/hGYH/+Yr+/r+C/2wfGXXs3/tw/p4sc85t72Jbd5+38Owvj5vZdvwvnm3ATPznurfm4X+WAhFtj+J/IfpY1L6Rn802fOidTvduxP9Frbvbl/vQb5FhQeFapPc+if+T/mLzdcOj8SNLLewuDRmDH2ntLjiP62F7b8T7D/9b+JHJx4Gz8P8hh+ugwxd4JdKHh/Ah6rxQScDFwB+cc+1dHeD8xXN/xQdqzGwmcDi7S0IAvo6vwfwv4D3zdcTnd9OPcKnKYnwwCd/CNZ2R5TT1zrmmqON3AFNC98Nfo9+38OOxJPY9DKuOetxK6D12zlUBJ+FHDR8CKsxPLbYPvZNIn8OqXP8vaK2Oehx+vvBnZzyQjR81j/x+3I8flZ1C7+2I8zgndK4xofP9Kup8Lfj3NrqcqqsQ3B9dPWe3n7dQTfjT+D5+BzgWH1DfZvf72RtTovsSCtq76Pw5gG4+m934AP/Lb3e3dT08RxUwKk77mNC2ZB8nMmBothCR3gsH6OgRRIBzzc8bXIUPpN0FjF09bA/P7pAX1T4GX2sZKd7I6rn4PyV/P9xgZgf3sg845xrM7EH8iPUm/OhxvBrXaIuA/zOzPfEhuwI/yhx+3mrgm8A3zWwWcDW+5nmFiz87QmXo6+X4utRokRdOFZtZYVTgmcjuoLw1om1XxD6TIs7VSM/fw4Q45/4FnGpmhcCJ+HrzPwFH9eJpIvscKbLPHafsSz97qRI/UnoM8WfNiA7KiYh+bRND59iJD4QOP6q6OM6xW6IeJ/oedPdzFq2r5+zp83Y0frT4JOdcRx2ymcULkYnYStR7FfpLwTg6fw766l78L9HdeRFfetKV1UTVSIf+6rcPvja9u+P2MLMi56/fCIup4RYZqDRyLdILoVKBT+EvlpofdfsOPugcH/pP4XXgC1EXgEV6FjjFzCZ1sX1z6OtBEeffg8Qv6inEj+pFujDq8bPA2AQuQLsHP9p2I74GNpH/5J7Gj5qdhw/Xj0T9GbuDc24FfkaHLLp+fe/ha473ds6Vxbntitr/M+E7oZKWk9h9Yd9KfHiOnkXjPGCNc64iwe9hrzjnmpxz/4cPL9G/6PRkMz5AxutzLbtnw+iNREYxu/IcfiR5VBffj76MnH8mzuNlzrlA6PvxL2BmF+eLDtfRokfew+L9nH0EX2rTp77H+bwVhr62ROzzUWLn3E70+/E68Jmo0pvP4gfMXulVr+O7kf6XhTwJzDWzvSLazsSXpXVXUhYuY4l8P6fi//3pb6mTSFpo5Fqkd87C1+r+3Dn3euQGM/snfvaJC/D1ltfiZzZ40szuxl9AdzRQ5pz7G/BT4Av4GQN+hJ894CCgyDm30Dm32fz8rTebWSM+eH6PxEemnsGPCr+Or/u9EH8RWvQ+/8DXA98EvIkfqf24c67jP0/n3Ovmp/n7GAnWWjrn2szsMfwvHVMIzXgRZmav4EtWVuJHBC/Dv0fxZrbAORc0s/8H/DE0A8eT+DCyD/5iu3Occ42h3ZuAH4VCzhb8bCx5wM9Dz1VpZj8D/tPM2vEXRH0WP7NE5KwvPX0Pe2Rmp+MvuvwL/s/t0/Dv4XPdHNbV678R+I2Z7cJ/7z6Bn73ie65v81ivBiaZXxVwJbDTObcxwf68Z2a/xs+MshD/HhYAhwAHOOcu7UN/Phn6WXgR//04Cf8zF3Y18KyZBfEX/9Xh/5JyOvB959yabp77A/zn4mIzqwHanHNl+M9bOfALM7seX1ZxNf4XlkR1+3nD/1JQD/w29F5NxwfY8qjnWQ2cZWafJvTLVBe/NPwQ/9ebv5jZXaHnuxX4h3PutV70O67QZ2BjP5/mEfy/h4+F3tdR+H/z/hRZy29mz4bOeULo62Yzuwf4WeiX2gr8e7UJPyNL+LgJ7J7FaAx+SslzQs8x6FazlCEm01dU6qbbYLrhp/Za0832X+FHa/NDjz8BvIQfJa3GT7U1O2L/vfDlE1Whfd4Gzo/Yvh++nrsBP3J7FvFnCymL05difPlGZej2O+AMomYgwY+q3Y7/z7wFX17xozjP98NQH0t68X6dGDpfOZAVte02/GhrXcR7c2wCz/lJ/KwRDfgAtDzUt5zQ9hvxZQTHhra1hN7Xj0c9TzZ+Kr8P8SF9FXBhnPN1+T0kgZkm8BetPRI6T0voff41MLaH19nxHFHt38DXu7bip/f7dtT2G/EhOZHvT0HoM7Ij9Dru7+ozhR9ldUTMEoOvSf8W8E7otVXgg/EXevlzdVzouU/B/9LUGHqfvhpn34/gRz5rQ5+BVfgym1HdfU9C2y4E1oTeOxfRPhe/0l8jPrQeE/3+E/VzF/1+J/B5OxX/C0wTfrrB06KfE19b/jj+59UBN3Z1bvwFpa/jy1p24P/tKY7znnY721Aqb/jQ/xf8Lxa78NN9jojTnxei2vJD39OK0Pd4MTCji89MzC0dr0033bq7mXPpKM0TkcHOzN7AT1N3Uab70p3Q6O7XnXPje9pXBgbz84k/DxzmnFuZ2d6IiPSPykJEpFvmV5E7Hj+6F7PqooiIiOymcC0iPVmKL4e4zjm3NMN9ERERGdBUFiIiIiIikiSaik9EREREJEkUrkVEREREkmRI1VyPHz/e7b333pnuhoiIiIgMYcuWLdvpnJsQb9uQCtd77703ZWVlme6GiIiIiAxhZrapq20qCxERERERSRKFaxERERGRJFG4FhERERFJkiFVcx1PW1sbmzdvprm5OdNdkTQpKChg+vTp5ObmZrorIiIiMswM+XC9efNmRo4cyd57742ZZbo7kmLOOXbt2sXmzZuZMWNGprsjIiIiw8yQLwtpbm5m3LhxCtbDhJkxbtw4/aVCREREMmLIh2tAwXqY0fdbREREMmVYhOtMy87OZvbs2Rx++OHMmTOHV199tU/Pc8kll/DII48kuXfpd//997Nly5aOx5deeimrVq3KYI9EREREkkPhOtLChfD8853bnn/et/dDYWEhy5cv5+233+bHP/4x1113Xb+er7/a29szev7ocP273/2Ogw8+OIM9EhEREUkOhetIc+fCeeftDtjPP+8fz52btFPU1tYyZswYAOrr6znhhBOYM2cOhx12GH/961879vvDH/7ArFmzOPzww7noootinuf666/nkksuIRAIdGo/7rjjuPLKK5k9ezaHHnoob7zxBgA33ngjF110EccccwwXXXQRGzdu5Pjjj2fWrFmccMIJfPDBB4AfHb/iiisoLS3lgAMO4G9/+xsAGzdu5Nhjj2XOnDmdRt+DwSBf/epXOfDAAznppJM47bTTOkbXb7rpJubOncuhhx7K5ZdfjnOORx55hLKyMi688EJmz55NU1MTxx13XMfKmn/+85857LDDOPTQQ7nmmms6XldxcTHf//73OfzwwznqqKPYvn17Ur4fIiIiIknlnBsytyOPPNJFW7VqVUxbt557zrnx4527/nr/9bnnend8HFlZWe7www93M2fOdCUlJa6srMw551xbW5urqalxzjlXUVHh9t13XxcMBt3KlSvd/vvv7yoqKpxzzu3atcs559zFF1/sHn74Yffd737XffnLX3bBYDDmXJ/4xCfcpZde6pxz7sUXX3SHHHKIc865G264wc2ZM8c1NjY655w744wz3P333++cc+6ee+5xZ511Vsc5TjnlFBcIBNyaNWvctGnTXFNTk2toaHBNTU3OOefWrFnjwu/1ww8/7D75yU+6QCDgtm7d6kaPHu0efvjhTv12zrnPf/7z7oknnujo49KlSzv1eenSpa68vNztsccebseOHa6trc3Nnz/fPf74484554CO46+66ip38803d/ue9/r7LiIiIpIgoMx1kUc1ch1t/nz4ylfg5pv91/nz+/2U4bKQ1atX89RTT/GFL3yh4xvwve99j1mzZnHiiSdSXl7O9u3bee655zj33HMZP348AGPHju14rptvvpmamhp+/etfd3nh3gUXXADAxz/+cWpra6murgbgzDPPpLCwEIDXXnuNz33ucwBcdNFFvPLKKx3Hn3feeWRlZbH//vuzzz77sHr1atra2rjssss47LDDOPfccztqpF955RXOPfdcsrKymDx5MvMj3q/nn3+ej3zkIxx22GE899xzvPPOO92+T0uXLuW4445jwoQJ5OTkcOGFF/LSSy8BkJeXxxlnnAHAkUceycaNGxN670VERETSacjPc91rzz8Pd90F11/vv86fn5SAHXb00Uezc+dOKioqWLx4MRUVFSxbtozc3Fz23nvvHqeQmzt3LsuWLaOysrJT6I4UHbrDj4uKihLqY7zjf/rTnzJp0iTefvttgsEgBQUF3T5Hc3MzX/3qVykrK2OPPfbgxhtv7Nf0eLm5uR39ys7OznjduIiIiEg8GrmOFK6xfughuOkm/zWyBjsJVq9eTSAQYNy4cdTU1DBx4kRyc3N5/vnn2bRpEwDHH388Dz/8MLt27QKgsrKy4/hTTz2Va6+9ltNPP526urq451i0aBHgR5VHjRrFqFGjYvb56Ec/yoMPPgjAAw88wLHHHtux7eGHHyYYDPL++++zfv16Zs6cSU1NDVOmTCErK4s//vGPHbXexxxzDI8++ijBYJDt27fzwgsvAHQE6fHjx1NfX99plpORI0fG7fu8efN48cUX2blzJ4FAgD//+c984hOfSOyNFRERERkANHIdaelSH6jDI9Xz5/vHS5f2a/S6qamJ2bNnA77G/fe//z3Z2dlceOGFfOpTn+Kwww6jtLSUAw88EIBDDjmE73//+3ziE58gOzubI444gvvvv7/j+c4991zq6uo488wzWbx4cUepR1hBQQFHHHEEbW1t3HvvvXH7dMcdd/DFL36R2267jQkTJnDfffd1bNtzzz2ZN28etbW1/PrXv6agoICvfvWrnH322fzhD3/g1FNP7RgFP/vss3n22Wc5+OCD2WOPPZgzZw6jRo1i9OjRXHbZZRx66KFMnjyZuREXhYYvmiwsLOS1117raJ8yZQq33HIL8+fPxznH6aefzllnndXn911EREQk3czXZA8NpaWlLjzrRNi7777LQQcdlKEepd9xxx3H7bffTmlpaZ+Ov+SSSzjjjDM455xzEj6mvr6e4uJidu3axbx58/jnP//J5MmT+3T+ZBlu33cREZHBzDlHe7CdtmCb/xpooy3Y1vE1uq092M62+m3c8MINXPuxa/nsQZ+lIKf7ktVkMrNlzrm4YUsj19JvZ5xxBtXV1bS2tnL99ddnPFiLiIhIejnnOgXfnoJx5PZwW2/99s3fsnLHSh5Y8QCfOuBTaQ3X3VG4HmLCNc99FVl+kq5zioiIyMAQDsmtgdaOW1ug8+PWQCttwTa21m3lBy/+gGuOuYaS/JK09rOyqZIl65fgcCxZv4TNtZs5aMLA+Iu1wrWIiIjIEBQIBhIOypHbE3XPW/ewcsdK/vTvP3FF6RUpfCWxFr2zCIcvbQ4SZOE/F3Lfp+/r4aj0ULgWERERGcDC9cjRYXhzzWa+8dQ3WHjiQkryS2K2B4KBnp+8j6JHjhccuoAxBWNSdr545w6XkrQH23lw5YP8+MQfM7k486WpCtciIiIiaRKuM44eRY5uS2Q0+a6yu1i2ZRm3v3p7xkeOF61c1O8+5GTlkJudS25Wbqf7udmhx6H71z93fcyxARfg5hdv5s7T7+xXH5JB4VpERESklwLBAJuqN3HRXy7i7jPuZkzhmG7DcrgtWbO0DbSR4yXrl3DhrAuZVDQpJhj3FJbD27taeTraih0raAt2/oWjLdjGq5tfTfpr7QuF6zQwM77zne/wk5/8BIDbb7+d+vp6brzxxoSfo7i4mPr6+oT333vvvRk5ciRmxuTJk/nDH/6Q8Cwe1dXV/OlPf+KrX/1qwucL6+1UgMuXL2fLli2cdtppADzxxBOsWrWKa6+9ttfnFhER6a2gC8bUI0ePGscLy0EX5K6yu3jtw9e4+pmrh8TIcVhudi552XnkZvmv4Vu4/Ybnb4g5xuH45wf/TMvI8Vtffivl5+iPlIZrM7sSuAww4LfOuZ+Z2Y2htorQbt9zzi2Oc+ypwM+BbOB3zrlbUtnXSFvrtnL+o+ez6JxFSandyc/P57HHHuO6665j/PjxSehhYp5//nnGjx/P9773Pf77v/+bX/ziFz0e097eTnV1Nb/61a96Ha7Dqzb2xvLlyykrK+sI12eeeSZnnnlmr59HRESGt6ALdiq56Cksh9v7Wpc8EEeOo/tgZruDcVRQjgzL0fv0NII80EeOMy1l4drMDsWH6HlAK/CUmf0ttPmnzrnbuzk2G7gTOAnYDCw1syecc6tS1d9IN790M6988ErSandycnK4/PLL+elPf8qPfvSjTts2btzIl770JXbu3NmxUuKee+7Jhg0b+NznPkd9fX3MKoW33XYbDz30EC0tLXzmM5/hBz/4Qbfn//jHP84vfvELAoEA1157LS+88AItLS187Wtf48tf/jIvvPAC119/PWPGjGH16tXMmTOH999/n9mzZ3PSSSdx+umnc/vtt/O3v/lv39e//nVKS0u55JJL2HvvvVmwYAHPPPMMV199NQB//OMfufTSS2lvb+fee+9l3rx5vPHGG1x55ZU0NzdTWFjIfffdx4wZM/iv//ovmpqaeOWVV7juuutoamqirKyMX/7yl12+N5dccgklJSWUlZWxbds2Fi5c2KtFb0REZGBzzsXMaBF5awm00BpoZWvdVm544Qau+dg1lOSldyq4VI4cx5OTldMRiO9bfl/HucMcjuc3PM9PT/1pR1DOzc5NSV8G+shxpqVy5Pog4HXnXCOAmb0IfDbBY+cB65xz60PHPgicBaQ8XG+t28p9y+8j6ILct/w+rv/E9UkZvf7a177GrFmzOgJo2De+8Q0uvvhiLr74Yu69916++c1v8pe//IUrr7ySr3zlK3zhC1/gzjt3B/ynn36atWvX8sYbb+Cc48wzz+Sll17i4x//eJfn/tvf/sZhhx3GPffcw6hRo1i6dCktLS0cc8wxnHzyyQC8+eabrFy5khkzZrBx40ZWrlzJ8uXLgZ7nsR43bhxvvvkmAL/+9a9pbGxk+fLlvPTSS3zpS19i5cqVHHjggbz88svk5OSwZMkSvve97/Hoo49y0003dYRp6DzPdlfvDcDWrVt55ZVXWL16NWeeeabCtYjIABY500VroJWW9pZuQ3Oi08GFFxH504r0TgWX6MhxPNlZ2TEjyfHKMCLbcrNzybKsjue48qkrYxZdaQu2sXz7ckYXjE7665XeSWW4Xgn8yMzGAU3AaUAZsAv4upl9IfT4/znnqqKOnQZ8GPF4M/CRFPa1w80v3UzQBYHkXnlaUlLCF77wBX7xi19QWFjY0f7aa6/x2GOPAXDRRRd1hO9//vOfPProox3t11xzDeDD9dNPP80RRxwB+KXH165dGzdcz58/n+zsbGbNmsUPf/hDLr30UlasWMEjjzwCQE1NDWvXriUvL4958+YxY8aMPr22BQsWdHp8wQUXAH7EvLa2lurqaurq6rj44otZu3YtZkZbW8//cHb13gB8+tOfJisri4MPPpjt27f3qd8iItI3gWCApvYmNlZv5NInLuWOT97BqIJRXYbn8P+ryZTJsozIUeuwIEH+vubv3HjcjZ3KLcKBOdwWGZL7SiPHA1vKwrVz7l0zuxV4GmgAlgMB4C7gZsCFvv4E+FJfz2NmlwOXA+y555796nN41Lo10ApAa6A1qaPX3/rWt5gzZw5f/OIXE9o/Xs2Tc47rrruOL3/5yz0eH665jjz2jjvu4JRTTum03wsvvEBRUVGXz5OTk0MwuPsfxubm5k7bo4+N7reZcf311zN//nwef/xxNm7cyHHHHddj/7uTn5/fcT9ZV16LiIivXW5ub6aprYmm9iaa2pr849D9pvamjpHlu8ru4o3yN7jxhRsH7QV94brk7kaSo8Py9c9fHzNy3B5sZ03lmgGzSqBkTkovaHTO3QPcA2Bm/w1sds51DDOa2W+Bv8U5tBzYI+Lx9FBbvHPcDdwNUFpa2q+UFTlqHZbM0euxY8dy3nnncc899/ClL/nfJz760Y/y4IMPctFFF/HAAw9w7LHHAnDMMcfw4IMP8vnPf54HHnig4zlOOeUUrr/+ei688EKKi4spLy8nNzeXiRMn9nj+U045hbvuuovjjz+e3Nxc1qxZw7Rp02L2GzlyJHV1dR2P99prL1atWkVLSwtNTU08++yzfOxjH+vyPIsWLWL+/Pm88sorjBo1ilGjRlFTU9NxrsjSj+hzRerqvRERkb4JuiAt7S1dhubm9mZa2lsSeq6BeEHfhbMuZHLx5F6VXeRk9T4KLb9ieZJfkQwlqZ4tZKJzboeZ7Ymvtz7KzKY457aGdvkMvnwk2lJgfzObgQ/V5wOfS2VfAV7b/FrHqHVYa6A1qVe//r//9/866osB7rjjDr74xS9y2223dVy0B/Dzn/+cz33uc9x6662dLmg8+eSTeffddzn66KMBP0Xf//7v/yYUri+99FI2btzInDlzcM4xYcKEjhrmSOPGjeOYY47h0EMP5ZOf/CS33XYb5513HoceeigzZszoKEnpSkFBAUcccQRtbW3ce++9AFx99dVcfPHF/PCHP+T000/v2Hf+/PnccsstzJ49m+uuu67T83T13oiISCznHC2Bli5Dc1NbEy2BlqT9tS/VF/RFz2SRn53fcf8HL8ZeyJ/OqeAkzdraoLl59+2OO2DmTJg1C5qaoKUFsrJg2TKIurYtEyyVf1I3s5eBcUAb8B3n3LNm9kdgNr4sZCPwZefcVjObip9y77TQsacBP8NPxXevc+5HsWforLS01JWVlXVqe/fddznoIP2JZrjR911EhhrnHM3tzTS2NdLQ1kBjWyObqjdxzZJr+N6x32NE7oi0lclVNlVy2f9d1mk6trysPH575m/jjl5nWdbukJyT3yk0xwvP0RfwRTviN0ewfNvymPbZk2erHnkwcc4H46amzuE5fAu3R0/1u2IF3HorXHOND9grVsBPfwoPPwzz56el62a2zDkXd1GPVJeFxPwd3zl3URf7bsFf9Bh+vBiImf9aRERkqAq6oA/PrQ2dQnT4cXTp4l1ld7Fi+wp+v/z3aa15jndBn8Px9PtPc8uJt8QE576UXnRHAXoQaG+PH5Qjby0tPmD31qxZPljfeiucdhosXgz/+79pC9Y90QqNIiIiadQWaOsIztEhuqmtKeHnSWXNc35OPoU5hRTmFlKYU0hBTgGFuaGvOYVxL+hrC7bxTsU7TC+ZnpQ+yAAVDMKPfwyHHQbz5u0Oyi+9BG+/Deec4x+3t/f8XP0xa5YP1osWwYIFMHduas/XCwrXIiIiSRSufW5obYgZeW5oa0h4Duee9LXmOS87LyY0h4N0QU4BBTkFPU4Xpwv6hqDWVj+S3NKye1Q53v3wVLqf/3znsoxwmUZ9fXr6u2KFH7FesMB//dSn4MIL03PuHgyLcO2c63EpTxk6NDWfiKRSePXATdWbuPivF/PzU39OUW5Rp5HoVMzrHKm72TKmjZwWNzSHw3R2VnZK+yYDSDDYc1gO3w/24jMbrywjHLRTISsLCgp2395+G37yE7jnHjjhBHj9dbjoIpg6dUCUhgz5cF1QUMCuXbsYN26cAvYw4Jxj165dFBQUZLorIjLIBIIBPxVdoIWW9pZO91sCLR3T1IVn3Lir7C5e3/w6P3jhB2mrd87NzmVE7gj+8PYfYrZptoxh5JZbfJCdO3d3OH7xRR86zz9/d2hOYMG2Posuy+hrsM7NhcLCzuE5+nFeXudjnn0WHn10d5D+5Cf9xYxLlypcp8P06dPZvHkzFRUVme6KpElBQQHTp6vmT0R2jzJHh+N496NriLuTynrngpwCRuSOoCiviKLcoo77I3JHkJftQ8aVT13ZaaYO8DXPyZw6VjIoPItGY2PsraHBz55x/vnxyzJ27kxPH6PLMg47rHPANosflKNv2X34S0q86fbmzx8QwRqGQbjOzc3t87LeIiIyMDnn2FC1gc8//nl+dfqvGJU/Km5obg20pqRUrD9zPGdZFoW5hRTlFnWE5hG5IzqCdCJlG5otYwhobY0fnhsb/cwa3ZVppLssI9q778Jtt/kLG485xo8cf/vb8Nvfwokn7h5tHqYVA0M+XIuIyOAUCAY6zaQReWFgY1sjdy69k39t/hfff/b7aZ2Grqt658jR65ysnI7gHBmii3KLKMgpUJnicBAIdB2eGxv7P5tGssoywrKzfSjOz/e3ru7n5/tw/fjju0eKZ8+GPff0ZRlnn92/fgwBCtciIpIxLe0tu4Nz1Mwaze3NXR6XyaW3u5rj+dn1z/KzU39GUV5RR/mGDFELF0JpKRx11O6w/Pzz8OabcO65/nFLYsvI91lPZRngR47z8hILzTm9iIQDvCwj0xSuRUQkZYIuSFNbU6cAHTn63Js650ipWHrbzMjPzqcgp4D8nPwu73c1x/OKHSsYU5iegC9p0t6+u8458mt2NnzmMz5kRtc8V1Wlvl8rVviA/8Mf+rKMk0+G734X7rrLB9xwaB7GpRmZpHAtIiL90h5s7xhxji7haGpvSnrNcyJlGZFys3NjgnJ+TuhxxP3crNyEyjU0x/MQEnnhYHSAbmjwddHxHHCAD9aprHkOz6JRVAQjRnS+vfMO/OUvu0eKjzwSZszwZRkTJyavD9InCtciIpKQtkAbda111LbUUtdSx/qq9fzn8//JVR+9Km0lGRC/LCNIkKfWPcV/n/DfMaG5pwVRZIgLBjvPtBH9tTfzO0fqb81zVlZsaI685eZ2fey118a2qSxjwFC4FhGRToIuSF1LXacgXdtSG1MD/Ztlv+GdHe8kpSSjK4W5hZ1m0ijKK+LD2g9jyjLag+28u/Nd9h69d0r6IQNceOaNcGiOvN+U+JLyvZLoVHRdhef8fJVsDFEK1yIiw5Rzjsa2Rh+gI4J0Q1tDj6UcybqgMDsrO2Yqusjp6eKNOv/7K//u9XlkEGtt9YuiRN7uusuXZhxyiA/QbW0+7K5dm57ZKsI11v/5n3D00X7E+Nprfb9OOMGH58JCPzotw47CtYjIMNDc3txpNLq2pZb61noCwUCfnq83FxTm5+R3GaALcrSa6rDV3h4bmqNvXS3LPX68r3GOt4hKMpl1rnsOf33rLXjsMT+nM8Cxx/qwv3QpTJiQ3D7IoGOpmFw/U0pLS11ZWVmmuyEikjHtwfaOEejI0ejWQBcXZvVBZVMll/3fZZ1WCMzLyuOxBY+xx6g9YhZHycnSOM6wEgjsDsbdBedA336x6xAO1P29oDAnp3NwjvxaUKDRZ4nLzJY550rjbdO/eCIig5BzjnWV67jo8YtYeNJCCnIKqG2ppaktRfWl+KnqinKL+MPbf4izERavXcydp9+ZsvPLAOCcD8a33AIHHeQXDwmH5ddeg5Ur4dOfTk9fenNBYbj2OV6AztOc5JJcCtciIgNcuDa6urm641bTUsMv3/glb5S/wa2v3Jr0CwoLcgooyS9hZP5ISvJLKMkvoTivmCzL4jtPf6fTqDVAa6CVVze/mtQ+SIYEg/4iwIaGzrNqhO8Hgz6QXn556ssyuhN5QeGTT8K8efCxj+2+YDByCrvsnpeUF0kWhWsRkQGmobWBmpaa3UG6uSZmdoxkXVCYm53LyLyRnYL0yLyR5GZ3PQ3YW19+q9fnkQEmvDR3ODRHhuimJj9C3Z1Zs3yQTuU8z2E5OX7kObwwSkGBr3n+yU/gj3+EU06BV1/1Ifuhh3zIFskghWsRkQxqamvqNBpd3VxNW6Ctx+N6u0JhlmVRnFfcMQodDtK6oHAIa2uLDdDhEN3c9dLyCUvGPM/h0Bx9C4fogoL4y3L/7W/w6KO753U+/ngfrJcu1VzPknEK1yIiadLc3txpNLq6ubpPFxr2tEJhUV5RzGh0UW5RQqsPyiCxcCHMnQsf/ejuwPzss/Dmm3DOOf5xV6sLJktX8zybdQ7H8YJzYWH3i6T05OqrY9u0iIoMEArXIiIp0NLe0mk0urq5mpb2lqQ8d7wVCh2Olza+xG8+9Ruys1RfOmS0tPgyjfDCKOH7OTn+wsGrr46tea6uTm2fcnPhvffg9tvhjjvguOPg3HN9Dfaf/gQnn6zFUWRYU7gWEemn1kBrx0h0+Ba9mmF/5WTlMKpgFKMLRrOpZlNMDXZbsI1l25YpWA82LS2dQ3Pk/aamrqer239/H6xTVfOcn+8vCIycVSN8y831Yf6xx3aPFO+1F4wd68syTjklOX0QGaQ0z7WISC8456hrrWNX4y7W7FrDVc9cxXc/+t0+XUzYleysbEbl+yAdDtQq6xik+hqeE/XAA7trni+8sHfHhhdHiQ7QI0bEr3MWkQ6a51pEpI+CLkh1czWVTZXsatxFVXNVxwWHd5XdxcodK3u8mLA7WZbFqIJRHWF6dMFoivOKFaQHsnC98/z5u8PzM89AWRl8/vOdA3S81QWTpaua57CsrO4DtBZHEUkJhWsRkQjtwXaqmqrY1bSLyqZKqpqqCLrYgNSXqfCyLIuS/JKO0ehwkM4yhZwBzzmor4eaGr/09mc+A9ddBwcf3LneedOm9PQnfM7rrvMXNX7iE/C978GvfuWX5C4q8sFav6SJpJ3CtYgMa62BVnY1+iC9q2kXtS21JFIu19NUeGbGyLyRHSF6dMFoRuaPVJAeDAIBqK31Qbqmxt+vrd09Cj1hAlx1FfzoR6mf4zk7e/dCKIWFu++/+SY88oi/eBB8uD7wQF/zPGFC8vshIglTzbWIDCuNbY0dJR6VTZXUt9b3+jkqmyq57P8u67RKYV5WHn+/8O/sN3Y/RuWPoiS/RBcXDgatrZ1DdE2NH6FORH/qncMiw3NkgA5/1dLcIgOSaq5FZFhyzlHfWt9R4rGrcVdSZvF4aNVDMVPhYfD4u49z5+l39vv5JUUaG2ODdF8XU+mp3jksJ6fziHP0fYVnkSFH4VpEhoygC1LTXNNR4lHZVJnQaoc9ycvOY9yIcYwtHMu4wnFc//z1MVPhtQZaeXXzq/0+lyRBMLi7PjqyvKO9vedjExFZY33EEXDUUXDjjfA//+PnfI4M0P1ZKEVEBiWFaxEZtDbXbOa8R85j4UkLybZsqpqrCAT7ObUZMCJ3hA/SoUBdnFfcafvyK5b3+xySJO3tuwN0+GtdXWpm6cjLg1GjoLISfvc7X29dVOS/Hn64r3feZ5/kn1dEBhXVXIvIoFLbUktFQwU7Gnbww5d+yJPrnuST+32yz1PhAYzMH8m4wnEdgbogpyCJPZakaW72Afr222Hfff1CKg0NftuKFbB2LZx9dnLONWIElJT4MB2+FehzISKeaq5FZNBqDbSys3EnOxp2UNFQ0VEzXdlUyTPrn+nVVHjgZ/EYXTC6o8RjbOFYcrP1p/sBJTztXfSIdGur3z5qlJ+tIzxDR2SZRm+ZwciRnYN0SYnKOUSkzxSuRWRAcc5R3Vztw3RjBVVNVXH362kqvLDsrGzGFIzpKPEYUzBGs3gMJOGyjsggHTntXTyzZvkg3dulv7Ozd4fncJAeOVKLqYhIUilci0jGNbc3s6NhBzsadrCzcWePFyGGF3AJX1TYHmzvGL2eVDSJsYVjO0o8SvJLNLf0QBEu64gcjQ6XdfTWrFk+WIenwosO1vn5sWUdI0ZoURURSTmFaxFJu6ALsqtxV8fodF1LXa+Ojxy1DnM4Xt70Mnd/6m4tHZ5pzvnQHBmia2v9UuHJEjkV3pNP+lk6Tjhh98i06qNFJEMUrkUkLepb6zvqpnc17erzrB5ZlsW6ynUxU+G1Bdso21qmYJ1u7e1+do7o2ToC/Z+1JUZWli/jWLMGfvIT+OMf4fTT4eWX4bzzYOZMf5GjiEgGKVyLSEq0BdrY1bSro9yjqa2pz89VlFfExKKJTBgxgfEjxnP6AacnsaeSsPCI9I9+5Gfr2G8/H6Qh+bN15OZ2vsBw1Cg/7V1WFvzrX/DoozB/vt93/nx46CE/FV64TUQkQxSuRSQpnHPUtNR0TJNX1VxFX6f6zMnKYfyI8UwomsDEoomMyB2R5N5KQtrbobraz+tcVeVvbW1QXAzf+U5yZusAH5rDATr8tbuyjquvjm2bP1/BWkQGBIVrEemzjVUbOf/R87lp/k0452gNtPb5uUYVjGLCCB+mxxSO0UWImdDQ4AN0OEzX1fnR6mh9na0jXNYRGaJLSvwS4SIiQ4T+RRORXmkLtLGtfhvldeXc9OJNvFH+Bj//1897vYhLXnZex8j0hBETyM/JT1GPJa5AwI9Kh0ekKyt3zyOdiJ5m6wiXdUSG6OJiTXsnIkOewrWI9Cjoguxo2MHm2s1sr99O0AU7psNLdBEXM2NMwRgmFk1kYtFESvJLdPFhOjU1dS7vqKmJPyqdqHizdRx//O4gXViYtK6LiAwmCtciEpdzjl1NuyivLWdr/daYuacTWcSlMLewI0yPKxynlRDTJRj04TkyTDc3J+e5s7Nh/Xq/BPn998OnPgWvvLJ7to4DDkjOeUREBimFaxHppKa5hvK6cspryzuWGo/W1SIunzvsc+w3dr+Oco/ivOJ0dn34am7uXCtdU9P9Coe9UVQEY8b429ixvmb6ttvgscc0W4eISBwK1yJCQ2tDR6Cub63vcf+uFnH55wf/5MJZF6aqmwK+lOPmm/18zjNn+kDd1JScqfCysmD0aB+iw4E6P04tvGbrEBHpksK1yDDV0t7ClrotlNeVU9VU1atj39v1XtxFXF4rfy2ZXRTwI9Dh6fDCt+xs+PKX+z8VXmFh51HpkhJdcCgi0k8K1yLDSHuwnW3129hcu5mdjTt7NQ91TlYOk4snM61kGuu+sU4XI6ZKW9vuEo9du3ywji7x6MtUeFlZ/mLDyDCtJcJFRJJO4VpkiAu6IBUNFX6mj4btvVp23MyYWDSR6SXTmVQ0ieys7BT2dJhqbt4dpCsr/RLiiehpKrz8/M7lHaNHa1RaRCQNFK5FhiDnHJVNlZTXlbOlbkvMTB89GVs4lukl05kycgp52Xkp6uUwVV/fOUw3NvbteaKnwjvqKDjxxN1heoRWtRQRyQSFa5EhpLallvLacsrrymlqa+rVsSX5JUwrmca0kdMozNUcxUkROSVe+NabhVq68s47fsaOX/7Sj14vXw4XXACHHw6HHtr/5xcRkT5TuBYZ5Jramthcu5nyunLqWup6dWxhbiHTRk5jesl0RuaPTFEPh5H29t310uFp8QKJl+F0KVziMXYsjBsHq1bB44/vnp3jxBM1FZ6IyABhvbmgaaArLS11ZWVlme6GSMoFXZCtdVsp21LG95/7Plcfc3W3qyNGysvOY+rIqUwrmcaYgjG6MLEvFi6EuXPhmGN2l3c88wy8/TZ89rP9f/6iot1BeuxY/1hERAYMM1vmnCuNty2lI9dmdiVwGWDAb51zPzOz24BPAa3A+8AXnXPVcY7dCNQBAaC9qxcgMpy0BlrZVL2JjdUbaW5v5jfLfsOqilVxV0eMlJ2V7Wf6GDmNCUUTyDJd2NZnjY0wdSp85jNw1VX9mwoPwMxPgRcO0mPHxp9bWkREBoWUhWszOxQfrOfhg/RTZvY34BngOudcu5ndClwHdPU/0nzn3M5U9VFksKhrqWN91Xo2124m6Py0bOFVEh2OJeuXsODQBZ1Gr82MCSMm+Jk+iieRk6UqsD6rrYWtW2HbNn9/1CgfrHszFV5YdvbuqfDCs3nk6HsjIjJUpPJf9IOA151zjQBm9iLwWefcwoh9/gWck8I+iAxazjl2NOxgQ/UGKhoqYrZHrpIYJNgxej22cCzTSqYxdeRUzfTRV875Uo9t2/wt3owePU2FF5aXtztIjx3rg7mmxBMRGbJSGa5XAj8ys3FAE3AaEF0Q/SVgURfHO+BpM3PAb5xzd6espyIDSHuwnQ9rPmRD9QYaWhvi7hMetQ6vktgebOfZDc9yx2l3sM+YfdLZ3aEjEICdO/0I9fbtPc/qETkV3uLFcNhhPmCPGNH54sOiIl/6ISIiw0LKwrVz7t1Q2cfTQAOwHF8/DYCZfR9oBx7o4ik+5pwrN7OJwDNmtto591L0TmZ2OXA5wJ577pncFyGSRo1tjWyo2sAHNR/ELC0eLXLUOszh+MmrP+HO0+9MZTeHlrY22LHDB+odOxKf2SOyxvroo+Gkk3yZyJ//DCeckNo+i4jIgJbSQj/n3D3APQBm9t/A5tD9S4AzgBNcF9OVOOfKQ193mNnj+NrtmHAdGtG+G/xsIcl/FSKptatxF+ur1rO9YXtCy5HnZueyoXpDTABvDbTy6uZXU9XNoaO5eXe5x86dvgSktz78EO68Ez79aRgZmsJwn338VHinnJLU7oqIyOCS0qn4zGxiKBzviR/BPip0+x/gE8652EJSf1wRkOWcqwvdfwa4yTn3VHfn01R8MlgEXZDy2nLWV62ntiWx5a6L8orYZ8w+TC+ZrosTe6u+fnegrqrq/fFmvsRj8mR/K9QiOyIiw1nGpuIDHg3VXLcBX3POVZvZL4F8fKkHwL+cc1eY2VTgd86504BJwOOh7TnAn3oK1iKDQUt7CxurN7KpZhMt7S0JHTOhaAL7jNmHCSMmaE7qRDnnV0bcts2XfNTX9/45srNhwgQfpidN8hcmioiI9CDVZSHHxmnbr4t9t+AvesQ5tx44PJV9E0mnmuYa1letZ0vdlo6p9LqTnZXN9JLpzBg9QysnJioY9Au6hEeom5t7/xy5uT5IT5nig3V2dvL7KSIiQ5r+tiySIs45ttVvY33VeiqbKhM6piCngBljZrDnqD01jV53wiskfvzj/kLEbdvg6afh3Xfh7LN791yFhbvLPcaO1TR5IiLSLwrXIknWFmjjg5oP2FC9gaa2poSOGVM4hn3G7MPk4slaPTERBx3klxm/+mo49NDer5A4cqQP01Om+HmnRUREkkThWiRJ6lvr2VC1gQ9rPyQQ7HlKNzNj6sip7DNmH0YXjE59Bwe79nYoL4eNG/3j734XfvzjxFdIHDNmd6AuKkpLl0VEZPhRuBbpp4qGCtZXrWdHw46E9s/LzmOv0Xux9+i9KcgpSHHvhoDqati0yQfryHmoe1ohMSvLz/AxZYqvoy7Qey0iIqmncC3SR1vqtvDah69xwws3cPUxVzOmYEy3+4/MH8k+Y/Zh2shpZGfpQrluhUepN23ys37EE2+FxDlzYOJEP0I9caK/QFFERCSNFK5FeqmqqYp3Kt6hqqmKe966h1UVq1i0chFXlF4Rd/9JxZPYZ8w+jB8xPs09HYRqanaPUrd3s0plZI31kUfCiSf6+w895B+LiIhkiMK1SIIa2xp5t+JdttRtAaCyqZIl65fgcCxZv4QFhy7oGL3Oycphj1F7MGP0DIryVN/brfZ22LLF11J3NUodbe1a+OEP4TOfgalT/ZR5++7rV0jU8uMiIpJBCtciPWgLtLG2ci0bqjZ0mqN60TuLcPgVToMEWbRyEd85+jvMGDODPUr2IDdbJQndSnSUOlJODkyfDr/4BZSUdN42f76/iYiIZJDCtUgXgi7IxuqNrNm1hrZAW6dt4VHr9qAPhe3Bdp7b8By/O/N3TBk5JRPdHRzCo9SbNvkLFRM1ejTstRdMm6aFXUREZEBTuBaJY2vdVt7d+S4NrQ1xt0eOWocFCfLDl37InaffmY4uDi61tT5Qb97c+1HqvfaKHaUWEREZoBSuRSJUN1fzzo53elxR8b1d73WMWoe1Blp5dfOrqeze4BII+JKPDz6AqqrEjwuPUk+d6gO2iIjIIKL/uUTwFyuu3rma8trybvfLzspm3zH78t7X3yMnSz8+cfV1lHraNB+qtWKiiIgMYkoHMqx1dbFiPHuM2oMDxx+ohV/iCQR211L3ZpR61KjdtdQapRYRkSFA/5vJsBR0QTZVb2LNrjW0Blq73Xf8iPEcMvEQSvJV99th4UKYOxdKS3ePUi9b5qfIO/vs7o/NzvZheu+9NUotIiJDjsK1DDvb6rexqmJVlxcrhhXnFXPIxEOYWDQxTT0bRA480M8xfdVVftnxyEVdulJS4gO1RqlFRGQI0/9wMmxUN1ezqmIVuxp3dbtffk4+M8fNZM9Re2JmaerdILFzpx+dNvPB+tZb4bTT/PLj11zjg3ak8Cj1Xnv5CxVFRESGOIVrGfKa2pp4d+e7PV6smGVZ7Dt2X/Ybu58uVoy2Y4cP1ZURs6jMmuWD9aJFsGBB52BdUrK7ljpXi+mIiMjwoQQhQ1Z7sJ11let4v/L9Hi9WnF4ynQPHH0hhbmGaejcIOAfbt8OaNfGXJV+xwo9YL1jgvx5+OHzykz5UjxmT/v6KiIgMAArXMuQ459hUs4n3dr7X48WK40aM45AJhzCqQBfWdXDOz/yxdi3U1cXfJ7LG+qij4Mwz4Zvf9Pdnz05rd0VERAYShWsZUrbXb2dVxSrqW+u73a84r5iDJxzMpOJJaerZIBAM+kVf1q6Fhu4v9mTtWrjxRjjvPL/YS1YWTJkCS5fC/Plp6a6IiMhAZM65nvcaJEpLS11ZWVmmuyEZUNNcw6qKVexs3NntfnnZecwc7y9WzLKsNPVugAsG/SqK69ZBU1PP+5eUwP77+zCtCz5FRGQYMrNlzrnSeNs0ci2D2oaqDZzz8Dlc+ZErGVPQdZ1vlmWxz5h92G/sfuRm6wI7wC/8smkTvP8+NDf3vP/o0T5UT5qkUC0iItIFhWsZtNZXrecbi7/BW1vfYtHKRVxRekXc/aaVTOOg8QfpYsWw9nbYuNGH6tbua9IBGDsWDjgAJkxIeddEREQGO4VrGXQCwQDLty1n5Y6VPLP+GRyOJeuXsODQBZ1Gr8cWjuWQiYcwumB05jo7kLS1wfr1sGGDv9+TCRP8SPW4canvm4iIyBChcC2DSkNrA2VbyqhtqWXRO4tw+GsGggQ7Rq+L8oo4eMLBTC6enOHeDhAtLT5Ub9zoR617MmmSD9WaTk9ERKTXFK5l0KhoqGDZ1mW0BdqobKpkyfoltAd9WGwPtrNk/RJuOO4GSqeW6mJF8HXU77/v66oDgZ73nzrVh+qSktT3TUREZIhSuJZBYV3lOt6teLfjceSodZjD8fvlv2fetHnp7t7A0tjoZ/748EM/E0h3zPwqivvvD8XF6emfiIjIEKZwLQNae7Cd5duWs7Vua6f21TtXd4xah7UF23h186vp7N7AUl/vQ/XmzX4hmO5kZcH06bDfflBUlJ7+iYiIDAMK1zJgNbQ2sHTLUupaYlcJ/PmpP2dE7gjmTptLSf4wL2OorfWLumzZ0vO+WVl+efJ994VCzZ4iIiKSbArXMiDtaNjBm1vfpC0Qf1aLCUUTmDNlDnnZeWnu2QCxcCEccghMnAjbtvm2FSt8yD777Nj9s7Nh7719qM7PT2tXRUREhhNd9SUDztpda3l98+tdBut9x+7LR6Z9ZPgG6/Z2P03e5z4HTz/t21asgFtv9bXTkXJz/RzVJ54IBx+sYC0iIpJiGrmWAaOr+uqw7KxsZk+ezdSRU9PcswFkyxZ45x0YPx6uvtoH6tNOg8WL4ZprYNYsv19eHuyzjx+tztWKlCIiIumicC0DQkNrA2+Uv0F9a33c7cO+vrq+Hv79b9i5c3fbrFk+WC9aBAsW+Mf5+f4ixT33hBz9eIuIiKSb/veVjNtev503t74ZM/tH2ISiCRw55Uhys4fhCGx7u6+jXr8+dlq9FSv8iPWCBfDkk3DWWXD66f6iRREREckIhWvJGOccayvX8t7O97rcZ7+x+3Hg+AMxszT2bIDYuhVWrvSLwUQL11hfd52/gPGyy+D882HyZJg/P/19FREREUDhWjKkPdjOW1vfYlv9trjbh3V9dX29D9UVFV3vs3atD9cXXwwjRviLFh96CJYuVbgWERHJIIVrSbv61nqWli/tsr66KK+IuVPnMjJ/ZJp7lmGBgA/N77/f/cqKI0b4YD1pUuf2+fMVrEVERDJM4VrSalv9Nt7a+laX9dUTiyYyZ8qc4VdfvW2bH61uaup6n6wsf7Hifvv5eatFRERkwFG4lrRwzrFm1xrW7FrT5T77j9ufmeNmDq/66oYGH6p37Oh+v4kT4dBDtVS5iIjIAKdwLSnXFmjjrW1vsb1+e9zt2VnZHDH5CKaMnJLmnmVQoiUghYU+VE+enL6+iYiISJ8pXEtK1bfW80b5GzS0NsTdPizrq1UCIiIiMmQpXEvK9FRfPal4EkdMPmL41FerBERERGTIU7iWpHPO8d6u91i7a22X+xww7gAOGHfA8KivDgRg3Tp/UwmIiIjIkKZwLUnVFmjjza1vsqMh/uhsTlYOR0w5gsnFwyRAbt/uR6sbG7veJysL9t0X9t9fJSAiIiKDnMK1JE1dSx1Ltyzttr563rR5FOcVp7lnGdDY6EP19vgXcXaYMAEOO0wlICIiIkOEwrUkxda6rSzftlz11cGgL/9Yu7b7EpCCAl8CMmUYzZAiIiIyDChcS78kUl89c/xM9h+7/9Cvr96xw49WN8QfuQd8Ccg++/jlylUCIiIiMuRkZboDMni1BdpYvHYx5zx0DlXNVTHbc7JymDtt7tC/cLGxEZYuhddf7z5Yjx8Pn/gEHHSQgrWIiMgQpXAtfdIWaOOfH/6TO5feyaqKVSxauajT9uK8Yo7d69ihe+HiwoXw7LO+/OOFF/zc1StWwKOPxu5bUABHHglHHw3Fw6DeXEREZBhTuJZec86xbOsyNlVvYsn6JTgcS9Yv6Ri9nlw8mWP3OnZoX7h4+OFw9tnw0EN+qr0VK+DWW/2MH2FmfhaQ+fNh6tTM9VVERETSRjXX0murKlZR0VDBoncW4XAABAmyaOUifnrqT4d+fXV1ta+dvuoqH6hPOw0WL4ZrroFZs/w+48f7CxZHDqOVJ0VERCS1I9dmdqWZrTSzd8zsW6G2sWb2jJmtDX0d08WxF4f2WWtmF6eyn5K4D2o+YH3VeiqbKlmyfknH7CDtwXae2/AcJfklQztYb9kC//wnNDf7IH3aabBokf86a5YvAZkzx5eAKFiLiIgMOykL12Z2KHAZMA84HDjDzPYDrgWedc7tDzwbehx97FjgBuAjoeNv6CqES/pUNlXy7+3/Bug0ah0WJMjNL96cia6lx3vvwbJlu6fYW7HCj1gvWABPPgk7d/oSkGnTMttPERERyZhUjlwfBLzunGt0zrUDLwKfBc4Cfh/a5/fAp+McewrwjHOu0jlXBTwDnJrCvkoPGtsaWVq+lKDzwXL1ztUxc1q3Blp5dfOrmeheagUCPlSvWbO7LVxjfc018KUvwR//CFdfDS+/nLl+ioiISMalsuZ6JfAjMxsHNAGnAWXAJOfc1tA+24BJcY6dBnwY8XhzqC2GmV0OXA6w5557Jqfn0kl7sJ2l5UtpDbR2tP381J8DsMeoPZg9eXaGepYGzc1+mr3q6s7ta9f6YH3ssTB3LhQW+osbly71o9ciIiIyLKUsXDvn3jWzW4GngQZgORCI2seZmYtzeG/OczdwN0BpaWm/nktiOed4a+tb1LbUxmwbWziWWZNmZaBXaVJd7cNyc3PstrPP9qsrHnHE7jmr589XsBYRERnmUnpBo3PuHufckc65jwNVwBpgu5lNAQh93RHn0HJgj4jH00Ntkmbv7XqPbfXbYtoLcwspnVpKlg3R2Ry3bIFXX40frMFPuXfkkVoMRkRERDpJ9WwhE0Nf98TXW/8JeAIIz/5xMfDXOIf+AzjZzMaELmQ8OdQmaVReWx53WfPsrGzmTZtHfk5+BnqVBmvW+BrrQCB2W1aWnw3kwAP9PNYiIiIiEVI9z/WjoZrrNuBrzrlqM7sFeMjM/gPYBJwHYGalwBXOuUudc5VmdjOwNPQ8NznnKlPcV4lQ3VzN8m3L4247YvIRlOSXpLdD6RAIwPLlftQ6nvx8X189RhPXiIiISHzm3NApUy4tLXVlZWWZ7sag19zezMubXqa5PbYkYub4mRww7oAM9CrFurpwMaykBObN8xcuioiIyLBmZsucc6XxtmmFRukkEAywtHxp3GA9deTUoRmsu7twEWIvXBQRERHpgsK1dPL29repbq6OaR9VMGpoTrm3ZYsvBYlXXw3+wsWZM1VfLSIiIglRuJYOa3etpbw2dlKW/Jx85k2bR3bWEBu5XbPGr7oYT1YWzJ6t1RZFRESkVxSuBYBt9dtYvXN1THuWZTF36lwKcgoy0KsU0YWLIiIikiIK10JtSy1vbn0z7rbDJx/OmMIhFDJ14aKIiIikkML1MNfS3sIb5W8QCMbWHO83dj+ml0zPQK9SpKYG3nhDFy6KiIhIyihcD2NBF6RsSxlNbU0x2yYVT+LA8QdmoFcpogsXRUREJA0Uroexf2//N5VNsWvzjMwfyZwpc7ChEjR14aKIiIikicL1MLW+aj0f1HwQ056Xnce8afPIyRoCHw1duCgiIiJpNgQSlPRWRUMFqypWxbSbGaVTSxmROyIDvUoyXbgoIiIiGaBwPczUt9azbOsy4i17f9jEwxg3YlwGepVkPV24OHkyzJmjCxdFREQk6RSuh5G2QBtvlL9BW6AtZtuMMTPYa/ReGehVkunCRREREckghethwjnHsq3LaGhtiNk2oWgCh0w4JAO9SjJduCgiIiIZpnA9TLxT8Q4VDRUx7UV5RRw55cjBPTNIIABvvw3lsUu3A7pwUURERNJG4XoY2FS9iQ1VG2Lac7NzmTdtHrnZuRnoVZLowkUREREZQBSuh7hdjbv4945/x7SbGXOmzKE4rzgDvUqChQvhoIOgoGD3hYsrVsDatXD22f6xLlwUERGRNMvKdAckdRrbGinbUhZ3ZpCDJxzMxKKJGehVkhx8MHz+835WEPDB+tZb/QWL4L+WlipYi4iISFpp5HqIag+280b5G7QGWmO27TFqD/YZs08GepUkjY2+jvrqq32gPu00WLwYrrnGX7SoCxdFREQkQxSuhyDnHG9ufZO6lrqYbWMLxzJr0qwM9CpJWlvh9dd9KcisWT5YL1oECxb4ixZ14aKIiIhkkMpChqDVO1ezvX57THthbiGlU0vJskH6bQ8EfBlIfb1/vGKFH7FesACefBKCQQVrERERyahBmrKkK5trN7Oucl1Me3ZWNvOmzSM/Jz8DvUqCYBDKyqCqyj8O11hfcw38x3/40euLLoLnn89sP0VERGRYU7geQqqaqnh729txt82ZMoeS/JI09yiJVqyAHTt2P1671gfrI4+Eo46CU0+Fhx7y0/KJiIiIZEhCNdfmVxh5HLjOOfduarskfdHc3szSLUsJumDMtgPHH8jk4skZ6FWSrF4NH37Yue3ss/2qi3PnwsiRvm3+fH8TERERyZBER65PBuYCl6awL9JHgWCAN8rfoKW9JWbbtJJp7D9u/wz0Kkk2bPCj1NHM/Kj1uHHp75OIiIhIFxIN1/+BD9afMjPNMDLALN+2nJrmmpj2UQWjOHzS4RnoUZJs2QIrV8bfdthhfpEYERERkQGkx3BtZuOBQ5xzTwJLgE+nulOSuDW71rClbktMe0FOAfOmzSM7a5AuorJzJ7z1VvxtBxwAe+2V3v6IiIiIJCCRkeuLgD+H7t+HSkMGjK11W3ntw9e47tnrqGqu6mjPsizmTptLQU5BBnvXD7W1/sLEYGz9OHvuCTNnpr9PIiIiIglIJFx/CR+qcc4tBaaY2R4p7ZX0qKW9hbe2vcWidxaxqmIVi1Yu6tg2e/JsRheMzlzn+qOxEf71L2hvj902ebJfOEZERERkgOo2XJvZaOCXzrnyiObvAuNT2Snp2Qc1H1DRUMGS9UtwOJasX0JVcxX7j9ufaSWDdOnv1lYfrFtiL8xk7FiYM8dfyCgiIiIyQHUbrp1z1c6530S1PeOc66IYVtJlR8MOFr2zCIcDIEiQJ957gpnjBmnJRCDglzVvaIjdNnIkzJsH2YO0flxERESGjV4tImNmb6aqI5K4tkAb71e9z5L1S2gP+vKJ9mA7f1/zd7Y3xC57PuCFV1+sro7dVlAAH/kI5OamvVsiIiIivdXbFRr1N/kBYGfjTh5c+WDHqHVYkCA3v3hzhnrVD9GrL4bl5vrVFwsL098nERERkT7obbj+e0p6Ib2yo2EHq3eu7hi1DmsNtPLq5lcz1Ks+evfd2NUXwa++OG/e7tUXRURERAaBXi0I45z7z1R1RBK3o2EHPz/1553a5k6bO/iWON+wAdati20Pr744dmz6+yQiIiLSD70duZYMq22ppbm9uVNblmUxfsQgm8BFqy+KiIjIEKRwPcjsaIitTR5bOJacrEG0Kn13qy/OnKnVF0VERGTQ6u1sIfua2WGp6oz0LF64nlg0MQM96aOamq5XX9xrL7+0uYiIiMgglfBwp5l9D9gPCJpZvnPuotR1S+JpD7ZT2VQZ0z5ownVjo5/LuqvVFw/T720iIiIyuHUZrs3sm8CdzrlAqOlw59yC0LYV6eicdFbRUIFznaffK8gpYGT+IJhRQ6svioiIyDDQXVnILuApMzsz9PhpM3vKzJ4G/pH6rkm0QVsS0t6u1RdFRERkWOgyXDvnHgA+BcwysyeAZcBngXOdc1elqX8SIV64nlQ8KQM96YWeVl886iitvigiIiJDRk8XNO4LPARcDnwN+Dmg5fIyoK6lLmYKPjMb2FPwOQdvvw0VFbHbwqsvFhSkv18iIiIiKdJdzfX9QBswAih3zl1mZkcAvzWzpc65m9LUR2GQTsG3ejVs3hzbrtUXRUREZIjqLpkd4Zw7HMDM3gJwzr0FfMrMzkpH52S3QVdvvX69Vl8UERGRYae7cP2kmf0DyAX+FLnBOffXlPZKOhl0U/CVl8M778TfNmuWVl8UERGRIavLcO2cu9bMSoCgc64+jX2SKDsbdxJ0nRddKcgpoCS/JEM96kZFBSxfHn/bzJmw555p7Y6IiIhIOnVbsOucq01XR6Rrg6YkpKbGzwyi1RdFRERkmOrV8ueSGYMiXDc0dL364pQpWn1RREREhgWF6wGurqWOpramTm0Dbgq+lhYfrLX6ooiIiAxzPYZrMzvXzEaG7v+nmT1mZnNS3zWB+KPWYwrGkJs9QBZeSWT1xSz9DiciIiLDQyKp53rnXJ2ZfQw4EbgHuCu13ZKwisbYBVgGREnIwoXw7LO+xrqmxretWAGPPurvFxZq9UUREREZdhIJ14HQ19OBu51zfwfyEnlyM/u2mb1jZivN7M9mVmBmL5vZ8tBti5n9pYtjAxH7PZHQqxli2oPt7GrcFdM+IJY8nzsXzjnHB2zwwfrWW2H//bX6ooiIiAxbiSzvV25mvwFOAm41s3wSKyeZBnwTONg512RmDwHnO+eOjdjnUaCrObObnHOzE+jfkLWrcdfAnYLv4IPhu9/1gfq002DxYrjmGjjiCF8KUlyc6R6KiIiIpF0iI9fnAf8ATnHOVQNjgasSfP4coNDMcvDLqG8JbwjNoX088Jde9HdYiVdvPaFoQgZ6EsU5v7T5rFk+WC9a5L8efrhWXxQREZFhrcdw7ZxrdM49BtSY2Z74FRtXJ3BcOXA78AGwFahxzj0dscungWe7mUu7wMzKzOxfZvbprs5jZpeH9iurqIitTx7MBuwUfFu3Qm2tLwVZvBgWLPBf6+pg0gAoWRERERHJkETKO840s7XABuDF0NcnEzhuDHAWMAOYChSZ2ecjdrkA+HM3T7GXc64U+BzwMzPbN95Ozrm7nXOlzrnSCRMGwKhuktS31tPY1tipzcyYMCLDr9E5eO+93TXW11wDF14IP/oRfPOb8Pzzme2fiIiISAYlUhZyM3AUsMY5NwM/Y8i/EjjuRGCDc67COdcGPAZ8FMDMxgPzgL93dXBo5Bvn3HrgBeCIBM45ZAzYKfjKy6G+Htau9cF61izffsEF8NBDsHRpZvsnIiIikkGJXNDY5pzbZWZZZpblnHvezH6WwHEfAEeZ2QigCTgBKAttOwf4m3OuOd6BoVHvRudcSyiIHwMsTOCcQ8aALAkJBv2oNcDZZ+9uHz/e3+bP9zcRERGRYSqRcF1tZsXAS8ADZrYDiLNiSGfOudfN7BHgTaAdeAu4O7T5fOCWyP3NrBS4wjl3KXAQ8BszC+JH129xzq1K8DUNeoFgIO4UfBkP1x9+CI2Nse0zZ6a/LyIiIiIDUCLh+iz8yPO3gQuBUcBNiTy5c+4G4IY47cfFaSsDLg3dfxU4LJFzDEU7G3fGTMGXn5Of2Sn4gkFYsya2feJEzQ4iIiIiEtJjuHbOhUepg8DvU9sdgfirMk4YMQEzy0BvQjZtguY4VTwHHpj+voiIiIgMUIlc0ChpNuDqrQMBfwFjtClTYNSo9PdHREREZIBSuB5gGlobaGjtXNJuZpkN1xs2QEtLbLtqrUVEREQ6SWSe60+ZmUJ4msQbtR5dMDpzU/C1t8O6dbHt06bByJHp74+IiIjIAJZIaF4ArDWzhWamAtsUG3AlIevXQ1tb5zYzjVqLiIiIxJHI8uefxy/g8j5wv5m9FlpyXMOWSRYIBtjZuDOmPWPhuq0N3n8/tn2PPaCoKP39ERERERngEir3cM7VAo8ADwJTgM8Ab5rZN1LYt2FnV9OumCn48rLzGJWfoYsG33/fl4VEysqC/ffPTH9EREREBrhEaq7PNLPH8UuQ5wLznHOfBA4H/l9quze8dFUSkpEp+FpafElItD33hBEj0t8fERERkUEgkUVkzgZ+6px7KbLROddoZv+Rmm4NTwOq3nrdOj8FXySNWouIiIh0K5FwfSOwNfzAzAqBSc65jc65Z1PVseEm3hR8ABOKJqS/M83NsHFjbPvee0NBQbp7IyIiIjJoJFJz/TB+dcawQKhNkijeqPWYwjHkZeelvzNr1/rlziNlZ8N++6W/LyIiIiKDSCLhOsc51xp+ELqfgcQ3tHW15HnaNTbCBx/Etu+zD+Tnp78/IiIiIoNIIuG6wszODD8ws7OA2PnipM+CLjhwpuBbsyZ21Do3F/bdN/19ERERERlkEqm5vgJ4wMx+CRjwIfCFlPZqmNnVuItAsPPFg3nZeYwuGJ3ejjQ0wObNse377OMDtoiIiIh0q8dw7Zx7HzjKzIpDj+tT3qthJl699YSiCemfgu+998C5zm15eT5ci4iIiEiPEhm5xsxOBw4BCsKBzzl3Uwr7NawMiCn4amuhvDy2fb/9ICehj4mIiIjIsJfIIjK/BhYA38CXhZwL7JXifg0bjW2N1LfG/jEg7RczvvdebFt+vp9+T0REREQSksgFjR91zn0BqHLO/QA4Gjggtd0aPuKNWo8uGE1+Thpn5qipgW3bYtv3399PwSciIiIiCUkkXDeHvjaa2VSgDZiSui4NLwOiJGT16ti2wkLYS3+gEBEREemNRIpp/8/MRgO3AW8CDvhtKjs1XAyIKfgqK2FHbMDngAP8cuciIiIikrBuw7WZZQHPOueqgUfN7G9AgXOuJh2dG+riTcGXm52b3in44o1ajxgB06enrw8iIiIiQ0S3Q5POuSBwZ8TjFgXr5Im3KuPEoonpm4Jv507YtSu2feZMjVqLiIiI9EEiCepZMzvb0j7p8tAXd37rdM4SEm/UurgYpk1LXx9EREREhpBEwvWXgYeBFjOrNbM6M6tNcb+GvKa2Jupa6mLa01ZvvX07VFXFts+cCfo9SkRERKRPElmhcWQ6OjLcxBu1HlUwKj1T8DkXf17rkhKYoolgRERERPqqx3BtZh+P1+6ceyn53Rk+MjoF37Ztfm7raAceqFFrERERkX5IZCq+qyLuFwDzgGXA8Snp0TCQ0Sn4uhq1Hj0aJk1K/flFREREhrBEykI+FfnYzPYAfpaqDg0HlU2VtAfbO7XlZucypmBM6k++ZQvUxdZ6c+CBqT+3iIiIyBDXl/nWNgMHJbsjw0lXs4SkfEKWYDD+qPW4cTAhjbOUiIiIiAxRidRc34FflRF8GJ+NX6lR+ihj9dabN0NDQ2z7zJmpP7eIiIjIMJBIzXVZxP124M/OuX+mqD9DXldT8E0oSvHIcTAIa9bEtk+Y4EeuRURERKTfEgnXjwDNzrkAgJllm9kI51xjars2NMVblbEkv4SCnILUnviDD6CpKbZdtdYiIiIiSZPQCo1AYcTjQmBJaroz9MUrCZlUnOJZOgKB+KPWkyf7WUJEREREJCkSCdcFzrn68IPQ/RGp69LQFXRBKhpiR65TvuT5xo3Q0hLbrlprERERkaRKJFw3mNmc8AMzOxKIU18gPalqqoqZgi8nK4cxhSmcgq+9Hdati22fOtWvyCgiIiIiSZNIzfW3gIfNbAtgwGRgQSo7NVTFnYKvaAJZ1pcZERO0YQO0tnZuM9OotYiIiEgKJLKIzFIzOxAIp7H3nHNtqe3W0JT2Kfja2uD992Pbp02D4uLUnVdERERkmOpxyNTMvgYUOedWOudWAsVm9tXUd21oaW5vpralNqY9peH6/fd9wI5kBgcckLpzioiIiAxjidQjXOacqw4/cM5VAZelrEdDVLxR65ROwdfa6ktCou25JxQVpeacIiIiIsNcIuE62yLW5TazbCAvdV0amtJeErJunb+YMVJWFuy/f+rOKSIiIjLMJXJB41PAIjP7Tejxl0NtkiDnHDsbd8a0pyxcNzfHH7Xeay8oLIxtFxEREZGkSCRcXwNcDnwl9PgZ4Lcp69EQVNVcRVugc+1zSqfgW7fOL3ceKTtbo9YiIiIiKdZjWYhzLuic+7Vz7hzn3DnAKuCO1Hdt6IhXEjJ+xPjUTMHX1ASbNsW2z5gB+fnJP5+IiIiIdEhk5BozOwK4ADgP2AA8lspODTVpXfJ8zZrYUeucHNh339ScT0REREQ6dBmuzewAfKC+ANgJLALMOTc/TX0bElraW6hprolpT8mS5w0N8OGHse377AN5ugZVREREJNW6G7leDbwMnOGcWwdgZt9OS6+GkHij1iPzR1KYm4ILC9esAec6t+Xm+nAtIiIiIinXXdHvZ4GtwPNm9lszOwG//Ln0Qtqm4Kurg82bY9v33dcHbBERERFJuS7DtXPuL86584EDgeeBbwETzewuMzs5Tf0b1JxzVDRWxLSnJFy/915sW16ev5BRRERERNIikdlCGpxzf3LOfQqYDryFn55PetDVFHxjC8cm90Q1NbB1a2z7/vv7ixlFREREJC16NRecc67KOXe3c+6EVHVoKEnbFHzxRq0LCvyiMSIiIiKSNimYaFnC0lJvXVUF27fHtu+/v184RkRERETSJqXh2sy+bWbvmNlKM/uzmRWY2f1mtsHMlodus7s49mIzWxu6XZzKfqZCV1PwJT1cxxu1LiyEPfdM7nlEREREpEcpK8g1s2nAN4GDnXNNZvYQcH5o81XOuUe6OXYscANQCjhgmZk94ZyrSlV/ky3ehYzFecXJm4Jv4UI44IDOo9MrVsDatXDzzZClP0qIiIiIpFuqE1gOUGhmOcAIYEuCx50CPOOcqwwF6meAU1PUx5RIeUnI3LlwySU+UIP/euutcNhhMH168s4jIiIiIglLWbh2zpUDtwMf4OfLrnHOPR3a/CMzW2FmPzWz/DiHTwMilxrcHGqLYWaXm1mZmZVVVMSOFmeCcy714frww+Gqq3ygfuAB//Waa+D888E0HbmIiIhIJqQsXJvZGOAsYAYwFSgys88D1+Hnzp4LjKWf0/qFZi8pdc6VTpiQgiXF+6C6uTpmCr7srGzGjRiXvJPs2AGzZsFpp8GiRf7rMcfA1KnJO4eIiIiI9Eoqy0JOBDY45yqcc23AY8BHnXNbndcC3AfMi3NsObBHxOPpobZBIS1T8FVW+lKQxYthwQL/dds2jVqLiIiIZFAqVxj5ADjKzEYATcAJQJmZTXHObTUzAz4NrIxz7D+A/w6NfgOcjB/xHhRSXhLiHLz44u5SkFmzfK31178OkybB/PnJO5eIiIiIJCyVNdevA48AbwL/Dp3rbuABM/t3qG088EMAMys1s9+Fjq0EbgaWhm43hdoGvNZAK9XN1THtSQ3XdXV+Cr5wsAY48kh46CFYujR55xERERGRXjHnXKb7kDSlpaWurKwso30ory3nza1vdmorzitm/owkjiZv2rR7lpCwiRPhIx9J3jlEREREJC4zW+acK423TZMhJ9n2htjVElOyKmO0MWNi20REREQkrRSuk8g5R0VD7HSACtciIiIiw4PCdRLVtNTQGmjt1Jb0Kfja2qC+PrZ99OjknUNERERE+kThOonizRIyrnBccqfgizdqXVwMubnJO4eIiIiI9InCdRKlfAo+UEmIiIiIyACmcJ0krYFWqppig29awvXYsck9h4iIiIj0icJ1ksS7kLEor4iivKLkncS5+OFa9dYiIiIiA4LCdZKkpSSkvh7a2zu35eTAyJHJPY+IiIiI9InCdRI45zJXbz16NJgl9zwiIiIi0icK10lQ21IbMwVflmUxrjCJU/CBLmYUERERGeAUrpMg3qj1+BHjyc7KTu6JFK5FREREBjSF6yRIS0lIWxvU1cW2K1yLiIiIDBgK1/3UFmijqjkNU/BVV8e2FRVBXl5yzyMiIiIifaZw3U8VjRU45zq1jcgdkdwp+CB+uNaotYiIiMiAonDdT2kpCQGorIxtU7gWERERGVAUrvshbVPwgS5mFBERERkEFK77oballpb2lk5tWZbF+BHjk3uihgZ/QWOk7GwoKUnueURERESkXxSu+yHeqPW4EePSMwWfFo8RERERGXAUrvthR8MOKpsque7Z6zpmDFFJiIiIiMjwpXDdR+Ep+Ba9s4hVFatYtHIRoHAtIiIiMpwpXPfRzsad7GrcxZL1S3A4lqxfQnN7M8V5xck9USAAtbWx7aNHJ/c8IiIiItJvCtd9tKNhB4veWYTDz3EdJMijqx5N/omqqyFqHm0KC6GgIPnnEhEREZF+Ubjuo3d3vsuS9UtoD7YD0B5s56/v/ZVt9duSe6J4JSFjxyb3HCIiIiKSFArXfeCc46l1T3WMWocFXICbX7w5uSfraqYQERERERlwFK77wMx4e/vbHaPWYW3BNl7d/GpyT6aLGUVEREQGjZxMd2CweuvLb6X+JI2N0NJ5kRqysmDUqNSfW0RERER6TSPXA1m8UetRo3zAFhEREZEBRyltIFNJiIiIiMigonA9kClci4iIiAwqCtcDVTAYf/EYhWsRERGRAUvheqCqrvYBO1JBgV9ARkREREQGJIXrgUolISIiIiKDjsL1QKVwLSIiIjLoKFwPVArXIiIiIoOOwvVA1Nzsb5HMtHiMiIiIyACncD0QdbV4THZ2+vsiIiIiIglTuB6IVBIiIiIiMigpXA9ECtciIiIig5LC9UATDPo5rqMpXIuIiIgMeArXA01tbeziMXl5MGJEZvojIiIiIglTuB5o4pWEjB2b/n6IiIiISK8pXA808cL16NFp74aIiIiI9J7C9UCjixlFREREBi2F64GkpQUaGzu3mWnkWkRERGSQULgeSOKNWo8cCTk56e+LiIiIiPSawvVAopIQERERkUFN4XogUbgWERERGdQUrgcK57R4jIiIiMggp3A9UNTVQSDQuS03F4qKMtMfEREREek1heuBorIytm3MGD9biIiIiIgMCikN12b2bTN7x8xWmtmfzazAzB4ws/dCbfeaWW4XxwbMbHno9kQq+zkgqN5aREREZNBLWbg2s2nAN4FS59yhQDZwPvAAcCBwGFAIXNrFUzQ552aHbmemqp8DhsK1iIiIyKCX6gmUc4BCM2sDRgBbnHNPhzea2RvA9BT3YeBrbYWGhth2LR4jIiIiMqikbOTaOVcO3A58AGwFaqKCdS5wEfBUF09RYGZlZvYvM/t0qvo5IMSbJWTkSH9Bo4iIiIgMGqksCxkDnAXMAKYCRWb2+YhdfgW85Jx7uYun2Ms5Vwp8DviZme3bxXkuD4XwsoqKiiS+gjRSSYiIiIjIkJDKCxpPBDY45yqcc23AY8BHAczsBmAC8J2uDg6NfOOcWw+8ABzRxX53O+dKnXOlEyZMSO4rSBeFaxEREZEhIZXh+gPgKDMbYWYGnAC8a2aXAqcAFzjngvEONLMxZpYfuj8eOAZYlcK+Zo5zCtciIiIiQ0Qqa65fBx4B3gT+HTrX3cCvgUnAa6Fp9v4LwMxKzex3ocMPAsrM7G3geeAW59zQDNf19dDe3rktJweKizPTHxERERHps5TOFuKcuwG4IZFzOufKCE3L55x7FT9V39DX1ai1Fo8RERERGXS0QmOmxQvXmoJPREREZFBSuM401VuLiIiIDBkK15nU1gZ1dbHtCtciIiIig5LCdSbFWzymqAjy8tLeFRERERHpP4XrTFJJiIiIiMiQonCdSQrXIiIiIkOKwnUmKVyLiIiIDCkK15nS0OAvaIyUnQ0lJZnpj4iIiIj0m8J1plRWxraNHq3FY0REREQGMYXrTFFJiIiIiMiQo3CdKQrXIiIiIkOOwnUmtLdr8RgRERGRIUjhOhNqasC5zm0jRkB+fmb6IyIiIiJJoXCdCSoJERERERmSFK4zQeFaREREZEhSuM4EhWsRERGRIUnhOt0aG6GlpXNbVpYWjxEREREZAhSu0y3eqPXo0T5gi4iIiMigpkSXbl2FaxEREREZ9BSu0y1euB47Nv39EBEREZGkU7hOp0DAz3EdTRczioiIiAwJCtfpFG/xmIICfxMRERGRQU/hOp00BZ+IiIjIkKZwnU4K1yIiIiJDmsJ1Oilci4iIiAxpCtfp0tzsb5GysmDUqMz0R0RERESSTuE6XSorY9tKSiA7O/19EREREZGUULhOF5WEiIiIiAx5CtfpUl0d26ZwLSIiIjKkKFynQzCocC0iIiIyDChcp0NtrQ/YkfLzYcSIzPRHRERERFJC4TodVG8tIiIiMiwoXKeDwrWIiIjIsKBwnQ4K1yIiIiLDgsJ1qrW0QGNj5zYzGD06I90RERERkdRRuE61eKPWI0dq8RgRERGRIUjhOtVUEiIiIiIybChcp1q8cD12bPr7ISIiIiIpp3CdSs5p8RgRERGRYUThOpVqayEQ6NyWmwtFRZnpj4iIiIiklMJ1KqneWkRERGRYUbhOJYVrERERkWFF4TqVFK5FREREhhWF61RpbYWGhth2LR4jIiIiMmQpXKdKV4vH5Oamvy8iIiIikhYK16mikhARERGRYUfhOlUUrkVERESGHYXrVNDiMSIiIiLDksJ1KtTXQ3t757acHCguzkx/RERERCQtFK5ToauSELP090VERERE0kbhOhVUby0iIiIyLKU0XJvZt83sHTNbaWZ/NrMCM5thZq+b2TozW2RmeV0ce11on/fM7JRU9jPpFK5FREREhqWUhWszmwZ8Eyh1zh0KZAPnA7cCP3XO7QdUAf8R59iDQ/seApwK/MrMslPV16Rqa4O6uth2LR4jIiIiMuSluiwkByg0sxxgBLAVOB54JLT998Cn4xx3FvCgc67FObcBWAfMS3FfkyPeLCFFRZAXd4BeRERERIaQlIVr51w5cDvwAT5U1wDLgGrnXHgqjc3AtDiHTwM+jHjc1X4Dj0pCRERERIatVJaFjMGPQM8ApgJF+BKPZJ/ncjMrM7OyioqKZD9978UL12PHpr8fIiIiIpJ2qSwLORHY4JyrcM61AY8BxwCjQ2UiANOB8jjHlgN7RDzuaj+cc3c750qdc6UTJkxIXu/7wjmNXIuIiIgMY6kM1x8AR5nZCDMz4ARgFfA8cE5on4uBv8Y59gngfDPLN7MZwP7AGynsa3I0NPgLGiNlZ8PIkZnpj4iIiIikVSprrl/HX7j4JvDv0LnuBq4BvmNm64BxwD0AZnammd0UOvYd4CF8GH8K+JpzLpCqviZNvFHr0aO1eIyIiIjIMGHOuUz3IWlKS0tdWVlZ5jqwYgVs2tS5bb/94KCDMtMfEREREUk6M1vmnCuNt00rNCaT6q1FREREhjWF62Rpb4+/eIzCtYiIiMiwoXCdLNXVfraQSCNGQH5+RrojIiIiIumncJ0sKgkRERERGfYUrpNF4VpERERk2FO4ThaFaxEREZFhT+E6GRobobW1c1tWFpSUZKY/IiIiIpIRCtfJ0NXiMVl6e0VERESGE6W/ZFBJiIiIiIigcJ0cCtciIiIigsJ1/wUCUFMT265wLSIiIjLsKFz3V01N7OIxBQX+JiIiIiLDisJ1f6kkRERERERCFK77K164Hjs2/f0QERERkYxTuO6vrqbhExEREZFhR+G6P5qaoLm5c1tWlsK1iIiIyDClcN0f8UatS0q0eIyIiIjIMKUU2B+6mFFEREREIihc94fCtYiIiIhEULjuq1tugZdf7ty2YgXcd19m+iMiIiIiGadw3VcHHww//rEP1OC/LlwIxxyT2X6JiIiISMbkZLoDg9ahh8I118Ctt8Jpp8Hixf7+/PmZ7pmIiIiIZIhGrvuqqgpmzfLBetEi//WkkzLdKxERERHJIIXrvqqu9qUgixfDggX+68qVme6ViIiIiGSQykL6yjn4n/+BO++EmTNhzhy49FIYNUqlISIiIiLDlMJ1X735JjzyyO4gPW8ezJ0LS5cqXIuIiIgMU+acy3Qfkqa0tNSVlZVluhsiIiIiMoSZ2TLnXGm8baq5FhERERFJEoVrEREREZEkUbgWEREREUkShWsRERERkSRRuBYRERERSRKFaxERERGRJFG4FhERERFJEoVrEREREZEkUbgWEREREUkShWsRERERkSRRuBYRERERSRKFaxERERGRJFG4FhERERFJEoVrEREREZEkUbgWEREREUkSc85lug9JY2YVwKYMnHo8sDMD5x0q9P71j96//tH71z96//pH71//6T3sH71/fbOXc25CvA1DKlxnipmVOedKM92PwUrvX//o/esfvX/9o/evf/T+9Z/ew/7R+5d8KgsREREREUkShWsRERERkSRRuE6OuzPdgUFO71//6P3rH71//aP3r3/0/vWf3sP+0fuXZKq5FhERERFJEo1ci4iIiIgkicJ1L5jZqWb2npmtM7Nr42zPN7NFoe2vm9neGejmgGRme5jZ82a2yszeMbMr4+xznJnVmNny0O2/MtHXgcrMNprZv0PvTVmc7WZmvwh9/laY2ZxM9HMgMrOZEZ+r5WZWa2bfitpHn78IZnavme0ws5URbWPN7BkzWxv6OqaLYy8O7bPWzC5OX68Hji7ev9vMbHXo5/NxMxvdxbHd/qwPF128hzeaWXnEz+lpXRzb7f/Xw0EX79+iiPduo5kt7+JYfQb7QWUhCTKzbGANcBKwGVgKXOCcWxWxz1eBWc65K8zsfOAzzrkFGenwAGNmU4Apzrk3zWwksAz4dNT7dxzwXefcGZnp5cBmZhuBUudc3PlIQ//JfAM4DfgI8HPn3EfS18PBIfSzXA58xDm3KaL9OPT562BmHwfqgT845w4NtS0EKp1zt4QCyxjn3DVRx40FyoBSwOF/1o90zlWl9QVkWBfv38nAc865djO7FSD6/Qvtt5FuftaHiy7ewxuBeufc7d0c1+P/18NBvPcvavtPgBrn3E1xtm1En8E+08h14uYB65xz651zrcCDwFlR+5wF/D50/xHgBDOzNPZxwHLObXXOvRm6Xwe8C0zLbK+GnLPw/4g659y/gNGhX2qksxOA9yODtcRyzr0EVEY1R/4b93vg03EOPQV4xjlXGQrUzwCnpqqfA1W8988597Rzrj308F/A9LR3bBDp4jOYiET+vx7yunv/QtnkPODPae3UMKFwnbhpwIcRjzcTGw479gn9A1oDjEtL7waRULnMEcDrcTYfbWZvm9mTZnZIens24DngaTNbZmaXx9meyGdU4Hy6/g9Fn7/uTXLObQ3d3wZMirOPPoeJ+RLwZBfbevpZH+6+HiqtubeL0iR9Bnt2LLDdObe2i+36DPaDwrWklZkVA48C33LO1UZtfhO/nOjhwB3AX9LcvYHuY865OcAnga+F/uQnvWBmecCZwMNxNuvz1wvO1xSqrrAPzOz7QDvwQBe76Ge9a3cB+wKzga3ATzLam8HrAroftdZnsB8UrhNXDuwR8Xh6qC3uPmaWA4wCdqWld4OAmeXig/UDzrnHorc752qdc/Wh+4uBXDMbn+ZuDljOufLQ1x3A4/g/fUZK5DM63H0SeNM5tz16gz5/CdkeLjUKfd0RZx99DrthZpcAZwAXui4uekrgZ33Ycs5td84FnHNB4LfEf2/0GexGKJ98FljU1T76DPaPwnXilgL7m9mM0OjX+cATUfs8AYSvjD8Hf+GKRnboqO+6B3jXOfc/XewzOVyjbmbz8J9P/XICmFlR6EJQzKwIOBlYGbXbE8AXzDsKf6HKViRSl6M1+vwlJPLfuIuBv8bZ5x/AyWY2JvQn+5NDbcOemZ0KXA2c6Zxr7GKfRH7Wh62o60g+Q/z3JpH/r4ezE4HVzrnN8TbqM9h/OZnuwGARurr76/j/JLKBe51z75jZTUCZc+4JfHj8o5mtw19EcH7mejzgHANcBPw7Yuqf7wF7Ajjnfo3/heQrZtYONAHn65eTDpOAx0PZLwf4k3PuKTO7Ajrev8X4mULWAY3AFzPU1wEp9J/EScCXI9oi3z99/iKY2Z+B44DxZrYZuAG4BXjIzP4D2IS/IAozKwWucM5d6pyrNLOb8QEH4CbnXF8uShvUunj/rgPygWdCP8v/Cs0uNRX4nXPuNLr4Wc/AS8i4Lt7D48xsNr4kaSOhn+fI97Cr/6/T/woyK97755y7hzjXnegzmFyaik9EREREJElUFiIiIiIikiQK1yIiIiIiSaJwLSIiIiKSJArXIiIiIiJJonAtIiIiIpIkCtciIgOYmQXMbHnE7dokPvfeZqb5a0VEkkjzXIuIDGxNzrnZme5ET8xsjHOuKtP9EBHJNI1ci4gMQma20cwWmtm/zewNM9sv1L63mT1nZivM7Fkz2zPUPsnMHjezt0O3j4aeKtvMfmtm75jZ02ZWGNr/m2a2KvQ8DybQpatC/fiymZWk5lWLiAx8CtciIgNbYVRZyIKIbTXOucOAXwI/C7XdAfzeOTcLeAD4Raj9F8CLzrnDgTlAeMW6/YE7nXOHANXA2aH2a4EjQs9zRU+ddM59D78K6z7Am2Z2n5l9rE+vWERkENMKjSIiA5iZ1TvniuO0bwSOd86tN7NcYJtzbpyZ7QSmOOfaQu1bnXPjzawCmO6ca4l4jr2BZ5xz+4ceXwPkOud+aGZPAfXAX4C/OOfqe9HnbOAC4E580P9m3169iMjgo5FrEZHBy3VxvzdaIu4H2H0tzun4cDwHWGpmna7RCY1MLzezxRFtZmbHA78H/gs/Wv6TPvZLRGRQUrgWERm8FkR8fS10/1Xg/ND9C4GXQ/efBb4CfmTZzEZ19aRmlgXs4Zx7HrgGGAV0Gj13zn3ROTfbOXda6JgLgdXA14A/AQc55653zm3q30sUERlcNFuIiMjAVmhmyyMeP+WcC0/HN8bMVuBHny8ItX0DuM/MrgIqgC+G2q8E7jaz/8CPUH8F2NrFObOB/w0FcAN+4Zyr7qGfm4CPOecqEn5lIiJDkGquRUQGoVDNdalzbmem+yIiIrupLEREREREJEk0ci0iIiIikiQauRYRERERSRKFaxERERGRJFG4FhERERFJEoVrEREREZEkUbgWEREREUkShWsRERERkST5//l3PU+Vupl6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainBP3= [i[0] for i in trainAccBoth]\n",
    "trainNP3 = [i[1] for i in trainAccBoth]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainBP3, 'rx')\n",
    "plt.plot(trainNP3, 'g^')\n",
    "plt.plot(trainBP3, 'r', linewidth=5,alpha=0.3)\n",
    "plt.plot(trainNP3, 'g', linewidth=5,alpha=0.3)\n",
    "plt.legend([\"Back propagation\", \"Node Perturbation\"])\n",
    "plt.title(\"Accuracy vs epochs for the  perturbation = 0.01\", size=15)\n",
    "plt.xlabel(\"Epochs ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "0.9999999999999997\n",
      "Iteration: 1::Train accuracy: 9.846031746031745::Val accuracy: 9.971428571428572::Train Acc BP::9.844444444444445 Val Acc BP::9.185714285714287\n",
      "0.9999995164409428\n",
      "Iteration: 2::Train accuracy: 9.974603174603175::Val accuracy: 10.085714285714285::Train Acc BP::11.007936507936508 Val Acc BP::10.4\n",
      "0.9999985844600019\n",
      "Iteration: 3::Train accuracy: 10.096825396825396::Val accuracy: 10.4::Train Acc BP::12.304761904761904 Val Acc BP::11.942857142857143\n",
      "0.9999975219493955\n",
      "Iteration: 4::Train accuracy: 10.217460317460317::Val accuracy: 10.585714285714285::Train Acc BP::13.561904761904762 Val Acc BP::13.385714285714286\n",
      "0.9999965073297103\n",
      "Iteration: 5::Train accuracy: 10.353968253968254::Val accuracy: 10.557142857142857::Train Acc BP::14.844444444444443 Val Acc BP::14.657142857142858\n",
      "0.9999955591551105\n",
      "Iteration: 6::Train accuracy: 10.47936507936508::Val accuracy: 10.614285714285714::Train Acc BP::15.946031746031746 Val Acc BP::15.814285714285713\n",
      "0.9999946449000549\n",
      "Iteration: 7::Train accuracy: 10.617460317460317::Val accuracy: 10.757142857142856::Train Acc BP::17.114285714285714 Val Acc BP::17.085714285714285\n",
      "0.9999937375289455\n",
      "Iteration: 8::Train accuracy: 10.769841269841269::Val accuracy: 11.042857142857143::Train Acc BP::18.252380952380953 Val Acc BP::18.0\n",
      "0.9999928258591593\n",
      "Iteration: 9::Train accuracy: 10.96031746031746::Val accuracy: 11.314285714285715::Train Acc BP::19.28095238095238 Val Acc BP::19.3\n",
      "0.9999919064576464\n",
      "Iteration: 10::Train accuracy: 11.192063492063491::Val accuracy: 11.514285714285714::Train Acc BP::20.247619047619047 Val Acc BP::20.3\n",
      "0.9999909854383202\n",
      "Iteration: 11::Train accuracy: 11.420634920634921::Val accuracy: 11.600000000000001::Train Acc BP::21.22222222222222 Val Acc BP::21.585714285714285\n",
      "0.9999900701356013\n",
      "Iteration: 12::Train accuracy: 11.65079365079365::Val accuracy: 11.714285714285715::Train Acc BP::22.174603174603174 Val Acc BP::22.7\n",
      "0.9999891657135772\n",
      "Iteration: 13::Train accuracy: 11.912698412698413::Val accuracy: 11.899999999999999::Train Acc BP::23.153968253968255 Val Acc BP::23.614285714285714\n",
      "0.9999882744471709\n",
      "Iteration: 14::Train accuracy: 12.115873015873015::Val accuracy: 12.0::Train Acc BP::24.08730158730159 Val Acc BP::24.6\n",
      "0.9999873954286769\n",
      "Iteration: 15::Train accuracy: 12.268253968253969::Val accuracy: 12.2::Train Acc BP::25.023809523809526 Val Acc BP::25.585714285714285\n",
      "0.9999865263059154\n",
      "Iteration: 16::Train accuracy: 12.457142857142857::Val accuracy: 12.385714285714286::Train Acc BP::25.98412698412698 Val Acc BP::26.385714285714286\n",
      "0.9999856644582005\n",
      "Iteration: 17::Train accuracy: 12.598412698412698::Val accuracy: 12.514285714285714::Train Acc BP::26.81269841269841 Val Acc BP::27.400000000000002\n",
      "0.9999848086325619\n",
      "Iteration: 18::Train accuracy: 12.760317460317461::Val accuracy: 12.614285714285714::Train Acc BP::27.750793650793646 Val Acc BP::28.199999999999996\n",
      "0.9999839573215036\n",
      "Iteration: 19::Train accuracy: 12.917460317460316::Val accuracy: 12.757142857142856::Train Acc BP::28.61269841269841 Val Acc BP::28.985714285714288\n",
      "0.9999831105250694\n",
      "Iteration: 20::Train accuracy: 13.080952380952382::Val accuracy: 13.0::Train Acc BP::29.44126984126984 Val Acc BP::29.814285714285717\n",
      "0.9999822680356802\n",
      "Iteration: 21::Train accuracy: 13.206349206349207::Val accuracy: 13.17142857142857::Train Acc BP::30.319047619047616 Val Acc BP::30.7\n",
      "0.9999814288380513\n",
      "Iteration: 22::Train accuracy: 13.37936507936508::Val accuracy: 13.242857142857142::Train Acc BP::31.17936507936508 Val Acc BP::31.514285714285716\n",
      "0.999980594434652\n",
      "Iteration: 23::Train accuracy: 13.476190476190474::Val accuracy: 13.428571428571429::Train Acc BP::31.998412698412697 Val Acc BP::32.142857142857146\n",
      "0.9999797662027383\n",
      "Iteration: 24::Train accuracy: 13.587301587301587::Val accuracy: 13.571428571428571::Train Acc BP::32.768253968253966 Val Acc BP::32.9\n",
      "0.9999789425223171\n",
      "Iteration: 25::Train accuracy: 13.674603174603176::Val accuracy: 13.742857142857142::Train Acc BP::33.5984126984127 Val Acc BP::33.72857142857143\n",
      "0.9999781252683322\n",
      "Iteration: 26::Train accuracy: 13.803174603174604::Val accuracy: 13.87142857142857::Train Acc BP::34.336507936507935 Val Acc BP::34.31428571428572\n",
      "0.999977314945538\n",
      "Iteration: 27::Train accuracy: 13.922222222222222::Val accuracy: 13.942857142857143::Train Acc BP::35.09206349206349 Val Acc BP::35.12857142857143\n",
      "0.9999765119253564\n",
      "Iteration: 28::Train accuracy: 14.026984126984127::Val accuracy: 14.028571428571428::Train Acc BP::35.84444444444445 Val Acc BP::35.84285714285714\n",
      "0.999975716400569\n",
      "Iteration: 29::Train accuracy: 14.126984126984127::Val accuracy: 14.099999999999998::Train Acc BP::36.56507936507937 Val Acc BP::36.542857142857144\n",
      "0.9999749283161928\n",
      "Iteration: 30::Train accuracy: 14.238095238095239::Val accuracy: 14.2::Train Acc BP::37.161904761904765 Val Acc BP::37.128571428571426\n",
      "0.9999741475838191\n",
      "Iteration: 31::Train accuracy: 14.355555555555554::Val accuracy: 14.314285714285715::Train Acc BP::37.83015873015873 Val Acc BP::37.81428571428572\n",
      "0.9999733747001303\n",
      "Iteration: 32::Train accuracy: 14.463492063492064::Val accuracy: 14.371428571428572::Train Acc BP::38.477777777777774 Val Acc BP::38.32857142857143\n",
      "0.9999726096738659\n",
      "Iteration: 33::Train accuracy: 14.561904761904762::Val accuracy: 14.457142857142857::Train Acc BP::39.10634920634921 Val Acc BP::38.971428571428575\n",
      "0.9999718536795815\n",
      "Iteration: 34::Train accuracy: 14.63968253968254::Val accuracy: 14.499999999999998::Train Acc BP::39.75396825396825 Val Acc BP::39.614285714285714\n",
      "0.9999711057358021\n",
      "Iteration: 35::Train accuracy: 14.746031746031745::Val accuracy: 14.557142857142857::Train Acc BP::40.34285714285714 Val Acc BP::40.05714285714286\n",
      "0.9999703651328559\n",
      "Iteration: 36::Train accuracy: 14.853968253968253::Val accuracy: 14.657142857142858::Train Acc BP::40.87936507936508 Val Acc BP::40.68571428571428\n",
      "0.9999696329784439\n",
      "Iteration: 37::Train accuracy: 14.947619047619048::Val accuracy: 14.771428571428572::Train Acc BP::41.474603174603175 Val Acc BP::41.199999999999996\n",
      "0.999968908798597\n",
      "Iteration: 38::Train accuracy: 15.03015873015873::Val accuracy: 14.857142857142858::Train Acc BP::42.03650793650794 Val Acc BP::41.72857142857143\n",
      "0.9999681927068615\n",
      "Iteration: 39::Train accuracy: 15.10793650793651::Val accuracy: 14.985714285714286::Train Acc BP::42.592063492063495 Val Acc BP::42.35714285714286\n",
      "0.999967484938454\n",
      "Iteration: 40::Train accuracy: 15.214285714285714::Val accuracy: 15.1::Train Acc BP::43.13492063492064 Val Acc BP::43.042857142857144\n",
      "0.9999667850215108\n",
      "Iteration: 41::Train accuracy: 15.306349206349207::Val accuracy: 15.228571428571428::Train Acc BP::43.67936507936508 Val Acc BP::43.528571428571425\n",
      "0.9999660927731331\n",
      "Iteration: 42::Train accuracy: 15.403174603174602::Val accuracy: 15.285714285714286::Train Acc BP::44.13492063492063 Val Acc BP::43.885714285714286\n",
      "0.9999654079231636\n",
      "Iteration: 43::Train accuracy: 15.507936507936506::Val accuracy: 15.385714285714286::Train Acc BP::44.641269841269846 Val Acc BP::44.285714285714285\n",
      "0.9999647307105496\n",
      "Iteration: 44::Train accuracy: 15.6::Val accuracy: 15.442857142857141::Train Acc BP::45.141269841269846 Val Acc BP::44.871428571428574\n",
      "0.9999640606468017\n",
      "Iteration: 45::Train accuracy: 15.711111111111112::Val accuracy: 15.571428571428573::Train Acc BP::45.63809523809524 Val Acc BP::45.385714285714286\n",
      "0.9999633973773256\n",
      "Iteration: 46::Train accuracy: 15.807936507936507::Val accuracy: 15.714285714285714::Train Acc BP::46.10634920634921 Val Acc BP::45.77142857142857\n",
      "0.9999627415245088\n",
      "Iteration: 47::Train accuracy: 15.898412698412697::Val accuracy: 15.771428571428572::Train Acc BP::46.56984126984127 Val Acc BP::46.41428571428571\n",
      "0.9999620928307383\n",
      "Iteration: 48::Train accuracy: 15.971428571428573::Val accuracy: 15.871428571428572::Train Acc BP::47.08571428571429 Val Acc BP::46.84285714285714\n",
      "0.9999614504288715\n",
      "Iteration: 49::Train accuracy: 16.06190476190476::Val accuracy: 15.928571428571429::Train Acc BP::47.50793650793651 Val Acc BP::47.385714285714286\n",
      "0.9999608156836131\n",
      "Iteration: 50::Train accuracy: 16.157142857142855::Val accuracy: 16.057142857142857::Train Acc BP::47.923809523809524 Val Acc BP::47.699999999999996\n",
      "0.9999601876799649\n",
      "Iteration: 51::Train accuracy: 16.255555555555556::Val accuracy: 16.114285714285714::Train Acc BP::48.32857142857143 Val Acc BP::48.142857142857146\n",
      "0.9999595663700823\n",
      "Iteration: 52::Train accuracy: 16.347619047619048::Val accuracy: 16.214285714285715::Train Acc BP::48.67936507936508 Val Acc BP::48.528571428571425\n",
      "0.9999589509619153\n",
      "Iteration: 53::Train accuracy: 16.466666666666665::Val accuracy: 16.242857142857144::Train Acc BP::49.047619047619044 Val Acc BP::48.91428571428572\n",
      "0.9999583424616668\n",
      "Iteration: 54::Train accuracy: 16.553968253968254::Val accuracy: 16.314285714285713::Train Acc BP::49.42857142857143 Val Acc BP::49.371428571428574\n",
      "0.9999577407988471\n",
      "Iteration: 55::Train accuracy: 16.63174603174603::Val accuracy: 16.37142857142857::Train Acc BP::49.834920634920636 Val Acc BP::49.72857142857143\n",
      "0.9999571454566911\n",
      "Iteration: 56::Train accuracy: 16.71904761904762::Val accuracy: 16.428571428571427::Train Acc BP::50.14761904761905 Val Acc BP::50.18571428571429\n",
      "0.9999565562271636\n",
      "Iteration: 57::Train accuracy: 16.825396825396826::Val accuracy: 16.52857142857143::Train Acc BP::50.4984126984127 Val Acc BP::50.6\n",
      "0.9999559728266323\n",
      "Iteration: 58::Train accuracy: 16.914285714285715::Val accuracy: 16.685714285714287::Train Acc BP::50.828571428571436 Val Acc BP::50.957142857142856\n",
      "0.9999553948606779\n",
      "Iteration: 59::Train accuracy: 17.02063492063492::Val accuracy: 16.8::Train Acc BP::51.16666666666667 Val Acc BP::51.37142857142857\n",
      "0.9999548227235879\n",
      "Iteration: 60::Train accuracy: 17.125396825396823::Val accuracy: 16.957142857142856::Train Acc BP::51.487301587301594 Val Acc BP::51.67142857142857\n",
      "0.9999542562479857\n",
      "Iteration: 61::Train accuracy: 17.201587301587303::Val accuracy: 17.057142857142857::Train Acc BP::51.82222222222222 Val Acc BP::51.97142857142857\n",
      "0.9999536955407411\n",
      "Iteration: 62::Train accuracy: 17.287301587301588::Val accuracy: 17.12857142857143::Train Acc BP::52.15396825396825 Val Acc BP::52.300000000000004\n",
      "0.9999531413684388\n",
      "Iteration: 63::Train accuracy: 17.377777777777776::Val accuracy: 17.15714285714286::Train Acc BP::52.46349206349207 Val Acc BP::52.55714285714286\n",
      "0.9999525924454208\n",
      "Iteration: 64::Train accuracy: 17.44761904761905::Val accuracy: 17.228571428571428::Train Acc BP::52.77460317460317 Val Acc BP::52.900000000000006\n",
      "0.9999520489978656\n",
      "Iteration: 65::Train accuracy: 17.51904761904762::Val accuracy: 17.285714285714285::Train Acc BP::53.076190476190476 Val Acc BP::53.25714285714286\n",
      "0.999951511083581\n",
      "Iteration: 66::Train accuracy: 17.584126984126986::Val accuracy: 17.299999999999997::Train Acc BP::53.41428571428571 Val Acc BP::53.614285714285714\n",
      "0.9999509780341992\n",
      "Iteration: 67::Train accuracy: 17.679365079365077::Val accuracy: 17.457142857142856::Train Acc BP::53.71111111111111 Val Acc BP::53.957142857142856\n",
      "0.9999504498641594\n",
      "Iteration: 68::Train accuracy: 17.744444444444444::Val accuracy: 17.5::Train Acc BP::53.98412698412698 Val Acc BP::54.22857142857143\n",
      "0.9999499262870707\n",
      "Iteration: 69::Train accuracy: 17.814285714285713::Val accuracy: 17.57142857142857::Train Acc BP::54.27301587301587 Val Acc BP::54.45714285714286\n",
      "0.999949408067556\n",
      "Iteration: 70::Train accuracy: 17.90793650793651::Val accuracy: 17.757142857142856::Train Acc BP::54.4984126984127 Val Acc BP::54.74285714285714\n",
      "0.9999488943777787\n",
      "Iteration: 71::Train accuracy: 17.995238095238093::Val accuracy: 17.87142857142857::Train Acc BP::54.72857142857143 Val Acc BP::55.08571428571428\n",
      "0.9999483853001079\n",
      "Iteration: 72::Train accuracy: 18.073015873015873::Val accuracy: 17.92857142857143::Train Acc BP::54.96825396825397 Val Acc BP::55.35714285714286\n",
      "0.9999478810652096\n",
      "Iteration: 73::Train accuracy: 18.152380952380952::Val accuracy: 18.0::Train Acc BP::55.20952380952381 Val Acc BP::55.614285714285714\n",
      "0.9999473813761124\n",
      "Iteration: 74::Train accuracy: 18.226984126984128::Val accuracy: 18.085714285714285::Train Acc BP::55.46984126984127 Val Acc BP::55.92857142857143\n",
      "0.9999468858518142\n",
      "Iteration: 75::Train accuracy: 18.29206349206349::Val accuracy: 18.142857142857142::Train Acc BP::55.76190476190476 Val Acc BP::56.24285714285714\n",
      "0.9999463948937064\n",
      "Iteration: 76::Train accuracy: 18.376190476190477::Val accuracy: 18.242857142857144::Train Acc BP::55.980952380952374 Val Acc BP::56.51428571428572\n",
      "0.9999459085023812\n",
      "Iteration: 77::Train accuracy: 18.455555555555556::Val accuracy: 18.357142857142858::Train Acc BP::56.27619047619048 Val Acc BP::56.72857142857143\n",
      "0.999945425648125\n",
      "Iteration: 78::Train accuracy: 18.530158730158732::Val accuracy: 18.385714285714286::Train Acc BP::56.4968253968254 Val Acc BP::56.99999999999999\n",
      "0.9999449471112105\n",
      "Iteration: 79::Train accuracy: 18.604761904761904::Val accuracy: 18.414285714285715::Train Acc BP::56.73809523809524 Val Acc BP::57.32857142857143\n",
      "0.9999444722440929\n",
      "Iteration: 80::Train accuracy: 18.66190476190476::Val accuracy: 18.45714285714286::Train Acc BP::56.938095238095244 Val Acc BP::57.61428571428572\n",
      "0.999944001361166\n",
      "Iteration: 81::Train accuracy: 18.73333333333333::Val accuracy: 18.52857142857143::Train Acc BP::57.16031746031746 Val Acc BP::57.8\n",
      "0.9999435347080066\n",
      "Iteration: 82::Train accuracy: 18.788888888888888::Val accuracy: 18.6::Train Acc BP::57.38412698412698 Val Acc BP::58.099999999999994\n",
      "0.9999430719304538\n",
      "Iteration: 83::Train accuracy: 18.86031746031746::Val accuracy: 18.628571428571426::Train Acc BP::57.5968253968254 Val Acc BP::58.32857142857143\n",
      "0.9999426125479574\n",
      "Iteration: 84::Train accuracy: 18.93015873015873::Val accuracy: 18.7::Train Acc BP::57.784126984126985 Val Acc BP::58.471428571428575\n",
      "0.9999421570189035\n",
      "Iteration: 85::Train accuracy: 19.001587301587303::Val accuracy: 18.771428571428572::Train Acc BP::58.025396825396825 Val Acc BP::58.81428571428572\n",
      "0.9999417050961348\n",
      "Iteration: 86::Train accuracy: 19.076190476190476::Val accuracy: 18.871428571428574::Train Acc BP::58.219047619047615 Val Acc BP::58.98571428571429\n",
      "0.9999412567016195\n",
      "Iteration: 87::Train accuracy: 19.14920634920635::Val accuracy: 18.95714285714286::Train Acc BP::58.439682539682536 Val Acc BP::59.15714285714285\n",
      "0.9999408117134039\n",
      "Iteration: 88::Train accuracy: 19.233333333333334::Val accuracy: 19.0::Train Acc BP::58.6031746031746 Val Acc BP::59.357142857142854\n",
      "0.999940369872262\n",
      "Iteration: 89::Train accuracy: 19.303174603174604::Val accuracy: 19.02857142857143::Train Acc BP::58.7952380952381 Val Acc BP::59.55714285714285\n",
      "0.9999399308941685\n",
      "Iteration: 90::Train accuracy: 19.37936507936508::Val accuracy: 19.085714285714285::Train Acc BP::58.99682539682539 Val Acc BP::59.74285714285714\n",
      "0.9999394949886351\n",
      "Iteration: 91::Train accuracy: 19.446031746031746::Val accuracy: 19.157142857142855::Train Acc BP::59.16825396825397 Val Acc BP::59.785714285714285\n",
      "0.9999390622922886\n",
      "Iteration: 92::Train accuracy: 19.523809523809526::Val accuracy: 19.257142857142856::Train Acc BP::59.357142857142854 Val Acc BP::60.014285714285705\n",
      "0.999938632335762\n",
      "Iteration: 93::Train accuracy: 19.58888888888889::Val accuracy: 19.357142857142858::Train Acc BP::59.56825396825397 Val Acc BP::60.21428571428571\n",
      "0.999938205310331\n",
      "Iteration: 94::Train accuracy: 19.63968253968254::Val accuracy: 19.371428571428574::Train Acc BP::59.738095238095234 Val Acc BP::60.385714285714286\n",
      "0.9999377804614717\n",
      "Iteration: 95::Train accuracy: 19.70952380952381::Val accuracy: 19.442857142857143::Train Acc BP::59.91111111111111 Val Acc BP::60.542857142857144\n",
      "0.9999373585399819\n",
      "Iteration: 96::Train accuracy: 19.8015873015873::Val accuracy: 19.485714285714288::Train Acc BP::60.098412698412695 Val Acc BP::60.699999999999996\n",
      "0.9999369390548026\n",
      "Iteration: 97::Train accuracy: 19.877777777777776::Val accuracy: 19.57142857142857::Train Acc BP::60.25873015873016 Val Acc BP::60.871428571428574\n",
      "0.9999365226654353\n",
      "Iteration: 98::Train accuracy: 19.942857142857143::Val accuracy: 19.685714285714287::Train Acc BP::60.42857142857143 Val Acc BP::61.1\n",
      "0.9999361085592147\n",
      "Iteration: 99::Train accuracy: 20.017460317460316::Val accuracy: 19.757142857142856::Train Acc BP::60.592063492063495 Val Acc BP::61.24285714285714\n",
      "0.9999356970216483\n",
      "Iteration: 100::Train accuracy: 20.06984126984127::Val accuracy: 19.8::Train Acc BP::60.73492063492063 Val Acc BP::61.357142857142854\n"
     ]
    }
   ],
   "source": [
    "w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, trainAccBoth, valAccBoth, _, _, _ = batchGDComp(x_train,y_train,100, 0.0001, 0.000005, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy as % ->')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainBP = [i[0] for i in trainAccBoth]\n",
    "trainNP = [i[1] for i in trainAccBoth]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(trainBP, 'r.')\n",
    "plt.plot(trainBP, 'r', linewidth=5,alpha=0.3)\n",
    "plt.plot(trainNP, 'g^')\n",
    "plt.plot(trainNP, 'g', linewidth=5,alpha=0.3)\n",
    "plt.legend([\"Back propagation\", \"Node Perturbation\"])\n",
    "plt.title(\"Accuracy vs epochs\", size=15)\n",
    "plt.xlabel(\"Epochs ->\", size=10)\n",
    "plt.ylabel(\"Accuracy as % ->\", size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cosine Similiarity of the NP and BP updates')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(w1Sim)\n",
    "plt.plot(b1Sim)\n",
    "plt.plot(w2Sim)\n",
    "plt.plot(b2Sim)\n",
    "plt.plot(w3Sim)\n",
    "plt.plot(b3Sim)\n",
    "plt.legend([\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"])\n",
    "plt.title(\"Cosine similarity of the weights of NP and BP with the epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cosine Similiarity of the NP and BP updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance of BP and NP with the full variability play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "  #print(\"b3 shape = \",b3.shape, \" A2fp shape = \", A2.shape,\" and W3 shape = \", W3.shape)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchGDCompOC(X,Y,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  w1Sim = []\n",
    "  b1Sim = []\n",
    "  w2Sim = []\n",
    "  b2Sim = []\n",
    "  w3Sim = []\n",
    "  b3Sim = []\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "\n",
    "  W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1bp, b1bp, W2bp, b2bp, W3bp, b3bp =params_init()\n",
    "\n",
    "  W1np, b1np, W2np, b2np, W3np, b3np = W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy()\n",
    "  #W1np, b1np, W2np, b2np, W3np, b3np = params_init()\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  dW1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  db1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  dW2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  db2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  dW3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  db3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "    #computing the cosine similarity between the weights at eacj epoch\n",
    "    w1Sim.append(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    b1Sim.append(cosine_similarity(b1bp.reshape(1,-1), b1np.reshape(1,-1))[0][0])\n",
    "    w2Sim.append(cosine_similarity(W2bp.reshape(1,-1), W2np.reshape(1,-1))[0][0])\n",
    "    b2Sim.append(cosine_similarity(b2bp.reshape(1,-1), b2np.reshape(1,-1))[0][0])\n",
    "    w3Sim.append(cosine_similarity(W3bp.reshape(1,-1), W3np.reshape(1,-1))[0][0])\n",
    "    b3Sim.append(cosine_similarity(b3bp.reshape(1,-1), b3np.reshape(1,-1))[0][0])\n",
    "\n",
    "    print(cosine_similarity(W1bp.reshape(1,-1), W1np.reshape(1,-1))[0][0])\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      W1varocnp = weightTransformWithVariability(W1np, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocnp = weightTransformWithVariability(b1np, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocnp = weightTransformWithVariability(W2np, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocnp = weightTransformWithVariability(b2np, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocnp = weightTransformWithVariability(W3np, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocnp = weightTransformWithVariability(b3np, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      #print(W3varocnp.shape)\n",
    "      #doing the node perturbation pass first\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocnp, b1varocnp, W2varocnp,b2varocnp, W3varocnp, b3varocnp) \n",
    "      print(f\"NP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1np, db1np, dW2np, db2np, dW3np, db3np = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1varocnp, W2varocnp, W3varocnp,b1varocnp, b2varocnp, b3varocnp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      dW1varocnp = weightTransformWithVariability(dW1np, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varocnp = weightTransformWithVariability(db1np.reshape(db1np.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varocnp = weightTransformWithVariability(dW2np, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varocnp = weightTransformWithVariability(db2np.reshape(db2np.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varocnp = weightTransformWithVariability(dW3np, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varocnp = weightTransformWithVariability(db3np.reshape(db3np.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "\n",
    "      W1np, b1np, W2np, b2np, W3np, b3np = param_update(W1varocnp, b1varocnp, W2varocnp,b2varocnp, W3varocnp, b3varocnp, dW1varocnp, db1varocnp, dW2varocnp, db2varocnp, dW3varocnp, db3varocnp, lr = lrNP)\n",
    "      #print(W1)\n",
    "\n",
    "\n",
    "\n",
    "      #doing the back propagation for the same data set sample\n",
    "\n",
    "      W1varocbp = weightTransformWithVariability(W1bp, W1Currents, precision, step, discreteSteps)\n",
    "      b1varocbp = weightTransformWithVariability(b1bp, b1Currents, precision, step, discreteSteps)\n",
    "      W2varocbp = weightTransformWithVariability(W2bp, W2Currents, precision, step, discreteSteps)\n",
    "      b2varocbp = weightTransformWithVariability(b2bp, b2Currents, precision, step, discreteSteps)\n",
    "      W3varocbp = weightTransformWithVariability(W3bp, W3Currents, precision, step, discreteSteps)\n",
    "      b3varocbp = weightTransformWithVariability(b3bp, b3Currents, precision, step, discreteSteps)\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varocbp, b1varocbp, W2varocbp,b2varocbp, W3varocbp, b3varocbp) \n",
    "      print(f\"                                                                        BP Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      dW1bp, db1bp, dW2bp, db2bp, dW3bp, db3bp = backprop(Z1, A1, Z2, A2, Z3, A3, W1varocbp, W2varocbp, W3varocbp, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "\n",
    "\n",
    "      dW1varocbp = weightTransformWithVariability(dW1bp, dW1Currents, precision, step, discreteSteps)\n",
    "      db1varocbp = weightTransformWithVariability(db1bp.reshape(db1bp.shape[0],1), db1Currents, precision, step, discreteSteps)\n",
    "      dW2varocbp = weightTransformWithVariability(dW2bp, dW2Currents, precision, step, discreteSteps)\n",
    "      db2varocbp = weightTransformWithVariability(db2bp.reshape(db2bp.shape[0],1), db2Currents, precision, step, discreteSteps)\n",
    "      dW3varocbp = weightTransformWithVariability(dW3bp, dW3Currents, precision, step, discreteSteps)\n",
    "      db3varocbp = weightTransformWithVariability(db3bp.reshape(db3bp.shape[0],1), db3Currents, precision, step, discreteSteps)\n",
    "\n",
    "\n",
    "      W1bp, b1bp, W2bp, b2bp, W3bp, b3bp = param_update(W1varocbp, b1varocbp, W2varocbp, b2varocbp, W3varocbp, b3varocbp, dW1varocbp, db1varocbp, dW2varocbp, db2varocbp, dW3varocbp, db3varocbp, lr = lrBP)\n",
    "      #plt.plot(dW1np.flatten(), dW1bp.flatten(), '.')\n",
    "      #plt.xlabel(\"Weight updates from Node perturbation\")\n",
    "      #plt.ylabel(\"Weight updates from Back prop\")\n",
    "      #plt.show()\n",
    "\n",
    "\n",
    "    #lrNP = lrNP*np.exp(-0.1)\n",
    "    #lrBP = lrBP*np.exp(-0.1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      #print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _, A3_train_bp = forward(X, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append([accuracy(predictions(A3_train_bp), Y), train_score])\n",
    "      #print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1np, b1np, W2np, b2np, W3np, b3np)\n",
    "      _, _, _, _, _,  A3_val_bp = forward(x_val, W1bp, b1bp, W2bp, b2bp, W3bp, b3bp)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append([accuracy(predictions(A3_val_bp), y_val), val_score])\n",
    "      #print(f'Val accuracy: {val_score}')\n",
    "      print(f\"Iteration: {i + 1}::Train accuracy: {round(train_score, 3)}::Val accuracy: {round(val_score, 3)}::Train Acc BP::{round(accuracy(predictions(A3_train_bp), Y), 3)} Val Acc BP::{round(accuracy(predictions(A3_val_bp), y_val), 3)}\")\n",
    "\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return w1Sim, b1Sim, w2Sim, b2Sim, w3Sim, b3Sim, train_acc, val_acc, train_loss, val_loss, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter=10\n",
    "lrBP=0.01\n",
    "lrNP=0.01\n",
    "pert=0.00001\n",
    "mu = 0.7\n",
    "sigma = 0.0001\n",
    "vDD = 5\n",
    "precision = 10#setting the precision value of the calculations\n",
    "wRange = 3\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization\n",
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "1.0000000000000004\n",
      "Iteration: 1::Train accuracy: 67.594::Val accuracy: 67.0::Train Acc BP::85.092 Val Acc BP::84.314: 82.380952380952385\n",
      "0.9981732882966751\n",
      "Iteration: 2::Train accuracy: 70.962::Val accuracy: 70.386::Train Acc BP::87.319 Val Acc BP::86.05784.92063492063492\n",
      "0.9970869298182345\n",
      "Iteration: 3::Train accuracy: 71.714::Val accuracy: 71.243::Train Acc BP::87.981 Val Acc BP::86.55785.39682539682539\n",
      "0.9956640534907408\n",
      "Iteration: 4::Train accuracy: 70.7::Val accuracy: 70.814::Train Acc BP::88.294 Val Acc BP::86.59 : 86.03174603174604\n",
      "0.993736637688105\n",
      "Iteration: 5::Train accuracy: 69.819::Val accuracy: 69.686::Train Acc BP::88.294 Val Acc BP::86.54386.50793650793653\n",
      "0.9911313751997478\n",
      "Iteration: 6::Train accuracy: 67.251::Val accuracy: 66.943::Train Acc BP::88.305 Val Acc BP::86.52986.98412698412699\n",
      "0.9881635750934692\n",
      "Iteration: 7::Train accuracy: 63.846::Val accuracy: 63.4::Train Acc BP::88.224 Val Acc BP::86.69 : 87.14285714285714\n",
      "0.9849371959106381\n",
      "Iteration: 8::Train accuracy: 61.194::Val accuracy: 61.086::Train Acc BP::88.289 Val Acc BP::86.75787.46031746031746\n",
      "0.9813626013785718\n",
      "Iteration: 9::Train accuracy: 60.011::Val accuracy: 59.614::Train Acc BP::88.427 Val Acc BP::86.94388.09523809523809\n",
      "0.9771443087979774\n",
      "Iteration: 10::Train accuracy: 58.517::Val accuracy: 58.271::Train Acc BP::88.779 Val Acc BP::87.0718.57142857142857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.0000000000000004,\n",
       "  0.9981732882966751,\n",
       "  0.9970869298182345,\n",
       "  0.9956640534907408,\n",
       "  0.993736637688105,\n",
       "  0.9911313751997478,\n",
       "  0.9881635750934692,\n",
       "  0.9849371959106381,\n",
       "  0.9813626013785718,\n",
       "  0.9771443087979774],\n",
       " [0.9999999999999999,\n",
       "  0.9980142017446489,\n",
       "  0.9968514212775206,\n",
       "  0.9955623580925386,\n",
       "  0.9939957456644724,\n",
       "  0.9916427128395326,\n",
       "  0.9892184991174243,\n",
       "  0.9866500062042548,\n",
       "  0.9836706788837867,\n",
       "  0.9806942436552365],\n",
       " [1.0000000000000009,\n",
       "  0.9967411336067554,\n",
       "  0.9944440385613139,\n",
       "  0.9922724303106552,\n",
       "  0.9900090288259958,\n",
       "  0.987554805174873,\n",
       "  0.9851014137647096,\n",
       "  0.9825177854022842,\n",
       "  0.979718247033706,\n",
       "  0.9762616415518605],\n",
       " [0.9999999999999999,\n",
       "  0.996896128627725,\n",
       "  0.9941293627250659,\n",
       "  0.9898526005792067,\n",
       "  0.9863031727671555,\n",
       "  0.9810580661694654,\n",
       "  0.975154936341623,\n",
       "  0.9667835987235799,\n",
       "  0.9604463588837814,\n",
       "  0.9552994084205463],\n",
       " [0.9999999999999999,\n",
       "  0.990696400118346,\n",
       "  0.9829944111248117,\n",
       "  0.9774461252295532,\n",
       "  0.9723872186114124,\n",
       "  0.9657506479450791,\n",
       "  0.9601663170695692,\n",
       "  0.9535833919648402,\n",
       "  0.9463323068121565,\n",
       "  0.9391467459208467],\n",
       " [0.9999999999999998,\n",
       "  0.9996401100188024,\n",
       "  0.9973629299701556,\n",
       "  0.9939011360076192,\n",
       "  0.9879122695193985,\n",
       "  0.9887382280133609,\n",
       "  0.9841889655062113,\n",
       "  0.9749803778976998,\n",
       "  0.9720343372261538,\n",
       "  0.9721042598432819],\n",
       " [[85.0920634920635, 67.5936507936508],\n",
       "  [87.31904761904762, 70.96190476190476],\n",
       "  [87.98095238095239, 71.71428571428572],\n",
       "  [88.29365079365078, 70.7],\n",
       "  [88.29365079365078, 69.81904761904761],\n",
       "  [88.3047619047619, 67.25079365079365],\n",
       "  [88.22380952380952, 63.84603174603175],\n",
       "  [88.28888888888888, 61.19365079365079],\n",
       "  [88.42698412698414, 60.01111111111111],\n",
       "  [88.77936507936508, 58.51746031746031]],\n",
       " [[84.31428571428572, 67.0],\n",
       "  [86.05714285714285, 70.38571428571429],\n",
       "  [86.55714285714285, 71.24285714285715],\n",
       "  [86.5, 70.81428571428572],\n",
       "  [86.54285714285714, 69.68571428571428],\n",
       "  [86.52857142857144, 66.94285714285715],\n",
       "  [86.6, 63.4],\n",
       "  [86.75714285714285, 61.08571428571429],\n",
       "  [86.94285714285715, 59.61428571428572],\n",
       "  [87.07142857142857, 58.27142857142857]],\n",
       " [],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchGDCompOC(x_train,y_train,iter, lrBP, lrNP, pert, mu, sigma, vDD, precision, discreteSteps, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
