{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z1\")\n",
    "    #print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "def batchGDNP(X,Y,iter, lr, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "      #print(W1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsToTrain = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertList = [1,0.1, 0.01, 0.001, 0.0001]\n",
    "trainAccPertList = []\n",
    "valAccPertList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 69.841269841269834\n",
      "Iteration: 1\n",
      "Train accuracy: 70.33809523809524\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 1 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 2\n",
      "Train accuracy: 79.0079365079365\n",
      "Val accuracy: 78.08571428571427\n",
      "Iter 2 -> sub iter 99 : 82.69841269841271\n",
      "Iteration: 3\n",
      "Train accuracy: 82.1079365079365\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 3 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 4\n",
      "Train accuracy: 84.05238095238096\n",
      "Val accuracy: 83.52857142857142\n",
      "Iter 4 -> sub iter 99 : 85.23809523809524\n",
      "Iteration: 5\n",
      "Train accuracy: 85.35396825396825\n",
      "Val accuracy: 84.88571428571429\n",
      "Iter 5 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 6\n",
      "Train accuracy: 86.41904761904762\n",
      "Val accuracy: 85.97142857142858\n",
      "Iter 6 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 7\n",
      "Train accuracy: 87.17777777777778\n",
      "Val accuracy: 86.62857142857143\n",
      "Iter 7 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 8\n",
      "Train accuracy: 87.86825396825397\n",
      "Val accuracy: 87.41428571428571\n",
      "Iter 8 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 9\n",
      "Train accuracy: 88.39047619047619\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 9 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 10\n",
      "Train accuracy: 88.84761904761905\n",
      "Val accuracy: 88.25714285714285\n",
      "Iter 10 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 11\n",
      "Train accuracy: 89.2095238095238\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 11 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 12\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 12 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 13\n",
      "Train accuracy: 89.89047619047619\n",
      "Val accuracy: 89.4\n",
      "Iter 13 -> sub iter 99 : 90.01111111111111\n",
      "Iteration: 14\n",
      "Train accuracy: 90.15555555555555\n",
      "Val accuracy: 89.64285714285715\n",
      "Iter 14 -> sub iter 99 : 90.08730158730158\n",
      "Iteration: 15\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.84285714285714\n",
      "Iter 15 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 16\n",
      "Train accuracy: 90.62857142857142\n",
      "Val accuracy: 90.17142857142856\n",
      "Iter 16 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 17\n",
      "Train accuracy: 90.86349206349207\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 17 -> sub iter 99 : 90.63492063492063\n",
      "Iteration: 18\n",
      "Train accuracy: 91.09047619047618\n",
      "Val accuracy: 90.64285714285715\n",
      "Iter 18 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 19\n",
      "Train accuracy: 91.25079365079365\n",
      "Val accuracy: 90.8\n",
      "Iter 19 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 20\n",
      "Train accuracy: 91.42857142857143\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 20 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 21\n",
      "Train accuracy: 91.58730158730158\n",
      "Val accuracy: 91.01428571428572\n",
      "Iter 21 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 22\n",
      "Train accuracy: 91.73174603174603\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 22 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 23\n",
      "Train accuracy: 91.87777777777778\n",
      "Val accuracy: 91.42857142857143\n",
      "Iter 23 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 24\n",
      "Train accuracy: 91.99841269841271\n",
      "Val accuracy: 91.41428571428571\n",
      "Iter 24 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 25\n",
      "Train accuracy: 92.0984126984127\n",
      "Val accuracy: 91.54285714285714\n",
      "Iter 25 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 26\n",
      "Train accuracy: 92.2031746031746\n",
      "Val accuracy: 91.60000000000001\n",
      "Iter 26 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 27\n",
      "Train accuracy: 92.3079365079365\n",
      "Val accuracy: 91.75714285714285\n",
      "Iter 27 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 28\n",
      "Train accuracy: 92.41428571428571\n",
      "Val accuracy: 91.88571428571429\n",
      "Iter 28 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 29\n",
      "Train accuracy: 92.5063492063492\n",
      "Val accuracy: 92.01428571428572\n",
      "Iter 29 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 30\n",
      "Train accuracy: 92.57619047619048\n",
      "Val accuracy: 92.02857142857142\n",
      "Iter 30 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 31\n",
      "Train accuracy: 92.67301587301587\n",
      "Val accuracy: 92.12857142857143\n",
      "Iter 31 -> sub iter 99 : 91.90476190476191\n",
      "Iteration: 32\n",
      "Train accuracy: 92.74761904761904\n",
      "Val accuracy: 92.14285714285714\n",
      "Iter 32 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 33\n",
      "Train accuracy: 92.83809523809524\n",
      "Val accuracy: 92.21428571428572\n",
      "Iter 33 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 34\n",
      "Train accuracy: 92.93174603174603\n",
      "Val accuracy: 92.34285714285714\n",
      "Iter 34 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 35\n",
      "Train accuracy: 92.9968253968254\n",
      "Val accuracy: 92.4\n",
      "Iter 35 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 36\n",
      "Train accuracy: 93.06349206349206\n",
      "Val accuracy: 92.5\n",
      "Iter 36 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 37\n",
      "Train accuracy: 93.12222222222222\n",
      "Val accuracy: 92.51428571428572\n",
      "Iter 37 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 38\n",
      "Train accuracy: 93.16349206349206\n",
      "Val accuracy: 92.58571428571429\n",
      "Iter 38 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 39\n",
      "Train accuracy: 93.2047619047619\n",
      "Val accuracy: 92.62857142857143\n",
      "Iter 39 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 40\n",
      "Train accuracy: 93.25873015873016\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 40 -> sub iter 99 : 92.69841269841278\n",
      "Iteration: 41\n",
      "Train accuracy: 93.32222222222222\n",
      "Val accuracy: 92.72857142857143\n",
      "Iter 41 -> sub iter 99 : 92.69841269841273\n",
      "Iteration: 42\n",
      "Train accuracy: 93.38253968253967\n",
      "Val accuracy: 92.77142857142857\n",
      "Iter 42 -> sub iter 99 : 93.01587301587301\n",
      "Iteration: 43\n",
      "Train accuracy: 93.42857142857143\n",
      "Val accuracy: 92.85714285714286\n",
      "Iter 43 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 44\n",
      "Train accuracy: 93.4888888888889\n",
      "Val accuracy: 92.95714285714286\n",
      "Iter 44 -> sub iter 99 : 93.17460317460318\n",
      "Iteration: 45\n",
      "Train accuracy: 93.55079365079365\n",
      "Val accuracy: 93.05714285714286\n",
      "Iter 45 -> sub iter 99 : 93.49206349206356\n",
      "Iteration: 46\n",
      "Train accuracy: 93.6015873015873\n",
      "Val accuracy: 93.12857142857143\n",
      "Iter 46 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 47\n",
      "Train accuracy: 93.66507936507936\n",
      "Val accuracy: 93.18571428571428\n",
      "Iter 47 -> sub iter 99 : 93.80952380952381\n",
      "Iteration: 48\n",
      "Train accuracy: 93.72222222222221\n",
      "Val accuracy: 93.22857142857143\n",
      "Iter 48 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 49\n",
      "Train accuracy: 93.76984126984127\n",
      "Val accuracy: 93.27142857142857\n",
      "Iter 49 -> sub iter 99 : 93.96825396825396\n",
      "Iteration: 50\n",
      "Train accuracy: 93.83968253968254\n",
      "Val accuracy: 93.31428571428572\n",
      "Iter 50 -> sub iter 99 : 94.12698412698413\n",
      "Iteration: 51\n",
      "Train accuracy: 93.88571428571429\n",
      "Val accuracy: 93.32857142857142\n",
      "Iter 51 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 93.92380952380952\n",
      "Val accuracy: 93.34285714285714\n",
      "Iter 52 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 53\n",
      "Train accuracy: 93.97777777777779\n",
      "Val accuracy: 93.38571428571429\n",
      "Iter 53 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 54\n",
      "Train accuracy: 94.01428571428572\n",
      "Val accuracy: 93.41428571428571\n",
      "Iter 54 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 55\n",
      "Train accuracy: 94.07142857142857\n",
      "Val accuracy: 93.4\n",
      "Iter 55 -> sub iter 99 : 94.28571428571428\n",
      "Iteration: 56\n",
      "Train accuracy: 94.12063492063491\n",
      "Val accuracy: 93.45714285714286\n",
      "Iter 56 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 57\n",
      "Train accuracy: 94.13650793650794\n",
      "Val accuracy: 93.47142857142858\n",
      "Iter 57 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 58\n",
      "Train accuracy: 94.16349206349206\n",
      "Val accuracy: 93.54285714285714\n",
      "Iter 58 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 59\n",
      "Train accuracy: 94.22222222222221\n",
      "Val accuracy: 93.58571428571429\n",
      "Iter 59 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 60\n",
      "Train accuracy: 94.24285714285713\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 60 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 61\n",
      "Train accuracy: 94.28412698412698\n",
      "Val accuracy: 93.57142857142857\n",
      "Iter 61 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 62\n",
      "Train accuracy: 94.33174603174604\n",
      "Val accuracy: 93.61428571428571\n",
      "Iter 62 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 63\n",
      "Train accuracy: 94.4031746031746\n",
      "Val accuracy: 93.60000000000001\n",
      "Iter 63 -> sub iter 99 : 94.60317460317466\n",
      "Iteration: 64\n",
      "Train accuracy: 94.43492063492064\n",
      "Val accuracy: 93.62857142857143\n",
      "Iter 64 -> sub iter 99 : 94.60317460317463\n",
      "Iteration: 65\n",
      "Train accuracy: 94.46190476190476\n",
      "Val accuracy: 93.64285714285714\n",
      "Iter 65 -> sub iter 99 : 94.60317460317467\n",
      "Iteration: 66\n",
      "Train accuracy: 94.4888888888889\n",
      "Val accuracy: 93.68571428571428\n",
      "Iter 66 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 94.51587301587303\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 67 -> sub iter 99 : 94.44444444444444\n",
      "Iteration: 68\n",
      "Train accuracy: 94.54444444444444\n",
      "Val accuracy: 93.71428571428572\n",
      "Iter 68 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 69\n",
      "Train accuracy: 94.57301587301588\n",
      "Val accuracy: 93.8\n",
      "Iter 69 -> sub iter 99 : 94.76190476190476\n",
      "Iteration: 70\n",
      "Train accuracy: 94.61587301587302\n",
      "Val accuracy: 93.8\n",
      "Iter 70 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 71\n",
      "Train accuracy: 94.63492063492063\n",
      "Val accuracy: 93.84285714285714\n",
      "Iter 71 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 72\n",
      "Train accuracy: 94.65396825396826\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 72 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 73\n",
      "Train accuracy: 94.67301587301587\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 73 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 74\n",
      "Train accuracy: 94.71269841269842\n",
      "Val accuracy: 93.88571428571429\n",
      "Iter 74 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 75\n",
      "Train accuracy: 94.74126984126984\n",
      "Val accuracy: 93.91428571428571\n",
      "Iter 75 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 76\n",
      "Train accuracy: 94.77777777777779\n",
      "Val accuracy: 93.92857142857143\n",
      "Iter 76 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 77\n",
      "Train accuracy: 94.80952380952381\n",
      "Val accuracy: 93.95714285714286\n",
      "Iter 77 -> sub iter 99 : 94.92063492063491\n",
      "Iteration: 78\n",
      "Train accuracy: 94.82698412698413\n",
      "Val accuracy: 93.98571428571428\n",
      "Iter 78 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 79\n",
      "Train accuracy: 94.83968253968254\n",
      "Val accuracy: 94.01428571428572\n",
      "Iter 79 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 80\n",
      "Train accuracy: 94.86825396825397\n",
      "Val accuracy: 94.04285714285714\n",
      "Iter 80 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 81\n",
      "Train accuracy: 94.89999999999999\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 81 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 82\n",
      "Train accuracy: 94.92063492063491\n",
      "Val accuracy: 94.07142857142857\n",
      "Iter 82 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 83\n",
      "Train accuracy: 94.94920634920635\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 83 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 84\n",
      "Train accuracy: 94.98253968253968\n",
      "Val accuracy: 94.05714285714286\n",
      "Iter 84 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 85\n",
      "Train accuracy: 95.0031746031746\n",
      "Val accuracy: 94.1\n",
      "Iter 85 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 86\n",
      "Train accuracy: 95.03174603174604\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 86 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 87\n",
      "Train accuracy: 95.06190476190476\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 87 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 88\n",
      "Train accuracy: 95.09206349206349\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 88 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 89\n",
      "Train accuracy: 95.11904761904762\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 89 -> sub iter 99 : 95.07936507936508\n",
      "Iteration: 90\n",
      "Train accuracy: 95.13174603174603\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 90 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 91\n",
      "Train accuracy: 95.18095238095238\n",
      "Val accuracy: 94.11428571428571\n",
      "Iter 91 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 92\n",
      "Train accuracy: 95.21111111111111\n",
      "Val accuracy: 94.15714285714286\n",
      "Iter 92 -> sub iter 99 : 95.23809523809523\n",
      "Iteration: 93\n",
      "Train accuracy: 95.24603174603175\n",
      "Val accuracy: 94.18571428571428\n",
      "Iter 93 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 94\n",
      "Train accuracy: 95.25396825396825\n",
      "Val accuracy: 94.21428571428572\n",
      "Iter 94 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 95\n",
      "Train accuracy: 95.3\n",
      "Val accuracy: 94.28571428571428\n",
      "Iter 95 -> sub iter 99 : 95.39682539682542\n",
      "Iteration: 96\n",
      "Train accuracy: 95.31904761904761\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 96 -> sub iter 99 : 95.39682539682549\n",
      "Iteration: 97\n",
      "Train accuracy: 95.33015873015873\n",
      "Val accuracy: 94.25714285714287\n",
      "Iter 97 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 98\n",
      "Train accuracy: 95.34444444444445\n",
      "Val accuracy: 94.22857142857143\n",
      "Iter 98 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 99\n",
      "Train accuracy: 95.36666666666666\n",
      "Val accuracy: 94.3\n",
      "Iter 99 -> sub iter 99 : 95.55555555555556\n",
      "Iteration: 100\n",
      "Train accuracy: 95.3920634920635\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 100 -> sub iter 99 : 95.71428571428572\n",
      "Iteration: 101\n",
      "Train accuracy: 95.41428571428571\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 101 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 102\n",
      "Train accuracy: 95.42222222222222\n",
      "Val accuracy: 94.32857142857142\n",
      "Iter 102 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 103\n",
      "Train accuracy: 95.43333333333334\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 103 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 104\n",
      "Train accuracy: 95.46507936507936\n",
      "Val accuracy: 94.38571428571429\n",
      "Iter 104 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 105\n",
      "Train accuracy: 95.47936507936508\n",
      "Val accuracy: 94.37142857142857\n",
      "Iter 105 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 106\n",
      "Train accuracy: 95.5079365079365\n",
      "Val accuracy: 94.39999999999999\n",
      "Iter 106 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 107\n",
      "Train accuracy: 95.51587301587303\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 107 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 108\n",
      "Train accuracy: 95.52857142857142\n",
      "Val accuracy: 94.41428571428571\n",
      "Iter 108 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 109\n",
      "Train accuracy: 95.54126984126984\n",
      "Val accuracy: 94.42857142857143\n",
      "Iter 109 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 110\n",
      "Train accuracy: 95.56031746031746\n",
      "Val accuracy: 94.47142857142858\n",
      "Iter 110 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 111\n",
      "Train accuracy: 95.55873015873016\n",
      "Val accuracy: 94.48571428571428\n",
      "Iter 111 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 112\n",
      "Train accuracy: 95.57460317460318\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 112 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 113\n",
      "Train accuracy: 95.6047619047619\n",
      "Val accuracy: 94.52857142857142\n",
      "Iter 113 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 114\n",
      "Train accuracy: 95.62380952380953\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 114 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 115\n",
      "Train accuracy: 95.63968253968254\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 115 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 116\n",
      "Train accuracy: 95.65079365079366\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 116 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 117\n",
      "Train accuracy: 95.67142857142858\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 117 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 118\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.55714285714286\n",
      "Iter 118 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 119\n",
      "Train accuracy: 95.6920634920635\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 119 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 120\n",
      "Train accuracy: 95.71111111111111\n",
      "Val accuracy: 94.57142857142857\n",
      "Iter 120 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 121\n",
      "Train accuracy: 95.72063492063492\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 121 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 122\n",
      "Train accuracy: 95.72539682539683\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 122 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 123\n",
      "Train accuracy: 95.73174603174604\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 123 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 124\n",
      "Train accuracy: 95.74920634920635\n",
      "Val accuracy: 94.6\n",
      "Iter 124 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 125\n",
      "Train accuracy: 95.74603174603175\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 125 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 126\n",
      "Train accuracy: 95.75396825396825\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 126 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 127\n",
      "Train accuracy: 95.75873015873016\n",
      "Val accuracy: 94.6\n",
      "Iter 127 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 128\n",
      "Train accuracy: 95.78571428571429\n",
      "Val accuracy: 94.58571428571429\n",
      "Iter 128 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 129\n",
      "Train accuracy: 95.80634920634921\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 129 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 130\n",
      "Train accuracy: 95.82380952380952\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 130 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 131\n",
      "Train accuracy: 95.83650793650794\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 131 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 132\n",
      "Train accuracy: 95.85555555555555\n",
      "Val accuracy: 94.62857142857143\n",
      "Iter 132 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 133\n",
      "Train accuracy: 95.86031746031746\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 133 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 134\n",
      "Train accuracy: 95.87142857142858\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 134 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 135\n",
      "Train accuracy: 95.87301587301587\n",
      "Val accuracy: 94.65714285714286\n",
      "Iter 135 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 136\n",
      "Train accuracy: 95.8873015873016\n",
      "Val accuracy: 94.67142857142858\n",
      "Iter 136 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 137\n",
      "Train accuracy: 95.91428571428573\n",
      "Val accuracy: 94.68571428571428\n",
      "Iter 137 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 138\n",
      "Train accuracy: 95.92222222222222\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 138 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 139\n",
      "Train accuracy: 95.94126984126984\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 139 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 140\n",
      "Train accuracy: 95.95714285714286\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 140 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 141\n",
      "Train accuracy: 95.97777777777777\n",
      "Val accuracy: 94.71428571428572\n",
      "Iter 141 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 142\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.72857142857143\n",
      "Iter 142 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 143\n",
      "Train accuracy: 96.02857142857142\n",
      "Val accuracy: 94.77142857142857\n",
      "Iter 143 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 144\n",
      "Train accuracy: 96.04285714285714\n",
      "Val accuracy: 94.8\n",
      "Iter 144 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 145\n",
      "Train accuracy: 96.06507936507937\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 145 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 146\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 146 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 147\n",
      "Train accuracy: 96.0936507936508\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 147 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 148\n",
      "Train accuracy: 96.10634920634921\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 148 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 149\n",
      "Train accuracy: 96.13174603174603\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 149 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 150\n",
      "Train accuracy: 96.15079365079366\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 150 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 151\n",
      "Train accuracy: 96.16666666666667\n",
      "Val accuracy: 94.84285714285714\n",
      "Iter 151 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 152\n",
      "Train accuracy: 96.18253968253968\n",
      "Val accuracy: 94.85714285714286\n",
      "Iter 152 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 153\n",
      "Train accuracy: 96.18571428571428\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 153 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 154\n",
      "Train accuracy: 96.2\n",
      "Val accuracy: 94.87142857142857\n",
      "Iter 154 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 155\n",
      "Train accuracy: 96.20793650793651\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 155 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 156\n",
      "Train accuracy: 96.21428571428572\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 156 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 157\n",
      "Train accuracy: 96.22539682539683\n",
      "Val accuracy: 94.89999999999999\n",
      "Iter 157 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 158\n",
      "Train accuracy: 96.22857142857143\n",
      "Val accuracy: 94.88571428571429\n",
      "Iter 158 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 159\n",
      "Train accuracy: 96.24126984126984\n",
      "Val accuracy: 94.91428571428571\n",
      "Iter 159 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 160\n",
      "Train accuracy: 96.24444444444444\n",
      "Val accuracy: 94.94285714285714\n",
      "Iter 160 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 161\n",
      "Train accuracy: 96.25396825396825\n",
      "Val accuracy: 94.97142857142858\n",
      "Iter 161 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 162\n",
      "Train accuracy: 96.27142857142857\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 162 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 163\n",
      "Train accuracy: 96.27777777777777\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 163 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 164\n",
      "Train accuracy: 96.3047619047619\n",
      "Val accuracy: 94.98571428571428\n",
      "Iter 164 -> sub iter 99 : 95.87301587301587\n",
      "Iteration: 165\n",
      "Train accuracy: 96.32063492063492\n",
      "Val accuracy: 95.0\n",
      "Iter 165 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 166\n",
      "Train accuracy: 96.33809523809524\n",
      "Val accuracy: 95.01428571428572\n",
      "Iter 166 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 167\n",
      "Train accuracy: 96.34126984126983\n",
      "Val accuracy: 95.04285714285714\n",
      "Iter 167 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 168\n",
      "Train accuracy: 96.34761904761905\n",
      "Val accuracy: 95.08571428571429\n",
      "Iter 168 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 169\n",
      "Train accuracy: 96.35873015873015\n",
      "Val accuracy: 95.12857142857143\n",
      "Iter 169 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 170\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 170 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 171\n",
      "Train accuracy: 96.3873015873016\n",
      "Val accuracy: 95.18571428571428\n",
      "Iter 171 -> sub iter 99 : 96.03174603174604\n",
      "Iteration: 172\n",
      "Train accuracy: 96.3984126984127\n",
      "Val accuracy: 95.15714285714286\n",
      "Iter 172 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 173\n",
      "Train accuracy: 96.41428571428573\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 173 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 174\n",
      "Train accuracy: 96.42857142857143\n",
      "Val accuracy: 95.17142857142858\n",
      "Iter 174 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 175\n",
      "Train accuracy: 96.44285714285714\n",
      "Val accuracy: 95.21428571428572\n",
      "Iter 175 -> sub iter 99 : 96.19047619047619\n",
      "Iteration: 176\n",
      "Train accuracy: 96.45396825396826\n",
      "Val accuracy: 95.22857142857143\n",
      "Iter 176 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 177\n",
      "Train accuracy: 96.46031746031746\n",
      "Val accuracy: 95.27142857142857\n",
      "Iter 177 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 178\n",
      "Train accuracy: 96.46666666666667\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 178 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 179\n",
      "Train accuracy: 96.46825396825398\n",
      "Val accuracy: 95.28571428571428\n",
      "Iter 179 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 180\n",
      "Train accuracy: 96.48095238095237\n",
      "Val accuracy: 95.3\n",
      "Iter 180 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 181\n",
      "Train accuracy: 96.49682539682539\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 181 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 182\n",
      "Train accuracy: 96.50634920634921\n",
      "Val accuracy: 95.3\n",
      "Iter 182 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 183\n",
      "Train accuracy: 96.52222222222223\n",
      "Val accuracy: 95.3\n",
      "Iter 183 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 184\n",
      "Train accuracy: 96.53015873015873\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 184 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 185\n",
      "Train accuracy: 96.54444444444444\n",
      "Val accuracy: 95.31428571428572\n",
      "Iter 185 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 186\n",
      "Train accuracy: 96.54761904761905\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 186 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 187\n",
      "Train accuracy: 96.56349206349206\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 187 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 188\n",
      "Train accuracy: 96.56031746031746\n",
      "Val accuracy: 95.35714285714286\n",
      "Iter 188 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 189\n",
      "Train accuracy: 96.56825396825397\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 189 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 190\n",
      "Train accuracy: 96.58571428571429\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 190 -> sub iter 99 : 96.34920634920636\n",
      "Iteration: 191\n",
      "Train accuracy: 96.5952380952381\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 191 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 192\n",
      "Train accuracy: 96.60317460317461\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 192 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 193\n",
      "Train accuracy: 96.61111111111111\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 193 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 194\n",
      "Train accuracy: 96.62380952380953\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 194 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 195\n",
      "Train accuracy: 96.62857142857143\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 195 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 196\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 196 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 197\n",
      "Train accuracy: 96.62698412698413\n",
      "Val accuracy: 95.38571428571429\n",
      "Iter 197 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 198\n",
      "Train accuracy: 96.63492063492065\n",
      "Val accuracy: 95.39999999999999\n",
      "Iter 198 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 199\n",
      "Train accuracy: 96.63809523809523\n",
      "Val accuracy: 95.37142857142857\n",
      "Iter 199 -> sub iter 99 : 96.50793650793654\n",
      "Iteration: 200\n",
      "Train accuracy: 96.65238095238095\n",
      "Val accuracy: 95.35714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 23.333333333333332\n",
      "Iteration: 1\n",
      "Train accuracy: 21.192063492063493\n",
      "Val accuracy: 21.185714285714287\n",
      "Iter 1 -> sub iter 99 : 32.380952380952387\n",
      "Iteration: 2\n",
      "Train accuracy: 30.338095238095235\n",
      "Val accuracy: 30.185714285714287\n",
      "Iter 2 -> sub iter 99 : 39.365079365079374\n",
      "Iteration: 3\n",
      "Train accuracy: 36.630158730158726\n",
      "Val accuracy: 35.885714285714286\n",
      "Iter 3 -> sub iter 99 : 43.333333333333336\n",
      "Iteration: 4\n",
      "Train accuracy: 40.7015873015873\n",
      "Val accuracy: 39.957142857142856\n",
      "Iter 4 -> sub iter 99 : 45.873015873015874\n",
      "Iteration: 5\n",
      "Train accuracy: 43.32222222222222\n",
      "Val accuracy: 43.042857142857144\n",
      "Iter 5 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 6\n",
      "Train accuracy: 45.303174603174604\n",
      "Val accuracy: 44.94285714285714\n",
      "Iter 6 -> sub iter 99 : 49.365079365079376\n",
      "Iteration: 7\n",
      "Train accuracy: 48.33174603174603\n",
      "Val accuracy: 47.3\n",
      "Iter 7 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 8\n",
      "Train accuracy: 53.77301587301587\n",
      "Val accuracy: 52.67142857142857\n",
      "Iter 8 -> sub iter 99 : 58.888888888888896\n",
      "Iteration: 9\n",
      "Train accuracy: 57.51904761904761\n",
      "Val accuracy: 56.471428571428575\n",
      "Iter 9 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 10\n",
      "Train accuracy: 60.73968253968254\n",
      "Val accuracy: 59.68571428571428\n",
      "Iter 10 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 11\n",
      "Train accuracy: 63.834920634920636\n",
      "Val accuracy: 62.97142857142857\n",
      "Iter 11 -> sub iter 99 : 64.444444444444445\n",
      "Iteration: 12\n",
      "Train accuracy: 66.26349206349207\n",
      "Val accuracy: 65.65714285714286\n",
      "Iter 12 -> sub iter 99 : 66.349206349206345\n",
      "Iteration: 13\n",
      "Train accuracy: 68.01904761904763\n",
      "Val accuracy: 67.37142857142857\n",
      "Iter 13 -> sub iter 99 : 67.619047619047625\n",
      "Iteration: 14\n",
      "Train accuracy: 69.34920634920636\n",
      "Val accuracy: 68.45714285714286\n",
      "Iter 14 -> sub iter 99 : 68.57142857142857\n",
      "Iteration: 15\n",
      "Train accuracy: 70.35714285714286\n",
      "Val accuracy: 69.27142857142857\n",
      "Iter 15 -> sub iter 99 : 69.68253968253968\n",
      "Iteration: 16\n",
      "Train accuracy: 71.15873015873015\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 16 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 17\n",
      "Train accuracy: 71.80952380952381\n",
      "Val accuracy: 70.6\n",
      "Iter 17 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 18\n",
      "Train accuracy: 72.47777777777777\n",
      "Val accuracy: 71.31428571428572\n",
      "Iter 18 -> sub iter 99 : 72.22222222222221\n",
      "Iteration: 19\n",
      "Train accuracy: 73.23015873015873\n",
      "Val accuracy: 72.17142857142858\n",
      "Iter 19 -> sub iter 99 : 73.49206349206354\n",
      "Iteration: 20\n",
      "Train accuracy: 74.12222222222222\n",
      "Val accuracy: 73.08571428571429\n",
      "Iter 20 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 21\n",
      "Train accuracy: 75.04603174603174\n",
      "Val accuracy: 73.75714285714285\n",
      "Iter 21 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 22\n",
      "Train accuracy: 75.86190476190477\n",
      "Val accuracy: 74.5142857142857\n",
      "Iter 22 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 23\n",
      "Train accuracy: 76.63809523809523\n",
      "Val accuracy: 74.9857142857143\n",
      "Iter 23 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 24\n",
      "Train accuracy: 77.25555555555556\n",
      "Val accuracy: 75.74285714285715\n",
      "Iter 24 -> sub iter 99 : 77.30158730158733\n",
      "Iteration: 25\n",
      "Train accuracy: 77.86031746031746\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 25 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 26\n",
      "Train accuracy: 78.41269841269842\n",
      "Val accuracy: 76.91428571428571\n",
      "Iter 26 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 27\n",
      "Train accuracy: 78.94285714285715\n",
      "Val accuracy: 77.58571428571429\n",
      "Iter 27 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 28\n",
      "Train accuracy: 79.43650793650794\n",
      "Val accuracy: 78.01428571428572\n",
      "Iter 28 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 29\n",
      "Train accuracy: 79.9\n",
      "Val accuracy: 78.65714285714286\n",
      "Iter 29 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 80.32857142857142\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 30 -> sub iter 99 : 80.07460317460318\n",
      "Iteration: 31\n",
      "Train accuracy: 80.68095238095238\n",
      "Val accuracy: 79.31428571428572\n",
      "Iter 31 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 32\n",
      "Train accuracy: 81.01428571428572\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 32 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 33\n",
      "Train accuracy: 81.36984126984127\n",
      "Val accuracy: 79.94285714285714\n",
      "Iter 33 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 34\n",
      "Train accuracy: 81.67619047619048\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 34 -> sub iter 99 : 81.90476190476196\n",
      "Iteration: 35\n",
      "Train accuracy: 81.95396825396826\n",
      "Val accuracy: 80.4\n",
      "Iter 35 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 36\n",
      "Train accuracy: 82.28095238095237\n",
      "Val accuracy: 80.7\n",
      "Iter 36 -> sub iter 99 : 81.90476190476198\n",
      "Iteration: 37\n",
      "Train accuracy: 82.53809523809524\n",
      "Val accuracy: 81.10000000000001\n",
      "Iter 37 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 38\n",
      "Train accuracy: 82.73492063492064\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 38 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 39\n",
      "Train accuracy: 82.9968253968254\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 39 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 40\n",
      "Train accuracy: 83.2\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 40 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 41\n",
      "Train accuracy: 83.46507936507936\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 41 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 42\n",
      "Train accuracy: 83.66349206349206\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 42 -> sub iter 99 : 84.28571428571429\n",
      "Iteration: 43\n",
      "Train accuracy: 83.87142857142858\n",
      "Val accuracy: 82.47142857142858\n",
      "Iter 43 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 44\n",
      "Train accuracy: 84.04761904761905\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 44 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 45\n",
      "Train accuracy: 84.21111111111111\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 45 -> sub iter 99 : 84.60317460317461\n",
      "Iteration: 46\n",
      "Train accuracy: 84.38571428571429\n",
      "Val accuracy: 83.05714285714285\n",
      "Iter 46 -> sub iter 99 : 85.07936507936508\n",
      "Iteration: 47\n",
      "Train accuracy: 84.57301587301588\n",
      "Val accuracy: 83.12857142857143\n",
      "Iter 47 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 48\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 83.34285714285714\n",
      "Iter 48 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 49\n",
      "Train accuracy: 84.91746031746031\n",
      "Val accuracy: 83.5\n",
      "Iter 49 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 50\n",
      "Train accuracy: 85.06984126984128\n",
      "Val accuracy: 83.61428571428571\n",
      "Iter 50 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 51\n",
      "Train accuracy: 85.22222222222223\n",
      "Val accuracy: 83.78571428571429\n",
      "Iter 51 -> sub iter 99 : 85.71428571428571\n",
      "Iteration: 52\n",
      "Train accuracy: 85.39999999999999\n",
      "Val accuracy: 83.84285714285714\n",
      "Iter 52 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 53\n",
      "Train accuracy: 85.54603174603174\n",
      "Val accuracy: 83.92857142857143\n",
      "Iter 53 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 54\n",
      "Train accuracy: 85.66984126984127\n",
      "Val accuracy: 84.12857142857143\n",
      "Iter 54 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 55\n",
      "Train accuracy: 85.8\n",
      "Val accuracy: 84.2\n",
      "Iter 55 -> sub iter 99 : 86.03174603174604\n",
      "Iteration: 56\n",
      "Train accuracy: 85.91587301587302\n",
      "Val accuracy: 84.31428571428572\n",
      "Iter 56 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 57\n",
      "Train accuracy: 85.99365079365079\n",
      "Val accuracy: 84.41428571428573\n",
      "Iter 57 -> sub iter 99 : 86.19047619047619\n",
      "Iteration: 58\n",
      "Train accuracy: 86.1\n",
      "Val accuracy: 84.52857142857142\n",
      "Iter 58 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 59\n",
      "Train accuracy: 86.23015873015873\n",
      "Val accuracy: 84.61428571428571\n",
      "Iter 59 -> sub iter 99 : 86.34920634920636\n",
      "Iteration: 60\n",
      "Train accuracy: 86.34444444444445\n",
      "Val accuracy: 84.68571428571428\n",
      "Iter 60 -> sub iter 99 : 86.50793650793652\n",
      "Iteration: 61\n",
      "Train accuracy: 86.42857142857143\n",
      "Val accuracy: 84.72857142857143\n",
      "Iter 61 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 62\n",
      "Train accuracy: 86.53968253968254\n",
      "Val accuracy: 84.85714285714285\n",
      "Iter 62 -> sub iter 99 : 86.82539682539682\n",
      "Iteration: 63\n",
      "Train accuracy: 86.65555555555555\n",
      "Val accuracy: 85.02857142857142\n",
      "Iter 63 -> sub iter 99 : 86.98412698412699\n",
      "Iteration: 64\n",
      "Train accuracy: 86.76984126984128\n",
      "Val accuracy: 85.08571428571429\n",
      "Iter 64 -> sub iter 99 : 87.14285714285714\n",
      "Iteration: 65\n",
      "Train accuracy: 86.89206349206349\n",
      "Val accuracy: 85.21428571428571\n",
      "Iter 65 -> sub iter 99 : 87.30158730158735\n",
      "Iteration: 66\n",
      "Train accuracy: 86.9920634920635\n",
      "Val accuracy: 85.3\n",
      "Iter 66 -> sub iter 99 : 87.30158730158732\n",
      "Iteration: 67\n",
      "Train accuracy: 87.06349206349206\n",
      "Val accuracy: 85.45714285714286\n",
      "Iter 67 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 68\n",
      "Train accuracy: 87.14603174603175\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 68 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 69\n",
      "Train accuracy: 87.1984126984127\n",
      "Val accuracy: 85.58571428571429\n",
      "Iter 69 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 70\n",
      "Train accuracy: 87.27936507936508\n",
      "Val accuracy: 85.8\n",
      "Iter 70 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 71\n",
      "Train accuracy: 87.34603174603176\n",
      "Val accuracy: 85.91428571428571\n",
      "Iter 71 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 72\n",
      "Train accuracy: 87.44761904761906\n",
      "Val accuracy: 86.02857142857144\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 87.55238095238094\n",
      "Val accuracy: 86.11428571428571\n",
      "Iter 73 -> sub iter 99 : 87.77777777777777\n",
      "Iteration: 74\n",
      "Train accuracy: 87.61904761904762\n",
      "Val accuracy: 86.18571428571428\n",
      "Iter 74 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 75\n",
      "Train accuracy: 87.69206349206348\n",
      "Val accuracy: 86.25714285714285\n",
      "Iter 75 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 76\n",
      "Train accuracy: 87.7968253968254\n",
      "Val accuracy: 86.34285714285714\n",
      "Iter 76 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 77\n",
      "Train accuracy: 87.87460317460317\n",
      "Val accuracy: 86.37142857142858\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.96666666666667\n",
      "Val accuracy: 86.5142857142857\n",
      "Iter 78 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 79\n",
      "Train accuracy: 88.00634920634921\n",
      "Val accuracy: 86.61428571428571\n",
      "Iter 79 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 80\n",
      "Train accuracy: 88.07301587301588\n",
      "Val accuracy: 86.71428571428571\n",
      "Iter 80 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 81\n",
      "Train accuracy: 88.16190476190476\n",
      "Val accuracy: 86.81428571428572\n",
      "Iter 81 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 82\n",
      "Train accuracy: 88.22539682539683\n",
      "Val accuracy: 86.88571428571429\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.27301587301586\n",
      "Val accuracy: 87.02857142857144\n",
      "Iter 83 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 84\n",
      "Train accuracy: 88.33333333333333\n",
      "Val accuracy: 87.08571428571429\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.38571428571429\n",
      "Val accuracy: 87.1\n",
      "Iter 85 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 86\n",
      "Train accuracy: 88.43174603174603\n",
      "Val accuracy: 87.14285714285714\n",
      "Iter 86 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 87\n",
      "Train accuracy: 88.47301587301587\n",
      "Val accuracy: 87.25714285714285\n",
      "Iter 87 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 88\n",
      "Train accuracy: 88.50634920634921\n",
      "Val accuracy: 87.37142857142857\n",
      "Iter 88 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 89\n",
      "Train accuracy: 88.55079365079365\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 89 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 90\n",
      "Train accuracy: 88.60317460317461\n",
      "Val accuracy: 87.34285714285714\n",
      "Iter 90 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 91\n",
      "Train accuracy: 88.66349206349207\n",
      "Val accuracy: 87.42857142857143\n",
      "Iter 91 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 92\n",
      "Train accuracy: 88.70476190476191\n",
      "Val accuracy: 87.44285714285715\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 88.75714285714285\n",
      "Val accuracy: 87.5142857142857\n",
      "Iter 93 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 94\n",
      "Train accuracy: 88.81269841269841\n",
      "Val accuracy: 87.55714285714285\n",
      "Iter 94 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 95\n",
      "Train accuracy: 88.87142857142857\n",
      "Val accuracy: 87.6\n",
      "Iter 95 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 96\n",
      "Train accuracy: 88.93015873015872\n",
      "Val accuracy: 87.64285714285714\n",
      "Iter 96 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 97\n",
      "Train accuracy: 89.0047619047619\n",
      "Val accuracy: 87.71428571428571\n",
      "Iter 97 -> sub iter 99 : 89.68253968253968\n",
      "Iteration: 98\n",
      "Train accuracy: 89.04761904761904\n",
      "Val accuracy: 87.78571428571429\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 87.81428571428572\n",
      "Iter 99 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 100\n",
      "Train accuracy: 89.13650793650794\n",
      "Val accuracy: 87.84285714285714\n",
      "Iter 100 -> sub iter 99 : 90.02222222222223\n",
      "Iteration: 101\n",
      "Train accuracy: 89.17142857142856\n",
      "Val accuracy: 87.92857142857143\n",
      "Iter 101 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 102\n",
      "Train accuracy: 89.23968253968255\n",
      "Val accuracy: 87.97142857142856\n",
      "Iter 102 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 103\n",
      "Train accuracy: 89.28888888888889\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 103 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 104\n",
      "Train accuracy: 89.33174603174604\n",
      "Val accuracy: 88.12857142857143\n",
      "Iter 104 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 105\n",
      "Train accuracy: 89.36666666666667\n",
      "Val accuracy: 88.2\n",
      "Iter 105 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 106\n",
      "Train accuracy: 89.4015873015873\n",
      "Val accuracy: 88.27142857142857\n",
      "Iter 106 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 107\n",
      "Train accuracy: 89.43968253968254\n",
      "Val accuracy: 88.34285714285714\n",
      "Iter 107 -> sub iter 99 : 90.15873015873017\n",
      "Iteration: 108\n",
      "Train accuracy: 89.46666666666667\n",
      "Val accuracy: 88.35714285714286\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 89.5047619047619\n",
      "Val accuracy: 88.38571428571429\n",
      "Iter 109 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 110\n",
      "Train accuracy: 89.55396825396825\n",
      "Val accuracy: 88.44285714285715\n",
      "Iter 110 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 111\n",
      "Train accuracy: 89.6015873015873\n",
      "Val accuracy: 88.54285714285714\n",
      "Iter 111 -> sub iter 99 : 90.79365079365084\n",
      "Iteration: 112\n",
      "Train accuracy: 89.64444444444445\n",
      "Val accuracy: 88.6\n",
      "Iter 112 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 113\n",
      "Train accuracy: 89.67936507936508\n",
      "Val accuracy: 88.62857142857142\n",
      "Iter 113 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 114\n",
      "Train accuracy: 89.72698412698412\n",
      "Val accuracy: 88.67142857142856\n",
      "Iter 114 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 115\n",
      "Train accuracy: 89.78095238095239\n",
      "Val accuracy: 88.68571428571428\n",
      "Iter 115 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 116\n",
      "Train accuracy: 89.8079365079365\n",
      "Val accuracy: 88.77142857142857\n",
      "Iter 116 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 117\n",
      "Train accuracy: 89.83968253968254\n",
      "Val accuracy: 88.81428571428572\n",
      "Iter 117 -> sub iter 99 : 90.79365079365088\n",
      "Iteration: 118\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 88.87142857142857\n",
      "Iter 118 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 119\n",
      "Train accuracy: 89.91428571428571\n",
      "Val accuracy: 88.88571428571429\n",
      "Iter 119 -> sub iter 99 : 90.79365079365082\n",
      "Iteration: 120\n",
      "Train accuracy: 89.95714285714286\n",
      "Val accuracy: 88.91428571428571\n",
      "Iter 120 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 121\n",
      "Train accuracy: 89.97777777777777\n",
      "Val accuracy: 88.97142857142856\n",
      "Iter 121 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 122\n",
      "Train accuracy: 90.0079365079365\n",
      "Val accuracy: 89.07142857142857\n",
      "Iter 122 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 123\n",
      "Train accuracy: 90.04126984126984\n",
      "Val accuracy: 89.12857142857142\n",
      "Iter 123 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 124\n",
      "Train accuracy: 90.07619047619048\n",
      "Val accuracy: 89.22857142857143\n",
      "Iter 124 -> sub iter 99 : 91.11111111111111\n",
      "Iteration: 125\n",
      "Train accuracy: 90.11587301587302\n",
      "Val accuracy: 89.24285714285715\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 90.14761904761905\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 90.16507936507936\n",
      "Val accuracy: 89.25714285714285\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 90.1968253968254\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 90.23492063492064\n",
      "Val accuracy: 89.31428571428572\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 90.26190476190476\n",
      "Val accuracy: 89.37142857142857\n",
      "Iter 130 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 131\n",
      "Train accuracy: 90.27936507936508\n",
      "Val accuracy: 89.34285714285714\n",
      "Iter 131 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 132\n",
      "Train accuracy: 90.32063492063493\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 132 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 133\n",
      "Train accuracy: 90.34920634920634\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 133 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 134\n",
      "Train accuracy: 90.36825396825397\n",
      "Val accuracy: 89.45714285714286\n",
      "Iter 134 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 135\n",
      "Train accuracy: 90.4\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 135 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 136\n",
      "Train accuracy: 90.42857142857143\n",
      "Val accuracy: 89.60000000000001\n",
      "Iter 136 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 137\n",
      "Train accuracy: 90.44761904761904\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 137 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 138\n",
      "Train accuracy: 90.46825396825396\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 138 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 139\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 139 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 140\n",
      "Train accuracy: 90.4952380952381\n",
      "Val accuracy: 89.62857142857142\n",
      "Iter 140 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 141\n",
      "Train accuracy: 90.51269841269841\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 141 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 142\n",
      "Train accuracy: 90.52222222222223\n",
      "Val accuracy: 89.65714285714286\n",
      "Iter 142 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 143\n",
      "Train accuracy: 90.54603174603174\n",
      "Val accuracy: 89.6857142857143\n",
      "Iter 143 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 144\n",
      "Train accuracy: 90.57777777777778\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 144 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 145\n",
      "Train accuracy: 90.6079365079365\n",
      "Val accuracy: 89.72857142857143\n",
      "Iter 145 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 146\n",
      "Train accuracy: 90.62222222222222\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 146 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 147\n",
      "Train accuracy: 90.63015873015873\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 147 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 148\n",
      "Train accuracy: 90.65396825396826\n",
      "Val accuracy: 89.77142857142857\n",
      "Iter 148 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 149\n",
      "Train accuracy: 90.67936507936508\n",
      "Val accuracy: 89.8\n",
      "Iter 149 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 150\n",
      "Train accuracy: 90.6920634920635\n",
      "Val accuracy: 89.81428571428572\n",
      "Training for 0.1\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 13.650793650793653\n",
      "Iteration: 1\n",
      "Train accuracy: 13.998412698412698\n",
      "Val accuracy: 14.32857142857143\n",
      "Iter 1 -> sub iter 99 : 19.365079365079367\n",
      "Iteration: 2\n",
      "Train accuracy: 20.185714285714283\n",
      "Val accuracy: 20.314285714285717\n",
      "Iter 2 -> sub iter 99 : 22.380952380952383\n",
      "Iteration: 3\n",
      "Train accuracy: 23.184126984126983\n",
      "Val accuracy: 23.32857142857143\n",
      "Iter 3 -> sub iter 99 : 25.555555555555554\n",
      "Iteration: 4\n",
      "Train accuracy: 26.05396825396825\n",
      "Val accuracy: 26.285714285714285\n",
      "Iter 4 -> sub iter 99 : 29.682539682539684\n",
      "Iteration: 5\n",
      "Train accuracy: 29.965079365079365\n",
      "Val accuracy: 30.542857142857144\n",
      "Iter 5 -> sub iter 99 : 39.841269841269845\n",
      "Iteration: 6\n",
      "Train accuracy: 39.268253968253966\n",
      "Val accuracy: 39.628571428571426\n",
      "Iter 6 -> sub iter 99 : 45.714285714285715\n",
      "Iteration: 7\n",
      "Train accuracy: 44.371428571428574\n",
      "Val accuracy: 44.800000000000004\n",
      "Iter 7 -> sub iter 99 : 48.888888888888886\n",
      "Iteration: 8\n",
      "Train accuracy: 47.52222222222222\n",
      "Val accuracy: 47.92857142857142\n",
      "Iter 8 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 9\n",
      "Train accuracy: 49.804761904761904\n",
      "Val accuracy: 50.2\n",
      "Iter 9 -> sub iter 99 : 53.174603174603185\n",
      "Iteration: 10\n",
      "Train accuracy: 51.74126984126984\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 10 -> sub iter 99 : 54.444444444444444\n",
      "Iteration: 11\n",
      "Train accuracy: 53.179365079365084\n",
      "Val accuracy: 53.214285714285715\n",
      "Iter 11 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 12\n",
      "Train accuracy: 54.36984126984127\n",
      "Val accuracy: 54.38571428571428\n",
      "Iter 12 -> sub iter 99 : 57.301587301587396\n",
      "Iteration: 13\n",
      "Train accuracy: 55.74761904761905\n",
      "Val accuracy: 55.51428571428572\n",
      "Iter 13 -> sub iter 99 : 60.793650793650794\n",
      "Iteration: 14\n",
      "Train accuracy: 58.53174603174603\n",
      "Val accuracy: 58.08571428571428\n",
      "Iter 14 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 15\n",
      "Train accuracy: 61.68730158730159\n",
      "Val accuracy: 61.01428571428571\n",
      "Iter 15 -> sub iter 99 : 66.507936507936544\n",
      "Iteration: 16\n",
      "Train accuracy: 63.53809523809524\n",
      "Val accuracy: 63.15714285714286\n",
      "Iter 16 -> sub iter 99 : 67.619047619047626\n",
      "Iteration: 17\n",
      "Train accuracy: 64.57619047619048\n",
      "Val accuracy: 64.31428571428572\n",
      "Iter 17 -> sub iter 99 : 68.730158730158735\n",
      "Iteration: 18\n",
      "Train accuracy: 65.37777777777778\n",
      "Val accuracy: 65.11428571428571\n",
      "Iter 18 -> sub iter 99 : 68.888888888888895\n",
      "Iteration: 19\n",
      "Train accuracy: 66.02698412698412\n",
      "Val accuracy: 65.75714285714285\n",
      "Iter 19 -> sub iter 99 : 69.523809523809526\n",
      "Iteration: 20\n",
      "Train accuracy: 66.6126984126984\n",
      "Val accuracy: 66.4\n",
      "Iter 20 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 21\n",
      "Train accuracy: 67.1952380952381\n",
      "Val accuracy: 66.81428571428572\n",
      "Iter 21 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 22\n",
      "Train accuracy: 68.56507936507936\n",
      "Val accuracy: 68.37142857142857\n",
      "Iter 22 -> sub iter 99 : 73.33333333333333\n",
      "Iteration: 23\n",
      "Train accuracy: 70.21904761904761\n",
      "Val accuracy: 70.35714285714286\n",
      "Iter 23 -> sub iter 99 : 73.80952380952381\n",
      "Iteration: 24\n",
      "Train accuracy: 71.43174603174603\n",
      "Val accuracy: 71.8\n",
      "Iter 24 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 25\n",
      "Train accuracy: 72.41587301587302\n",
      "Val accuracy: 73.2\n",
      "Iter 25 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 26\n",
      "Train accuracy: 73.25714285714285\n",
      "Val accuracy: 73.87142857142858\n",
      "Iter 26 -> sub iter 99 : 76.98412698412699\n",
      "Iteration: 27\n",
      "Train accuracy: 73.98095238095237\n",
      "Val accuracy: 74.58571428571429\n",
      "Iter 27 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 28\n",
      "Train accuracy: 74.55555555555556\n",
      "Val accuracy: 75.17142857142856\n",
      "Iter 28 -> sub iter 99 : 77.77777777777779\n",
      "Iteration: 29\n",
      "Train accuracy: 75.05873015873016\n",
      "Val accuracy: 75.72857142857143\n",
      "Iter 29 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 30\n",
      "Train accuracy: 75.51904761904763\n",
      "Val accuracy: 76.25714285714285\n",
      "Iter 30 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 31\n",
      "Train accuracy: 75.84444444444445\n",
      "Val accuracy: 76.5142857142857\n",
      "Iter 31 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 32\n",
      "Train accuracy: 76.22698412698414\n",
      "Val accuracy: 76.84285714285714\n",
      "Iter 32 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 33\n",
      "Train accuracy: 76.47460317460317\n",
      "Val accuracy: 77.14285714285715\n",
      "Iter 33 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 34\n",
      "Train accuracy: 76.77777777777777\n",
      "Val accuracy: 77.4\n",
      "Iter 34 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 35\n",
      "Train accuracy: 77.0015873015873\n",
      "Val accuracy: 77.65714285714286\n",
      "Iter 35 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 36\n",
      "Train accuracy: 77.21904761904761\n",
      "Val accuracy: 77.81428571428572\n",
      "Iter 36 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 37\n",
      "Train accuracy: 77.44920634920635\n",
      "Val accuracy: 78.02857142857142\n",
      "Iter 37 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 38\n",
      "Train accuracy: 77.64603174603174\n",
      "Val accuracy: 78.18571428571428\n",
      "Iter 38 -> sub iter 99 : 79.52380952380952\n",
      "Iteration: 39\n",
      "Train accuracy: 77.83333333333333\n",
      "Val accuracy: 78.3\n",
      "Iter 39 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 40\n",
      "Train accuracy: 78.01904761904763\n",
      "Val accuracy: 78.54285714285714\n",
      "Iter 40 -> sub iter 99 : 79.84126984126985\n",
      "Iteration: 41\n",
      "Train accuracy: 78.1984126984127\n",
      "Val accuracy: 78.74285714285715\n",
      "Iter 41 -> sub iter 99 : 80.00634920634929\n",
      "Iteration: 42\n",
      "Train accuracy: 78.37936507936509\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 42 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 43\n",
      "Train accuracy: 78.54603174603174\n",
      "Val accuracy: 78.98571428571428\n",
      "Iter 43 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 44\n",
      "Train accuracy: 78.71904761904761\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 44 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 45\n",
      "Train accuracy: 78.84761904761905\n",
      "Val accuracy: 79.27142857142857\n",
      "Iter 45 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 46\n",
      "Train accuracy: 78.98730158730159\n",
      "Val accuracy: 79.47142857142858\n",
      "Iter 46 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 47\n",
      "Train accuracy: 79.11587301587302\n",
      "Val accuracy: 79.65714285714286\n",
      "Iter 47 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 48\n",
      "Train accuracy: 79.23492063492064\n",
      "Val accuracy: 79.74285714285713\n",
      "Iter 48 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 49\n",
      "Train accuracy: 79.34126984126985\n",
      "Val accuracy: 79.87142857142857\n",
      "Iter 49 -> sub iter 99 : 80.79365079365089\n",
      "Iteration: 50\n",
      "Train accuracy: 79.43015873015872\n",
      "Val accuracy: 79.88571428571429\n",
      "Iter 50 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 51\n",
      "Train accuracy: 79.53492063492064\n",
      "Val accuracy: 80.01428571428572\n",
      "Iter 51 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 52\n",
      "Train accuracy: 79.62857142857143\n",
      "Val accuracy: 80.10000000000001\n",
      "Iter 52 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 53\n",
      "Train accuracy: 79.74285714285713\n",
      "Val accuracy: 80.27142857142857\n",
      "Iter 53 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 54\n",
      "Train accuracy: 79.84126984126985\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 54 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 55\n",
      "Train accuracy: 79.94920634920635\n",
      "Val accuracy: 80.37142857142857\n",
      "Iter 55 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 80.04603174603174\n",
      "Val accuracy: 80.44285714285714\n",
      "Iter 56 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 57\n",
      "Train accuracy: 80.12698412698413\n",
      "Val accuracy: 80.52857142857142\n",
      "Iter 57 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 58\n",
      "Train accuracy: 80.22857142857143\n",
      "Val accuracy: 80.57142857142857\n",
      "Iter 58 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 59\n",
      "Train accuracy: 80.32380952380952\n",
      "Val accuracy: 80.64285714285714\n",
      "Iter 59 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 60\n",
      "Train accuracy: 80.4047619047619\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 60 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 61\n",
      "Train accuracy: 80.51428571428572\n",
      "Val accuracy: 80.85714285714286\n",
      "Iter 61 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 62\n",
      "Train accuracy: 80.61111111111111\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 62 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 63\n",
      "Train accuracy: 80.72857142857143\n",
      "Val accuracy: 80.94285714285714\n",
      "Iter 63 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 64\n",
      "Train accuracy: 80.81428571428572\n",
      "Val accuracy: 81.01428571428572\n",
      "Iter 64 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 65\n",
      "Train accuracy: 80.95079365079366\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 65 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 66\n",
      "Train accuracy: 81.2047619047619\n",
      "Val accuracy: 81.21428571428572\n",
      "Iter 66 -> sub iter 99 : 84.44444444444444\n",
      "Iteration: 67\n",
      "Train accuracy: 81.55238095238096\n",
      "Val accuracy: 81.55714285714286\n",
      "Iter 67 -> sub iter 99 : 85.39682539682539\n",
      "Iteration: 68\n",
      "Train accuracy: 82.17142857142858\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 68 -> sub iter 99 : 85.87301587301587\n",
      "Iteration: 69\n",
      "Train accuracy: 82.97301587301588\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 69 -> sub iter 99 : 86.50793650793659\n",
      "Iteration: 70\n",
      "Train accuracy: 83.9015873015873\n",
      "Val accuracy: 83.6\n",
      "Iter 70 -> sub iter 99 : 87.30158730158731\n",
      "Iteration: 71\n",
      "Train accuracy: 84.74444444444444\n",
      "Val accuracy: 84.48571428571428\n",
      "Iter 71 -> sub iter 99 : 87.46031746031746\n",
      "Iteration: 72\n",
      "Train accuracy: 85.50952380952381\n",
      "Val accuracy: 85.28571428571429\n",
      "Iter 72 -> sub iter 99 : 87.61904761904762\n",
      "Iteration: 73\n",
      "Train accuracy: 86.03650793650793\n",
      "Val accuracy: 85.54285714285714\n",
      "Iter 73 -> sub iter 99 : 87.93650793650794\n",
      "Iteration: 74\n",
      "Train accuracy: 86.52063492063492\n",
      "Val accuracy: 85.94285714285715\n",
      "Iter 74 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 75\n",
      "Train accuracy: 86.90952380952382\n",
      "Val accuracy: 86.52857142857144\n",
      "Iter 75 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 76\n",
      "Train accuracy: 87.25714285714285\n",
      "Val accuracy: 86.92857142857143\n",
      "Iter 76 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 77\n",
      "Train accuracy: 87.59523809523809\n",
      "Val accuracy: 87.22857142857143\n",
      "Iter 77 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 78\n",
      "Train accuracy: 87.83492063492064\n",
      "Val accuracy: 87.4\n",
      "Iter 78 -> sub iter 99 : 88.09523809523809\n",
      "Iteration: 79\n",
      "Train accuracy: 88.07460317460317\n",
      "Val accuracy: 87.62857142857143\n",
      "Iter 79 -> sub iter 99 : 88.41269841269849\n",
      "Iteration: 80\n",
      "Train accuracy: 88.23968253968253\n",
      "Val accuracy: 87.8\n",
      "Iter 80 -> sub iter 99 : 88.41269841269842\n",
      "Iteration: 81\n",
      "Train accuracy: 88.4063492063492\n",
      "Val accuracy: 87.94285714285715\n",
      "Iter 81 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 82\n",
      "Train accuracy: 88.58095238095238\n",
      "Val accuracy: 88.07142857142857\n",
      "Iter 82 -> sub iter 99 : 88.57142857142857\n",
      "Iteration: 83\n",
      "Train accuracy: 88.73174603174603\n",
      "Val accuracy: 88.15714285714286\n",
      "Iter 83 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 84\n",
      "Train accuracy: 88.86349206349206\n",
      "Val accuracy: 88.3\n",
      "Iter 84 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 85\n",
      "Train accuracy: 88.97142857142856\n",
      "Val accuracy: 88.37142857142857\n",
      "Iter 85 -> sub iter 99 : 88.41269841269845\n",
      "Iteration: 86\n",
      "Train accuracy: 89.09206349206349\n",
      "Val accuracy: 88.45714285714286\n",
      "Iter 86 -> sub iter 99 : 88.41269841269844\n",
      "Iteration: 87\n",
      "Train accuracy: 89.19365079365079\n",
      "Val accuracy: 88.52857142857142\n",
      "Iter 87 -> sub iter 99 : 88.25396825396825\n",
      "Iteration: 88\n",
      "Train accuracy: 89.29206349206349\n",
      "Val accuracy: 88.65714285714286\n",
      "Iter 88 -> sub iter 99 : 88.41269841269848\n",
      "Iteration: 89\n",
      "Train accuracy: 89.40476190476191\n",
      "Val accuracy: 88.8\n",
      "Iter 89 -> sub iter 99 : 88.41269841269843\n",
      "Iteration: 90\n",
      "Train accuracy: 89.51269841269841\n",
      "Val accuracy: 88.9857142857143\n",
      "Iter 90 -> sub iter 99 : 88.73015873015872\n",
      "Iteration: 91\n",
      "Train accuracy: 89.63333333333333\n",
      "Val accuracy: 89.1\n",
      "Iter 91 -> sub iter 99 : 88.88888888888889\n",
      "Iteration: 92\n",
      "Train accuracy: 89.71746031746032\n",
      "Val accuracy: 89.17142857142856\n",
      "Iter 92 -> sub iter 99 : 89.04761904761904\n",
      "Iteration: 93\n",
      "Train accuracy: 89.80317460317461\n",
      "Val accuracy: 89.2\n",
      "Iter 93 -> sub iter 99 : 89.20634920634922\n",
      "Iteration: 94\n",
      "Train accuracy: 89.88571428571429\n",
      "Val accuracy: 89.28571428571429\n",
      "Iter 94 -> sub iter 99 : 89.36507936507937\n",
      "Iteration: 95\n",
      "Train accuracy: 89.94920634920635\n",
      "Val accuracy: 89.35714285714286\n",
      "Iter 95 -> sub iter 99 : 89.52380952380953\n",
      "Iteration: 96\n",
      "Train accuracy: 90.01904761904763\n",
      "Val accuracy: 89.38571428571429\n",
      "Iter 96 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 97\n",
      "Train accuracy: 90.09206349206349\n",
      "Val accuracy: 89.41428571428571\n",
      "Iter 97 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 98\n",
      "Train accuracy: 90.14126984126985\n",
      "Val accuracy: 89.47142857142858\n",
      "Iter 98 -> sub iter 99 : 89.84126984126985\n",
      "Iteration: 99\n",
      "Train accuracy: 90.2095238095238\n",
      "Val accuracy: 89.54285714285714\n",
      "Iter 99 -> sub iter 99 : 90.02857142857143\n",
      "Iteration: 100\n",
      "Train accuracy: 90.2936507936508\n",
      "Val accuracy: 89.58571428571429\n",
      "Iter 100 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 101\n",
      "Train accuracy: 90.36031746031746\n",
      "Val accuracy: 89.67142857142856\n",
      "Iter 101 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 102\n",
      "Train accuracy: 90.42539682539682\n",
      "Val accuracy: 89.75714285714285\n",
      "Iter 102 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 103\n",
      "Train accuracy: 90.48412698412699\n",
      "Val accuracy: 89.81428571428572\n",
      "Iter 103 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 104\n",
      "Train accuracy: 90.54920634920634\n",
      "Val accuracy: 89.87142857142857\n",
      "Iter 104 -> sub iter 99 : 90.31746031746032\n",
      "Iteration: 105\n",
      "Train accuracy: 90.60952380952381\n",
      "Val accuracy: 89.9\n",
      "Iter 105 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 106\n",
      "Train accuracy: 90.65873015873017\n",
      "Val accuracy: 89.97142857142858\n",
      "Iter 106 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 107\n",
      "Train accuracy: 90.7031746031746\n",
      "Val accuracy: 90.04285714285714\n",
      "Iter 107 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 108\n",
      "Train accuracy: 90.73174603174603\n",
      "Val accuracy: 90.08571428571429\n",
      "Iter 108 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 109\n",
      "Train accuracy: 90.78412698412698\n",
      "Val accuracy: 90.12857142857142\n",
      "Iter 109 -> sub iter 99 : 90.47619047619048\n",
      "Iteration: 110\n",
      "Train accuracy: 90.84603174603174\n",
      "Val accuracy: 90.15714285714286\n",
      "Iter 110 -> sub iter 99 : 90.79365079365083\n",
      "Iteration: 111\n",
      "Train accuracy: 90.89365079365079\n",
      "Val accuracy: 90.2\n",
      "Iter 111 -> sub iter 99 : 90.95238095238095\n",
      "Iteration: 112\n",
      "Train accuracy: 90.94920634920635\n",
      "Val accuracy: 90.24285714285715\n",
      "Iter 112 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 113\n",
      "Train accuracy: 91.0079365079365\n",
      "Val accuracy: 90.25714285714285\n",
      "Iter 113 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 114\n",
      "Train accuracy: 91.06349206349206\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 114 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 115\n",
      "Train accuracy: 91.10000000000001\n",
      "Val accuracy: 90.27142857142857\n",
      "Iter 115 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 116\n",
      "Train accuracy: 91.14603174603174\n",
      "Val accuracy: 90.31428571428572\n",
      "Iter 116 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 117\n",
      "Train accuracy: 91.1952380952381\n",
      "Val accuracy: 90.34285714285714\n",
      "Iter 117 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 118\n",
      "Train accuracy: 91.23968253968255\n",
      "Val accuracy: 90.4\n",
      "Iter 118 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 119\n",
      "Train accuracy: 91.28571428571428\n",
      "Val accuracy: 90.4\n",
      "Iter 119 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 120\n",
      "Train accuracy: 91.33015873015873\n",
      "Val accuracy: 90.42857142857143\n",
      "Iter 120 -> sub iter 99 : 91.26984126984127\n",
      "Iteration: 121\n",
      "Train accuracy: 91.35555555555555\n",
      "Val accuracy: 90.47142857142858\n",
      "Iter 121 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 122\n",
      "Train accuracy: 91.41269841269842\n",
      "Val accuracy: 90.52857142857142\n",
      "Iter 122 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 123\n",
      "Train accuracy: 91.43968253968254\n",
      "Val accuracy: 90.58571428571427\n",
      "Iter 123 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 124\n",
      "Train accuracy: 91.48412698412697\n",
      "Val accuracy: 90.60000000000001\n",
      "Iter 124 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 125\n",
      "Train accuracy: 91.53333333333333\n",
      "Val accuracy: 90.62857142857142\n",
      "Iter 125 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 126\n",
      "Train accuracy: 91.57142857142857\n",
      "Val accuracy: 90.65714285714286\n",
      "Iter 126 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 127\n",
      "Train accuracy: 91.60634920634921\n",
      "Val accuracy: 90.68571428571428\n",
      "Iter 127 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 128\n",
      "Train accuracy: 91.63650793650794\n",
      "Val accuracy: 90.71428571428571\n",
      "Iter 128 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 129\n",
      "Train accuracy: 91.66349206349206\n",
      "Val accuracy: 90.74285714285715\n",
      "Iter 129 -> sub iter 99 : 91.42857142857143\n",
      "Iteration: 130\n",
      "Train accuracy: 91.70158730158731\n",
      "Val accuracy: 90.8\n",
      "Iter 130 -> sub iter 99 : 91.58730158730158\n",
      "Iteration: 131\n",
      "Train accuracy: 91.72063492063492\n",
      "Val accuracy: 90.82857142857142\n",
      "Iter 131 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 132\n",
      "Train accuracy: 91.74285714285715\n",
      "Val accuracy: 90.84285714285714\n",
      "Iter 132 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 133\n",
      "Train accuracy: 91.76507936507936\n",
      "Val accuracy: 90.87142857142857\n",
      "Iter 133 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 134\n",
      "Train accuracy: 91.8015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 134 -> sub iter 99 : 91.74603174603175\n",
      "Iteration: 135\n",
      "Train accuracy: 91.83015873015873\n",
      "Val accuracy: 90.9\n",
      "Iter 135 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 136\n",
      "Train accuracy: 91.86666666666666\n",
      "Val accuracy: 90.9\n",
      "Iter 136 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 137\n",
      "Train accuracy: 91.91746031746032\n",
      "Val accuracy: 90.94285714285715\n",
      "Iter 137 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 138\n",
      "Train accuracy: 91.94761904761904\n",
      "Val accuracy: 90.97142857142858\n",
      "Iter 138 -> sub iter 99 : 91.90476190476198\n",
      "Iteration: 139\n",
      "Train accuracy: 91.98888888888888\n",
      "Val accuracy: 91.05714285714286\n",
      "Iter 139 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 140\n",
      "Train accuracy: 92.02698412698412\n",
      "Val accuracy: 91.11428571428571\n",
      "Iter 140 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 141\n",
      "Train accuracy: 92.05238095238096\n",
      "Val accuracy: 91.12857142857142\n",
      "Iter 141 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 142\n",
      "Train accuracy: 92.08571428571429\n",
      "Val accuracy: 91.15714285714286\n",
      "Iter 142 -> sub iter 99 : 91.90476190476194\n",
      "Iteration: 143\n",
      "Train accuracy: 92.1079365079365\n",
      "Val accuracy: 91.17142857142856\n",
      "Iter 143 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 144\n",
      "Train accuracy: 92.13809523809525\n",
      "Val accuracy: 91.21428571428571\n",
      "Iter 144 -> sub iter 99 : 92.06349206349206\n",
      "Iteration: 145\n",
      "Train accuracy: 92.17142857142858\n",
      "Val accuracy: 91.24285714285715\n",
      "Iter 145 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 146\n",
      "Train accuracy: 92.1968253968254\n",
      "Val accuracy: 91.25714285714285\n",
      "Iter 146 -> sub iter 99 : 92.22222222222223\n",
      "Iteration: 147\n",
      "Train accuracy: 92.23174603174603\n",
      "Val accuracy: 91.27142857142857\n",
      "Iter 147 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 148\n",
      "Train accuracy: 92.25873015873016\n",
      "Val accuracy: 91.3\n",
      "Iter 148 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 149\n",
      "Train accuracy: 92.28253968253968\n",
      "Val accuracy: 91.3\n",
      "Iter 149 -> sub iter 99 : 92.38095238095238\n",
      "Iteration: 150\n",
      "Train accuracy: 92.3015873015873\n",
      "Val accuracy: 91.3\n",
      "Training for 0.01\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 24.285714285714285\n",
      "Iteration: 1\n",
      "Train accuracy: 21.285714285714285\n",
      "Val accuracy: 20.42857142857143\n",
      "Iter 1 -> sub iter 99 : 26.666666666666668\n",
      "Iteration: 2\n",
      "Train accuracy: 24.23968253968254\n",
      "Val accuracy: 23.414285714285715\n",
      "Iter 2 -> sub iter 99 : 29.523809523809526\n",
      "Iteration: 3\n",
      "Train accuracy: 26.51904761904762\n",
      "Val accuracy: 25.785714285714285\n",
      "Iter 3 -> sub iter 99 : 34.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 30.831746031746032\n",
      "Val accuracy: 30.242857142857144\n",
      "Iter 4 -> sub iter 99 : 37.301587301587304\n",
      "Iteration: 5\n",
      "Train accuracy: 33.77460317460317\n",
      "Val accuracy: 33.15714285714286\n",
      "Iter 5 -> sub iter 99 : 39.047619047619054\n",
      "Iteration: 6\n",
      "Train accuracy: 35.75238095238095\n",
      "Val accuracy: 35.25714285714286\n",
      "Iter 6 -> sub iter 99 : 39.523809523809526\n",
      "Iteration: 7\n",
      "Train accuracy: 37.7\n",
      "Val accuracy: 37.25714285714285\n",
      "Iter 7 -> sub iter 99 : 42.698412698412696\n",
      "Iteration: 8\n",
      "Train accuracy: 41.15555555555556\n",
      "Val accuracy: 40.34285714285714\n",
      "Iter 8 -> sub iter 99 : 46.190476190476195\n",
      "Iteration: 9\n",
      "Train accuracy: 44.84444444444444\n",
      "Val accuracy: 43.67142857142857\n",
      "Iter 9 -> sub iter 99 : 51.904761904761916\n",
      "Iteration: 10\n",
      "Train accuracy: 52.49206349206349\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 10 -> sub iter 99 : 55.238095238095246\n",
      "Iteration: 11\n",
      "Train accuracy: 56.10793650793651\n",
      "Val accuracy: 55.442857142857136\n",
      "Iter 11 -> sub iter 99 : 58.412698412698425\n",
      "Iteration: 12\n",
      "Train accuracy: 58.439682539682536\n",
      "Val accuracy: 57.72857142857143\n",
      "Iter 12 -> sub iter 99 : 60.317460317460316\n",
      "Iteration: 13\n",
      "Train accuracy: 60.098412698412695\n",
      "Val accuracy: 59.61428571428572\n",
      "Iter 13 -> sub iter 99 : 62.698412698412696\n",
      "Iteration: 14\n",
      "Train accuracy: 61.66031746031746\n",
      "Val accuracy: 61.42857142857143\n",
      "Iter 14 -> sub iter 99 : 65.238095238095245\n",
      "Iteration: 15\n",
      "Train accuracy: 63.32698412698413\n",
      "Val accuracy: 62.857142857142854\n",
      "Iter 15 -> sub iter 99 : 66.190476190476196\n",
      "Iteration: 16\n",
      "Train accuracy: 64.60317460317461\n",
      "Val accuracy: 64.14285714285714\n",
      "Iter 16 -> sub iter 99 : 66.507936507936545\n",
      "Iteration: 17\n",
      "Train accuracy: 65.4952380952381\n",
      "Val accuracy: 65.17142857142856\n",
      "Iter 17 -> sub iter 99 : 67.777777777777796\n",
      "Iteration: 18\n",
      "Train accuracy: 66.21904761904761\n",
      "Val accuracy: 65.9\n",
      "Iter 18 -> sub iter 99 : 68.730158730158736\n",
      "Iteration: 19\n",
      "Train accuracy: 66.91111111111111\n",
      "Val accuracy: 66.60000000000001\n",
      "Iter 19 -> sub iter 99 : 71.111111111111116\n",
      "Iteration: 20\n",
      "Train accuracy: 68.81111111111112\n",
      "Val accuracy: 68.54285714285714\n",
      "Iter 20 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 21\n",
      "Train accuracy: 70.85396825396826\n",
      "Val accuracy: 70.61428571428571\n",
      "Iter 21 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 22\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 72.0\n",
      "Iter 22 -> sub iter 99 : 73.49206349206358\n",
      "Iteration: 23\n",
      "Train accuracy: 73.44444444444444\n",
      "Val accuracy: 73.17142857142858\n",
      "Iter 23 -> sub iter 99 : 74.44444444444444\n",
      "Iteration: 24\n",
      "Train accuracy: 74.31428571428572\n",
      "Val accuracy: 74.11428571428571\n",
      "Iter 24 -> sub iter 99 : 74.92063492063492\n",
      "Iteration: 25\n",
      "Train accuracy: 74.92857142857143\n",
      "Val accuracy: 74.67142857142856\n",
      "Iter 25 -> sub iter 99 : 75.23809523809524\n",
      "Iteration: 26\n",
      "Train accuracy: 75.4968253968254\n",
      "Val accuracy: 75.0\n",
      "Iter 26 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 27\n",
      "Train accuracy: 75.91904761904762\n",
      "Val accuracy: 75.44285714285715\n",
      "Iter 27 -> sub iter 99 : 76.03174603174602\n",
      "Iteration: 28\n",
      "Train accuracy: 76.29047619047618\n",
      "Val accuracy: 75.68571428571428\n",
      "Iter 28 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 29\n",
      "Train accuracy: 76.65238095238095\n",
      "Val accuracy: 76.0\n",
      "Iter 29 -> sub iter 99 : 76.19047619047619\n",
      "Iteration: 30\n",
      "Train accuracy: 76.95714285714286\n",
      "Val accuracy: 76.31428571428572\n",
      "Iter 30 -> sub iter 99 : 76.34920634920634\n",
      "Iteration: 31\n",
      "Train accuracy: 77.22063492063492\n",
      "Val accuracy: 76.6\n",
      "Iter 31 -> sub iter 99 : 76.82539682539684\n",
      "Iteration: 32\n",
      "Train accuracy: 77.4920634920635\n",
      "Val accuracy: 76.92857142857143\n",
      "Iter 32 -> sub iter 99 : 77.14285714285715\n",
      "Iteration: 33\n",
      "Train accuracy: 77.72222222222223\n",
      "Val accuracy: 77.08571428571429\n",
      "Iter 33 -> sub iter 99 : 77.30158730158731\n",
      "Iteration: 34\n",
      "Train accuracy: 77.96190476190476\n",
      "Val accuracy: 77.21428571428571\n",
      "Iter 34 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 35\n",
      "Train accuracy: 78.16984126984127\n",
      "Val accuracy: 77.47142857142858\n",
      "Iter 35 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 36\n",
      "Train accuracy: 78.37460317460318\n",
      "Val accuracy: 77.8\n",
      "Iter 36 -> sub iter 99 : 77.61904761904762\n",
      "Iteration: 37\n",
      "Train accuracy: 78.57936507936508\n",
      "Val accuracy: 77.91428571428571\n",
      "Iter 37 -> sub iter 99 : 78.09523809523812\n",
      "Iteration: 38\n",
      "Train accuracy: 78.75555555555556\n",
      "Val accuracy: 78.04285714285714\n",
      "Iter 38 -> sub iter 99 : 78.41269841269842\n",
      "Iteration: 39\n",
      "Train accuracy: 78.92063492063492\n",
      "Val accuracy: 78.22857142857143\n",
      "Iter 39 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 40\n",
      "Train accuracy: 79.0936507936508\n",
      "Val accuracy: 78.37142857142857\n",
      "Iter 40 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 41\n",
      "Train accuracy: 79.26666666666667\n",
      "Val accuracy: 78.51428571428572\n",
      "Iter 41 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 79.4\n",
      "Val accuracy: 78.58571428571427\n",
      "Iter 42 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 43\n",
      "Train accuracy: 79.52539682539683\n",
      "Val accuracy: 78.77142857142857\n",
      "Iter 43 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 44\n",
      "Train accuracy: 79.62539682539682\n",
      "Val accuracy: 78.85714285714286\n",
      "Iter 44 -> sub iter 99 : 78.57142857142857\n",
      "Iteration: 45\n",
      "Train accuracy: 79.71587301587302\n",
      "Val accuracy: 79.04285714285714\n",
      "Iter 45 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 46\n",
      "Train accuracy: 79.83492063492064\n",
      "Val accuracy: 79.21428571428571\n",
      "Iter 46 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 47\n",
      "Train accuracy: 79.95396825396826\n",
      "Val accuracy: 79.37142857142857\n",
      "Iter 47 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 48\n",
      "Train accuracy: 80.04444444444444\n",
      "Val accuracy: 79.45714285714286\n",
      "Iter 48 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 49\n",
      "Train accuracy: 80.13333333333334\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 49 -> sub iter 99 : 78.88888888888889\n",
      "Iteration: 50\n",
      "Train accuracy: 80.21111111111111\n",
      "Val accuracy: 79.7\n",
      "Iter 50 -> sub iter 99 : 79.04761904761905\n",
      "Iteration: 51\n",
      "Train accuracy: 80.2984126984127\n",
      "Val accuracy: 79.84285714285714\n",
      "Iter 51 -> sub iter 99 : 79.20634920634924\n",
      "Iteration: 52\n",
      "Train accuracy: 80.38253968253967\n",
      "Val accuracy: 79.9\n",
      "Iter 52 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 53\n",
      "Train accuracy: 80.48095238095239\n",
      "Val accuracy: 79.91428571428571\n",
      "Iter 53 -> sub iter 99 : 80.15873015873017\n",
      "Iteration: 54\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 80.05714285714286\n",
      "Iter 54 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 55\n",
      "Train accuracy: 80.64126984126983\n",
      "Val accuracy: 80.12857142857143\n",
      "Iter 55 -> sub iter 99 : 80.47619047619048\n",
      "Iteration: 56\n",
      "Train accuracy: 80.70793650793651\n",
      "Val accuracy: 80.21428571428572\n",
      "Iter 56 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 57\n",
      "Train accuracy: 80.78888888888889\n",
      "Val accuracy: 80.32857142857142\n",
      "Iter 57 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 58\n",
      "Train accuracy: 80.85873015873017\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 58 -> sub iter 99 : 80.79365079365082\n",
      "Iteration: 59\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.48571428571428\n",
      "Iter 59 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 60\n",
      "Train accuracy: 80.99047619047619\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 60 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 61\n",
      "Train accuracy: 81.06507936507936\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 61 -> sub iter 99 : 80.79365079365087\n",
      "Iteration: 62\n",
      "Train accuracy: 81.12857142857143\n",
      "Val accuracy: 80.60000000000001\n",
      "Iter 62 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 63\n",
      "Train accuracy: 81.1920634920635\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 63 -> sub iter 99 : 81.11111111111111\n",
      "Iteration: 64\n",
      "Train accuracy: 81.25555555555556\n",
      "Val accuracy: 80.65714285714286\n",
      "Iter 64 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 65\n",
      "Train accuracy: 81.32857142857142\n",
      "Val accuracy: 80.7\n",
      "Iter 65 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 66\n",
      "Train accuracy: 81.37936507936509\n",
      "Val accuracy: 80.78571428571428\n",
      "Iter 66 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 67\n",
      "Train accuracy: 81.43333333333334\n",
      "Val accuracy: 80.80000000000001\n",
      "Iter 67 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 68\n",
      "Train accuracy: 81.48253968253968\n",
      "Val accuracy: 80.84285714285714\n",
      "Iter 68 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 69\n",
      "Train accuracy: 81.52539682539683\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 69 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 70\n",
      "Train accuracy: 81.5904761904762\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 70 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 71\n",
      "Train accuracy: 81.65238095238095\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 71 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 72\n",
      "Train accuracy: 81.70793650793651\n",
      "Val accuracy: 80.95714285714286\n",
      "Iter 72 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 73\n",
      "Train accuracy: 81.76825396825397\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 73 -> sub iter 99 : 81.90476190476192\n",
      "Iteration: 74\n",
      "Train accuracy: 81.81904761904762\n",
      "Val accuracy: 81.08571428571429\n",
      "Iter 74 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 75\n",
      "Train accuracy: 81.85238095238095\n",
      "Val accuracy: 81.12857142857143\n",
      "Iter 75 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 76\n",
      "Train accuracy: 81.8968253968254\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 76 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 77\n",
      "Train accuracy: 81.93015873015874\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 77 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 78\n",
      "Train accuracy: 82.0\n",
      "Val accuracy: 81.24285714285713\n",
      "Iter 78 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 79\n",
      "Train accuracy: 82.04444444444444\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 79 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 80\n",
      "Train accuracy: 82.1\n",
      "Val accuracy: 81.3\n",
      "Iter 80 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 81\n",
      "Train accuracy: 82.13333333333334\n",
      "Val accuracy: 81.34285714285714\n",
      "Iter 81 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 82\n",
      "Train accuracy: 82.15396825396826\n",
      "Val accuracy: 81.32857142857142\n",
      "Iter 82 -> sub iter 99 : 82.06349206349206\n",
      "Iteration: 83\n",
      "Train accuracy: 82.2015873015873\n",
      "Val accuracy: 81.37142857142857\n",
      "Iter 83 -> sub iter 99 : 82.22222222222221\n",
      "Iteration: 84\n",
      "Train accuracy: 82.24761904761905\n",
      "Val accuracy: 81.38571428571429\n",
      "Iter 84 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 85\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.41428571428571\n",
      "Iter 85 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 86\n",
      "Train accuracy: 82.33015873015873\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 86 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 87\n",
      "Train accuracy: 82.37301587301587\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 87 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 88\n",
      "Train accuracy: 82.43809523809524\n",
      "Val accuracy: 81.54285714285714\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 82.46507936507936\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 89 -> sub iter 99 : 82.69841269841273\n",
      "Iteration: 90\n",
      "Train accuracy: 82.5047619047619\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 90 -> sub iter 99 : 82.69841269841278\n",
      "Iteration: 91\n",
      "Train accuracy: 82.53650793650795\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 91 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 92\n",
      "Train accuracy: 82.57142857142857\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 92 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 93\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.74285714285713\n",
      "Iter 93 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 94\n",
      "Train accuracy: 82.62857142857143\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 94 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 95\n",
      "Train accuracy: 82.65714285714286\n",
      "Val accuracy: 81.81428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 82.6920634920635\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 96 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 97\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.89999999999999\n",
      "Iter 97 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 98\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.94285714285714\n",
      "Iter 98 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 99\n",
      "Train accuracy: 82.78095238095237\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 99 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 100\n",
      "Train accuracy: 82.81746031746032\n",
      "Val accuracy: 81.95714285714286\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.84603174603174\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.87777777777777\n",
      "Val accuracy: 81.98571428571428\n",
      "Iter 102 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 103\n",
      "Train accuracy: 82.89047619047619\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 104\n",
      "Train accuracy: 82.92380952380952\n",
      "Val accuracy: 82.01428571428572\n",
      "Iter 104 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 105\n",
      "Train accuracy: 82.95555555555556\n",
      "Val accuracy: 82.02857142857142\n",
      "Iter 105 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 106\n",
      "Train accuracy: 82.97777777777777\n",
      "Val accuracy: 82.05714285714286\n",
      "Iter 106 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 107\n",
      "Train accuracy: 83.0\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 107 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 108\n",
      "Train accuracy: 83.02539682539683\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 108 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 109\n",
      "Train accuracy: 83.05079365079365\n",
      "Val accuracy: 82.14285714285714\n",
      "Iter 109 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 110\n",
      "Train accuracy: 83.07460317460318\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 110 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 111\n",
      "Train accuracy: 83.1\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 111 -> sub iter 99 : 83.65079365079366\n",
      "Iteration: 112\n",
      "Train accuracy: 83.13015873015873\n",
      "Val accuracy: 82.21428571428572\n",
      "Iter 112 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 113\n",
      "Train accuracy: 83.16190476190476\n",
      "Val accuracy: 82.24285714285713\n",
      "Iter 113 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 114\n",
      "Train accuracy: 83.17936507936507\n",
      "Val accuracy: 82.27142857142857\n",
      "Iter 114 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 115\n",
      "Train accuracy: 83.20476190476191\n",
      "Val accuracy: 82.28571428571428\n",
      "Iter 115 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 116\n",
      "Train accuracy: 83.22698412698412\n",
      "Val accuracy: 82.32857142857142\n",
      "Iter 116 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 117\n",
      "Train accuracy: 83.25396825396825\n",
      "Val accuracy: 82.37142857142857\n",
      "Iter 117 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 118\n",
      "Train accuracy: 83.28253968253968\n",
      "Val accuracy: 82.38571428571429\n",
      "Iter 118 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 119\n",
      "Train accuracy: 83.30634920634921\n",
      "Val accuracy: 82.39999999999999\n",
      "Iter 119 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 120\n",
      "Train accuracy: 83.31904761904761\n",
      "Val accuracy: 82.42857142857143\n",
      "Iter 120 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 121\n",
      "Train accuracy: 83.34603174603174\n",
      "Val accuracy: 82.45714285714286\n",
      "Iter 121 -> sub iter 99 : 83.80952380952381\n",
      "Iteration: 122\n",
      "Train accuracy: 83.37460317460318\n",
      "Val accuracy: 82.48571428571428\n",
      "Iter 122 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 123\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.51428571428572\n",
      "Iter 123 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 124\n",
      "Train accuracy: 83.4095238095238\n",
      "Val accuracy: 82.55714285714286\n",
      "Iter 124 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 125\n",
      "Train accuracy: 83.44444444444444\n",
      "Val accuracy: 82.6\n",
      "Iter 125 -> sub iter 99 : 84.12698412698413\n",
      "Iteration: 126\n",
      "Train accuracy: 83.46190476190476\n",
      "Val accuracy: 82.62857142857143\n",
      "Iter 126 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 127\n",
      "Train accuracy: 83.49047619047619\n",
      "Val accuracy: 82.67142857142858\n",
      "Iter 127 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 128\n",
      "Train accuracy: 83.5079365079365\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 128 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 129\n",
      "Train accuracy: 83.53333333333333\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 129 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 130\n",
      "Train accuracy: 83.54920634920634\n",
      "Val accuracy: 82.71428571428572\n",
      "Iter 130 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 131\n",
      "Train accuracy: 83.57460317460318\n",
      "Val accuracy: 82.72857142857143\n",
      "Iter 131 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 132\n",
      "Train accuracy: 83.6015873015873\n",
      "Val accuracy: 82.78571428571428\n",
      "Iter 132 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 133\n",
      "Train accuracy: 83.62857142857143\n",
      "Val accuracy: 82.8\n",
      "Iter 133 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 134\n",
      "Train accuracy: 83.65714285714286\n",
      "Val accuracy: 82.81428571428572\n",
      "Iter 134 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 135\n",
      "Train accuracy: 83.67460317460318\n",
      "Val accuracy: 82.84285714285714\n",
      "Iter 135 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 136\n",
      "Train accuracy: 83.69682539682539\n",
      "Val accuracy: 82.85714285714286\n",
      "Iter 136 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 137\n",
      "Train accuracy: 83.72539682539683\n",
      "Val accuracy: 82.87142857142857\n",
      "Iter 137 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 138\n",
      "Train accuracy: 83.74444444444444\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 139\n",
      "Train accuracy: 83.75873015873016\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 139 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 140\n",
      "Train accuracy: 83.77619047619046\n",
      "Val accuracy: 82.88571428571429\n",
      "Iter 140 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 141\n",
      "Train accuracy: 83.7968253968254\n",
      "Val accuracy: 82.94285714285714\n",
      "Iter 141 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 142\n",
      "Train accuracy: 83.82063492063492\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 142 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 143\n",
      "Train accuracy: 83.83809523809524\n",
      "Val accuracy: 82.95714285714286\n",
      "Iter 143 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 144\n",
      "Train accuracy: 83.86190476190475\n",
      "Val accuracy: 82.97142857142858\n",
      "Iter 144 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 145\n",
      "Train accuracy: 83.88571428571429\n",
      "Val accuracy: 82.98571428571428\n",
      "Iter 145 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 146\n",
      "Train accuracy: 83.9031746031746\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 146 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 147\n",
      "Train accuracy: 83.91904761904762\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 147 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 148\n",
      "Train accuracy: 83.95238095238096\n",
      "Val accuracy: 83.0\n",
      "Iter 148 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 149\n",
      "Train accuracy: 83.97301587301588\n",
      "Val accuracy: 83.01428571428572\n",
      "Iter 149 -> sub iter 99 : 83.96825396825398\n",
      "Iteration: 150\n",
      "Train accuracy: 83.97460317460317\n",
      "Val accuracy: 83.04285714285714\n",
      "Training for 0.001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 20.158730158730158\n",
      "Iteration: 1\n",
      "Train accuracy: 16.414285714285715\n",
      "Val accuracy: 15.814285714285713\n",
      "Iter 1 -> sub iter 99 : 30.158730158730158\n",
      "Iteration: 2\n",
      "Train accuracy: 27.836507936507935\n",
      "Val accuracy: 27.15714285714286\n",
      "Iter 2 -> sub iter 99 : 37.936507936507946\n",
      "Iteration: 3\n",
      "Train accuracy: 36.93015873015873\n",
      "Val accuracy: 36.15714285714286\n",
      "Iter 3 -> sub iter 99 : 44.285714285714285\n",
      "Iteration: 4\n",
      "Train accuracy: 42.15238095238095\n",
      "Val accuracy: 41.27142857142857\n",
      "Iter 4 -> sub iter 99 : 47.777777777777786\n",
      "Iteration: 5\n",
      "Train accuracy: 46.84285714285714\n",
      "Val accuracy: 45.92857142857143\n",
      "Iter 5 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 6\n",
      "Train accuracy: 51.34603174603175\n",
      "Val accuracy: 50.55714285714286\n",
      "Iter 6 -> sub iter 99 : 54.603174603174605\n",
      "Iteration: 7\n",
      "Train accuracy: 54.85079365079365\n",
      "Val accuracy: 53.51428571428571\n",
      "Iter 7 -> sub iter 99 : 56.825396825396824\n",
      "Iteration: 8\n",
      "Train accuracy: 57.369841269841274\n",
      "Val accuracy: 56.07142857142857\n",
      "Iter 8 -> sub iter 99 : 58.253968253968264\n",
      "Iteration: 9\n",
      "Train accuracy: 59.23492063492064\n",
      "Val accuracy: 57.8\n",
      "Iter 9 -> sub iter 99 : 59.841269841269844\n",
      "Iteration: 10\n",
      "Train accuracy: 60.663492063492065\n",
      "Val accuracy: 59.5\n",
      "Iter 10 -> sub iter 99 : 60.476190476190474\n",
      "Iteration: 11\n",
      "Train accuracy: 61.850793650793655\n",
      "Val accuracy: 60.785714285714285\n",
      "Iter 11 -> sub iter 99 : 61.587301587301596\n",
      "Iteration: 12\n",
      "Train accuracy: 62.790476190476184\n",
      "Val accuracy: 62.02857142857143\n",
      "Iter 12 -> sub iter 99 : 62.539682539682545\n",
      "Iteration: 13\n",
      "Train accuracy: 63.549206349206344\n",
      "Val accuracy: 62.94285714285714\n",
      "Iter 13 -> sub iter 99 : 63.492063492063495\n",
      "Iteration: 14\n",
      "Train accuracy: 64.18253968253968\n",
      "Val accuracy: 63.67142857142857\n",
      "Iter 14 -> sub iter 99 : 63.809523809523835\n",
      "Iteration: 15\n",
      "Train accuracy: 64.75555555555556\n",
      "Val accuracy: 64.24285714285715\n",
      "Iter 15 -> sub iter 99 : 64.603174603174615\n",
      "Iteration: 16\n",
      "Train accuracy: 65.28412698412698\n",
      "Val accuracy: 64.85714285714286\n",
      "Iter 16 -> sub iter 99 : 65.079365079365085\n",
      "Iteration: 17\n",
      "Train accuracy: 65.6952380952381\n",
      "Val accuracy: 65.18571428571428\n",
      "Iter 17 -> sub iter 99 : 66.031746031746024\n",
      "Iteration: 18\n",
      "Train accuracy: 66.10317460317461\n",
      "Val accuracy: 65.84285714285714\n",
      "Iter 18 -> sub iter 99 : 66.190476190476195\n",
      "Iteration: 19\n",
      "Train accuracy: 66.45555555555556\n",
      "Val accuracy: 66.17142857142856\n",
      "Iter 19 -> sub iter 99 : 66.507936507936514\n",
      "Iteration: 20\n",
      "Train accuracy: 66.79047619047618\n",
      "Val accuracy: 66.57142857142857\n",
      "Iter 20 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 21\n",
      "Train accuracy: 67.14126984126985\n",
      "Val accuracy: 66.8\n",
      "Iter 21 -> sub iter 99 : 66.666666666666665\n",
      "Iteration: 22\n",
      "Train accuracy: 67.43492063492063\n",
      "Val accuracy: 67.0\n",
      "Iter 22 -> sub iter 99 : 67.142857142857146\n",
      "Iteration: 23\n",
      "Train accuracy: 67.76507936507936\n",
      "Val accuracy: 67.31428571428572\n",
      "Iter 23 -> sub iter 99 : 67.460317460317474\n",
      "Iteration: 24\n",
      "Train accuracy: 68.03968253968254\n",
      "Val accuracy: 67.52857142857142\n",
      "Iter 24 -> sub iter 99 : 68.09523809523813\n",
      "Iteration: 25\n",
      "Train accuracy: 68.27619047619048\n",
      "Val accuracy: 67.92857142857143\n",
      "Iter 25 -> sub iter 99 : 68.25396825396825\n",
      "Iteration: 26\n",
      "Train accuracy: 68.53174603174604\n",
      "Val accuracy: 68.14285714285714\n",
      "Iter 26 -> sub iter 99 : 68.73015873015873\n",
      "Iteration: 27\n",
      "Train accuracy: 68.77301587301588\n",
      "Val accuracy: 68.27142857142857\n",
      "Iter 27 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 28\n",
      "Train accuracy: 68.98412698412699\n",
      "Val accuracy: 68.51428571428572\n",
      "Iter 28 -> sub iter 99 : 69.20634920634922\n",
      "Iteration: 29\n",
      "Train accuracy: 69.19047619047619\n",
      "Val accuracy: 68.77142857142857\n",
      "Iter 29 -> sub iter 99 : 69.36507936507937\n",
      "Iteration: 30\n",
      "Train accuracy: 69.39047619047619\n",
      "Val accuracy: 68.94285714285714\n",
      "Iter 30 -> sub iter 99 : 69.52380952380952\n",
      "Iteration: 31\n",
      "Train accuracy: 69.55238095238096\n",
      "Val accuracy: 69.07142857142857\n",
      "Iter 31 -> sub iter 99 : 70.15873015873015\n",
      "Iteration: 32\n",
      "Train accuracy: 69.72857142857143\n",
      "Val accuracy: 69.22857142857143\n",
      "Iter 32 -> sub iter 99 : 70.31746031746032\n",
      "Iteration: 33\n",
      "Train accuracy: 69.92063492063491\n",
      "Val accuracy: 69.32857142857142\n",
      "Iter 33 -> sub iter 99 : 70.47619047619048\n",
      "Iteration: 34\n",
      "Train accuracy: 70.0904761904762\n",
      "Val accuracy: 69.44285714285714\n",
      "Iter 34 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 35\n",
      "Train accuracy: 70.22380952380952\n",
      "Val accuracy: 69.69999999999999\n",
      "Iter 35 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 36\n",
      "Train accuracy: 70.36984126984127\n",
      "Val accuracy: 69.88571428571429\n",
      "Iter 36 -> sub iter 99 : 70.63492063492063\n",
      "Iteration: 37\n",
      "Train accuracy: 70.5\n",
      "Val accuracy: 69.98571428571428\n",
      "Iter 37 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 38\n",
      "Train accuracy: 70.63650793650794\n",
      "Val accuracy: 70.02857142857142\n",
      "Iter 38 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 39\n",
      "Train accuracy: 70.74126984126984\n",
      "Val accuracy: 70.11428571428571\n",
      "Iter 39 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 40\n",
      "Train accuracy: 70.84920634920636\n",
      "Val accuracy: 70.15714285714286\n",
      "Iter 40 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 41\n",
      "Train accuracy: 70.92539682539683\n",
      "Val accuracy: 70.3\n",
      "Iter 41 -> sub iter 99 : 70.95238095238095\n",
      "Iteration: 42\n",
      "Train accuracy: 71.02539682539683\n",
      "Val accuracy: 70.34285714285714\n",
      "Iter 42 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 43\n",
      "Train accuracy: 71.13650793650793\n",
      "Val accuracy: 70.39999999999999\n",
      "Iter 43 -> sub iter 99 : 71.11111111111111\n",
      "Iteration: 44\n",
      "Train accuracy: 71.22222222222221\n",
      "Val accuracy: 70.48571428571428\n",
      "Iter 44 -> sub iter 99 : 71.26984126984127\n",
      "Iteration: 45\n",
      "Train accuracy: 71.28730158730158\n",
      "Val accuracy: 70.55714285714285\n",
      "Iter 45 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 46\n",
      "Train accuracy: 71.36984126984127\n",
      "Val accuracy: 70.71428571428572\n",
      "Iter 46 -> sub iter 99 : 71.58730158730158\n",
      "Iteration: 47\n",
      "Train accuracy: 71.44603174603175\n",
      "Val accuracy: 70.84285714285714\n",
      "Iter 47 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 48\n",
      "Train accuracy: 71.5047619047619\n",
      "Val accuracy: 70.94285714285714\n",
      "Iter 48 -> sub iter 99 : 71.90476190476196\n",
      "Iteration: 49\n",
      "Train accuracy: 71.56825396825397\n",
      "Val accuracy: 70.98571428571428\n",
      "Iter 49 -> sub iter 99 : 71.90476190476193\n",
      "Iteration: 50\n",
      "Train accuracy: 71.615873015873\n",
      "Val accuracy: 71.02857142857142\n",
      "Iter 50 -> sub iter 99 : 71.74603174603175\n",
      "Iteration: 51\n",
      "Train accuracy: 71.67301587301587\n",
      "Val accuracy: 71.11428571428571\n",
      "Iter 51 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 52\n",
      "Train accuracy: 71.73650793650793\n",
      "Val accuracy: 71.21428571428572\n",
      "Iter 52 -> sub iter 99 : 71.90476190476192\n",
      "Iteration: 53\n",
      "Train accuracy: 71.8047619047619\n",
      "Val accuracy: 71.25714285714285\n",
      "Iter 53 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 54\n",
      "Train accuracy: 71.88253968253969\n",
      "Val accuracy: 71.28571428571429\n",
      "Iter 54 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 55\n",
      "Train accuracy: 71.96984126984127\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 55 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 56\n",
      "Train accuracy: 72.04126984126984\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 56 -> sub iter 99 : 72.06349206349206\n",
      "Iteration: 57\n",
      "Train accuracy: 72.0920634920635\n",
      "Val accuracy: 71.35714285714285\n",
      "Iter 57 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 58\n",
      "Train accuracy: 72.16984126984127\n",
      "Val accuracy: 71.41428571428573\n",
      "Iter 58 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 59\n",
      "Train accuracy: 72.22698412698414\n",
      "Val accuracy: 71.45714285714286\n",
      "Iter 59 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 60\n",
      "Train accuracy: 72.27936507936508\n",
      "Val accuracy: 71.55714285714285\n",
      "Iter 60 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 61\n",
      "Train accuracy: 72.33333333333334\n",
      "Val accuracy: 71.64285714285714\n",
      "Iter 61 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 62\n",
      "Train accuracy: 72.38888888888889\n",
      "Val accuracy: 71.68571428571428\n",
      "Iter 62 -> sub iter 99 : 72.53968253968253\n",
      "Iteration: 63\n",
      "Train accuracy: 72.45238095238096\n",
      "Val accuracy: 71.7\n",
      "Iter 63 -> sub iter 99 : 72.69841269841276\n",
      "Iteration: 64\n",
      "Train accuracy: 72.52222222222223\n",
      "Val accuracy: 71.75714285714285\n",
      "Iter 64 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 65\n",
      "Train accuracy: 72.59523809523809\n",
      "Val accuracy: 71.77142857142857\n",
      "Iter 65 -> sub iter 99 : 72.69841269841278\n",
      "Iteration: 66\n",
      "Train accuracy: 72.65079365079366\n",
      "Val accuracy: 71.82857142857144\n",
      "Iter 66 -> sub iter 99 : 72.85714285714285\n",
      "Iteration: 67\n",
      "Train accuracy: 72.73650793650793\n",
      "Val accuracy: 71.97142857142858\n",
      "Iter 67 -> sub iter 99 : 73.01587301587301\n",
      "Iteration: 68\n",
      "Train accuracy: 73.05396825396825\n",
      "Val accuracy: 72.27142857142857\n",
      "Iter 68 -> sub iter 99 : 75.39682539682539\n",
      "Iteration: 69\n",
      "Train accuracy: 74.32857142857144\n",
      "Val accuracy: 73.97142857142858\n",
      "Iter 69 -> sub iter 99 : 78.73015873015873\n",
      "Iteration: 70\n",
      "Train accuracy: 76.29206349206349\n",
      "Val accuracy: 75.61428571428571\n",
      "Iter 70 -> sub iter 99 : 79.36507936507937\n",
      "Iteration: 71\n",
      "Train accuracy: 77.46507936507938\n",
      "Val accuracy: 76.81428571428572\n",
      "Iter 71 -> sub iter 99 : 79.68253968253968\n",
      "Iteration: 72\n",
      "Train accuracy: 78.24126984126984\n",
      "Val accuracy: 77.54285714285714\n",
      "Iter 72 -> sub iter 99 : 80.31746031746032\n",
      "Iteration: 73\n",
      "Train accuracy: 78.8\n",
      "Val accuracy: 77.95714285714286\n",
      "Iter 73 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 74\n",
      "Train accuracy: 79.25555555555556\n",
      "Val accuracy: 78.38571428571429\n",
      "Iter 74 -> sub iter 99 : 80.63492063492063\n",
      "Iteration: 75\n",
      "Train accuracy: 79.56031746031746\n",
      "Val accuracy: 78.7\n",
      "Iter 75 -> sub iter 99 : 80.95238095238095\n",
      "Iteration: 76\n",
      "Train accuracy: 79.81428571428572\n",
      "Val accuracy: 78.95714285714286\n",
      "Iter 76 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 77\n",
      "Train accuracy: 79.98571428571428\n",
      "Val accuracy: 79.11428571428571\n",
      "Iter 77 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 78\n",
      "Train accuracy: 80.18571428571428\n",
      "Val accuracy: 79.34285714285714\n",
      "Iter 78 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 79\n",
      "Train accuracy: 80.33015873015873\n",
      "Val accuracy: 79.48571428571428\n",
      "Iter 79 -> sub iter 99 : 81.42857142857143\n",
      "Iteration: 80\n",
      "Train accuracy: 80.43492063492064\n",
      "Val accuracy: 79.60000000000001\n",
      "Iter 80 -> sub iter 99 : 81.26984126984127\n",
      "Iteration: 81\n",
      "Train accuracy: 80.56349206349206\n",
      "Val accuracy: 79.72857142857143\n",
      "Iter 81 -> sub iter 99 : 81.58730158730158\n",
      "Iteration: 82\n",
      "Train accuracy: 80.71428571428572\n",
      "Val accuracy: 79.82857142857142\n",
      "Iter 82 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 83\n",
      "Train accuracy: 80.80952380952381\n",
      "Val accuracy: 79.97142857142858\n",
      "Iter 83 -> sub iter 99 : 81.74603174603175\n",
      "Iteration: 84\n",
      "Train accuracy: 80.91904761904762\n",
      "Val accuracy: 80.07142857142857\n",
      "Iter 84 -> sub iter 99 : 81.90476190476199\n",
      "Iteration: 85\n",
      "Train accuracy: 81.0047619047619\n",
      "Val accuracy: 80.17142857142858\n",
      "Iter 85 -> sub iter 99 : 81.90476190476194\n",
      "Iteration: 86\n",
      "Train accuracy: 81.10952380952381\n",
      "Val accuracy: 80.30000000000001\n",
      "Iter 86 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 87\n",
      "Train accuracy: 81.17460317460318\n",
      "Val accuracy: 80.34285714285714\n",
      "Iter 87 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 88\n",
      "Train accuracy: 81.26031746031745\n",
      "Val accuracy: 80.42857142857143\n",
      "Iter 88 -> sub iter 99 : 82.38095238095238\n",
      "Iteration: 89\n",
      "Train accuracy: 81.33968253968254\n",
      "Val accuracy: 80.47142857142858\n",
      "Iter 89 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 90\n",
      "Train accuracy: 81.41428571428571\n",
      "Val accuracy: 80.51428571428572\n",
      "Iter 90 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 91\n",
      "Train accuracy: 81.48571428571428\n",
      "Val accuracy: 80.54285714285714\n",
      "Iter 91 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 92\n",
      "Train accuracy: 81.54444444444444\n",
      "Val accuracy: 80.55714285714286\n",
      "Iter 92 -> sub iter 99 : 82.53968253968253\n",
      "Iteration: 93\n",
      "Train accuracy: 81.6047619047619\n",
      "Val accuracy: 80.61428571428571\n",
      "Iter 93 -> sub iter 99 : 82.69841269841272\n",
      "Iteration: 94\n",
      "Train accuracy: 81.66984126984127\n",
      "Val accuracy: 80.62857142857143\n",
      "Iter 94 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 95\n",
      "Train accuracy: 81.72539682539683\n",
      "Val accuracy: 80.71428571428572\n",
      "Iter 95 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 96\n",
      "Train accuracy: 81.78730158730158\n",
      "Val accuracy: 80.77142857142857\n",
      "Iter 96 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 97\n",
      "Train accuracy: 81.83809523809525\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 97 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 98\n",
      "Train accuracy: 81.87619047619049\n",
      "Val accuracy: 80.87142857142857\n",
      "Iter 98 -> sub iter 99 : 82.85714285714286\n",
      "Iteration: 99\n",
      "Train accuracy: 81.93333333333334\n",
      "Val accuracy: 80.88571428571429\n",
      "Iter 99 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 100\n",
      "Train accuracy: 81.99047619047619\n",
      "Val accuracy: 80.91428571428571\n",
      "Iter 100 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 101\n",
      "Train accuracy: 82.02063492063492\n",
      "Val accuracy: 80.98571428571428\n",
      "Iter 101 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 102\n",
      "Train accuracy: 82.05873015873016\n",
      "Val accuracy: 81.0\n",
      "Iter 102 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 103\n",
      "Train accuracy: 82.11746031746033\n",
      "Val accuracy: 81.02857142857142\n",
      "Iter 103 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 104\n",
      "Train accuracy: 82.15714285714286\n",
      "Val accuracy: 81.05714285714286\n",
      "Iter 104 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 105\n",
      "Train accuracy: 82.1968253968254\n",
      "Val accuracy: 81.11428571428571\n",
      "Iter 105 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 106\n",
      "Train accuracy: 82.24285714285713\n",
      "Val accuracy: 81.15714285714286\n",
      "Iter 106 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 107\n",
      "Train accuracy: 82.28412698412698\n",
      "Val accuracy: 81.18571428571428\n",
      "Iter 107 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 108\n",
      "Train accuracy: 82.32857142857142\n",
      "Val accuracy: 81.17142857142858\n",
      "Iter 108 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 109\n",
      "Train accuracy: 82.37619047619049\n",
      "Val accuracy: 81.22857142857143\n",
      "Iter 109 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 110\n",
      "Train accuracy: 82.41746031746032\n",
      "Val accuracy: 81.25714285714287\n",
      "Iter 110 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 111\n",
      "Train accuracy: 82.46190476190476\n",
      "Val accuracy: 81.28571428571428\n",
      "Iter 111 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 112\n",
      "Train accuracy: 82.4952380952381\n",
      "Val accuracy: 81.27142857142857\n",
      "Iter 112 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 113\n",
      "Train accuracy: 82.53015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 113 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 114\n",
      "Train accuracy: 82.55396825396826\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 114 -> sub iter 99 : 83.01587301587303\n",
      "Iteration: 115\n",
      "Train accuracy: 82.6015873015873\n",
      "Val accuracy: 81.35714285714286\n",
      "Iter 115 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 116\n",
      "Train accuracy: 82.63174603174603\n",
      "Val accuracy: 81.39999999999999\n",
      "Iter 116 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 117\n",
      "Train accuracy: 82.67142857142858\n",
      "Val accuracy: 81.42857142857143\n",
      "Iter 117 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 118\n",
      "Train accuracy: 82.7031746031746\n",
      "Val accuracy: 81.47142857142858\n",
      "Iter 118 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 119\n",
      "Train accuracy: 82.72222222222221\n",
      "Val accuracy: 81.48571428571428\n",
      "Iter 119 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 120\n",
      "Train accuracy: 82.74603174603175\n",
      "Val accuracy: 81.52857142857142\n",
      "Iter 120 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 121\n",
      "Train accuracy: 82.77460317460317\n",
      "Val accuracy: 81.57142857142857\n",
      "Iter 121 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 122\n",
      "Train accuracy: 82.79841269841269\n",
      "Val accuracy: 81.58571428571429\n",
      "Iter 122 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 123\n",
      "Train accuracy: 82.82380952380952\n",
      "Val accuracy: 81.6\n",
      "Iter 123 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 124\n",
      "Train accuracy: 82.86825396825397\n",
      "Val accuracy: 81.61428571428571\n",
      "Iter 124 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 125\n",
      "Train accuracy: 82.88412698412698\n",
      "Val accuracy: 81.62857142857143\n",
      "Iter 125 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 126\n",
      "Train accuracy: 82.91111111111111\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 126 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 127\n",
      "Train accuracy: 82.93174603174603\n",
      "Val accuracy: 81.64285714285714\n",
      "Iter 127 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 128\n",
      "Train accuracy: 82.94920634920635\n",
      "Val accuracy: 81.65714285714286\n",
      "Iter 128 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 129\n",
      "Train accuracy: 82.98253968253968\n",
      "Val accuracy: 81.68571428571428\n",
      "Iter 129 -> sub iter 99 : 83.17460317460318\n",
      "Iteration: 130\n",
      "Train accuracy: 83.01746031746032\n",
      "Val accuracy: 81.67142857142858\n",
      "Iter 130 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 131\n",
      "Train accuracy: 83.03968253968253\n",
      "Val accuracy: 81.71428571428572\n",
      "Iter 131 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 132\n",
      "Train accuracy: 83.06825396825397\n",
      "Val accuracy: 81.72857142857143\n",
      "Iter 132 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 133\n",
      "Train accuracy: 83.0952380952381\n",
      "Val accuracy: 81.75714285714287\n",
      "Iter 133 -> sub iter 99 : 83.33333333333334\n",
      "Iteration: 134\n",
      "Train accuracy: 83.12698412698413\n",
      "Val accuracy: 81.77142857142857\n",
      "Iter 134 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 135\n",
      "Train accuracy: 83.15714285714286\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 135 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 136\n",
      "Train accuracy: 83.18412698412698\n",
      "Val accuracy: 81.78571428571428\n",
      "Iter 136 -> sub iter 99 : 83.49206349206359\n",
      "Iteration: 137\n",
      "Train accuracy: 83.1984126984127\n",
      "Val accuracy: 81.82857142857142\n",
      "Iter 137 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 138\n",
      "Train accuracy: 83.21904761904761\n",
      "Val accuracy: 81.88571428571429\n",
      "Iter 138 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 139\n",
      "Train accuracy: 83.23968253968253\n",
      "Val accuracy: 81.92857142857143\n",
      "Iter 139 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 140\n",
      "Train accuracy: 83.27460317460319\n",
      "Val accuracy: 81.97142857142858\n",
      "Iter 140 -> sub iter 99 : 83.49206349206356\n",
      "Iteration: 141\n",
      "Train accuracy: 83.3015873015873\n",
      "Val accuracy: 82.04285714285714\n",
      "Iter 141 -> sub iter 99 : 83.49206349206351\n",
      "Iteration: 142\n",
      "Train accuracy: 83.32698412698413\n",
      "Val accuracy: 82.07142857142857\n",
      "Iter 142 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 143\n",
      "Train accuracy: 83.34444444444445\n",
      "Val accuracy: 82.12857142857143\n",
      "Iter 143 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 144\n",
      "Train accuracy: 83.36031746031746\n",
      "Val accuracy: 82.17142857142858\n",
      "Iter 144 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 145\n",
      "Train accuracy: 83.38888888888889\n",
      "Val accuracy: 82.18571428571428\n",
      "Iter 145 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 146\n",
      "Train accuracy: 83.40793650793651\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 146 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 147\n",
      "Train accuracy: 83.43015873015874\n",
      "Val accuracy: 82.19999999999999\n",
      "Iter 147 -> sub iter 99 : 83.49206349206357\n",
      "Iteration: 148\n",
      "Train accuracy: 83.45555555555556\n",
      "Val accuracy: 82.22857142857143\n",
      "Iter 148 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 149\n",
      "Train accuracy: 83.47777777777777\n",
      "Val accuracy: 82.25714285714287\n",
      "Iter 149 -> sub iter 99 : 83.49206349206352\n",
      "Iteration: 150\n",
      "Train accuracy: 83.4920634920635\n",
      "Val accuracy: 82.27142857142857\n",
      "Training for 0.0001\n",
      "Params Initialised\n",
      "Iter 0 -> sub iter 99 : 21.904761904761905\n",
      "Iteration: 1\n",
      "Train accuracy: 22.957142857142856\n",
      "Val accuracy: 24.22857142857143\n",
      "Iter 1 -> sub iter 99 : 27.777777777777785\n",
      "Iteration: 2\n",
      "Train accuracy: 29.887301587301586\n",
      "Val accuracy: 31.157142857142855\n",
      "Iter 2 -> sub iter 99 : 31.904761904761975\n",
      "Iteration: 3\n",
      "Train accuracy: 34.33968253968254\n",
      "Val accuracy: 35.65714285714286\n",
      "Iter 3 -> sub iter 99 : 38.571428571428584\n",
      "Iteration: 4\n",
      "Train accuracy: 39.353968253968254\n",
      "Val accuracy: 40.87142857142857\n",
      "Iter 4 -> sub iter 99 : 41.111111111111116\n",
      "Iteration: 5\n",
      "Train accuracy: 42.35714285714286\n",
      "Val accuracy: 43.9\n",
      "Iter 5 -> sub iter 99 : 42.539682539682546\n",
      "Iteration: 6\n",
      "Train accuracy: 44.34603174603174\n",
      "Val accuracy: 46.1\n",
      "Iter 6 -> sub iter 99 : 44.603174603174614\n",
      "Iteration: 7\n",
      "Train accuracy: 45.76190476190476\n",
      "Val accuracy: 47.599999999999994\n",
      "Iter 7 -> sub iter 99 : 46.031746031746034\n",
      "Iteration: 8\n",
      "Train accuracy: 46.68571428571428\n",
      "Val accuracy: 48.22857142857143\n",
      "Iter 8 -> sub iter 99 : 46.825396825396824\n",
      "Iteration: 9\n",
      "Train accuracy: 47.41746031746032\n",
      "Val accuracy: 48.871428571428574\n",
      "Iter 9 -> sub iter 99 : 48.095238095238095\n",
      "Iteration: 10\n",
      "Train accuracy: 48.03809523809524\n",
      "Val accuracy: 49.542857142857144\n",
      "Iter 10 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 11\n",
      "Train accuracy: 48.53174603174603\n",
      "Val accuracy: 49.81428571428572\n",
      "Iter 11 -> sub iter 99 : 48.253968253968254\n",
      "Iteration: 12\n",
      "Train accuracy: 48.94761904761904\n",
      "Val accuracy: 50.24285714285715\n",
      "Iter 12 -> sub iter 99 : 48.571428571428575\n",
      "Iteration: 13\n",
      "Train accuracy: 49.32857142857143\n",
      "Val accuracy: 50.5\n",
      "Iter 13 -> sub iter 99 : 49.206349206349296\n",
      "Iteration: 14\n",
      "Train accuracy: 49.71111111111111\n",
      "Val accuracy: 50.91428571428571\n",
      "Iter 14 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 15\n",
      "Train accuracy: 50.00476190476191\n",
      "Val accuracy: 51.28571428571429\n",
      "Iter 15 -> sub iter 99 : 49.841269841269844\n",
      "Iteration: 16\n",
      "Train accuracy: 50.3111111111111\n",
      "Val accuracy: 51.4\n",
      "Iter 16 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 17\n",
      "Train accuracy: 50.56349206349206\n",
      "Val accuracy: 51.72857142857142\n",
      "Iter 17 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 18\n",
      "Train accuracy: 50.7984126984127\n",
      "Val accuracy: 52.028571428571425\n",
      "Iter 18 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 19\n",
      "Train accuracy: 51.01269841269841\n",
      "Val accuracy: 52.214285714285715\n",
      "Iter 19 -> sub iter 99 : 50.158730158730165\n",
      "Iteration: 20\n",
      "Train accuracy: 51.217460317460315\n",
      "Val accuracy: 52.51428571428571\n",
      "Iter 20 -> sub iter 99 : 50.158730158730164\n",
      "Iteration: 21\n",
      "Train accuracy: 51.42539682539683\n",
      "Val accuracy: 52.65714285714286\n",
      "Iter 21 -> sub iter 99 : 50.317460317460316\n",
      "Iteration: 22\n",
      "Train accuracy: 51.65238095238095\n",
      "Val accuracy: 52.87142857142857\n",
      "Iter 22 -> sub iter 99 : 50.476190476190474\n",
      "Iteration: 23\n",
      "Train accuracy: 51.866666666666674\n",
      "Val accuracy: 53.128571428571426\n",
      "Iter 23 -> sub iter 99 : 50.952380952380956\n",
      "Iteration: 24\n",
      "Train accuracy: 52.05714285714286\n",
      "Val accuracy: 53.38571428571428\n",
      "Iter 24 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 25\n",
      "Train accuracy: 52.23174603174603\n",
      "Val accuracy: 53.57142857142857\n",
      "Iter 25 -> sub iter 99 : 51.428571428571426\n",
      "Iteration: 26\n",
      "Train accuracy: 52.38095238095239\n",
      "Val accuracy: 53.7\n",
      "Iter 26 -> sub iter 99 : 51.587301587301596\n",
      "Iteration: 27\n",
      "Train accuracy: 52.549206349206344\n",
      "Val accuracy: 53.800000000000004\n",
      "Iter 27 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 28\n",
      "Train accuracy: 52.7079365079365\n",
      "Val accuracy: 54.0\n",
      "Iter 28 -> sub iter 99 : 51.746031746031754\n",
      "Iteration: 29\n",
      "Train accuracy: 52.84920634920635\n",
      "Val accuracy: 54.142857142857146\n",
      "Iter 29 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 30\n",
      "Train accuracy: 52.957142857142856\n",
      "Val accuracy: 54.27142857142857\n",
      "Iter 30 -> sub iter 99 : 51.904761904761914\n",
      "Iteration: 31\n",
      "Train accuracy: 53.07777777777778\n",
      "Val accuracy: 54.371428571428574\n",
      "Iter 31 -> sub iter 99 : 52.222222222222234\n",
      "Iteration: 32\n",
      "Train accuracy: 53.20952380952381\n",
      "Val accuracy: 54.51428571428571\n",
      "Iter 32 -> sub iter 99 : 52.222222222222235\n",
      "Iteration: 33\n",
      "Train accuracy: 53.31428571428572\n",
      "Val accuracy: 54.65714285714286\n",
      "Iter 33 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 34\n",
      "Train accuracy: 53.42857142857142\n",
      "Val accuracy: 54.75714285714286\n",
      "Iter 34 -> sub iter 99 : 52.539682539682545\n",
      "Iteration: 35\n",
      "Train accuracy: 53.51904761904762\n",
      "Val accuracy: 54.95714285714286\n",
      "Iter 35 -> sub iter 99 : 52.698412698412785\n",
      "Iteration: 36\n",
      "Train accuracy: 53.642857142857146\n",
      "Val accuracy: 55.01428571428571\n",
      "Iter 36 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 37\n",
      "Train accuracy: 53.73015873015873\n",
      "Val accuracy: 55.08571428571428\n",
      "Iter 37 -> sub iter 99 : 52.698412698412746\n",
      "Iteration: 38\n",
      "Train accuracy: 53.78571428571428\n",
      "Val accuracy: 55.22857142857143\n",
      "Iter 38 -> sub iter 99 : 52.857142857142865\n",
      "Iteration: 39\n",
      "Train accuracy: 53.86825396825397\n",
      "Val accuracy: 55.371428571428574\n",
      "Iter 39 -> sub iter 99 : 52.857142857142866\n",
      "Iteration: 40\n",
      "Train accuracy: 53.955555555555556\n",
      "Val accuracy: 55.385714285714286\n",
      "Iter 40 -> sub iter 99 : 53.174603174603184\n",
      "Iteration: 41\n",
      "Train accuracy: 54.03174603174603\n",
      "Val accuracy: 55.471428571428575\n",
      "Iter 41 -> sub iter 99 : 53.174603174603186\n",
      "Iteration: 42\n",
      "Train accuracy: 54.1\n",
      "Val accuracy: 55.58571428571428\n",
      "Iter 42 -> sub iter 99 : 53.492063492063494\n",
      "Iteration: 43\n",
      "Train accuracy: 54.179365079365084\n",
      "Val accuracy: 55.68571428571428\n",
      "Iter 43 -> sub iter 99 : 53.650793650793654\n",
      "Iteration: 44\n",
      "Train accuracy: 54.250793650793646\n",
      "Val accuracy: 55.74285714285714\n",
      "Iter 44 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 45\n",
      "Train accuracy: 54.31587301587302\n",
      "Val accuracy: 55.84285714285714\n",
      "Iter 45 -> sub iter 99 : 53.809523809523814\n",
      "Iteration: 46\n",
      "Train accuracy: 54.37777777777778\n",
      "Val accuracy: 55.871428571428574\n",
      "Iter 46 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 47\n",
      "Train accuracy: 54.44126984126984\n",
      "Val accuracy: 55.91428571428572\n",
      "Iter 47 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 48\n",
      "Train accuracy: 54.50000000000001\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 48 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 49\n",
      "Train accuracy: 54.541269841269845\n",
      "Val accuracy: 56.04285714285714\n",
      "Iter 49 -> sub iter 99 : 53.968253968253975\n",
      "Iteration: 50\n",
      "Train accuracy: 54.6063492063492\n",
      "Val accuracy: 56.128571428571426\n",
      "Iter 50 -> sub iter 99 : 53.968253968253976\n",
      "Iteration: 51\n",
      "Train accuracy: 54.646031746031746\n",
      "Val accuracy: 56.15714285714286\n",
      "Iter 51 -> sub iter 99 : 54.126984126984136\n",
      "Iteration: 52\n",
      "Train accuracy: 54.68730158730158\n",
      "Val accuracy: 56.214285714285715\n",
      "Iter 52 -> sub iter 99 : 54.285714285714285\n",
      "Iteration: 53\n",
      "Train accuracy: 54.72380952380952\n",
      "Val accuracy: 56.27142857142857\n",
      "Iter 53 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 54\n",
      "Train accuracy: 54.75714285714286\n",
      "Val accuracy: 56.34285714285714\n",
      "Iter 54 -> sub iter 99 : 54.444444444444446\n",
      "Iteration: 55\n",
      "Train accuracy: 54.801587301587304\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 55 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 56\n",
      "Train accuracy: 54.82857142857143\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 56 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 57\n",
      "Train accuracy: 54.87301587301587\n",
      "Val accuracy: 56.385714285714286\n",
      "Iter 57 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 58\n",
      "Train accuracy: 54.919047619047625\n",
      "Val accuracy: 56.41428571428572\n",
      "Iter 58 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 59\n",
      "Train accuracy: 54.96507936507936\n",
      "Val accuracy: 56.442857142857136\n",
      "Iter 59 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 60\n",
      "Train accuracy: 55.00158730158731\n",
      "Val accuracy: 56.49999999999999\n",
      "Iter 60 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 61\n",
      "Train accuracy: 55.05238095238095\n",
      "Val accuracy: 56.51428571428572\n",
      "Iter 61 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 62\n",
      "Train accuracy: 55.093650793650795\n",
      "Val accuracy: 56.528571428571425\n",
      "Iter 62 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 63\n",
      "Train accuracy: 55.141269841269846\n",
      "Val accuracy: 56.557142857142864\n",
      "Iter 63 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 64\n",
      "Train accuracy: 55.18253968253968\n",
      "Val accuracy: 56.58571428571428\n",
      "Iter 64 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 65\n",
      "Train accuracy: 55.21904761904762\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 65 -> sub iter 99 : 54.761904761904766\n",
      "Iteration: 66\n",
      "Train accuracy: 55.24444444444444\n",
      "Val accuracy: 56.614285714285714\n",
      "Iter 66 -> sub iter 99 : 54.920634920634924\n",
      "Iteration: 67\n",
      "Train accuracy: 55.269841269841265\n",
      "Val accuracy: 56.67142857142857\n",
      "Iter 67 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 68\n",
      "Train accuracy: 55.304761904761904\n",
      "Val accuracy: 56.714285714285715\n",
      "Iter 68 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 69\n",
      "Train accuracy: 55.33968253968254\n",
      "Val accuracy: 56.74285714285714\n",
      "Iter 69 -> sub iter 99 : 55.079365079365084\n",
      "Iteration: 70\n",
      "Train accuracy: 55.37460317460317\n",
      "Val accuracy: 56.81428571428572\n",
      "Iter 70 -> sub iter 99 : 55.238095238095244\n",
      "Iteration: 71\n",
      "Train accuracy: 55.4031746031746\n",
      "Val accuracy: 56.84285714285714\n",
      "Iter 71 -> sub iter 99 : 55.396825396825435\n",
      "Iteration: 72\n",
      "Train accuracy: 55.43492063492064\n",
      "Val accuracy: 56.871428571428574\n",
      "Iter 72 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 73\n",
      "Train accuracy: 55.474603174603175\n",
      "Val accuracy: 56.89999999999999\n",
      "Iter 73 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 74\n",
      "Train accuracy: 55.4952380952381\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 74 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 75\n",
      "Train accuracy: 55.52222222222222\n",
      "Val accuracy: 56.92857142857143\n",
      "Iter 75 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 76\n",
      "Train accuracy: 55.55238095238095\n",
      "Val accuracy: 56.91428571428572\n",
      "Iter 76 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 77\n",
      "Train accuracy: 55.574603174603176\n",
      "Val accuracy: 56.98571428571428\n",
      "Iter 77 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 78\n",
      "Train accuracy: 55.6063492063492\n",
      "Val accuracy: 56.99999999999999\n",
      "Iter 78 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 79\n",
      "Train accuracy: 55.62222222222222\n",
      "Val accuracy: 57.028571428571425\n",
      "Iter 79 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 80\n",
      "Train accuracy: 55.63809523809524\n",
      "Val accuracy: 57.04285714285714\n",
      "Iter 80 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 81\n",
      "Train accuracy: 55.65714285714286\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 81 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 82\n",
      "Train accuracy: 55.68730158730158\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 82 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 83\n",
      "Train accuracy: 55.70952380952381\n",
      "Val accuracy: 57.08571428571428\n",
      "Iter 83 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 84\n",
      "Train accuracy: 55.72380952380952\n",
      "Val accuracy: 57.15714285714286\n",
      "Iter 84 -> sub iter 99 : 55.396825396825475\n",
      "Iteration: 85\n",
      "Train accuracy: 55.73968253968255\n",
      "Val accuracy: 57.17142857142857\n",
      "Iter 85 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 86\n",
      "Train accuracy: 55.76190476190476\n",
      "Val accuracy: 57.18571428571428\n",
      "Iter 86 -> sub iter 99 : 55.396825396825476\n",
      "Iteration: 87\n",
      "Train accuracy: 55.78412698412698\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 87 -> sub iter 99 : 55.396825396825474\n",
      "Iteration: 88\n",
      "Train accuracy: 55.7952380952381\n",
      "Val accuracy: 57.214285714285715\n",
      "Iter 88 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 89\n",
      "Train accuracy: 55.80952380952381\n",
      "Val accuracy: 57.24285714285714\n",
      "Iter 89 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 90\n",
      "Train accuracy: 55.822222222222216\n",
      "Val accuracy: 57.25714285714286\n",
      "Iter 90 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 91\n",
      "Train accuracy: 55.83968253968254\n",
      "Val accuracy: 57.27142857142857\n",
      "Iter 91 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 92\n",
      "Train accuracy: 55.85555555555556\n",
      "Val accuracy: 57.285714285714285\n",
      "Iter 92 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 93\n",
      "Train accuracy: 55.87777777777778\n",
      "Val accuracy: 57.3\n",
      "Iter 93 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 94\n",
      "Train accuracy: 55.888888888888886\n",
      "Val accuracy: 57.3\n",
      "Iter 94 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 95\n",
      "Train accuracy: 55.907936507936505\n",
      "Val accuracy: 57.32857142857143\n",
      "Iter 95 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 96\n",
      "Train accuracy: 55.919047619047625\n",
      "Val accuracy: 57.371428571428574\n",
      "Iter 96 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 97\n",
      "Train accuracy: 55.93015873015873\n",
      "Val accuracy: 57.385714285714286\n",
      "Iter 97 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 98\n",
      "Train accuracy: 55.941269841269836\n",
      "Val accuracy: 57.44285714285714\n",
      "Iter 98 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 99\n",
      "Train accuracy: 55.94920634920635\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 99 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 100\n",
      "Train accuracy: 55.96666666666666\n",
      "Val accuracy: 57.45714285714286\n",
      "Iter 100 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 101\n",
      "Train accuracy: 55.98888888888889\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 101 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 102\n",
      "Train accuracy: 56.007936507936506\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 102 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 103\n",
      "Train accuracy: 56.019047619047626\n",
      "Val accuracy: 57.48571428571429\n",
      "Iter 103 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 104\n",
      "Train accuracy: 56.03174603174603\n",
      "Val accuracy: 57.49999999999999\n",
      "Iter 104 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 105\n",
      "Train accuracy: 56.046031746031744\n",
      "Val accuracy: 57.51428571428572\n",
      "Iter 105 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 106\n",
      "Train accuracy: 56.06666666666666\n",
      "Val accuracy: 57.54285714285714\n",
      "Iter 106 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 107\n",
      "Train accuracy: 56.092063492063495\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 107 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 108\n",
      "Train accuracy: 56.1031746031746\n",
      "Val accuracy: 57.557142857142864\n",
      "Iter 108 -> sub iter 99 : 55.555555555555565\n",
      "Iteration: 109\n",
      "Train accuracy: 56.12380952380952\n",
      "Val accuracy: 57.57142857142858\n",
      "Iter 109 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 110\n",
      "Train accuracy: 56.14285714285714\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 110 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 111\n",
      "Train accuracy: 56.15555555555556\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 111 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 112\n",
      "Train accuracy: 56.18253968253968\n",
      "Val accuracy: 57.58571428571428\n",
      "Iter 112 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 113\n",
      "Train accuracy: 56.19047619047619\n",
      "Val accuracy: 57.61428571428572\n",
      "Iter 113 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 114\n",
      "Train accuracy: 56.2047619047619\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 114 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 115\n",
      "Train accuracy: 56.21587301587302\n",
      "Val accuracy: 57.64285714285714\n",
      "Iter 115 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 116\n",
      "Train accuracy: 56.23650793650794\n",
      "Val accuracy: 57.657142857142865\n",
      "Iter 116 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 117\n",
      "Train accuracy: 56.25238095238095\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 117 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 118\n",
      "Train accuracy: 56.268253968253966\n",
      "Val accuracy: 57.68571428571428\n",
      "Iter 118 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 119\n",
      "Train accuracy: 56.29206349206349\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 119 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 120\n",
      "Train accuracy: 56.3031746031746\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 120 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 121\n",
      "Train accuracy: 56.31587301587302\n",
      "Val accuracy: 57.67142857142857\n",
      "Iter 121 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 122\n",
      "Train accuracy: 56.33015873015873\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 122 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 123\n",
      "Train accuracy: 56.34603174603174\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 123 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 124\n",
      "Train accuracy: 56.353968253968254\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 124 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 125\n",
      "Train accuracy: 56.35714285714286\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 125 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 126\n",
      "Train accuracy: 56.36190476190476\n",
      "Val accuracy: 57.699999999999996\n",
      "Iter 126 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 127\n",
      "Train accuracy: 56.371428571428574\n",
      "Val accuracy: 57.714285714285715\n",
      "Iter 127 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 128\n",
      "Train accuracy: 56.38253968253968\n",
      "Val accuracy: 57.74285714285714\n",
      "Iter 128 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 129\n",
      "Train accuracy: 56.3968253968254\n",
      "Val accuracy: 57.77142857142857\n",
      "Iter 129 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 130\n",
      "Train accuracy: 56.4015873015873\n",
      "Val accuracy: 57.785714285714285\n",
      "Iter 130 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 131\n",
      "Train accuracy: 56.41111111111111\n",
      "Val accuracy: 57.8\n",
      "Iter 131 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 132\n",
      "Train accuracy: 56.423809523809524\n",
      "Val accuracy: 57.8\n",
      "Iter 132 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 133\n",
      "Train accuracy: 56.426984126984124\n",
      "Val accuracy: 57.8\n",
      "Iter 133 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 134\n",
      "Train accuracy: 56.442857142857136\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 134 -> sub iter 99 : 55.555555555555566\n",
      "Iteration: 135\n",
      "Train accuracy: 56.44603174603174\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 135 -> sub iter 99 : 55.396825396825436\n",
      "Iteration: 136\n",
      "Train accuracy: 56.45873015873016\n",
      "Val accuracy: 57.81428571428572\n",
      "Iter 136 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 137\n",
      "Train accuracy: 56.461904761904755\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 137 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 138\n",
      "Train accuracy: 56.46984126984127\n",
      "Val accuracy: 57.82857142857143\n",
      "Iter 138 -> sub iter 99 : 55.396825396825434\n",
      "Iteration: 139\n",
      "Train accuracy: 56.48253968253968\n",
      "Val accuracy: 57.84285714285714\n",
      "Iter 139 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 140\n",
      "Train accuracy: 56.48412698412698\n",
      "Val accuracy: 57.871428571428574\n",
      "Iter 140 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 141\n",
      "Train accuracy: 56.48888888888889\n",
      "Val accuracy: 57.885714285714286\n",
      "Iter 141 -> sub iter 99 : 55.555555555555564\n",
      "Iteration: 142\n",
      "Train accuracy: 56.5031746031746\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 142 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 143\n",
      "Train accuracy: 56.50952380952381\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 143 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 144\n",
      "Train accuracy: 56.51587301587302\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 144 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 145\n",
      "Train accuracy: 56.52063492063492\n",
      "Val accuracy: 57.91428571428572\n",
      "Iter 145 -> sub iter 99 : 55.714285714285715\n",
      "Iteration: 146\n",
      "Train accuracy: 56.526984126984125\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 146 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 147\n",
      "Train accuracy: 56.528571428571425\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 147 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 148\n",
      "Train accuracy: 56.53650793650794\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 148 -> sub iter 99 : 55.873015873015874\n",
      "Iteration: 149\n",
      "Train accuracy: 56.54920634920635\n",
      "Val accuracy: 57.92857142857143\n",
      "Iter 149 -> sub iter 99 : 55.873015873015875\n",
      "Iteration: 150\n",
      "Train accuracy: 56.56031746031746\n",
      "Val accuracy: 57.92857142857143\n"
     ]
    }
   ],
   "source": [
    "for pert in pertList:\n",
    "    print(f\"Training for {pert}\")\n",
    "    W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.01, pert, 1)\n",
    "    trainAccPertList.append(train_acc)\n",
    "    valAccPertList.append(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 81.73650793650793\n",
      "Val accuracy: 81.02857142857142\n",
      "Iteration: 2\n",
      "Train accuracy: 86.43174603174603\n",
      "Val accuracy: 85.91428571428571\n",
      "Iteration: 3\n",
      "Train accuracy: 88.5984126984127\n",
      "Val accuracy: 88.18571428571428\n",
      "Iteration: 4\n",
      "Train accuracy: 89.78571428571429\n",
      "Val accuracy: 89.44285714285715\n",
      "Iteration: 5\n",
      "Train accuracy: 90.70158730158731\n",
      "Val accuracy: 90.21428571428571\n",
      "Iteration: 6\n",
      "Train accuracy: 91.39206349206349\n",
      "Val accuracy: 90.72857142857143\n",
      "Iteration: 7\n",
      "Train accuracy: 91.88730158730158\n",
      "Val accuracy: 91.15714285714286\n",
      "Iteration: 8\n",
      "Train accuracy: 92.33650793650794\n",
      "Val accuracy: 91.45714285714286\n",
      "Iteration: 9\n",
      "Train accuracy: 92.70793650793651\n",
      "Val accuracy: 91.75714285714285\n",
      "Iteration: 10\n",
      "Train accuracy: 93.04126984126984\n",
      "Val accuracy: 92.08571428571429\n",
      "Iteration: 11\n",
      "Train accuracy: 93.31746031746032\n",
      "Val accuracy: 92.41428571428571\n",
      "Iteration: 12\n",
      "Train accuracy: 93.57460317460318\n",
      "Val accuracy: 92.55714285714286\n",
      "Iteration: 13\n",
      "Train accuracy: 93.7984126984127\n",
      "Val accuracy: 92.84285714285714\n",
      "Iteration: 14\n",
      "Train accuracy: 94.04285714285714\n",
      "Val accuracy: 93.02857142857142\n",
      "Iteration: 15\n",
      "Train accuracy: 94.20952380952382\n",
      "Val accuracy: 93.10000000000001\n",
      "Iteration: 16\n",
      "Train accuracy: 94.3984126984127\n",
      "Val accuracy: 93.30000000000001\n",
      "Iteration: 17\n",
      "Train accuracy: 94.57619047619048\n",
      "Val accuracy: 93.5\n",
      "Iteration: 18\n",
      "Train accuracy: 94.70952380952382\n",
      "Val accuracy: 93.61428571428571\n",
      "Iteration: 19\n",
      "Train accuracy: 94.86507936507937\n",
      "Val accuracy: 93.71428571428572\n",
      "Iteration: 20\n",
      "Train accuracy: 95.01428571428572\n",
      "Val accuracy: 93.75714285714287\n",
      "Iteration: 21\n",
      "Train accuracy: 95.12063492063491\n",
      "Val accuracy: 93.92857142857143\n",
      "Iteration: 22\n",
      "Train accuracy: 95.23809523809523\n",
      "Val accuracy: 94.04285714285714\n",
      "Iteration: 23\n",
      "Train accuracy: 95.36190476190475\n",
      "Val accuracy: 94.18571428571428\n",
      "Iteration: 24\n",
      "Train accuracy: 95.46825396825398\n",
      "Val accuracy: 94.25714285714287\n",
      "Iteration: 25\n",
      "Train accuracy: 95.5936507936508\n",
      "Val accuracy: 94.32857142857142\n",
      "Iteration: 26\n",
      "Train accuracy: 95.72222222222221\n",
      "Val accuracy: 94.38571428571429\n",
      "Iteration: 27\n",
      "Train accuracy: 95.83015873015873\n",
      "Val accuracy: 94.45714285714286\n",
      "Iteration: 28\n",
      "Train accuracy: 95.93015873015874\n",
      "Val accuracy: 94.58571428571429\n",
      "Iteration: 29\n",
      "Train accuracy: 96.0047619047619\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 30\n",
      "Train accuracy: 96.08253968253968\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 31\n",
      "Train accuracy: 96.16031746031746\n",
      "Val accuracy: 94.65714285714286\n",
      "Iteration: 32\n",
      "Train accuracy: 96.23968253968253\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 33\n",
      "Train accuracy: 96.3031746031746\n",
      "Val accuracy: 94.71428571428572\n",
      "Iteration: 34\n",
      "Train accuracy: 96.36984126984127\n",
      "Val accuracy: 94.77142857142857\n",
      "Iteration: 35\n",
      "Train accuracy: 96.42380952380952\n",
      "Val accuracy: 94.8\n",
      "Iteration: 36\n",
      "Train accuracy: 96.48412698412699\n",
      "Val accuracy: 94.87142857142857\n",
      "Iteration: 37\n",
      "Train accuracy: 96.58412698412698\n",
      "Val accuracy: 94.85714285714286\n",
      "Iteration: 38\n",
      "Train accuracy: 96.65396825396826\n",
      "Val accuracy: 94.95714285714286\n",
      "Iteration: 39\n",
      "Train accuracy: 96.72222222222221\n",
      "Val accuracy: 94.94285714285714\n",
      "Iteration: 40\n",
      "Train accuracy: 96.75555555555555\n",
      "Val accuracy: 95.01428571428572\n",
      "Iteration: 41\n",
      "Train accuracy: 96.81904761904761\n",
      "Val accuracy: 95.07142857142857\n",
      "Iteration: 42\n",
      "Train accuracy: 96.88412698412698\n",
      "Val accuracy: 95.1\n",
      "Iteration: 43\n",
      "Train accuracy: 96.93650793650794\n",
      "Val accuracy: 95.11428571428571\n",
      "Iteration: 44\n",
      "Train accuracy: 97.0079365079365\n",
      "Val accuracy: 95.17142857142858\n",
      "Iteration: 45\n",
      "Train accuracy: 97.06190476190476\n",
      "Val accuracy: 95.15714285714286\n",
      "Iteration: 46\n",
      "Train accuracy: 97.12380952380953\n",
      "Val accuracy: 95.21428571428572\n",
      "Iteration: 47\n",
      "Train accuracy: 97.17619047619047\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 48\n",
      "Train accuracy: 97.22380952380952\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 49\n",
      "Train accuracy: 97.27460317460317\n",
      "Val accuracy: 95.24285714285713\n",
      "Iteration: 50\n",
      "Train accuracy: 97.31269841269842\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 51\n",
      "Train accuracy: 97.35079365079365\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 52\n",
      "Train accuracy: 97.38095238095238\n",
      "Val accuracy: 95.3\n",
      "Iteration: 53\n",
      "Train accuracy: 97.42698412698412\n",
      "Val accuracy: 95.32857142857142\n",
      "Iteration: 54\n",
      "Train accuracy: 97.47142857142858\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 55\n",
      "Train accuracy: 97.5047619047619\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 56\n",
      "Train accuracy: 97.55079365079365\n",
      "Val accuracy: 95.31428571428572\n",
      "Iteration: 57\n",
      "Train accuracy: 97.6\n",
      "Val accuracy: 95.37142857142857\n",
      "Iteration: 58\n",
      "Train accuracy: 97.64761904761905\n",
      "Val accuracy: 95.38571428571429\n",
      "Iteration: 59\n",
      "Train accuracy: 97.68730158730159\n",
      "Val accuracy: 95.41428571428571\n",
      "Iteration: 60\n",
      "Train accuracy: 97.73174603174604\n",
      "Val accuracy: 95.42857142857143\n",
      "Iteration: 61\n",
      "Train accuracy: 97.76984126984128\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 62\n",
      "Train accuracy: 97.78730158730158\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 63\n",
      "Train accuracy: 97.82222222222222\n",
      "Val accuracy: 95.5\n",
      "Iteration: 64\n",
      "Train accuracy: 97.85238095238095\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 65\n",
      "Train accuracy: 97.87936507936507\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 66\n",
      "Train accuracy: 97.93015873015874\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 67\n",
      "Train accuracy: 97.96349206349207\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 68\n",
      "Train accuracy: 98.00634920634921\n",
      "Val accuracy: 95.48571428571428\n",
      "Iteration: 69\n",
      "Train accuracy: 98.03809523809524\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 70\n",
      "Train accuracy: 98.08571428571429\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 71\n",
      "Train accuracy: 98.12539682539682\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 72\n",
      "Train accuracy: 98.17142857142858\n",
      "Val accuracy: 95.67142857142858\n",
      "Iteration: 73\n",
      "Train accuracy: 98.1920634920635\n",
      "Val accuracy: 95.71428571428572\n",
      "Iteration: 74\n",
      "Train accuracy: 98.21587301587302\n",
      "Val accuracy: 95.7\n",
      "Iteration: 75\n",
      "Train accuracy: 98.25079365079365\n",
      "Val accuracy: 95.7\n",
      "Iteration: 76\n",
      "Train accuracy: 98.27619047619048\n",
      "Val accuracy: 95.7\n",
      "Iteration: 77\n",
      "Train accuracy: 98.30793650793652\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 78\n",
      "Train accuracy: 98.33333333333333\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 79\n",
      "Train accuracy: 98.35873015873015\n",
      "Val accuracy: 95.74285714285715\n",
      "Iteration: 80\n",
      "Train accuracy: 98.38571428571429\n",
      "Val accuracy: 95.75714285714285\n",
      "Iteration: 81\n",
      "Train accuracy: 98.40476190476191\n",
      "Val accuracy: 95.72857142857143\n",
      "Iteration: 82\n",
      "Train accuracy: 98.41904761904762\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 83\n",
      "Train accuracy: 98.43968253968254\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 84\n",
      "Train accuracy: 98.47142857142858\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 85\n",
      "Train accuracy: 98.4984126984127\n",
      "Val accuracy: 95.8\n",
      "Iteration: 86\n",
      "Train accuracy: 98.52380952380952\n",
      "Val accuracy: 95.78571428571429\n",
      "Iteration: 87\n",
      "Train accuracy: 98.55238095238094\n",
      "Val accuracy: 95.8\n",
      "Iteration: 88\n",
      "Train accuracy: 98.56666666666666\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 89\n",
      "Train accuracy: 98.58888888888889\n",
      "Val accuracy: 95.8\n",
      "Iteration: 90\n",
      "Train accuracy: 98.62222222222222\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 91\n",
      "Train accuracy: 98.64761904761905\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 92\n",
      "Train accuracy: 98.66666666666667\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 93\n",
      "Train accuracy: 98.67936507936508\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 94\n",
      "Train accuracy: 98.70476190476191\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 95\n",
      "Train accuracy: 98.72222222222223\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 96\n",
      "Train accuracy: 98.73333333333333\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 97\n",
      "Train accuracy: 98.74920634920635\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 98\n",
      "Train accuracy: 98.78412698412698\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 99\n",
      "Train accuracy: 98.79523809523809\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 100\n",
      "Train accuracy: 98.80793650793652\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 101\n",
      "Train accuracy: 98.84126984126983\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 102\n",
      "Train accuracy: 98.86984126984127\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 103\n",
      "Train accuracy: 98.88888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 104\n",
      "Train accuracy: 98.89682539682539\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 105\n",
      "Train accuracy: 98.91428571428571\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 106\n",
      "Train accuracy: 98.92380952380952\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 107\n",
      "Train accuracy: 98.94603174603175\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 108\n",
      "Train accuracy: 98.95873015873016\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 109\n",
      "Train accuracy: 98.97142857142858\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 110\n",
      "Train accuracy: 98.98095238095237\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 111\n",
      "Train accuracy: 99.0031746031746\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 112\n",
      "Train accuracy: 99.01269841269841\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 113\n",
      "Train accuracy: 99.02539682539683\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 114\n",
      "Train accuracy: 99.05079365079365\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 115\n",
      "Train accuracy: 99.06031746031746\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 116\n",
      "Train accuracy: 99.06666666666666\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 117\n",
      "Train accuracy: 99.08888888888889\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 118\n",
      "Train accuracy: 99.12222222222222\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 119\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 120\n",
      "Train accuracy: 99.13968253968254\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 121\n",
      "Train accuracy: 99.16507936507936\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 122\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 123\n",
      "Train accuracy: 99.18095238095238\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 124\n",
      "Train accuracy: 99.21904761904761\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 125\n",
      "Train accuracy: 99.21111111111112\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 126\n",
      "Train accuracy: 99.22857142857143\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 127\n",
      "Train accuracy: 99.25079365079365\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 128\n",
      "Train accuracy: 99.26031746031747\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 129\n",
      "Train accuracy: 99.27936507936508\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 130\n",
      "Train accuracy: 99.28095238095239\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 131\n",
      "Train accuracy: 99.30952380952381\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 132\n",
      "Train accuracy: 99.32857142857144\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 133\n",
      "Train accuracy: 99.33809523809524\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 134\n",
      "Train accuracy: 99.35238095238094\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 135\n",
      "Train accuracy: 99.35079365079366\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 136\n",
      "Train accuracy: 99.36507936507937\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 137\n",
      "Train accuracy: 99.37777777777778\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 138\n",
      "Train accuracy: 99.38412698412698\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 139\n",
      "Train accuracy: 99.39365079365079\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 140\n",
      "Train accuracy: 99.41587301587302\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 141\n",
      "Train accuracy: 99.42857142857143\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 142\n",
      "Train accuracy: 99.44603174603175\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 143\n",
      "Train accuracy: 99.46031746031746\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 144\n",
      "Train accuracy: 99.46825396825398\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 145\n",
      "Train accuracy: 99.47619047619047\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 146\n",
      "Train accuracy: 99.4904761904762\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 147\n",
      "Train accuracy: 99.4968253968254\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 148\n",
      "Train accuracy: 99.50952380952381\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 149\n",
      "Train accuracy: 99.51746031746032\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 150\n",
      "Train accuracy: 99.52857142857144\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 151\n",
      "Train accuracy: 99.53809523809524\n",
      "Val accuracy: 95.81428571428572\n",
      "Iteration: 152\n",
      "Train accuracy: 99.54126984126984\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 153\n",
      "Train accuracy: 99.55079365079365\n",
      "Val accuracy: 95.84285714285714\n",
      "Iteration: 154\n",
      "Train accuracy: 99.55238095238094\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 155\n",
      "Train accuracy: 99.56825396825397\n",
      "Val accuracy: 95.82857142857144\n",
      "Iteration: 156\n",
      "Train accuracy: 99.57777777777778\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 157\n",
      "Train accuracy: 99.58253968253969\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 158\n",
      "Train accuracy: 99.58571428571429\n",
      "Val accuracy: 95.85714285714285\n",
      "Iteration: 159\n",
      "Train accuracy: 99.6\n",
      "Val accuracy: 95.87142857142858\n",
      "Iteration: 160\n",
      "Train accuracy: 99.60793650793651\n",
      "Val accuracy: 95.88571428571429\n",
      "Iteration: 161\n",
      "Train accuracy: 99.615873015873\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 162\n",
      "Train accuracy: 99.62063492063493\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 163\n",
      "Train accuracy: 99.63492063492063\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 164\n",
      "Train accuracy: 99.63650793650794\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 165\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 166\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 167\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 168\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 169\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 170\n",
      "Train accuracy: 99.66666666666667\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 171\n",
      "Train accuracy: 99.68095238095238\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 172\n",
      "Train accuracy: 99.68253968253968\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 173\n",
      "Train accuracy: 99.69047619047619\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 174\n",
      "Train accuracy: 99.69365079365079\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 175\n",
      "Train accuracy: 99.6984126984127\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 176\n",
      "Train accuracy: 99.70476190476191\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 177\n",
      "Train accuracy: 99.71269841269842\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 178\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 179\n",
      "Train accuracy: 99.72222222222223\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 180\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.91428571428573\n",
      "Iteration: 181\n",
      "Train accuracy: 99.73174603174603\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 182\n",
      "Train accuracy: 99.73968253968253\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 183\n",
      "Train accuracy: 99.74603174603175\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 184\n",
      "Train accuracy: 99.74444444444444\n",
      "Val accuracy: 95.89999999999999\n",
      "Iteration: 185\n",
      "Train accuracy: 99.75238095238095\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 186\n",
      "Train accuracy: 99.76031746031747\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 187\n",
      "Train accuracy: 99.77460317460317\n",
      "Val accuracy: 95.92857142857143\n",
      "Iteration: 188\n",
      "Train accuracy: 99.77301587301586\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 189\n",
      "Train accuracy: 99.78730158730158\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 190\n",
      "Train accuracy: 99.79206349206349\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 191\n",
      "Train accuracy: 99.7984126984127\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 192\n",
      "Train accuracy: 99.80317460317461\n",
      "Val accuracy: 95.97142857142858\n",
      "Iteration: 193\n",
      "Train accuracy: 99.80952380952381\n",
      "Val accuracy: 95.94285714285714\n",
      "Iteration: 194\n",
      "Train accuracy: 99.81269841269841\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 195\n",
      "Train accuracy: 99.81587301587301\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 196\n",
      "Train accuracy: 99.82857142857144\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 197\n",
      "Train accuracy: 99.82698412698413\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 198\n",
      "Train accuracy: 99.83015873015873\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 199\n",
      "Train accuracy: 99.84126984126985\n",
      "Val accuracy: 95.95714285714286\n",
      "Iteration: 200\n",
      "Train accuracy: 99.84603174603176\n",
      "Val accuracy: 95.95714285714286\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_bp, val_acc_bp, train_loss_bp, val_loss_bp, sum_weights_bp = batch_grad_descent(x_train,y_train,epochsToTrain, 0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22a64942500>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAHwCAYAAABkJOM0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABCfklEQVR4nO3de5xcdX3/8deHhBhQLFcRRJqolKqIqBt1EfllTS+iVQrlF7By8fITSb0WLZq2UUqq2JSq8LPdij+0CCpEFEQrrTZuRG3EXZBSUCxgQCGAAUHkZkjy+f1xziyTzV5mz87szOy+no/HPmbmzJkz3z17Nnnvdz7f7zcyE0mSJEmTt0O7GyBJkiR1K8O0JEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMS5IkSRUZpqUWi4iMiIci4kMtfp8rIuKkZu+r5omI/SPiwYiY0+62jCciTo+IC9vdjmZp9vcTEf8cESuadKy9IuLGiNipfLw2Iv5PM449iTbcGhG/V97/y4j4f0045t4R8eOIeMIYz/9O+buwZbq/X6nZ5ra7AdIs8fzMvBkgIhYAazNzQUQ8WLfPzsBvgC3l47dm5ucafYPMPKIV+6p5MvNnwJPa3Y5uEhG3Av8nM/+jTe//hvL9D6tty8xTmvgW7wf+JTMfaeIxK8vMDzfpOHdHxABwMvB/ofijpnzu9Mz8H+BJEbG2Ge8ntZM901IbZeaTal/Az4DX1G0bDtIR4R++DfA8zRzN+ll28jVR9tqeBMyYTwFG+Bzw1nY3Qmo1w7TUgSJicUTcHhHvi4i7gM9ExG4R8bWI2BgR95X396t7zfDHwxHxhoj4bkScVe67PiKOqLjvwoi4MiJ+HRH/ERH/ONZH5g20cfeI+ExEbCifv6zuuSMj4tqIeCAibomIV5bbhz+CLh8Pf2QfEQvKMpo3R8TPgG+V278YEXdFxK/Ktj+37vU7RcQ/RMRt5fPfLbf9a0S8Y8T3c11EHDXWz2fEtvqPyl8cEUPl93J3RHx0RHvn1v0cVkbE98rz+42I2LPumCeW7bw3IlaMPBcj3v9fyp/Nv5bHuioinln3/KERMVh+z4MRceiIn/G3y9d9E9hzxLFfGhH/GRH3R8R/RcTi0dpQdx6WR8SPyp/xZyJift3zf1T+nO8vj3nwiNe+LyKuAx6KiC8A+wNfjaIk4LQGzv3pEXFJRFwYEQ8Abyh3mx8RF5ff4zUR8fy617+/vOZ+Xbb7qHL7s4F/BnrL97+/7lz/bd3r3xIRN0fELyPi8ojYt+65jIhTIuKm8nv+x4iI8umXAPdn5jbfD/DMiPhBef18JSJ2rzveeNf2q8r2/zoi7oiI9zZy3kecy9F+v06KiJ9FxD0R8Vd1++5Qd+7ujYjV9W0FrgKeERG/Pdp7STOFYVqaZpl5a2YuaGDXpwK7A79N8VHpDsBnysf7A48Anxjn9S8BfkIRjFYB59X9Jz6ZfT8P/ADYAzgdOGGc95yojRdQlLM8F3gK8DEowifwWeAvgF2Bw4Fbx3mfkf4X8GzgD8vHVwAHlO9xDUUPWc1ZwIuAQynO72nAVuB84PjaTmXYehrwr5NoR83ZwNmZ+WTgmcDqcfb9U+CNZVvnAe8t3/85wD8Brwf2AX6rbM94jgP+BtgNuBn4UHms3cvv4xyKn+NHgX+NiD3K130euJri57+SoreU8rW1c/C3FOfrvcCXImKvcdrxeoqfxTOB3wH+ujzWC4BPU/RW7gF8Erg8tq2rfR3wamDXzHwd235is2qC77/mSOASimvpc3Xbvlh+D58HLouIHcvnbgFeTnGO/wa4MCL2ycwfA6cA68r333XkG0XEK4AzgaUUP6fbgItG7PZHwCLg4HK/2nX6PIrfu5FOBN5UHm8zxc+tZrxr+zyK8rBdgIN4/I/LRs77eA4DDgSWAB8o/8gAeAfwxxS/f/sC9wH/WHtRZm6muA6fXz4+PTNPb/A9pa5hmJY611bgg5n5m8x8JDPvzcwvZebDmflriqD0v8Z5/W2Z+anM3EIRFPcB9p7MvhGxP0UI+EBmbsrM7wKXj/WG47UxIvYBjgBOycz7MvOxzPx2+dI3A5/OzG9m5tbMvCMzb2zsNAFwemY+VKs7zcxPZ+avM/M3FH8APD8ifisidqAIKe8q32NLZv5nud/lwO9ExAHlMU8ALs7MTZNoR81jwLMiYs/MfDAzvz/Ovp/JzP8p274aOKTcfgzw1cz8btmGDwA5wftempk/KEPM5+qO9Wrgpsy8IDM3Z+YXgBuB19T9jFeU19qVwFfrjnk88PXM/Hr5s/kmMAS8apx2fCIzf56Zv6S4Bl5Xbj8Z+GRmXlWe+/Mpxgm8tO6155SvnUoN8brMvKxsb+04V2fmJZn5GMUfE/Nr75uZX8zMDeX+FwM3AS9u8L1eT3HtXlNeR8sperIX1O3zkcy8v6yZH+Dxn8uuwK9HOeYFmXl9Zj4ErACWRjlodaxru3zdY8BzIuLJ5e/YNeX2Rs77eP6m/Dfov4D/ogzHFH9o/FVm3l7XnmNi29KaX5ffpzRjGaalzrUxMx+tPYiInSPik1F87P8AcCWwa4w9M8RdtTuZ+XB5d6zBb2Ptuy/wy7ptAD8fq8ETtPHp5bHuG+WlT6foHaxquE0RMSciPlJ+9PwAj/dw71l+zR/tvcpzfTFwfBm6X0fRk17Fmyl6ZG+MoqTij8bZ9666+w/z+M9oX+q+r/JncO8E7zvesW4bse9tFD3d+wL3lcGt/rma3wb+d1kecH9Z6nAYxR9cY6m/Rm4r36N2rPeMONbT654f+dqqRjtG/bncCtxee98oymmurWvTQYwodRnHNuc2Mx+k+DnVf4ow1s/lPmCXCdp/G7AjsOcE1zbAn1D8kXNbFGU7veX2Rs77eMZq/28Dl9Yd88cUA6jr/2jfBbi/wfeRupJhWupcI3sh30PxUetLyvKBw8vtY5VuNMOdwO4RsXPdtqePs/94bfx5eaxdR3ndzylKAkbzEEVpSM1TR9mn/lz9KcVH+r9H8bH9gro23AM8Os57nU/R07gEeDgz1zXSpvKPheGyh8y8qSxReArwd8AlEfHEMY41ljuB+nrznSg+oq9iA0Xwqbc/cEf5PruNaN/+dfd/TtFTumvd1xMz8yPjvF/9NbJ/+f61Y31oxLF2LnvKa0Ze9yMfj3vux3jNNm0q/1jaD9hQ1vN+Cng7sEdZynE9j/9eTfRpwDbntjyPe1Cc24lcR/FH15htpTh/j1Fcu+Nd22TmYGYeSXHdXcbj5UWNnPcqfg4cMeK48zPzDhge/Pksit5sacYyTEvdYxeKGuT7yxrYD7b6DTPzNoqP9E+PiHllT9drqrQxM++kqPf8pygGKu4YEbWwfR7wxohYUg5qelpE/G753LXAceX+PRTlD+PZheIj7HspQtfwVF9lj+SngY9GxL5lT19vrXa0DM9bgX9g/F7p/6EY0Pbqsu72r4Hh+tOIOD4i9irf7/5y89YJ2j3SJRRlGIdGxDyKj9Cr/uH0dYoSlj+NiLkRcSzwHOBrdT/jvyl/xoex7c/4wrIdf1ier/lRDALcb/u3Gfa2iNivvAb+iqLHH4rQekpEvCQKTyzP4Wi9szV3A8+oezzuuR/HiyLi6DLgvZviGvk+8ESKwLwRICLeSNEzXf/++5U/g9F8geLaPaS8jj4MXJWZtzbQph9QfHIzshb++Ih4TvlH7BnAJWUJ1pjXdvmze31E/FZZyvIAj19zVc57I/4Z+FD5B0ltzuwj655/MXBreY1JM5ZhWuoeHwd2ouih+j7wb9P0vq8Hein+A/9bimD0mzH2/Tjjt/EEil62G4FfUIQaMvMHFIPwPgb8Cvg2j/f2raDoSb6PYnDY5ydo72cpPhq/A/hR2Y567wX+GxgEfknRc7zDiNc/j3GmK8vMXwF/Bvy/8n0eoigbqHklcEMU84ifDRw32RrgzLyBYoDXRRS9xw9SnLOxzv14x7qXYhDceyh+jqcBf5SZ95S7/CnFINRfUvwB9Nm61/6cojf0LykC588pBoqO9//H54FvAD+lKKn52/JYQ8BbKAal3kcxOO0NEzT/TOCvy1KC9zZw7sfyFeDY8n1PAI7Oom7/RxR/PK2jCM7PA75X97pvATcAd0XEPYyQxfzXK4AvUfycnkkxEHRCZS38v1A38LV0Qbn9LoqypHeW2ye6tk8Abi1LQE6h+N2tet4bcTbFWINvRMSvy/a8pO7511MEbmlGi8yJPsGSNBUR8ShFADonM5uyalo7RcTFwI2Z2fKe8XaIiBOBk7NukY5OEBFPoujlPiAz17e5OWOKNi+y0m2imBXlO8ALpjjosqNExFMo/ih+Qf3Yj7rnD6D4g3Ye8GeZ+S/T20KpeTp2MntppsjM+RPv1bkiYhFFj+V64A8oeinHq5ftWuXH6n9GMSVd20XEa4A1FOUdZ1H0qN/azjapuTJzI/C7E+7YZTLzFxTTVY71/E04y4dmCMs8JE3kqcBaijKDc4BlmfnDtraoBSLiDynKGO5m4lKS6XIkxQC3DRRzCx+XfpwoSR3FMg9JkiSpInumJUmSpIoM05IkSVJFXT0Acc8998wFCxa0uxmSJEma4a6++up7MnPkIlHdHaYXLFjA0NBQu5shSZKkGS4iRl2AyDIPSZIkqSLDtCRJklSRYVqSJEmqqKtrpkfz2GOPcfvtt/Poo9utXto15s+fz3777ceOO+7Y7qZIkiRpHDMuTN9+++3ssssuLFiwgIhod3MmLTO59957uf3221m4cGG7myNJkqRxzLgyj0cffZQ99tijK4M0QESwxx57dHXPuiRJ0mwx48I00LVBuqbb2y9JkjRbzMgw3W4RwXve857hx2eddRann346AKeffjpPe9rTOOSQQzjooIO4/PLL29RKSZIkTZVhugWe8IQn8OUvf5l77rln1Of//M//nGuvvZYvfvGLvOlNb2Lr1q3T3EJJkiQ1g2EaYN06OPPM4rYJ5s6dy8knn8zHPvaxcfd79rOfzdy5c8cM3ZIkSepsM242j0lbtw6WLIFNm2DePFizBnp7p3zYt73tbRx88MGcdtppY+5z1VVXscMOO7DXXtst8y5JkqQuYJheu7YI0lu2FLdr1zYlTD/5yU/mxBNP5JxzzmGnnXba5rmPfexjXHjhheyyyy5cfPHFDjiUJEnqUi0r84iIT0fELyLi+rptu0fENyPipvJ2t3J7RMQ5EXFzRFwXES9sVbu2s3hx0SM9Z05xu3hx0w797ne/m/POO4+HHnpom+21munvfOc7vPzlL2/a+0mSJGl6tbJm+l+AV47Y9n5gTWYeAKwpHwMcARxQfp0M9LewXdvq7S1KO1aubFqJR83uu+/O0qVLOe+885p2TEmSJHWOloXpzLwS+OWIzUcC55f3zwf+uG77Z7PwfWDXiNinVW3bTm8vLF/e1CBd8573vMcBhpIkSTPUdNdM752Zd5b37wL2Lu8/Dfh53X63l9vupAs9+OCDw/f33ntvHn744eHHtfmmJUmSOt2q761i0b6L6FvYN3wf4O//8+/5i0P/YtrvD24Y5LSXncbA+oHh++3WtgGImZkRkZN9XUScTFEKwv7779/0dkmSJLVDJwTXwQ2DAMzdYS6bt25m0b6LWHrJUpYftpxbfnkLH/7Oh0mSD/6vD3LUxUdN+/3Ljr2MgfUDLL1kKauPWd3yn0kjpjtM3x0R+2TmnWUZxy/K7XcAT6/bb79y23Yy81zgXICenp5Jh3FJkjQ7tSKsjgyfQOXj3vLLW/j7//z7tgbXy469jB/e9UPe+433ctYfnEXfwj6WH7ac937jvRx/8PEkSRDc/+j9bbk/cOsA/UP9rD5mNX0L+1p4tTRuusP05cBJwEfK26/UbX97RFwEvAT4VV05iCRJmqGmsze2FWF1ZPh8wVNfMKVjHbjngW0NrrWwetYfnMWZ3z2T+x+9n/6hfo4/+HguuO4CVhy+AoCVV65s6/1OCdLQwjAdEV8AFgN7RsTtwAcpQvTqiHgzcBuwtNz968CrgJuBh4E3tqpdkiRpbK0Ot+0sI2hFWB0ZPpf1LJvysTohuJ7aeyr3P3o/K69cyQkHn8AVN1/BisNXcPZVZxNEW+/3D/XTt6CvYwJ1y8J0Zr5ujKeWjLJvAm9rVVskSZrpmhWCW11q0M4yglaG1frwOZVjdUJw7R/qZ9f5u9I/1M8JB5/AhdddONzrfs5V55Aku87flSCm/X7fgj76FvQN10x3QqB2BURJkqbRZENvozW5zQrBrS41aHcZQSvCan34nMqxOiG49i3oY9f5uw7/sbN56+bhn9XRv3s0lx576fC11477tRk8Vh+zmsENg4bpmWrOnDk873nPIzOZM2cOn/jEJzj00EO59dZbefazn82BBx7Ipk2bOPzww/mnf/ondtihlWvnSJJaabLheLKht9Ga3GaF4OkqNWhHGUErwurI8DmV4+40d6e2B9faH2+1IF2beu4FT33BNuG1PsS2437fwllQ5tEN6v8BrGnGvIU77bQT1157LQD//u//zvLly/n2t78NwDOf+UyuvfZaNm/ezCte8Qouu+wyjj766Cl9H5KkqZlKiUSVcDyZ0NtoTW4zQ3Crw227yghaEVZHhs/BDYNTOtapvad2THCt10nhtdPM6jBdG/RQq7lpxbyFDzzwALvtttt22+fOncuhhx7KzTff3LT3kqTZZKwAXGWqsqmUSFQNx5MNvY3U5DYjBLc63LazjKDVYXWkmdDrqonN6jDdt7CP1cesZuklS1nWs6xp8xY+8sgjHHLIITz66KPceeedfOtb39pun4cffpg1a9ZwxhlnTOm9JGmmaFa5RJWpyqZSIlElHE829DZSk9usENzqUoNOKCMwrKqZZnWYhuIXalnPsqbOW1hf5rFu3TpOPPFErr/+egBuueUWDjnkECKCI488kiOOOGLK7ydJnaITyiWqTFU21RKJyYTjyYbeRmtymxWCp7PUoJ4BV91q1ofpgfUDw3/pt2Lewt7eXu655x42btwIPF4zLUmdajoDcSvLJSY7VVnVEonJhuPJht5Ga3KbHYINt1JjZnWYrq+R7lvYmnkLb7zxRrZs2cIee+zBww8/3JRjSlIVjdYYT2URjU4pl5jsVGVTKZGoEo6rht6RDMFS+83qMD24YXCb4FyroZ7qvIW1mmmAzOT8889nzpw5zWiyJA1rVY3xVBbR6IRyiSpTlU2lRKJqODb0SjPDrA7To01/14x/3LZs2TLq9gULFgzXTksSdGaN8VQX0Wh3uUSVqcqaUSJhOJZmp1kdpiVpuowVmju1xni6AnGryyVGMhBLajbDtCRV0MwSi06rMZ7OQGy5hKRuZ5iWpDGMN2Bv0b6LOOriozj2ucdy3EHHVe5F7sQa46ksomG5hKTZZkaG6cwkItrdjMoys91NkGakZvYmAyTJxTdczN5P2ntKvcidVmPcjEU0DMSSZosZF6bnz5/Pvffeyx577NGVgTozuffee5k/f367myJ1lUaCcrMH7F127GUM3DowpV7kTq4xNhBL0sRmXJjeb7/9uP3224cXSelG8+fPZ7/99mt3M6SONJWBfM0esAdMeR5ja4wlqbvNuDC94447snDhwnY3Q1IFU+ldbiQoN3PA3tlXnc05V50zHGynUmJhjbEkda8ZF6Yldb5W9C43GpSbNWDv7gfv5qIbLgJoyjzGBmJJ6k6GaUktUR+Ya48nWq56qr3LEwXlZg7Y++RrPslxBx3H4IbBbRaAshdZkmYXw7SkKWmkl3nz1s3M3WHuhMtVT6V3uZGg3OwBewZjSVJ08zRsPT09OTQ01O5mSLPCWKH5ousv4ss3fpnlhy3nJ/f8hItvuHi4l/mHd/1wODBfcfMVLD9sOWd+90yW9Syjf6ifI551xJi9y8t6lg0H5Xe+5J3D91974Gu3Cc218o/jnnscB+554HBQPu6g44Bt661rvcgD6we261GWJGk8EXF1ZvaM3G7PtKRtTLaeeTI1zBMtVz2V3uXJDOSzR1mS1CyGaWmWamVohu3LMRpZrrqRMgxLLyRJncQwLc1w0x2aR+tlPrX31IaWq7Z3WZLUbQzT0gzRCaF5rF7mFzz1BcMBupHlqg3KkqRuYZiWulh9gJ7sdHOtCM3j9TKPNtjP0CxJ6nbO5iF1gUZm0qiffu74g4/nKz/5yvBMGJOZOWOys2U4Q4YkaTYYazYPw7TUIRpd5GSi6ecMzZIkNZ9T40kdopWLnLSiPMN6ZkmSxmaYllpkqgMCr7j5iuGwe/+j909Y32xoliRp+hmmpSZq9oDAySxyUnW6OUOzJEnVGaalKRorQG/eurmh0oxmLXKy+pjV9C3sMzRLkjSNHIAoVVAfoAfWD2wToOtn1Kg6IPDU3lP56LqPbrPIydwd5jooUJKkNnE2D6mCKlPS1QfoqrNorD5mNYMbBodn86iFZEOzJEntYZiWGjRWr3OjU9LVAvQRzzrCqeckSZohDNPSOBot2xhrIZTRArSlGZIkzRyGaYmpl22MVvc8VoCuDQg0NEuS1P0M05q1mlW2MVbdswFakqRpsG4drF0LixdDb++0v70rIGrWqk1XVwu6k1lJsOqUdE5DJ0masepDLUzP/T32gHe/GzZtgnnzYM2atgTq0dgzrRmpvjcailkwjrr4KHr27eG/7v6vSmUb1j1Lkjpes4Jub+/ox6oPtXPmQARs3tz6+xGwdWvxNWcOrFwJy5e37jyOwjIPzTgjA/Oq760ankpu5OIpi/ZdxKs//2oe2fyIZRuSpPZpZa9us4LuvHnw8Y+Pfqz6UBtRvHdm6+/vsEPRjq1b29YzbZmHZoTxVhusDRo86w/O2q6c40Pf+RDz5szjmOccY9mGJKm6scLwWD2509mrWx90t24t3jdz8vc3bYIvfam43bJl2+dqoTZienumawH/3nvbVjM9FsO0Ol6jy3VfcfMVw4H4/kfv36YGeucdd+Zrr/sagxsGh/c5+neP5tJjLwWKso1Te081QEvSTNWMHuGxwvB4PbmtCLtj3W9W0J03D/7kT+A73xn7e62F2qrnsmrpSQeyzEMdqepy3Wf0ncEHBj6wTQ308/d+PoMbBrns2Mu2qaG2bEOSOkg3lD+MVeIwZw4sWVKUHmzZ0lipwnT03k7l3I3X096hobbVrJlWx6sSoOvrn/uH+ll+2HLO/O6Z2wwiPLX31OHj1Uo4JElNMJUAXB/WprP8oVl1u1V6pqejV3eWBt3pYJhWR5pKgB4ZmD+67qPDNdMOIpSkCiYTjqcSgEeGz1YPamtWj/B4YbiRmmnDblczTKtjNCtAjwzMgxsGh2fzqAVmA7QklZo9OG4qAXhkWUS3lT9oVjJMq61aFaDtcZY0q1Qtq2gkKE82HE8lAI/smbb8QV3AMK22GlmzXCvJMEBLmvGaObBuyZJqZRWNBOXJhuOpBuCRZRGGXXU4w7SmXaOrEBqgJXWV6aorHhlcTzoJPvWp8WeLmEpQrhKODcCaRVy0RdOuNid0/Qwam7ZsYs36NcMB+oSDT9gmQLtwiqRJm8oiGq0olxirR3iqi2hAEXinMuBvoqA8MhzXPx7rvjTL2TOtphqvN3pwwyAu3S1pQs3o+W10qrJWlEu0ahaJNWsmPh/2IkstY5mHWma8wYWL9l3Eqz//ah7Z/Mg2qxAaoKUZrpUD5RoJtyNni5hsWUSn1BXX3zcMS21lmFbLjDe48Cs/+QpBDPdMuwqh1EWmKxC3oue3VT3T1hVLs5ZhWk3V6ODCWm90fa+1qxBKbdDoYhLNWJFusoG4VT2/raiZNhxLs5ZhWlPWaDlHbXDh8/d+vr3RUqs0e0aJZq5I14z5h+35ldRhDNOaskbKOeoHF57ae6q90VIV7VipbmSN8XQH4vr7hmNJHcgwrUomW87h4EJpEkYLze1aqa7ZK9IZiCXNMB01z3REvAt4CxDApzLz4xFxerltY7nbX2bm19vRvtmuPkDX5oquL+cYOVf0koVLGNwwCDAcmJ0fWrNe1d7l+qA81pzDtXAc0fw5hp/3vKnNOWyIljTLTHvPdEQcBFwEvBjYBPwbcApwPPBgZp7V6LHsmW4NyzmkSWh277Ir1UlSR+qYMo+I+N/AKzPzzeXjFcBvgJ0xTLeN5RzSCI0O8FuyZPze5apTuk12pTpJUkt1UpnH9cCHImIP4BHgVcAQcC/w9og4sXz8nsy8rw3tm5UaWfrbcg7NGM0Y4DdvHpx0UrHPli2TL8OYaEq3epZRSFLHassAxIh4M/BnwEPADRQ902cC9wAJrAT2ycw3jfLak4GTAfbff/8X3XbbbdPV7BmvVqqxrGcZZ191tuUc6k7NCMqNzn7xlrfA+eePvZS1vcuSNGN0TJnHdg2I+DBwe2b+U922BcDXMvOg8V5rmcfUjCztADjx0hMt51Dnmq6g3Gjd8po1o7fDoCxJM04nlXkQEU/JzF9ExP7A0cBLI2KfzLyz3OUoinIQNdHI8Lxo30UcdfFRHPvcY/nkaz7JR9d9lAuvu5AXPvWF3HzfzYDlHGqTZgzqm8pMGJPtWbYMQ5JmrbaEaeBLZc30Y8DbMvP+iPi/EXEIRZnHrcBb29S2GWu0uugkufiGi3lk8yPjlnMYoNU07Z4yrmoJhoFZkjSKtpd5TIVlHpNXXxfdP9TP6mNWM3DrACuvXMkJB5/AZ4/67Db7Ws6hhk1l9ovpnjLOMCxJmqSOKvPQ9BlZ2tG3sI8jnnUEK69cyYrDVwDQP9TPisNX0D/Uz8D6gW32tTda26lagjHe7BeN9C7boyxJ6kCG6RluZGlHrS76hINP4Oyrzuacq87h0mMvLYLzgj5n6tDoagF6KiUYmzYV9+fNmzh0O2WcJKlLGKZnoPre6L6Ffaw+ZjVHXXwUz9jtGVx717XDddFv/epbueiGi4ZfV9u3NtBQs9BEvc5jheZGa5VPPLH4muxqfQZlSVKHsmZ6Bho5eHBg/QCv/vyreWTzI9ZFqzCZUo36AO2qfZKkWapj55meCsP048ZbDnxwwyBB8M6XvHN40KE9zzNYM2bLGGvg32RKMCRJmkEcgDjDjbcceG0BFuuiZ7CJapobqW9udOCfJRiSJA0zTM8QtXrn+uXA582Zx6FPP5TBDYPb7WdddJeqWtM81dkyDMySJI3KMo8Z5gMDH2DllSu36Y0eWUOtLjOZmTSmOhezoVmSpFFZ5jEDjVYnfc5V5/Cs3Z7FLx7+xfB+9kZ3gfHqnGuLnDRSntGMuZglSVLDDNNdrL5OGuCoi48iSc59zbkALgfe6SbqcR65yEkzapoNzZIkNZVhuovV10k/f+/nkySXHXvZcGi2N7pDVK1zHrnIieUZkiR1HGumu8zI0g6AEy89kQuuu4AVh6/gjL4z2tg6DWtGnfO8ebBmTXE8Q7MkSW1lzfQMMd7y4P1D/fQtsJyjbUYL0FOtc66FZ0O0JEkdyTDdZepLO4541hFceN2Fw8uDO2tHG0wUoJ27WZKkGc0w3QVGlnb0LezjiGcdwQXXXcAJB5/Aqb2nDm+3TrpFJlP3PNleZ0mS1LUM011gvNKOK26+goH1A9sEbYN0k1Spex4tQNvrLEnSjGWY7gKWdkyjZtU9G5glSZoVDNNdom9hH8t6lrHyypWWdjRD1bKNydQ9S5KkGc8w3aHGWt1wycIllnZU1ayyDbDuWZIkAYbpjjXW6oZ/9fK/ArC0YzxVF0mx7lmSJE2SYbpDubrhJIwMz0uWNN7rbNmGJEmaAsN0B6uvk15x+IptgrOlHaV16x4Pz/PmwUknFfe3bJnaIimSJEkNMEx3kPHqpF3dcIRab/TPfvZ4eN60qXhu3rzte6Yt25AkSS1gmO4g1klPYKwBhHPLy3jePDjxxOJrZM20vc6SJKkFDNMdxDrpUUw07zPAW94C+++/bWCuD86GaEmS1CKG6Q5jnTQTB+iR9c8nnmhgliRJbWGY7jAD6wfoH+pnxeErZled9GQDtLNuSJKkDmCYbrP6QYe1pcGXH7aczVs3D5d8zNg6aQO0JEnqcobpNqsfdDi4YZDlhy3nzO+eORygZ2yddP2UdgZoSZLUpQzTbVY/6HBZzzL6h/q36YmecXXSo01pZ4CWJEldyjDdAcYbdDgjNDKlnQFakiR1IcN0B5iRgw6rTmknSZLURQzTbVYbdFgr7ehb0Nf9gw4brYd2SjtJktTlDNNtUD+Dx+CGweEVD1d9bxWnvey07h10aD20JEmaZQzTbVA/g8dpLzttm95p6LJBh9ZDS5KkWcww3QYTzeDRNcYq5wDroSVJ0qxgmG6Trp7Bo5FyDuuhJUnSLGCYbpOuncGjvjfacg5JkjTLGabboCtn8BitNxos55AkSbOaYboNajN41K9y2NEzeIzXG205hyRJmsUiM9vdhsp6enpyaGio3c1oSP10eDUD6wcY3DDIaS87rY0tG0d9b/SnPlX0Rs+ZY2+0JEmadSLi6szsGbndnulpUj8dXt/Cvu2mw+s49kZLkiRNyDA9TbpuOry1a62NliRJmoBhehp1xXR49YuwzJtXBGp7oyVJkkZlmJ5GHT8dXn1ph1PdSZIkTcgwPU06ejq80aa927SpCNLLl7e3bZIkSR3MMD1NOnY6vPEGGi5e3L52SZIkdQHD9DQZbfq7voUdUObhQENJkqTKDNOzUa2sY/Hi4suBhpIkSZUYpluoIxdqGTnIcM2a4qsWrg3SkiRJDduh3Q2YyWoLtQysHwAeH4S4aN9F7WtUfVnHpk3F497eYqChQVqSJGlS7JluoY5aqGWs+aMdZChJklSZYbrFOmKhFuePliRJagnLPFps5EIttZKPaTWytKM2f7RBWpIkaUrsmW6hti/UYmmHJElSSxmmW6itC7VY2iFJktRyhukWautCLWOVdkiSJKlprJmeqWqLscyZY2mHJElSi7QlTEfEuyLi+oi4ISLeXW7bPSK+GRE3lbe7taNtXW/dOjjzzOL+mjWwcmVxa2mHJElS0017mUdEHAS8BXgxsAn4t4j4GnAysCYzPxIR7wfeD7xvutvX1UZb3dDSDkmSpJZpR8/0s4GrMvPhzNwMfBs4GjgSOL/c53zgj9vQtu422uqGkiRJapl2hOnrgZdHxB4RsTPwKuDpwN6ZeWe5z13A3m1o25St+t6q7eaSHlg/wKrvrWrdm9ZKO2pT4FknLUmSNC2mvcwjM38cEX8HfAN4CLgW2DJin4yIHO31EXEyRUkI+++/f2sbW8GifRdtM5d0/VzTLeEUeJIkSW3TlgGImXleZr4oMw8H7gP+B7g7IvYBKG9/McZrz83Mnszs2Wuvvaav0Q2qzSW99JKlfGDgA61fpMXVDSVJktqmXbN5PKW83Z+iXvrzwOXASeUuJwFfaUfbmqFvYR/Lepax8sqVLOtZ1tp5pZ0CT5IkqW3atWjLlyJiD+Ax4G2ZeX9EfARYHRFvBm4DlrapbVM2sH6A/qF+Vhy+gv6hfvoWtGChltpS4YsXF7N21O7bIy1JkjRt2hKmM/Plo2y7F1jShuY0VX2NdN/CPvoW9DW/1MMp8CRJkjqCKyA22eCGwW2Cc62GenDDYPPexCnwJEmSOkK7yjxmrNNedtp22/oWNrnMo1YnXeuZtk5akiSpLQzT3ai31zppSZKkDmCY7ib1gw57ew3RkiRJbWaY7hajDTo0TEuSJLWVAxC7hYMOJUmSOo5hulu4OIskSVLHscyjWzjoUJIkqeMYpjudgw4lSZI6lmG6kznoUJIkqaNZM90Eq763ioH1A9tsG1g/wKrvrZragR10KEmS1NEM002waN9FLL1k6XCgHlg/wNJLlrJo30VTO7CDDiVJkjqaZR5N0Lewj9XHrGbpJUtZ1rOM/qF+Vh+zeupLiDvoUJIkqaMZppukb2Efy3qWsfLKlaw4fMXUg3SNgw4lSZI6lmUeTTKwfoD+oX5WHL6C/qH+7WqoJ2XdOjjzzOJWkiRJHcue6Sao1UjXSjv6FvRt83hSnMFDkiSpa9gz3QSDGwa3Cc61GurBDYOTP5gzeEiSJHUNe6ab4LSXnbbdtr6FfdXqpmszeNR6pp3BQ5IkqWNNGKYj4h3AhZl53zS0R87gIUmS1DUa6ZneGxiMiGuATwP/npnZ2mbNcs7gIUmS1BUmrJnOzL8GDgDOA94A3BQRH46IZ7a4bZIkSVJHa2gAYtkTfVf5tRnYDbgkIqa4XraGOR2eJElS12mkZvpdwInAPcD/A/4iMx+LiB2Am4DtR99pcpwOT5IkqSs1UjO9O3B0Zt5WvzEzt0bEH7WmWbPMaNPhGaYlSZI6XiNlHlcAv6w9iIgnR8RLADLzx61q2KxSmw5vzhynw5MkSeoijfRM9wMvrHv84CjbNBVOhydJktSVGgnTUT8VXlne4WIvzeZ0eJIkSV2nkTKPn0bEOyNix/LrXcBPW90wSZIkqdM1EqZPAQ4F7gBuB14CnNzKRkmSJEndYMJyjcz8BXDcNLRFkiRJ6iqNzDM9H3gz8Fxgfm17Zr6phe2aHdatc9ChJElSF2tkIOEFwI3AHwJnAK8HnBJvqlyoRZIkqes1UjP9rMxcATyUmecDr6aom9ZUjLZQiyRJkrpKI2H6sfL2/og4CPgt4Cmta9Is4UItkiRJXa+RMo9zI2I34K+By4EnASta2qrZwIVaJEmSut64YToidgAeyMz7gCuBZ0xLq2YLF2qRJEnqauOWeWTmVuC0aWqLJEmS1FUaqZn+j4h4b0Q8PSJ2r321vGWSJElSh2ukZvrY8vZtddsSSz4kSZI0y03YM52ZC0f5mvVBetX3VjGwfmCbbQPrB1j1vVVtapEkSZKmWyMrIJ442vbM/Gzzm9M9Fu27iKWXLGX1MavpW9jHwPqB4ceSJEmaHRop81hUd38+sAS4BpjVYbpvYR+rj1nN0kuWsqxnGf1D/cPBelwuIS5JkjRjTBimM/Md9Y8jYlfgolY1qJv0LexjWc8yVl65khWHr2gsSLuEuCRJ0ozRyGweIz0ELGx2Q7rRwPoB+of6WXH4CvqH+rerod6OS4hLkiTNKI3UTH+VYvYOKML3c4BZXxhcXyPdt7CPvgV92zweVW0J8VrPtEuIS5IkdbVGaqbPqru/GbgtM29vUXu6xuCGwW2Cc62GenDD4Nhh2iXEJUmSZpTIzPF3iFgI3JmZj5aPdwL2zsxbW9+88fX09OTQ0FC7myFJkqQZLiKuzsyekdsbqZn+IrC17vGWcpskSZI0qzUSpudm5qbag/L+vNY1SZIkSeoOjYTpjRHx2tqDiDgSuKd1TZIkSZK6QyMDEE8BPhcRnygf3w6MuiqiJEmSNJs0smjLLcBLI+JJ5eMHW94qSZIkqQtMWOYRER+OiF0z88HMfDAidouIv52OxkmSJEmdrJGa6SMy8/7ag8y8D3hVy1okSZIkdYlGwvSciHhC7UE5z/QTxtlfkiRJmhUaGYD4OWBNRHymfPxG4PzWNWkGWrfOVQ8lSZJmoEYGIP5dRFwHLCk3rczMf29ts2aQdetgyRLYtAnmzSuWEzdQS5IkzQiN9EyTmVcAV7S4LTPT2rVFkN6ypbhdu9YwLUmSNEM0MpvHSyNiMCIejIhNEbElIh6YyptGxJ9HxA0RcX1EfCEi5kfEv0TE+oi4tvw6ZCrv0TEWLy56pOfMKW4XL253iyRJktQkjfRMfwI4Dvgi0EOxYMvvVH3DiHga8E7gOZn5SESsLo8P8BeZeUnVY3ek3t6itMOaaUmSpBmn0TKPmyNiTmZuAT4TET8Elk/xfXeKiMeAnYENUzhW5+vtNURLkiTNQI1MjfdwRMwDro2IVRHx5w2+blSZeQdwFvAz4E7gV5n5jfLpD0XEdRHxsfrp+CRJkqRO1EgoPqHc7+3AQ8DTgT+p+oYRsRtwJLAQ2Bd4YkQcT9HT/bvAImB34H1jvP7kiBiKiKGNGzdWbYYkSZI0ZROG6cy8LTMfzcwHMvNvMvPUzLx5Cu/5e8D6zNyYmY8BXwYOzcw7s/Ab4DPAi8doz7mZ2ZOZPXvttdcUmiFJkiRNTeVyjSn4GfDSiNg5IoJi/uofR8Q+AOW2Pwaub0PbJEmSpIY1NACxmTLzqoi4BLgG2Az8EDgXuCIi9gICuBY4ZbrbJkmSJE3GtIdpgMz8IPDBEZtf0Y62SJIkSVVNGKYj4qtAjtj8K2AI+GRmPtqKhkmSJEmdrpGa6Z8CDwKfKr8eAH5NsXDLp1rXNEmSJKmzNVLmcWhmLqp7/NWIGMzMRRFxQ6saJkmSJHW6RnqmnxQR+9celPefVD7c1JJWSZIkSV2gkZ7p9wDfjYhbKGbaWAj8WUQ8ETi/lY2TJEmSOtmEYTozvx4RB1CsTgjwk7pBhx9vVcO63rp1sHYtLF4Mvb3tbo0kSZJaoNGp8V4ELCj3f35EkJmfbVmrut26dbBkCWzaBPPmwZo1BmpJkqQZqJGp8S4AnkmxkMqWcnMChumxrF1bBOktW4rbtWsN05IkSTNQIz3TPcBzMnPkXNMay+LFRY90rWd68eJ2t0iSJEkt0EiYvh54KnBni9syc/T2FqUd1kxLkiTNaI2E6T2BH0XED4Df1DZm5mtb1qqZoLfXEC1JkjTDNRKmT291IyRJkqRu1MjUeN+ejoZIkiRJ3WbMMB0R383MwyLi1xSzdww/BWRmPrnlrZMkSZI62JhhOjMPK293mb7mSJIkSd2joUVbImIOsHf9/pn5s1Y1SpIkSeoGjSza8g7gg8DdwNZycwIHt7BdkiRJUsdrpGf6XcCBmXlvqxsjSZIkdZMdGtjn58CvWt0QSZIkqds00jP9U2BtRPwr2y7a8tGWtUqSJEnqAo2E6Z+VX/PKL0mSJEk0tmjL30xHQyRJkqRuM96iLR/PzHdHxFfZdtEWADLztS1tmSRJktThxuuZvqC8PWs6GiJJkiR1m/FWQLy6vP329DVHkiRJ6h6NLNpyAHAm8Bxgfm17Zj6jhe2SJEmSOl4j80x/BugHNgN9wGeBC1vZKEmSJKkbNBKmd8rMNUBk5m2ZeTrw6tY2S5IkSep8jcwz/ZuI2AG4KSLeDtwBPKm1zZIkSZI6XyM90+8CdgbeCbwIOB44qZWNkiRJkrrBuD3TETEHODYz3ws8CLxxWlolSZIkdYExe6YjYm5mbgEOm8b2dLd16+DMM4tbSZIkzXjj9Uz/AHgh8MOIuBz4IvBQ7cnM/HKL29Zd1q2DJUtg0yaYNw/WrIHe3na3SpIkSS3UyADE+cC9wCsolhWP8tYwXW/t2iJIb9lS3K5da5iWJEma4cYL00+JiFOB63k8RNdkS1vVjRYvLnqkaz3Tixe3u0WSJElqsfHC9ByKKfBilOdmZZhe9b1VLNp3EX0L+4a3DawfYHDDIKe97LSitGPt2iJI2ystSZI0440Xpu/MzDOmrSVdYNG+i1h6yVJWH7OavoV9DKwfGH4MFAHaEC1JkjRrjBemR+uRntX6Fvax+pjVLL1kKct6ltE/1D8crCVJkjT7jLdoy5Jpa0UX6VvYx7KeZay8ciXLepYZpCVJkmaxMcN0Zv5yOhvSLQbWD9A/1M+Kw1fQP9TPwPqBdjdJkiRJbdLIcuIq1ddIn9F3xnDJh4FakiRpdjJMT8LghsFtaqRrNdSDGwbb3DJJkiS1Q2R27yx3PT09OTQ01O5mSJIkaYaLiKszs2fkdnumJUmSpIoM05IkSVJFhmlJkiSpIsO0JEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMS5IkSRUZpiVJkqSKDNOSJElSRYZpSZIkqSLDtCRJklSRYVqSJEmqyDAtSZIkVWSYliRJkipqS5iOiD+PiBsi4vqI+EJEzI+IhRFxVUTcHBEXR8S8drRNkiRJatS0h+mIeBrwTqAnMw8C5gDHAX8HfCwznwXcB7x5utsmSZIkTUa7yjzmAjtFxFxgZ+BO4BXAJeXz5wN/3J6mSZIkSY2Z9jCdmXcAZwE/owjRvwKuBu7PzM3lbrcDT5vutkmSJEmT0Y4yj92AI4GFwL7AE4FXTuL1J0fEUEQMbdy4sUWtlCRJkibWjjKP3wPWZ+bGzHwM+DLwMmDXsuwDYD/gjtFenJnnZmZPZvbstdde09NiSZIkaRTtCNM/A14aETtHRABLgB8BA8Ax5T4nAV9pQ9skSZKkhrWjZvoqioGG1wD/XbbhXOB9wKkRcTOwB3DedLdNkiRJmoy5E+/SfJn5QeCDIzb/FHhxG5ojSZIkVeIKiJIkSVJFhmlJkiSpIsO0JEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMT9W6dXDmmcWtJEmSZpW2zDM9Y6xbB0uWwKZNMG8erFkDvb3tbpUkSZKmiT3TU7F2bRGkt2wpbteubXeLJEmSNI0M01OxeHHRIz1nTnG7eHG7WyRJkqRpZJnHVPT2FqUda9cWQdoSD0mSpFnFMD1Vvb2GaEmSpFnKMg9JkiSpIsO0JEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMS5IkSRUZpiVJkqSKDNOSJElSRYZpSZIkqSLDtCRJklSRYVqSJEmqyDAtSZIkVWSYliRJkioyTEuSJEkVGaYlSZKkigzTkiRJUkWGaUmSJKkiw7QkSZJUkWFakiRJqsgwLUmSJFVkmJYkSZIqMkxLkiRJFRmmJUmSpIoM05IkSVJFhmlJkiSpIsO0JEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMS5IkSRUZpiVJkqSKDNOSJElSRYZpSZIkqSLDtCRJklSRYVqSJEmqyDAtSZIkVWSYliRJkioyTEuSJEkVGaYlSZKkigzTkiRJUkWGaUmSJKkiw7QkSZJUkWFakiRJqsgwLUmSJFVkmJYkSZIqMkxLkiRJFc2d7jeMiAOBi+s2PQP4ALAr8BZgY7n9LzPz69PbOkmSJKlx0x6mM/MnwCEAETEHuAO4FHgj8LHMPGu62yRJkiRV0e4yjyXALZl5W5vbIUmSJE1au8P0ccAX6h6/PSKui4hPR8Ru7WqUJEmS1Ii2hemImAe8FvhiuakfeCZFCcidwD+M8bqTI2IoIoY2btw42i6SJEnStGhnz/QRwDWZeTdAZt6dmVsycyvwKeDFo70oM8/NzJ7M7Nlrr72msbmSJEnSttoZpl9HXYlHROxT99xRwPXT3iJJkiRpEqZ9Ng+AiHgi8PvAW+s2r4qIQ4AEbh3xnCRJktRx2hKmM/MhYI8R205oR1skSZKkqto9m4ckSZLUtQzTkiRJUkWGaUmSJKkiw7QkSZJUkWFakiRJqsgwLUmSJFVkmJYkSZIqMkxLkiRJFRmmJUmSpIoM05IkSVJFhmlJkiSpIsO0JEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMS5IkSRUZpiVJkqSKDNOSJElSRYZpSZIkqSLDtCRJklSRYVqSJEmqyDAtSZIkVWSYliRJkioyTEuSJEkVGaYlSZKkigzTkiRJUkWGaUmSJKkiw7QkSZJUkWFakiRJqsgwLUmSJFVkmJYkSZIqMkxLkiRJFRmmJUmSpIoM05IkSVJFhmlJkiSpIsO0JEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMS5IkSRUZpiVJkqSKDNOSJElSRYZpSZIkqSLDtCRJklSRYVqSJEmqyDAtSZIkVWSYliRJkioyTEuSJEkVGaYlSZKkigzTkiRJUkWGaUmSJKkiw7QkSZJUkWFakiRJqsgwLUmSJFVkmJYkSZIqMkxXsW4dnHlmcStJkqRZa267G9B11q2DJUtg0yaYNw/WrIHe3na3SpIkSW1gz/RkrV1bBOktW4rbtWvb3SJJkiS1ybSH6Yg4MCKurft6ICLeHRG7R8Q3I+Km8na36W5bQxYvLnqk58wpbhcvbneLJEmS1CbTHqYz8yeZeUhmHgK8CHgYuBR4P7AmMw8A1pSPO09vb1HasXKlJR6SJEmzXLtrppcAt2TmbRFxJLC43H4+sBZ4X5vaNb7eXkO0JEmS2l4zfRzwhfL+3pl5Z3n/LmDv9jRJkiRJakzbwnREzANeC3xx5HOZmUCO8bqTI2IoIoY2btzY4lZKkiRJY2tnz/QRwDWZeXf5+O6I2AegvP3FaC/KzHMzsycze/baa69paqokSZK0vXaG6dfxeIkHwOXASeX9k4CvTHuLJEmSpEloS5iOiCcCvw98uW7zR4Dfj4ibgN8rH0uSJEkdqy2zeWTmQ8AeI7bdSzG7hyRJktQV2j2bhyRJktS1DNOSJElSRYZpSZIkqSLDtCRJklSRYVqSJEmqyDAtSZIkVWSYliRJkioyTEuSJEkVGaYlSZKkigzTkiRJUkWRme1uQ2URsRG4rU1vvydwT5veuxt5vibH8zV5nrPJ8XxNnudscjxfk+c5m5zpPl+/nZl7jdzY1WG6nSJiKDN72t2ObuH5mhzP1+R5zibH8zV5nrPJ8XxNnudscjrlfFnmIUmSJFVkmJYkSZIqMkxXd267G9BlPF+T4/maPM/Z5Hi+Js9zNjmer8nznE1OR5wva6YlSZKkiuyZliRJkioyTE9SRLwyIn4SETdHxPvb3Z5OExFPj4iBiPhRRNwQEe8qt58eEXdExLXl16va3dZOEhG3RsR/l+dmqNy2e0R8MyJuKm93a3c7O0FEHFh3HV0bEQ9ExLu9xrYVEZ+OiF9ExPV120a9pqJwTvnv2nUR8cL2tbw9xjhffx8RN5bn5NKI2LXcviAiHqm71v65bQ1vozHO2Zi/hxGxvLzGfhIRf9ieVrfPGOfr4rpzdWtEXFtu9xpj3EzRUf+WWeYxCRExB/gf4PeB24FB4HWZ+aO2NqyDRMQ+wD6ZeU1E7AJcDfwxsBR4MDPPamf7OlVE3Ar0ZOY9ddtWAb/MzI+Uf7jtlpnva1cbO1H5O3kH8BLgjXiNDYuIw4EHgc9m5kHltlGvqTLwvAN4FcW5PDszX9KutrfDGOfrD4BvZebmiPg7gPJ8LQC+VttvthrjnJ3OKL+HEfEc4AvAi4F9gf8Aficzt0xro9totPM14vl/AH6VmWd4jRXGyRRvoIP+LbNnenJeDNycmT/NzE3ARcCRbW5TR8nMOzPzmvL+r4EfA09rb6u61pHA+eX98yn+AdG2lgC3ZGa7Fm/qWJl5JfDLEZvHuqaOpPgPPjPz+8Cu5X9is8Zo5yszv5GZm8uH3wf2m/aGdbAxrrGxHAlclJm/ycz1wM0U/6fOGuOdr4gIik6nL0xrozrcOJmio/4tM0xPztOAn9c9vh2D4pjKv6xfAFxVbnp7+bHLpy1Z2E4C34iIqyPi5HLb3pl5Z3n/LmDv9jStox3Htv/5eI2Nb6xryn/bJvYm4Iq6xwsj4ocR8e2IeHm7GtWhRvs99Bob38uBuzPzprptXmN1RmSKjvq3zDCtloiIJwFfAt6dmQ8A/cAzgUOAO4F/aF/rOtJhmflC4AjgbeXHgcOyqMeyJqtORMwDXgt8sdzkNTYJXlONi4i/AjYDnys33Qnsn5kvAE4FPh8RT25X+zqMv4fVvI5tOwa8xuqMkimGdcK/ZYbpybkDeHrd4/3KbaoTETtSXPSfy8wvA2Tm3Zm5JTO3Ap9iln28N5HMvKO8/QVwKcX5ubv28VR5+4v2tbAjHQFck5l3g9dYg8a6pvy3bQwR8Qbgj4DXl/9pU5Yq3Fvevxq4BfidtjWyg4zze+g1NoaImAscDVxc2+Y19rjRMgUd9m+ZYXpyBoEDImJh2St2HHB5m9vUUcq6r/OAH2fmR+u219csHQVcP/K1s1VEPLEcWEFEPBH4A4rzczlwUrnbScBX2tPCjrVNT47XWEPGuqYuB04sR8K/lGIQ1J2jHWA2iYhXAqcBr83Mh+u271UOfiUingEcAPy0Pa3sLOP8Hl4OHBcRT4iIhRTn7AfT3b4O9XvAjZl5e22D11hhrExBh/1bNrfVbzCTlCO63w78OzAH+HRm3tDmZnWalwEnAP9dm+IH+EvgdRFxCMVHMbcCb21H4zrU3sClxb8ZzAU+n5n/FhGDwOqIeDNwG8XgFDH8R8fvs+11tMpr7HER8QVgMbBnRNwOfBD4CKNfU1+nGP1+M/Awxcwos8oY52s58ATgm+Xv5/cz8xTgcOCMiHgM2AqckpmNDsSbMcY4Z4tH+z3MzBsiYjXwI4qSmbfNppk8YPTzlZnnsf3YD/AaqxkrU3TUv2VOjSdJkiRVZJmHJEmSVJFhWpIkSarIMC1JkiRVZJiWJEmSKjJMS5IkSRUZpiWpi0TEloi4tu7r/U089oKIcH5uSZoE55mWpO7ySGYe0u5GSJIK9kxL0gwQEbdGxKqI+O+I+EFEPKvcviAivhUR10XEmojYv9y+d0RcGhH/VX4dWh5qTkR8KiJuiIhvRMRO5f7vjIgflce5qE3fpiR1HMO0JHWXnUaUeRxb99yvMvN5wCeAj5fb/i9wfmYeDHwOOKfcfg7w7cx8PvBCoLaa6wHAP2bmc4H7gT8pt78feEF5nFNa861JUvdxBURJ6iIR8WBmPmmU7bcCr8jMn0bEjsBdmblHRNwD7JOZj5Xb78zMPSNiI7BfZv6m7hgLgG9m5gHl4/cBO2bm30bEvwEPApcBl2Xmgy3+ViWpK9gzLUkzR45xfzJ+U3d/C4+PrXk18I8UvdiDEeGYG0nCMC1JM8mxdbfryvv/CRxX3n898J3y/hpgGUBEzImI3xrroBGxA/D0zBwA3gf8FrBd77gkzUb2LEhSd9kpIq6te/xvmVmbHm+3iLiOonf5deW2dwCfiYi/ADYCbyy3vws4NyLeTNEDvQy4c4z3nANcWAbuAM7JzPub9P1IUlezZlqSZoCyZronM+9pd1skaTaxzEOSJEmqyJ5pSZIkqSJ7piVJkqSKDNOSJElSRYZpSZIkqSLDtCRJklSRYVqSJEmqyDAtSZIkVfT/AQgzavB22wQRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP\", \"BP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imposing variability and seeing the effect of variability on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.7\n",
    "sigma = 0.1\n",
    "vDD = 5\n",
    "precision = 10#setting the precision value of the calculations\n",
    "wRange = 3\n",
    "noOfLevels = 2**precision - 1 #no of levels of quantization\n",
    "step = round(wRange/noOfLevels, precision) #step size of each of the step after quantization\n",
    "discreteSteps = [round(step*i, precision) for i in range(0, noOfLevels)] #storing the values of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVth(mu, sigma, shape):\n",
    "  #last dimension represents the binary rep for each weight\n",
    "  return np.random.normal(loc=mu, scale=sigma, size=shape) #each bit is represented by an sram so we need those many vth values for each mosfet in this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initMosParam(shape, mu, sigma, vDD, precision):\n",
    "    dim1, dim2 = shape\n",
    "    sizeI = (dim1, dim2, precision)\n",
    "\n",
    "    Vth = getVth(mu, sigma, sizeI)#get the array of Vth values \n",
    "\n",
    "    iOn = ((vDD - Vth)**2)*1e-06#scaling the current according to Ioff values arbitraryfor now!!\n",
    "\n",
    "\n",
    "    iOnNominal = 1e-06*(vDD**2 - (2*vDD*mu) + (sigma**2  + mu**2))\n",
    "\n",
    "\n",
    "    iOff = np.random.uniform(low=0, high=1e-10, size = sizeI)#no negative value\n",
    "    return (iOn, iOnNominal, iOff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightTransformWithVariability(weightArray, currents, precision, step, discreteSteps):\n",
    "  dim1, dim2 = weightArray.shape\n",
    "  sizeI = (dim1, dim2, precision)\n",
    "\n",
    "  clippedWeightIndexArray = np.digitize(np.abs(weightArray), discreteSteps) #finds the index value of the weights\n",
    "\n",
    "  #vDD = 5\n",
    "  #mu = 0.7#mean of the distribution\n",
    "  #sigma = 0.00001\n",
    "  #! work with sigma/mu\n",
    "  \n",
    "  iOn, iOnNominal, iOff = currents\n",
    "\n",
    "  \n",
    "  analogWeightArray = np.zeros_like(weightArray, dtype=float)\n",
    "\n",
    "  for bitLevel in range(precision):\n",
    "    analogWeightArray += np.sign(weightArray) * np.where(np.bitwise_and(clippedWeightIndexArray, 2**bitLevel)>=1, iOn[:, :, bitLevel], iOff[:, :, bitLevel]) * (2**bitLevel)\n",
    "\n",
    "\n",
    "\n",
    "  weightWithVariability = (analogWeightArray/iOnNominal)*step\n",
    "  return weightWithVariability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descentFPOCBP(X,Y,iter, lr, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z1\")\n",
    "    #print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_grad_descentFPOCNP(X,Y,iter, lr, pert, mu, sigma, vDD, precision, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "\n",
    "  W1Currents = initMosParam((200, 784), mu, sigma, vDD, precision)\n",
    "  b1Currents = initMosParam((200, 1), mu, sigma, vDD, precision)\n",
    "  W2Currents = initMosParam((50, 200) ,mu, sigma, vDD, precision)\n",
    "  b2Currents = initMosParam((50, 1), mu, sigma, vDD, precision)\n",
    "  W3Currents = initMosParam((10, 50),mu, sigma, vDD, precision)\n",
    "  b3Currents = initMosParam((10, 1), mu, sigma, vDD, precision)\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes \n",
    "\n",
    "      startin = time.time()\n",
    "      W1varoc = weightTransformWithVariability(W1, W1Currents, precision, step, discreteSteps)\n",
    "      b1varoc = weightTransformWithVariability(b1, b1Currents, precision, step, discreteSteps)\n",
    "      W2varoc = weightTransformWithVariability(W2, W2Currents, precision, step, discreteSteps)\n",
    "      b2varoc = weightTransformWithVariability(b2, b2Currents, precision, step, discreteSteps)\n",
    "      W3varoc = weightTransformWithVariability(W3, W3Currents, precision, step, discreteSteps)\n",
    "      b3varoc = weightTransformWithVariability(b3, b3Currents, precision, step, discreteSteps)\n",
    "      endin = time.time()\n",
    "      #print(f\"#Fin processing weights {endin-startin}                                                     \", end = \"\\r\", flush= True)\n",
    "\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1varoc, b1varoc, W2varoc, b2varoc, W3varoc, b3varoc) \n",
    "\n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      \n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/Y.shape[0]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 82.28888888888889\n",
      "Val accuracy: 81.10000000000001\n",
      "Iteration: 2\n",
      "Train accuracy: 86.56666666666666\n",
      "Val accuracy: 85.91428571428571\n",
      "Iteration: 3\n",
      "Train accuracy: 88.72539682539683\n",
      "Val accuracy: 87.8\n",
      "Iteration: 4\n",
      "Train accuracy: 89.97936507936508\n",
      "Val accuracy: 88.9857142857143\n",
      "Iteration: 5\n",
      "Train accuracy: 90.88888888888889\n",
      "Val accuracy: 89.94285714285715\n",
      "Iteration: 6\n",
      "Train accuracy: 91.55873015873016\n",
      "Val accuracy: 90.54285714285714\n",
      "Iteration: 7\n",
      "Train accuracy: 92.1047619047619\n",
      "Val accuracy: 91.2\n",
      "Iteration: 8\n",
      "Train accuracy: 92.60952380952381\n",
      "Val accuracy: 91.71428571428571\n",
      "Iteration: 9\n",
      "Train accuracy: 92.97142857142858\n",
      "Val accuracy: 92.01428571428572\n",
      "Iteration: 10\n",
      "Train accuracy: 93.30634920634921\n",
      "Val accuracy: 92.32857142857142\n",
      "Iteration: 11\n",
      "Train accuracy: 93.60634920634921\n",
      "Val accuracy: 92.5\n",
      "Iteration: 12\n",
      "Train accuracy: 93.87301587301587\n",
      "Val accuracy: 92.72857142857143\n",
      "Iteration: 13\n",
      "Train accuracy: 94.0936507936508\n",
      "Val accuracy: 92.92857142857143\n",
      "Iteration: 14\n",
      "Train accuracy: 94.2968253968254\n",
      "Val accuracy: 93.10000000000001\n",
      "Iteration: 15\n",
      "Train accuracy: 94.50158730158729\n",
      "Val accuracy: 93.15714285714286\n",
      "Iteration: 16\n",
      "Train accuracy: 94.66507936507936\n",
      "Val accuracy: 93.37142857142857\n",
      "Iteration: 17\n",
      "Train accuracy: 94.82857142857142\n",
      "Val accuracy: 93.52857142857142\n",
      "Iteration: 18\n",
      "Train accuracy: 95.02063492063492\n",
      "Val accuracy: 93.64285714285714\n",
      "Iteration: 19\n",
      "Train accuracy: 95.16190476190476\n",
      "Val accuracy: 93.77142857142857\n",
      "Iteration: 20\n",
      "Train accuracy: 95.27142857142857\n",
      "Val accuracy: 93.87142857142857\n",
      "Iteration: 21\n",
      "Train accuracy: 95.3968253968254\n",
      "Val accuracy: 93.92857142857143\n",
      "Iteration: 22\n",
      "Train accuracy: 95.52539682539683\n",
      "Val accuracy: 94.05714285714286\n",
      "Iteration: 23\n",
      "Train accuracy: 95.63333333333334\n",
      "Val accuracy: 94.1\n",
      "Iteration: 24\n",
      "Train accuracy: 95.74126984126984\n",
      "Val accuracy: 94.14285714285714\n",
      "Iteration: 25\n",
      "Train accuracy: 95.84126984126983\n",
      "Val accuracy: 94.24285714285713\n",
      "Iteration: 26\n",
      "Train accuracy: 95.92380952380952\n",
      "Val accuracy: 94.27142857142857\n",
      "Iteration: 27\n",
      "Train accuracy: 95.98412698412699\n",
      "Val accuracy: 94.28571428571428\n",
      "Iteration: 28\n",
      "Train accuracy: 96.06666666666666\n",
      "Val accuracy: 94.32857142857142\n",
      "Iteration: 29\n",
      "Train accuracy: 96.12698412698413\n",
      "Val accuracy: 94.35714285714286\n",
      "Iteration: 30\n",
      "Train accuracy: 96.19047619047619\n",
      "Val accuracy: 94.41428571428571\n",
      "Iteration: 31\n",
      "Train accuracy: 96.24603174603175\n",
      "Val accuracy: 94.44285714285714\n",
      "Iteration: 32\n",
      "Train accuracy: 96.30952380952381\n",
      "Val accuracy: 94.51428571428572\n",
      "Iteration: 33\n",
      "Train accuracy: 96.4031746031746\n",
      "Val accuracy: 94.57142857142857\n",
      "Iteration: 34\n",
      "Train accuracy: 96.46190476190476\n",
      "Val accuracy: 94.58571428571429\n",
      "Iteration: 35\n",
      "Train accuracy: 96.51428571428572\n",
      "Val accuracy: 94.58571428571429\n",
      "Iteration: 36\n",
      "Train accuracy: 96.5920634920635\n",
      "Val accuracy: 94.58571428571429\n",
      "Iteration: 37\n",
      "Train accuracy: 96.69047619047619\n",
      "Val accuracy: 94.67142857142858\n",
      "Iteration: 38\n",
      "Train accuracy: 96.75873015873016\n",
      "Val accuracy: 94.61428571428571\n",
      "Iteration: 39\n",
      "Train accuracy: 96.81269841269841\n",
      "Val accuracy: 94.67142857142858\n",
      "Iteration: 40\n",
      "Train accuracy: 96.86031746031746\n",
      "Val accuracy: 94.72857142857143\n",
      "Iteration: 41\n",
      "Train accuracy: 96.9031746031746\n",
      "Val accuracy: 94.72857142857143\n",
      "Iteration: 42\n",
      "Train accuracy: 96.94603174603175\n",
      "Val accuracy: 94.81428571428572\n",
      "Iteration: 43\n",
      "Train accuracy: 97.01746031746032\n",
      "Val accuracy: 94.82857142857142\n",
      "Iteration: 44\n",
      "Train accuracy: 97.05238095238096\n",
      "Val accuracy: 94.84285714285714\n",
      "Iteration: 45\n",
      "Train accuracy: 97.0920634920635\n",
      "Val accuracy: 94.95714285714286\n",
      "Iteration: 46\n",
      "Train accuracy: 97.14761904761905\n",
      "Val accuracy: 94.91428571428571\n",
      "Iteration: 47\n",
      "Train accuracy: 97.18730158730159\n",
      "Val accuracy: 94.92857142857143\n",
      "Iteration: 48\n",
      "Train accuracy: 97.23650793650793\n",
      "Val accuracy: 94.92857142857143\n",
      "Iteration: 49\n",
      "Train accuracy: 97.28888888888889\n",
      "Val accuracy: 94.92857142857143\n",
      "Iteration: 50\n",
      "Train accuracy: 97.33809523809524\n",
      "Val accuracy: 94.94285714285714\n",
      "Iteration: 51\n",
      "Train accuracy: 97.3873015873016\n",
      "Val accuracy: 94.91428571428571\n",
      "Iteration: 52\n",
      "Train accuracy: 97.43015873015874\n",
      "Val accuracy: 94.92857142857143\n",
      "Iteration: 53\n",
      "Train accuracy: 97.48095238095237\n",
      "Val accuracy: 94.98571428571428\n",
      "Iteration: 54\n",
      "Train accuracy: 97.53492063492062\n",
      "Val accuracy: 95.0\n",
      "Iteration: 55\n",
      "Train accuracy: 97.55079365079365\n",
      "Val accuracy: 94.98571428571428\n",
      "Iteration: 56\n",
      "Train accuracy: 97.61269841269842\n",
      "Val accuracy: 95.04285714285714\n",
      "Iteration: 57\n",
      "Train accuracy: 97.65396825396824\n",
      "Val accuracy: 95.07142857142857\n",
      "Iteration: 58\n",
      "Train accuracy: 97.67619047619047\n",
      "Val accuracy: 95.08571428571429\n",
      "Iteration: 59\n",
      "Train accuracy: 97.71904761904761\n",
      "Val accuracy: 95.1\n",
      "Iteration: 60\n",
      "Train accuracy: 97.77301587301588\n",
      "Val accuracy: 95.12857142857143\n",
      "Iteration: 61\n",
      "Train accuracy: 97.79841269841269\n",
      "Val accuracy: 95.19999999999999\n",
      "Iteration: 62\n",
      "Train accuracy: 97.82222222222222\n",
      "Val accuracy: 95.21428571428572\n",
      "Iteration: 63\n",
      "Train accuracy: 97.84285714285714\n",
      "Val accuracy: 95.31428571428572\n",
      "Iteration: 64\n",
      "Train accuracy: 97.9015873015873\n",
      "Val accuracy: 95.3\n",
      "Iteration: 65\n",
      "Train accuracy: 97.92222222222222\n",
      "Val accuracy: 95.28571428571428\n",
      "Iteration: 66\n",
      "Train accuracy: 97.95873015873016\n",
      "Val accuracy: 95.27142857142857\n",
      "Iteration: 67\n",
      "Train accuracy: 98.0047619047619\n",
      "Val accuracy: 95.3\n",
      "Iteration: 68\n",
      "Train accuracy: 98.03015873015873\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 69\n",
      "Train accuracy: 98.05396825396825\n",
      "Val accuracy: 95.35714285714286\n",
      "Iteration: 70\n",
      "Train accuracy: 98.0873015873016\n",
      "Val accuracy: 95.37142857142857\n",
      "Iteration: 71\n",
      "Train accuracy: 98.11904761904762\n",
      "Val accuracy: 95.35714285714286\n",
      "Iteration: 72\n",
      "Train accuracy: 98.14761904761905\n",
      "Val accuracy: 95.32857142857142\n",
      "Iteration: 73\n",
      "Train accuracy: 98.17142857142858\n",
      "Val accuracy: 95.34285714285714\n",
      "Iteration: 74\n",
      "Train accuracy: 98.2063492063492\n",
      "Val accuracy: 95.37142857142857\n",
      "Iteration: 75\n",
      "Train accuracy: 98.23650793650793\n",
      "Val accuracy: 95.39999999999999\n",
      "Iteration: 76\n",
      "Train accuracy: 98.26507936507937\n",
      "Val accuracy: 95.42857142857143\n",
      "Iteration: 77\n",
      "Train accuracy: 98.28412698412698\n",
      "Val accuracy: 95.44285714285714\n",
      "Iteration: 78\n",
      "Train accuracy: 98.31111111111112\n",
      "Val accuracy: 95.5\n",
      "Iteration: 79\n",
      "Train accuracy: 98.33968253968254\n",
      "Val accuracy: 95.48571428571428\n",
      "Iteration: 80\n",
      "Train accuracy: 98.37619047619047\n",
      "Val accuracy: 95.47142857142858\n",
      "Iteration: 81\n",
      "Train accuracy: 98.3920634920635\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 82\n",
      "Train accuracy: 98.41428571428571\n",
      "Val accuracy: 95.5\n",
      "Iteration: 83\n",
      "Train accuracy: 98.43968253968254\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 84\n",
      "Train accuracy: 98.46349206349207\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 85\n",
      "Train accuracy: 98.48253968253968\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 86\n",
      "Train accuracy: 98.5\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 87\n",
      "Train accuracy: 98.4984126984127\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 88\n",
      "Train accuracy: 98.51746031746032\n",
      "Val accuracy: 95.48571428571428\n",
      "Iteration: 89\n",
      "Train accuracy: 98.52539682539683\n",
      "Val accuracy: 95.47142857142858\n",
      "Iteration: 90\n",
      "Train accuracy: 98.56031746031746\n",
      "Val accuracy: 95.47142857142858\n",
      "Iteration: 91\n",
      "Train accuracy: 98.6\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 92\n",
      "Train accuracy: 98.60793650793651\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 93\n",
      "Train accuracy: 98.62380952380953\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 94\n",
      "Train accuracy: 98.66190476190476\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 95\n",
      "Train accuracy: 98.68571428571428\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 96\n",
      "Train accuracy: 98.6984126984127\n",
      "Val accuracy: 95.65714285714286\n",
      "Iteration: 97\n",
      "Train accuracy: 98.71587301587302\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 98\n",
      "Train accuracy: 98.75079365079365\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 99\n",
      "Train accuracy: 98.76507936507937\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 100\n",
      "Train accuracy: 98.79365079365078\n",
      "Val accuracy: 95.6\n",
      "Iteration: 101\n",
      "Train accuracy: 98.82380952380953\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 102\n",
      "Train accuracy: 98.83809523809524\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 103\n",
      "Train accuracy: 98.85714285714286\n",
      "Val accuracy: 95.6\n",
      "Iteration: 104\n",
      "Train accuracy: 98.87142857142858\n",
      "Val accuracy: 95.64285714285714\n",
      "Iteration: 105\n",
      "Train accuracy: 98.8873015873016\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 106\n",
      "Train accuracy: 98.89682539682539\n",
      "Val accuracy: 95.6\n",
      "Iteration: 107\n",
      "Train accuracy: 98.91428571428571\n",
      "Val accuracy: 95.6\n",
      "Iteration: 108\n",
      "Train accuracy: 98.94761904761906\n",
      "Val accuracy: 95.6\n",
      "Iteration: 109\n",
      "Train accuracy: 98.94285714285715\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 110\n",
      "Train accuracy: 98.96190476190476\n",
      "Val accuracy: 95.6\n",
      "Iteration: 111\n",
      "Train accuracy: 98.98412698412699\n",
      "Val accuracy: 95.6\n",
      "Iteration: 112\n",
      "Train accuracy: 98.9984126984127\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 113\n",
      "Train accuracy: 98.99365079365079\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 114\n",
      "Train accuracy: 99.02222222222223\n",
      "Val accuracy: 95.6\n",
      "Iteration: 115\n",
      "Train accuracy: 99.02698412698413\n",
      "Val accuracy: 95.6\n",
      "Iteration: 116\n",
      "Train accuracy: 99.04920634920636\n",
      "Val accuracy: 95.64285714285714\n",
      "Iteration: 117\n",
      "Train accuracy: 99.06031746031746\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 118\n",
      "Train accuracy: 99.07619047619048\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 119\n",
      "Train accuracy: 99.08412698412698\n",
      "Val accuracy: 95.6\n",
      "Iteration: 120\n",
      "Train accuracy: 99.08253968253969\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 121\n",
      "Train accuracy: 99.0984126984127\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 122\n",
      "Train accuracy: 99.0984126984127\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 123\n",
      "Train accuracy: 99.13174603174603\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 124\n",
      "Train accuracy: 99.14603174603175\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 125\n",
      "Train accuracy: 99.16031746031746\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 126\n",
      "Train accuracy: 99.17301587301587\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 127\n",
      "Train accuracy: 99.16666666666667\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 128\n",
      "Train accuracy: 99.18412698412699\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 129\n",
      "Train accuracy: 99.19206349206348\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 130\n",
      "Train accuracy: 99.19365079365079\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 131\n",
      "Train accuracy: 99.20476190476191\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 132\n",
      "Train accuracy: 99.21904761904761\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 133\n",
      "Train accuracy: 99.24126984126984\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 134\n",
      "Train accuracy: 99.25555555555555\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 135\n",
      "Train accuracy: 99.27936507936508\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 136\n",
      "Train accuracy: 99.29841269841269\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 137\n",
      "Train accuracy: 99.33015873015873\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 138\n",
      "Train accuracy: 99.34126984126983\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 139\n",
      "Train accuracy: 99.35238095238094\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 140\n",
      "Train accuracy: 99.36507936507937\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 141\n",
      "Train accuracy: 99.37619047619047\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 142\n",
      "Train accuracy: 99.37460317460317\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 143\n",
      "Train accuracy: 99.38095238095238\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 144\n",
      "Train accuracy: 99.4126984126984\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 145\n",
      "Train accuracy: 99.42857142857143\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 146\n",
      "Train accuracy: 99.43333333333332\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 147\n",
      "Train accuracy: 99.46507936507936\n",
      "Val accuracy: 95.5\n",
      "Iteration: 148\n",
      "Train accuracy: 99.45873015873016\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 149\n",
      "Train accuracy: 99.45873015873016\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 150\n",
      "Train accuracy: 99.47142857142856\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 151\n",
      "Train accuracy: 99.48253968253968\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 152\n",
      "Train accuracy: 99.4904761904762\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 153\n",
      "Train accuracy: 99.50634920634921\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 154\n",
      "Train accuracy: 99.5031746031746\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 155\n",
      "Train accuracy: 99.5142857142857\n",
      "Val accuracy: 95.64285714285714\n",
      "Iteration: 156\n",
      "Train accuracy: 99.52698412698413\n",
      "Val accuracy: 95.64285714285714\n",
      "Iteration: 157\n",
      "Train accuracy: 99.52222222222223\n",
      "Val accuracy: 95.65714285714286\n",
      "Iteration: 158\n",
      "Train accuracy: 99.53174603174602\n",
      "Val accuracy: 95.64285714285714\n",
      "Iteration: 159\n",
      "Train accuracy: 99.54444444444445\n",
      "Val accuracy: 95.65714285714286\n",
      "Iteration: 160\n",
      "Train accuracy: 99.55555555555556\n",
      "Val accuracy: 95.64285714285714\n",
      "Iteration: 161\n",
      "Train accuracy: 99.58095238095238\n",
      "Val accuracy: 95.6\n",
      "Iteration: 162\n",
      "Train accuracy: 99.5873015873016\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 163\n",
      "Train accuracy: 99.59206349206349\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 164\n",
      "Train accuracy: 99.5984126984127\n",
      "Val accuracy: 95.62857142857143\n",
      "Iteration: 165\n",
      "Train accuracy: 99.58095238095238\n",
      "Val accuracy: 95.64285714285714\n",
      "Iteration: 166\n",
      "Train accuracy: 99.58412698412698\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 167\n",
      "Train accuracy: 99.5904761904762\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 168\n",
      "Train accuracy: 99.58888888888889\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 169\n",
      "Train accuracy: 99.59523809523809\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 170\n",
      "Train accuracy: 99.59206349206349\n",
      "Val accuracy: 95.65714285714286\n",
      "Iteration: 171\n",
      "Train accuracy: 99.60634920634921\n",
      "Val accuracy: 95.65714285714286\n",
      "Iteration: 172\n",
      "Train accuracy: 99.615873015873\n",
      "Val accuracy: 95.6\n",
      "Iteration: 173\n",
      "Train accuracy: 99.62222222222222\n",
      "Val accuracy: 95.61428571428571\n",
      "Iteration: 174\n",
      "Train accuracy: 99.62857142857143\n",
      "Val accuracy: 95.6\n",
      "Iteration: 175\n",
      "Train accuracy: 99.64126984126985\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 176\n",
      "Train accuracy: 99.64285714285714\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 177\n",
      "Train accuracy: 99.64603174603175\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 178\n",
      "Train accuracy: 99.64603174603175\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 179\n",
      "Train accuracy: 99.64920634920635\n",
      "Val accuracy: 95.6\n",
      "Iteration: 180\n",
      "Train accuracy: 99.66031746031746\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 181\n",
      "Train accuracy: 99.64444444444445\n",
      "Val accuracy: 95.58571428571429\n",
      "Iteration: 182\n",
      "Train accuracy: 99.65238095238095\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 183\n",
      "Train accuracy: 99.65873015873015\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 184\n",
      "Train accuracy: 99.65714285714286\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 185\n",
      "Train accuracy: 99.66190476190476\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 186\n",
      "Train accuracy: 99.66984126984127\n",
      "Val accuracy: 95.5\n",
      "Iteration: 187\n",
      "Train accuracy: 99.68730158730159\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 188\n",
      "Train accuracy: 99.69682539682539\n",
      "Val accuracy: 95.5\n",
      "Iteration: 189\n",
      "Train accuracy: 99.69682539682539\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 190\n",
      "Train accuracy: 99.69682539682539\n",
      "Val accuracy: 95.5\n",
      "Iteration: 191\n",
      "Train accuracy: 99.70634920634922\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 192\n",
      "Train accuracy: 99.72380952380952\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 193\n",
      "Train accuracy: 99.72539682539683\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 194\n",
      "Train accuracy: 99.73333333333333\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 195\n",
      "Train accuracy: 99.73492063492063\n",
      "Val accuracy: 95.57142857142857\n",
      "Iteration: 196\n",
      "Train accuracy: 99.74603174603175\n",
      "Val accuracy: 95.52857142857142\n",
      "Iteration: 197\n",
      "Train accuracy: 99.75396825396825\n",
      "Val accuracy: 95.54285714285714\n",
      "Iteration: 198\n",
      "Train accuracy: 99.75714285714285\n",
      "Val accuracy: 95.51428571428572\n",
      "Iteration: 199\n",
      "Train accuracy: 99.76507936507937\n",
      "Val accuracy: 95.55714285714285\n",
      "Iteration: 200\n",
      "Train accuracy: 99.77142857142857\n",
      "Val accuracy: 95.54285714285714\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_bpVar, val_acc_bpVar, train_loss_bpVar, val_loss_bpVar, sum_weights_bpVar = batch_grad_descentFPOCBP(x_train,y_train,epochsToTrain , 0.1, mu, sigma, vDD, precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1b iter 99 : 11.111111111111115\n",
      "Train accuracy: 11.233333333333333\n",
      "Val accuracy: 11.428571428571429\n",
      "Iteration: 2b iter 99 : 11.111111111111115\n",
      "Train accuracy: 11.233333333333333\n",
      "Val accuracy: 11.428571428571429\n",
      "Iteration: 3b iter 99 : 11.111111111111115\n",
      "Train accuracy: 11.233333333333333\n",
      "Val accuracy: 11.428571428571429\n",
      "Iteration: 4b iter 99 : 11.111111111111115\n",
      "Train accuracy: 11.233333333333333\n",
      "Val accuracy: 11.428571428571429\n",
      "Iter 4 -> sub iter 69 : 12.222222222222221\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000044?line=0'>1</a>\u001b[0m W1, b1, W2, b2, W3, b3, train_acc_npVar, val_acc_npVar, train_loss_npVar, val_loss_npVar, sum_weights_npVar \u001b[39m=\u001b[39m batch_grad_descentFPOCNP(x_train,y_train,epochsToTrain , \u001b[39m0.1\u001b[39;49m, \u001b[39m0.000001\u001b[39;49m, mu, sigma, vDD, precision, print_op\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 17'\u001b[0m in \u001b[0;36mbatch_grad_descentFPOCNP\u001b[1;34m(X, Y, iter, lr, pert, mu, sigma, vDD, precision, print_op)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=323'>324</a>\u001b[0m   lossBeforePert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((A3\u001b[39m-\u001b[39mone_hot_encoding(Y1))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=325'>326</a>\u001b[0m   \u001b[39m#print(f\"Main iter: {i} Sub iter : {j}\\n\")\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=326'>327</a>\u001b[0m   \u001b[39m#dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=327'>328</a>\u001b[0m   dW1, db1, dW2, db2, dW3, db3 \u001b[39m=\u001b[39m NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=331'>332</a>\u001b[0m   W1, b1, W2, b2, W3, b3 \u001b[39m=\u001b[39m param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr \u001b[39m=\u001b[39m lr)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=333'>334</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m(print_op) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 17'\u001b[0m in \u001b[0;36mNP\u001b[1;34m(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=248'>249</a>\u001b[0m Z1pert \u001b[39m=\u001b[39m Z1\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=249'>250</a>\u001b[0m Z1pert[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pert\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=251'>252</a>\u001b[0m A1pert \u001b[39m=\u001b[39m relu(Z1pert)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=253'>254</a>\u001b[0m Z2pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2,A1pert) \u001b[39m+\u001b[39m b2 \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=254'>255</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 17'\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000037?line=48'>49</a>\u001b[0m    \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mmaximum(x,\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_npVar, val_acc_npVar, train_loss_npVar, val_loss_npVar, sum_weights_npVar = batch_grad_descentFPOCNP(x_train,y_train,epochsToTrain , 0.1, 0.000001, mu, sigma, vDD, precision, print_op=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22aa7a7dbd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.plot(train_acc_bpVar, \"c^\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP(software)\", \"BP(software)\", \"BP(with variability)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.5873015873016"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc_bpVar[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000049?line=0'>1</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
