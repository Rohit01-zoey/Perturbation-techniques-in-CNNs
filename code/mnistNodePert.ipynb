{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "#fetch the mnist dataset\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_n = x.to_numpy()\n",
    "x_n = x\n",
    "#y_n = y.to_numpy()\n",
    "y_n = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (7000, 784) (63000,) (7000,)\n",
      "(784, 63000) (784, 7000)\n"
     ]
    }
   ],
   "source": [
    "y_n = y_n.astype('int') #convert output to integers 0-9\n",
    "x_norm = x_n/255.0 #normalise input data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_norm, y_n, test_size=0.1, random_state=42) #split the data into train and validation\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "x_train = x_train.T #take the transpose of the training data m*784 -> 784*m\n",
    "x_val = x_val.T #take the transpose of the test data m*784 -> 784*m\n",
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to change with different number of layers\n",
    "def params_init():\n",
    "\n",
    "  #np.random.seed(2)\n",
    "  W1 = np.random.rand(200,784) - 0.5\n",
    "  b1 = np.random.rand(200,1) - 0.5\n",
    "  W2 = np.random.rand(50,200) - 0.5\n",
    "  b2 = np.random.rand(50,1) - 0.5\n",
    "  W3 = np.random.rand(10,50) - 0.5 \n",
    "  b3 = np.random.rand(10,1) - 0.5\n",
    "  #W4 = np.random.rand(50,200) - 0.5   \n",
    "  #b4 = np.random.rand(50,1) - 0.5    \n",
    "  #W5 = np.random.rand(10,50) - 0.5  \n",
    "  #b5 = np.random.rand(10,1) - 0.5    \n",
    "  print(\"Params Initialised\")\n",
    "\n",
    "  return (W1, b1, W2, b2, W3, b3)\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def forward(x_train, W1, b1, W2, b2, W3, b3):\n",
    "  #print(\"Entered FP\")\n",
    "  Z1 = np.matmul(W1,x_train) + b1 #W1 is 50*784, x_train is 748*m, Z1 is 50*m\n",
    "  A1 = relu(Z1)\n",
    "\n",
    "  Z2 = np.matmul(W2,A1) + b2 \n",
    "  A2 = relu(Z2)\n",
    "\n",
    "  Z3 = np.matmul(W3,A2) + b3\n",
    "  A3 = softmax(Z3)\n",
    "  \n",
    "  #Z4 = np.matmul(W4,A3) + b4\n",
    "  #A4 = relu(Z4)\n",
    "\n",
    "  #Z5 = np.matmul(W5,A4) + b5\n",
    "  #A5 = softmax(Z5)\n",
    "\n",
    "  #W2 is 10*50, A1 is 50*m\n",
    "  # print(np.exp(Z2))\n",
    "  # print(np.sum(np.exp(Z2)))\n",
    "\n",
    "  #A2 is 10*m, final predictions\n",
    "  # print(\"Fp Done\")\n",
    "\n",
    "  return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "   return np.maximum(x,0)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "  #return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "  Z = Z-np.max(Z, axis=0)\n",
    "  return np.exp(Z) / np.sum(np.exp(Z),0)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "  return x>0\n",
    "\n",
    "\n",
    "def one_hot_encoding(y):\n",
    "  shape = (y.shape[0], 10)\n",
    "  one_hot = np.zeros(shape)\n",
    "  rows = np.arange(y.size)\n",
    "  one_hot[rows, y] = 1\n",
    "  return one_hot.T\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, y):\n",
    "  # print(\"Entered Backprop\")\n",
    "  m = y.shape[0] #m is the number of training examples\n",
    "  Y = one_hot_encoding(y)\n",
    "\n",
    "  dZ3 = (A3 - Y)\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "  dZ2 = np.matmul(W3.T, dZ3)*relu_d(Z2) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  dZ1 = np.matmul(W2.T, dZ2)*relu_d(Z1) #W2 is 10*50, dZ2 = 10*m, dZ1 = 50*m\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr):\n",
    "\n",
    "  #updates the parameters based on backpropogation\n",
    "\n",
    "  W1 = W1 - lr*dW1\n",
    "  b1 = b1 - lr*(db1.reshape(b1.shape))\n",
    "  W2 = W2 - lr*dW2\n",
    "  b2 = b2 - lr*(db2.reshape(b2.shape))\n",
    "  W3 = W3 - lr*dW3\n",
    "  b3 = b3 - lr*(db3.reshape(b3.shape))\n",
    "  #W4 = W4 - lr*dW4\n",
    "  #b4 = b4 - lr*db4\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "\n",
    "#have to change with different number of layers\n",
    "def batch_grad_descent(X,Y,iter, lr, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "      dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X,Y,iter, lr, print_op, decay_factor=0):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  mu = 1\n",
    "  sigma = 0.4\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "    X1, Y1 = X.T, Y\n",
    "    X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "\n",
    "    dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "\n",
    "    W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr, factor = decay_factor)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'Iteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "def NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1):\n",
    "  #calculating dw3 and db3\n",
    "  #first approximate dZ3\n",
    "  m = Z3.shape[1]\n",
    "  lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "  for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z3\")\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "  \n",
    "  dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "  db3 = 1/m*np.sum(dZ3, axis=1)\n",
    "\n",
    "\n",
    "  #calculating the dZ2 and db2\n",
    "\n",
    "  lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "  for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z2\")\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  \n",
    "  dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "  dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n",
    "\n",
    "  #calculating the dZ1 and db1\n",
    "  lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "  for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    #print(\"Z1\")\n",
    "    #print(f\"sub sub in iter{i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "  dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "  dW1 = 1/m*np.matmul(dZ1,X1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "  db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1\n",
    "\n",
    "  return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "\n",
    "\n",
    "def batchGDNP(X,Y,iter, lr, pert, print_op=1):\n",
    "\n",
    "  # print(\"Entered Grad Descent\")\n",
    "  #performs minibatch grad descent for given iterations and learning rate\n",
    "  n = Y.shape[0]\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  sum_weights = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "\n",
    "  weightsDict = {}\n",
    "\n",
    "  W1, b1, W2, b2, W3, b3 = params_init()\n",
    "  #print(W1)\n",
    "  #gaussian_W1, gaussian_b1, gaussian_W2, gaussian_b2, gaussian_W3, gaussian_b3, gaussian_W4, gaussian_b4, gaussian_W5, gaussian_b5 = Gaussian_init (mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "  weightsDict[\"W1\"] = W1\n",
    "  weightsDict[\"b1\"] = b1\n",
    "  weightsDict[\"W2\"] = W2\n",
    "  weightsDict[\"b2\"] = b2\n",
    "  weightsDict[\"W3\"] = W3\n",
    "  weightsDict[\"b3\"] = b3\n",
    "\n",
    "  for i in range(iter): #loop over \n",
    "    train_loss_score = 0\n",
    "    val_loss_score = 0\n",
    "\n",
    "\n",
    "    for j in range(100): #loop over batches\n",
    "      # print(\"Entered for loops in grad descent\")\n",
    "      #total training samples = 63000, batch size = 630\n",
    "      X1, Y1 = shuffle(X[:, j*630: (j+1)*630].T,Y[j*630: (j+1)*630]) #shuffle each batch\n",
    "      X1 = X1.T #take transpose to match the sizes\n",
    "\n",
    "      Z1, A1, Z2, A2, Z3, A3 = forward(X1, W1, b1, W2, b2, W3, b3) \n",
    "      print(f\"Iter {i} -> sub iter {j} : {accuracy(predictions(A3), Y1)}\", end = \"\\r\", flush = True)\n",
    "      lossBeforePert = np.sum((A3-one_hot_encoding(Y1))**2, axis=0)\n",
    "\n",
    "      #print(f\"Main iter: {i} Sub iter : {j}\\n\")\n",
    "      #dW1, db1, dW2, db2, dW3, db3 = backprop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X1, Y1)\n",
    "      dW1, db1, dW2, db2, dW3, db3 = NP(pert, lossBeforePert, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, b1, b2, b3, X1, Y1)\n",
    "      #print(f\"iter in iter{j}\")\n",
    "      W1, b1, W2, b2, W3, b3 = param_update(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, lr = lr)\n",
    "      #print(W1)\n",
    "\n",
    "    if (i+1)%(print_op) == 0:\n",
    "      print(f'\\nIteration: {i + 1}')\n",
    "\n",
    "      #obtain training loss\n",
    "      _, _, _, _, _, A3_train = forward(X, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, Y.shape[0]):\n",
    "       # train_loss_score = train_loss_score + (-1*(np.log(A5_train[Y[i], i])))\n",
    "      #train_loss.append(train_loss_score)\n",
    "      #print(f'Train Loss: {train_loss_score}')\n",
    "\n",
    "      #obtain training accuracy\n",
    "      train_score = accuracy(predictions(A3_train), Y)\n",
    "      train_acc.append(train_score)\n",
    "      print(f'Train accuracy: {train_score}')\n",
    "\n",
    "      ##obtain validation loss\n",
    "      _, _, _, _, _,  A3_val = forward(x_val, W1, b1, W2, b2, W3, b3)\n",
    "      #for i in range(0, y_val.shape[0]):\n",
    "       # val_loss_score = val_loss_score + (-1*(np.log(A5_val[y_val[i], i]))) \n",
    "      #val_loss.append(val_loss_score)\n",
    "      #print(f'Validation Loss: {val_loss_score}')\n",
    "\n",
    "      ##obtain validation accuracy\n",
    "      val_score = accuracy(predictions(A3_val), y_val)\n",
    "      val_acc.append(val_score)\n",
    "      print(f'Val accuracy: {val_score}')\n",
    "\n",
    "      #obtain the sum of weights and append to the sum array\n",
    "      #sum_w = np.sum(abs(W1)) + np.sum(abs(W2)) + np.sum(abs(W3)) + np.sum(abs(W4)) + np.sum(abs(W5)) + np.sum(abs(b1)) + np.sum(abs(b2)) + np.sum(abs(b3))  + np.sum(abs(b4)) + np.sum(abs(b5))\n",
    "      #sum_weights.append(sum_w)\n",
    "      #print(f'Sum of weights: {sum_w}')\n",
    "\n",
    "  return W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predictions(A):\n",
    "  #argmax returns the index of maximum value, we will feed the sigmoid output to this function \n",
    "  return np.argmax(A,0)\n",
    "\n",
    "\n",
    "def accuracy(A,Y):\n",
    "  #this will compare the predicted output to the ground truth\n",
    "  return np.sum(A == Y)/(Y.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsToTrain = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1b iter 99 : 17.619047619047624\n",
      "Train accuracy: 17.08730158730159\n",
      "Val accuracy: 17.4\n",
      "Iteration: 2b iter 99 : 21.269841269841275\n",
      "Train accuracy: 20.144444444444446\n",
      "Val accuracy: 20.65714285714286\n",
      "Iteration: 3b iter 99 : 23.492063492063492\n",
      "Train accuracy: 22.306349206349207\n",
      "Val accuracy: 22.642857142857142\n",
      "Iteration: 4b iter 99 : 26.031746031746035\n",
      "Train accuracy: 24.312698412698413\n",
      "Val accuracy: 24.442857142857143\n",
      "Iteration: 5b iter 99 : 31.587301587301585\n",
      "Train accuracy: 30.160317460317458\n",
      "Val accuracy: 30.685714285714287\n",
      "Iteration: 6b iter 99 : 34.761904761904763\n",
      "Train accuracy: 34.26825396825397\n",
      "Val accuracy: 34.85714285714286\n",
      "Iteration: 7b iter 99 : 40.634920634920635\n",
      "Train accuracy: 39.51269841269841\n",
      "Val accuracy: 40.24285714285714\n",
      "Iteration: 8b iter 99 : 43.015873015873026\n",
      "Train accuracy: 43.67777777777778\n",
      "Val accuracy: 44.471428571428575\n",
      "Iteration: 9b iter 99 : 46.349206349206354\n",
      "Train accuracy: 46.52222222222222\n",
      "Val accuracy: 46.84285714285714\n",
      "Iteration: 10 iter 99 : 52.380952380952394\n",
      "Train accuracy: 51.48888888888889\n",
      "Val accuracy: 51.68571428571429\n",
      "Iteration: 11b iter 99 : 54.920634920634924\n",
      "Train accuracy: 54.4968253968254\n",
      "Val accuracy: 54.45714285714286\n",
      "Iteration: 12b iter 99 : 58.730158730158735\n",
      "Train accuracy: 58.68095238095238\n",
      "Val accuracy: 58.81428571428572\n",
      "Iteration: 13b iter 99 : 62.063492063492064\n",
      "Train accuracy: 61.974603174603175\n",
      "Val accuracy: 62.142857142857146\n",
      "Iteration: 14b iter 99 : 64.444444444444444\n",
      "Train accuracy: 63.817460317460316\n",
      "Val accuracy: 64.3\n",
      "Iteration: 15b iter 99 : 65.079365079365086\n",
      "Train accuracy: 64.95714285714286\n",
      "Val accuracy: 65.38571428571429\n",
      "Iteration: 16b iter 99 : 65.238095238095245\n",
      "Train accuracy: 65.84285714285714\n",
      "Val accuracy: 66.07142857142857\n",
      "Iteration: 17b iter 99 : 66.190476190476195\n",
      "Train accuracy: 66.54920634920634\n",
      "Val accuracy: 66.77142857142857\n",
      "Iteration: 18b iter 99 : 67.777777777777794\n",
      "Train accuracy: 67.06507936507936\n",
      "Val accuracy: 67.31428571428572\n",
      "Iteration: 19b iter 99 : 68.73015873015873\n",
      "Train accuracy: 67.55714285714286\n",
      "Val accuracy: 67.77142857142857\n",
      "Iteration: 20b iter 99 : 69.20634920634921\n",
      "Train accuracy: 67.97619047619048\n",
      "Val accuracy: 68.12857142857143\n",
      "Iteration: 21b iter 99 : 69.20634920634922\n",
      "Train accuracy: 68.36031746031746\n",
      "Val accuracy: 68.45714285714286\n",
      "Iteration: 22b iter 99 : 69.84126984126983\n",
      "Train accuracy: 68.6968253968254\n",
      "Val accuracy: 68.7\n",
      "Iteration: 23b iter 99 : 69.84126984126983\n",
      "Train accuracy: 68.96507936507936\n",
      "Val accuracy: 68.94285714285714\n",
      "Iteration: 24b iter 99 : 69.84126984126983\n",
      "Train accuracy: 69.21587301587302\n",
      "Val accuracy: 69.27142857142857\n",
      "Iteration: 25b iter 99 : 70.06031746031747\n",
      "Train accuracy: 69.42857142857143\n",
      "Val accuracy: 69.48571428571428\n",
      "Iteration: 26b iter 99 : 70.15873015873015\n",
      "Train accuracy: 69.64603174603174\n",
      "Val accuracy: 69.65714285714286\n",
      "Iteration: 27b iter 99 : 70.15873015873015\n",
      "Train accuracy: 69.83809523809525\n",
      "Val accuracy: 69.85714285714286\n",
      "Iteration: 28b iter 99 : 70.31746031746032\n",
      "Train accuracy: 70.0079365079365\n",
      "Val accuracy: 70.08571428571429\n",
      "Iteration: 29b iter 99 : 70.31746031746032\n",
      "Train accuracy: 70.2063492063492\n",
      "Val accuracy: 70.22857142857143\n",
      "Iteration: 30b iter 99 : 70.15873015873015\n",
      "Train accuracy: 70.37619047619049\n",
      "Val accuracy: 70.48571428571428\n",
      "Iteration: 31b iter 99 : 70.05396825396825\n",
      "Train accuracy: 70.5031746031746\n",
      "Val accuracy: 70.61428571428571\n",
      "Iteration: 32b iter 99 : 70.31746031746032\n",
      "Train accuracy: 70.63650793650794\n",
      "Val accuracy: 70.71428571428572\n",
      "Iteration: 33b iter 99 : 70.95238095238095\n",
      "Train accuracy: 70.81746031746032\n",
      "Val accuracy: 70.85714285714285\n",
      "Iteration: 34b iter 99 : 72.22222222222221\n",
      "Train accuracy: 71.41269841269842\n",
      "Val accuracy: 71.58571428571429\n",
      "Iteration: 35b iter 99 : 74.28571428571429\n",
      "Train accuracy: 72.56666666666666\n",
      "Val accuracy: 72.5142857142857\n",
      "Iteration: 36b iter 99 : 74.76190476190476\n",
      "Train accuracy: 73.58571428571429\n",
      "Val accuracy: 73.75714285714285\n",
      "Iteration: 37b iter 99 : 75.87301587301587\n",
      "Train accuracy: 74.42063492063492\n",
      "Val accuracy: 74.41428571428571\n",
      "Iteration: 38b iter 99 : 76.98412698412699\n",
      "Train accuracy: 75.10793650793651\n",
      "Val accuracy: 75.21428571428571\n",
      "Iteration: 39b iter 99 : 77.77777777777779\n",
      "Train accuracy: 75.69365079365079\n",
      "Val accuracy: 75.72857142857143\n",
      "Iteration: 40b iter 99 : 78.88888888888889\n",
      "Train accuracy: 76.15079365079364\n",
      "Val accuracy: 76.18571428571428\n",
      "Iteration: 41b iter 99 : 79.36507936507937\n",
      "Train accuracy: 76.57619047619048\n",
      "Val accuracy: 76.54285714285714\n",
      "Iteration: 42b iter 99 : 79.68253968253968\n",
      "Train accuracy: 76.92698412698412\n",
      "Val accuracy: 76.95714285714286\n",
      "Iteration: 43b iter 99 : 79.84126984126985\n",
      "Train accuracy: 77.23174603174603\n",
      "Val accuracy: 77.25714285714285\n",
      "Iteration: 44b iter 99 : 80.09523809523819\n",
      "Train accuracy: 77.50317460317461\n",
      "Val accuracy: 77.68571428571428\n",
      "Iteration: 45b iter 99 : 80.31746031746032\n",
      "Train accuracy: 77.78412698412698\n",
      "Val accuracy: 77.92857142857143\n",
      "Iteration: 46b iter 99 : 80.47619047619048\n",
      "Train accuracy: 78.04761904761904\n",
      "Val accuracy: 78.08571428571427\n",
      "Iteration: 47b iter 99 : 80.63492063492063\n",
      "Train accuracy: 78.26984126984127\n",
      "Val accuracy: 78.25714285714285\n",
      "Iteration: 48b iter 99 : 80.63492063492063\n",
      "Train accuracy: 78.51269841269841\n",
      "Val accuracy: 78.4\n",
      "Iteration: 49b iter 99 : 80.79365079365089\n",
      "Train accuracy: 78.72063492063492\n",
      "Val accuracy: 78.62857142857142\n",
      "Iteration: 50b iter 99 : 80.79365079365085\n",
      "Train accuracy: 78.91904761904762\n",
      "Val accuracy: 78.81428571428572\n",
      "Iteration: 51b iter 99 : 80.95238095238095\n",
      "Train accuracy: 79.15396825396826\n",
      "Val accuracy: 79.04285714285714\n",
      "Iteration: 52b iter 99 : 81.11111111111111\n",
      "Train accuracy: 79.41587301587302\n",
      "Val accuracy: 79.3\n",
      "Iteration: 53b iter 99 : 81.42857142857143\n",
      "Train accuracy: 79.65555555555555\n",
      "Val accuracy: 79.42857142857143\n",
      "Iteration: 54b iter 99 : 82.22222222222221\n",
      "Train accuracy: 79.9952380952381\n",
      "Val accuracy: 79.74285714285713\n",
      "Iteration: 55b iter 99 : 82.06349206349206\n",
      "Train accuracy: 80.33809523809524\n",
      "Val accuracy: 80.05714285714286\n",
      "Iteration: 56b iter 99 : 82.06349206349206\n",
      "Train accuracy: 80.73809523809524\n",
      "Val accuracy: 80.5\n",
      "Iteration: 57b iter 99 : 82.06349206349206\n",
      "Train accuracy: 81.37777777777778\n",
      "Val accuracy: 81.07142857142857\n",
      "Iteration: 58b iter 99 : 82.53968253968253\n",
      "Train accuracy: 82.16190476190476\n",
      "Val accuracy: 81.95714285714286\n",
      "Iteration: 59b iter 99 : 83.17460317460318\n",
      "Train accuracy: 83.15238095238095\n",
      "Val accuracy: 83.0\n",
      "Iteration: 60b iter 99 : 84.92063492063492\n",
      "Train accuracy: 84.06031746031746\n",
      "Val accuracy: 83.84285714285714\n",
      "Iteration: 61b iter 99 : 85.23809523809524\n",
      "Train accuracy: 84.85396825396826\n",
      "Val accuracy: 84.58571428571429\n",
      "Iteration: 62b iter 99 : 85.23809523809524\n",
      "Train accuracy: 85.43809523809523\n",
      "Val accuracy: 85.08571428571429\n",
      "Iteration: 63b iter 99 : 85.55555555555556\n",
      "Train accuracy: 85.87619047619047\n",
      "Val accuracy: 85.55714285714285\n",
      "Iteration: 64b iter 99 : 86.03174603174604\n",
      "Train accuracy: 86.26984126984128\n",
      "Val accuracy: 85.82857142857144\n",
      "Iteration: 65b iter 99 : 86.03174603174604\n",
      "Train accuracy: 86.55555555555556\n",
      "Val accuracy: 86.07142857142858\n",
      "Iteration: 66b iter 99 : 86.50793650793658\n",
      "Train accuracy: 86.78888888888889\n",
      "Val accuracy: 86.34285714285714\n",
      "Iteration: 67b iter 99 : 86.66666666666667\n",
      "Train accuracy: 87.03492063492064\n",
      "Val accuracy: 86.58571428571429\n",
      "Iteration: 68b iter 99 : 87.14285714285714\n",
      "Train accuracy: 87.23333333333333\n",
      "Val accuracy: 86.77142857142857\n",
      "Iteration: 69b iter 99 : 87.30158730158738\n",
      "Train accuracy: 87.41428571428571\n",
      "Val accuracy: 86.94285714285715\n",
      "Iteration: 70b iter 99 : 87.46031746031746\n",
      "Train accuracy: 87.57936507936508\n",
      "Val accuracy: 87.08571428571429\n",
      "Iteration: 71b iter 99 : 87.61904761904762\n",
      "Train accuracy: 87.73650793650793\n",
      "Val accuracy: 87.21428571428571\n",
      "Iteration: 72b iter 99 : 87.61904761904762\n",
      "Train accuracy: 87.9015873015873\n",
      "Val accuracy: 87.32857142857144\n",
      "Iteration: 73b iter 99 : 87.93650793650794\n",
      "Train accuracy: 88.02698412698413\n",
      "Val accuracy: 87.47142857142856\n",
      "Iteration: 74b iter 99 : 88.41269841269849\n",
      "Train accuracy: 88.15555555555555\n",
      "Val accuracy: 87.64285714285714\n",
      "Iteration: 75b iter 99 : 88.41269841269842\n",
      "Train accuracy: 88.27460317460317\n",
      "Val accuracy: 87.74285714285715\n",
      "Iteration: 76b iter 99 : 88.41269841269842\n",
      "Train accuracy: 88.40476190476191\n",
      "Val accuracy: 87.84285714285714\n",
      "Iteration: 77b iter 99 : 88.73015873015872\n",
      "Train accuracy: 88.52380952380953\n",
      "Val accuracy: 87.88571428571429\n",
      "Iteration: 78b iter 99 : 88.73015873015872\n",
      "Train accuracy: 88.64761904761906\n",
      "Val accuracy: 88.08571428571429\n",
      "Iteration: 79b iter 99 : 88.88888888888889\n",
      "Train accuracy: 88.76190476190476\n",
      "Val accuracy: 88.15714285714286\n",
      "Iteration: 80b iter 99 : 88.88888888888889\n",
      "Train accuracy: 88.87936507936509\n",
      "Val accuracy: 88.27142857142857\n",
      "Iteration: 81b iter 99 : 88.88888888888889\n",
      "Train accuracy: 88.9857142857143\n",
      "Val accuracy: 88.37142857142857\n",
      "Iteration: 82b iter 99 : 89.04761904761904\n",
      "Train accuracy: 89.08095238095238\n",
      "Val accuracy: 88.5142857142857\n",
      "Iteration: 83b iter 99 : 89.20634920634922\n",
      "Train accuracy: 89.20476190476191\n",
      "Val accuracy: 88.58571428571429\n",
      "Iteration: 84b iter 99 : 89.20634920634922\n",
      "Train accuracy: 89.3111111111111\n",
      "Val accuracy: 88.7\n",
      "Iteration: 85b iter 99 : 89.20634920634922\n",
      "Train accuracy: 89.42539682539683\n",
      "Val accuracy: 88.8\n",
      "Iteration: 86b iter 99 : 89.20634920634922\n",
      "Train accuracy: 89.51746031746032\n",
      "Val accuracy: 88.84285714285714\n",
      "Iteration: 87b iter 99 : 89.68253968253968\n",
      "Train accuracy: 89.59047619047618\n",
      "Val accuracy: 88.87142857142857\n",
      "Iteration: 88b iter 99 : 90.05238095238095\n",
      "Train accuracy: 89.67301587301587\n",
      "Val accuracy: 88.9857142857143\n",
      "Iteration: 89b iter 99 : 90.06984126984127\n",
      "Train accuracy: 89.74444444444445\n",
      "Val accuracy: 89.0142857142857\n",
      "Iteration: 90b iter 99 : 90.15873015873017\n",
      "Train accuracy: 89.83174603174604\n",
      "Val accuracy: 89.05714285714285\n",
      "Iteration: 91b iter 99 : 90.31746031746032\n",
      "Train accuracy: 89.91111111111111\n",
      "Val accuracy: 89.15714285714286\n",
      "Iteration: 92b iter 99 : 90.47619047619048\n",
      "Train accuracy: 89.9952380952381\n",
      "Val accuracy: 89.27142857142857\n",
      "Iteration: 93b iter 99 : 90.63492063492063\n",
      "Train accuracy: 90.06507936507937\n",
      "Val accuracy: 89.37142857142857\n",
      "Iteration: 94b iter 99 : 90.63492063492063\n",
      "Train accuracy: 90.14285714285715\n",
      "Val accuracy: 89.45714285714286\n",
      "Iteration: 95b iter 99 : 90.79365079365088\n",
      "Train accuracy: 90.23650793650793\n",
      "Val accuracy: 89.51428571428572\n",
      "Iteration: 96b iter 99 : 90.95238095238095\n",
      "Train accuracy: 90.29206349206349\n",
      "Val accuracy: 89.58571428571429\n",
      "Iteration: 97b iter 99 : 90.95238095238095\n",
      "Train accuracy: 90.38888888888889\n",
      "Val accuracy: 89.64285714285715\n",
      "Iteration: 98b iter 99 : 91.11111111111111\n",
      "Train accuracy: 90.44444444444444\n",
      "Val accuracy: 89.6857142857143\n",
      "Iteration: 99b iter 99 : 91.11111111111111\n",
      "Train accuracy: 90.5079365079365\n",
      "Val accuracy: 89.75714285714285\n",
      "Iteration: 100 iter 99 : 91.26984126984127\n",
      "Train accuracy: 90.55238095238096\n",
      "Val accuracy: 89.8\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc, val_acc, train_loss, val_loss, sum_weights = batchGDNP(x_train,y_train,epochsToTrain, 0.01, 0.01, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n",
      "Iteration: 1\n",
      "Train accuracy: 60.57142857142858\n",
      "Val accuracy: 59.65714285714285\n",
      "Iteration: 2\n",
      "Train accuracy: 70.14285714285714\n",
      "Val accuracy: 68.72857142857143\n",
      "Iteration: 3\n",
      "Train accuracy: 74.4920634920635\n",
      "Val accuracy: 73.72857142857143\n",
      "Iteration: 4\n",
      "Train accuracy: 77.22063492063492\n",
      "Val accuracy: 76.67142857142856\n",
      "Iteration: 5\n",
      "Train accuracy: 79.03333333333333\n",
      "Val accuracy: 78.37142857142857\n",
      "Iteration: 6\n",
      "Train accuracy: 80.48730158730159\n",
      "Val accuracy: 79.82857142857142\n",
      "Iteration: 7\n",
      "Train accuracy: 81.65238095238095\n",
      "Val accuracy: 80.94285714285714\n",
      "Iteration: 8\n",
      "Train accuracy: 82.57460317460318\n",
      "Val accuracy: 81.65714285714286\n",
      "Iteration: 9\n",
      "Train accuracy: 83.30634920634921\n",
      "Val accuracy: 82.54285714285714\n",
      "Iteration: 10\n",
      "Train accuracy: 84.01111111111112\n",
      "Val accuracy: 83.2\n",
      "Iteration: 11\n",
      "Train accuracy: 84.6\n",
      "Val accuracy: 83.85714285714285\n",
      "Iteration: 12\n",
      "Train accuracy: 85.0920634920635\n",
      "Val accuracy: 84.35714285714285\n",
      "Iteration: 13\n",
      "Train accuracy: 85.5079365079365\n",
      "Val accuracy: 84.72857142857143\n",
      "Iteration: 14\n",
      "Train accuracy: 85.89365079365079\n",
      "Val accuracy: 85.15714285714286\n",
      "Iteration: 15\n",
      "Train accuracy: 86.2904761904762\n",
      "Val accuracy: 85.68571428571428\n",
      "Iteration: 16\n",
      "Train accuracy: 86.6015873015873\n",
      "Val accuracy: 85.9857142857143\n",
      "Iteration: 17\n",
      "Train accuracy: 86.9063492063492\n",
      "Val accuracy: 86.3\n",
      "Iteration: 18\n",
      "Train accuracy: 87.18412698412699\n",
      "Val accuracy: 86.71428571428571\n",
      "Iteration: 19\n",
      "Train accuracy: 87.45238095238095\n",
      "Val accuracy: 86.9857142857143\n",
      "Iteration: 20\n",
      "Train accuracy: 87.68730158730159\n",
      "Val accuracy: 87.22857142857143\n",
      "Iteration: 21\n",
      "Train accuracy: 87.93492063492063\n",
      "Val accuracy: 87.44285714285715\n",
      "Iteration: 22\n",
      "Train accuracy: 88.12063492063493\n",
      "Val accuracy: 87.75714285714285\n",
      "Iteration: 23\n",
      "Train accuracy: 88.30793650793652\n",
      "Val accuracy: 87.94285714285715\n",
      "Iteration: 24\n",
      "Train accuracy: 88.45714285714286\n",
      "Val accuracy: 88.17142857142856\n",
      "Iteration: 25\n",
      "Train accuracy: 88.63015873015873\n",
      "Val accuracy: 88.41428571428571\n",
      "Iteration: 26\n",
      "Train accuracy: 88.8047619047619\n",
      "Val accuracy: 88.55714285714285\n",
      "Iteration: 27\n",
      "Train accuracy: 88.96349206349207\n",
      "Val accuracy: 88.74285714285715\n",
      "Iteration: 28\n",
      "Train accuracy: 89.14126984126985\n",
      "Val accuracy: 88.94285714285715\n",
      "Iteration: 29\n",
      "Train accuracy: 89.28095238095239\n",
      "Val accuracy: 88.9857142857143\n",
      "Iteration: 30\n",
      "Train accuracy: 89.42063492063492\n",
      "Val accuracy: 89.17142857142856\n",
      "Iteration: 31\n",
      "Train accuracy: 89.57142857142857\n",
      "Val accuracy: 89.34285714285714\n",
      "Iteration: 32\n",
      "Train accuracy: 89.71428571428571\n",
      "Val accuracy: 89.54285714285714\n",
      "Iteration: 33\n",
      "Train accuracy: 89.83650793650794\n",
      "Val accuracy: 89.64285714285715\n",
      "Iteration: 34\n",
      "Train accuracy: 89.94126984126984\n",
      "Val accuracy: 89.71428571428571\n",
      "Iteration: 35\n",
      "Train accuracy: 90.06031746031746\n",
      "Val accuracy: 89.82857142857142\n",
      "Iteration: 36\n",
      "Train accuracy: 90.1920634920635\n",
      "Val accuracy: 89.9\n",
      "Iteration: 37\n",
      "Train accuracy: 90.32857142857142\n",
      "Val accuracy: 90.02857142857142\n",
      "Iteration: 38\n",
      "Train accuracy: 90.43492063492063\n",
      "Val accuracy: 90.12857142857142\n",
      "Iteration: 39\n",
      "Train accuracy: 90.53809523809524\n",
      "Val accuracy: 90.27142857142857\n",
      "Iteration: 40\n",
      "Train accuracy: 90.62539682539682\n",
      "Val accuracy: 90.37142857142857\n",
      "Iteration: 41\n",
      "Train accuracy: 90.6984126984127\n",
      "Val accuracy: 90.45714285714286\n",
      "Iteration: 42\n",
      "Train accuracy: 90.79047619047618\n",
      "Val accuracy: 90.65714285714286\n",
      "Iteration: 43\n",
      "Train accuracy: 90.86666666666666\n",
      "Val accuracy: 90.74285714285715\n",
      "Iteration: 44\n",
      "Train accuracy: 90.96190476190476\n",
      "Val accuracy: 90.8\n",
      "Iteration: 45\n",
      "Train accuracy: 91.03650793650793\n",
      "Val accuracy: 90.9\n",
      "Iteration: 46\n",
      "Train accuracy: 91.13174603174603\n",
      "Val accuracy: 91.02857142857142\n",
      "Iteration: 47\n",
      "Train accuracy: 91.2063492063492\n",
      "Val accuracy: 91.08571428571427\n",
      "Iteration: 48\n",
      "Train accuracy: 91.28571428571428\n",
      "Val accuracy: 91.2\n",
      "Iteration: 49\n",
      "Train accuracy: 91.35238095238095\n",
      "Val accuracy: 91.28571428571428\n",
      "Iteration: 50\n",
      "Train accuracy: 91.42380952380952\n",
      "Val accuracy: 91.3\n",
      "Iteration: 51\n",
      "Train accuracy: 91.48571428571428\n",
      "Val accuracy: 91.34285714285714\n",
      "Iteration: 52\n",
      "Train accuracy: 91.54444444444444\n",
      "Val accuracy: 91.35714285714286\n",
      "Iteration: 53\n",
      "Train accuracy: 91.60317460317461\n",
      "Val accuracy: 91.45714285714286\n",
      "Iteration: 54\n",
      "Train accuracy: 91.67460317460318\n",
      "Val accuracy: 91.5\n",
      "Iteration: 55\n",
      "Train accuracy: 91.74444444444444\n",
      "Val accuracy: 91.57142857142857\n",
      "Iteration: 56\n",
      "Train accuracy: 91.83015873015873\n",
      "Val accuracy: 91.62857142857142\n",
      "Iteration: 57\n",
      "Train accuracy: 91.88730158730158\n",
      "Val accuracy: 91.67142857142856\n",
      "Iteration: 58\n",
      "Train accuracy: 91.93809523809524\n",
      "Val accuracy: 91.71428571428571\n",
      "Iteration: 59\n",
      "Train accuracy: 92.01587301587301\n",
      "Val accuracy: 91.75714285714285\n",
      "Iteration: 60\n",
      "Train accuracy: 92.06349206349206\n",
      "Val accuracy: 91.8\n",
      "Iteration: 61\n",
      "Train accuracy: 92.1126984126984\n",
      "Val accuracy: 91.84285714285714\n",
      "Iteration: 62\n",
      "Train accuracy: 92.16507936507936\n",
      "Val accuracy: 91.91428571428571\n",
      "Iteration: 63\n",
      "Train accuracy: 92.22222222222223\n",
      "Val accuracy: 91.9\n",
      "Iteration: 64\n",
      "Train accuracy: 92.26190476190477\n",
      "Val accuracy: 91.97142857142858\n",
      "Iteration: 65\n",
      "Train accuracy: 92.3063492063492\n",
      "Val accuracy: 92.02857142857142\n",
      "Iteration: 66\n",
      "Train accuracy: 92.36031746031746\n",
      "Val accuracy: 92.08571428571429\n",
      "Iteration: 67\n",
      "Train accuracy: 92.40317460317459\n",
      "Val accuracy: 92.08571428571429\n",
      "Iteration: 68\n",
      "Train accuracy: 92.44126984126984\n",
      "Val accuracy: 92.17142857142858\n",
      "Iteration: 69\n",
      "Train accuracy: 92.5015873015873\n",
      "Val accuracy: 92.22857142857143\n",
      "Iteration: 70\n",
      "Train accuracy: 92.56190476190476\n",
      "Val accuracy: 92.25714285714287\n",
      "Iteration: 71\n",
      "Train accuracy: 92.60634920634921\n",
      "Val accuracy: 92.31428571428572\n",
      "Iteration: 72\n",
      "Train accuracy: 92.67777777777778\n",
      "Val accuracy: 92.38571428571429\n",
      "Iteration: 73\n",
      "Train accuracy: 92.72380952380952\n",
      "Val accuracy: 92.42857142857143\n",
      "Iteration: 74\n",
      "Train accuracy: 92.76190476190476\n",
      "Val accuracy: 92.51428571428572\n",
      "Iteration: 75\n",
      "Train accuracy: 92.80158730158729\n",
      "Val accuracy: 92.52857142857142\n",
      "Iteration: 76\n",
      "Train accuracy: 92.83492063492064\n",
      "Val accuracy: 92.54285714285714\n",
      "Iteration: 77\n",
      "Train accuracy: 92.86031746031746\n",
      "Val accuracy: 92.57142857142857\n",
      "Iteration: 78\n",
      "Train accuracy: 92.8952380952381\n",
      "Val accuracy: 92.58571428571429\n",
      "Iteration: 79\n",
      "Train accuracy: 92.93650793650794\n",
      "Val accuracy: 92.61428571428571\n",
      "Iteration: 80\n",
      "Train accuracy: 92.97301587301587\n",
      "Val accuracy: 92.62857142857143\n",
      "Iteration: 81\n",
      "Train accuracy: 92.99841269841271\n",
      "Val accuracy: 92.67142857142858\n",
      "Iteration: 82\n",
      "Train accuracy: 93.05238095238096\n",
      "Val accuracy: 92.68571428571428\n",
      "Iteration: 83\n",
      "Train accuracy: 93.08412698412698\n",
      "Val accuracy: 92.7\n",
      "Iteration: 84\n",
      "Train accuracy: 93.13492063492063\n",
      "Val accuracy: 92.78571428571428\n",
      "Iteration: 85\n",
      "Train accuracy: 93.17142857142858\n",
      "Val accuracy: 92.80000000000001\n",
      "Iteration: 86\n",
      "Train accuracy: 93.23015873015873\n",
      "Val accuracy: 92.81428571428572\n",
      "Iteration: 87\n",
      "Train accuracy: 93.25079365079365\n",
      "Val accuracy: 92.84285714285714\n",
      "Iteration: 88\n",
      "Train accuracy: 93.28095238095237\n",
      "Val accuracy: 92.87142857142857\n",
      "Iteration: 89\n",
      "Train accuracy: 93.32063492063493\n",
      "Val accuracy: 92.94285714285714\n",
      "Iteration: 90\n",
      "Train accuracy: 93.35873015873017\n",
      "Val accuracy: 92.94285714285714\n",
      "Iteration: 91\n",
      "Train accuracy: 93.39365079365079\n",
      "Val accuracy: 92.98571428571428\n",
      "Iteration: 92\n",
      "Train accuracy: 93.43809523809524\n",
      "Val accuracy: 93.02857142857142\n",
      "Iteration: 93\n",
      "Train accuracy: 93.47619047619048\n",
      "Val accuracy: 93.04285714285714\n",
      "Iteration: 94\n",
      "Train accuracy: 93.4968253968254\n",
      "Val accuracy: 93.04285714285714\n",
      "Iteration: 95\n",
      "Train accuracy: 93.51587301587301\n",
      "Val accuracy: 93.05714285714286\n",
      "Iteration: 96\n",
      "Train accuracy: 93.55873015873016\n",
      "Val accuracy: 93.07142857142857\n",
      "Iteration: 97\n",
      "Train accuracy: 93.5952380952381\n",
      "Val accuracy: 93.10000000000001\n",
      "Iteration: 98\n",
      "Train accuracy: 93.62539682539682\n",
      "Val accuracy: 93.15714285714286\n",
      "Iteration: 99\n",
      "Train accuracy: 93.64920634920635\n",
      "Val accuracy: 93.18571428571428\n",
      "Iteration: 100\n",
      "Train accuracy: 93.67777777777778\n",
      "Val accuracy: 93.2\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3, train_acc_bp, val_acc_bp, train_loss_bp, val_loss_bp, sum_weights_bp = batch_grad_descent(x_train,y_train,epochsToTrain, 0.01,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x219c5d4a1a0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_acc, \"r.\")\n",
    "plt.plot(train_acc_bp, \"gx\")\n",
    "plt.title([\"Training accuracy using node perturbation(baseline)\"])\n",
    "plt.xlabel(\"Epochs\", size=10)\n",
    "plt.ylabel(\"Training accuracy\", size = 10)\n",
    "plt.legend([\"NP\", \"BP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 63000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = one_hot_encoding(y_train)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Initialised\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = params_init()\n",
    "Z1, A1, Z2, A2, Z3, A3 = forward(x_train, W1, b1, W2, b2, W3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.026984126984125"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions(A3), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = Z1- np.max(Z1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-12.25566136,  -9.77496698,  -8.79695744, ...,  -7.80636298,\n",
       "        -15.92526741, -20.02611282],\n",
       "       [ -9.72121696,  -6.22091867,  -4.8731629 , ...,  -2.55752564,\n",
       "         -6.59775347,  -8.95087339],\n",
       "       [ -9.92643277,  -7.37246957,  -4.52480223, ...,  -4.3854183 ,\n",
       "         -7.4639615 , -14.91604123],\n",
       "       ...,\n",
       "       [ -9.19078321, -10.62944845,  -5.44019187, ...,  -7.29793901,\n",
       "         -8.65648547, -15.0731984 ],\n",
       "       [-10.07853589,  -7.45615704,  -4.86311216, ...,  -3.71657699,\n",
       "         -6.66754462,  -9.58466006],\n",
       "       [ -9.76332042,  -9.69017676,  -7.12985019, ...,  -3.66609792,\n",
       "         -7.81213824, -11.37297805]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.79819857e-06, 1.57063805e-05, 2.90428273e-05, ...,\n",
       "        4.92898014e-05, 5.26667133e-08, 1.95759529e-09],\n",
       "       [4.78930925e-05, 5.49009277e-04, 1.46933629e-03, ...,\n",
       "        9.38205907e-03, 5.92139527e-04, 1.26368340e-04],\n",
       "       [3.90075601e-05, 1.73567131e-04, 2.08167210e-03, ...,\n",
       "        1.50818448e-03, 2.49020261e-04, 3.24338723e-07],\n",
       "       ...,\n",
       "       [8.14024228e-05, 6.68312413e-06, 8.33419493e-04, ...,\n",
       "        8.19526110e-05, 7.55662908e-05, 2.77170056e-07],\n",
       "       [3.35035820e-05, 1.59632927e-04, 1.48417867e-03, ...,\n",
       "        2.94393730e-03, 5.52222552e-04, 6.70483701e-05],\n",
       "       [4.59184882e-05, 1.70962177e-05, 1.53834721e-04, ...,\n",
       "        3.09635923e-03, 1.75801852e-04, 1.12132523e-05]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(Z1)/np.sum(np.exp(Z1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Z3.shape[1]\n",
    "pert = 0.1\n",
    "lossBeforePert = np.sum((A3-one_hot_encoding(y_train))**2, axis=0)\n",
    "lossArrayAfterPertZ3 = np.zeros_like(Z3)\n",
    "for i in range(Z3.shape[0]):\n",
    "    Z3pert = Z3.copy() #creates a local copy of the array since python arrays are sent by reference andnot copy!!\n",
    "    Z3pert[i] +=  pert\n",
    "    A3pert = softmax(Z3pert)\n",
    "    lossArrayAfterPertZ3[i] = np.sum((A3pert-one_hot_encoding(y_train))**2, axis=0)\n",
    "\n",
    "\n",
    "dZ3 = (lossArrayAfterPertZ3 - lossBeforePert)/pert\n",
    "\n",
    "dW3 = 1/m*np.matmul(dZ3,A2.T)\n",
    "\n",
    "db3 = 1/m*np.sum(dZ3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossArrayAfterPertZ2 = np.zeros_like(Z2)\n",
    "\n",
    "for i in range(Z2.shape[0]):\n",
    "    Z2pert = Z2.copy()\n",
    "    Z2pert[i] += pert\n",
    "\n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    lossArrayAfterPertZ2[i] = np.sum((A3pert-one_hot_encoding(y_train))**2, axis=0)\n",
    "\n",
    "\n",
    "dZ2 = (lossArrayAfterPertZ2 - lossBeforePert)/pert\n",
    "\n",
    "dW2 = 1/m*np.matmul(dZ2,A1.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "db2 = 1/m*np.sum(dZ2, axis=1) #db1 is 50*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.44652722e-05, -1.91071142e-03, -6.83915406e-04, ...,\n",
       "         1.04829187e-04, -1.18278904e-03, -7.32349391e-05],\n",
       "       [ 6.67987678e-05, -6.42835995e-03, -3.27877653e-04, ...,\n",
       "         7.59078499e-04,  5.46900503e-04, -5.25011768e-03],\n",
       "       [ 1.47084256e-04,  7.05337784e-03,  3.20616679e-03, ...,\n",
       "         8.04822963e-04,  1.13577690e-03,  9.12609766e-03],\n",
       "       ...,\n",
       "       [ 7.65987155e-05,  5.07250451e-03, -2.77136700e-03, ...,\n",
       "        -7.51528691e-04,  1.75562552e-03, -5.69986091e-04],\n",
       "       [ 1.66104750e-04,  4.44860294e-03, -1.58931222e-04, ...,\n",
       "        -2.10381943e-04,  2.30341943e-04,  3.00307195e-03],\n",
       "       [ 2.78581124e-07,  1.40669897e-04,  9.54493959e-05, ...,\n",
       "         3.54828288e-05,  1.59083036e-04,  2.20893989e-04]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub iter: 0\n",
      "sub iter: 1\n",
      "sub iter: 2\n",
      "sub iter: 3\n",
      "sub iter: 4\n",
      "sub iter: 5\n",
      "sub iter: 6\n",
      "sub iter: 7\n",
      "sub iter: 8\n",
      "sub iter: 9\n",
      "sub iter: 10\n",
      "sub iter: 11\n",
      "sub iter: 12\n",
      "sub iter: 13\n",
      "sub iter: 14\n",
      "sub iter: 15\n",
      "sub iter: 16\n",
      "sub iter: 17\n",
      "sub iter: 18\n",
      "sub iter: 19\n",
      "sub iter: 20\n",
      "sub iter: 21\n",
      "sub iter: 22\n",
      "sub iter: 23\n",
      "sub iter: 24\n",
      "sub iter: 25\n",
      "sub iter: 26\n",
      "sub iter: 27\n",
      "sub iter: 28\n",
      "sub iter: 29\n",
      "sub iter: 30\n",
      "sub iter: 31\n",
      "sub iter: 32\n",
      "sub iter: 33\n",
      "sub iter: 34\n",
      "sub iter: 35\n",
      "sub iter: 36\n",
      "sub iter: 37\n",
      "sub iter: 38\n",
      "sub iter: 39\n",
      "sub iter: 40\n",
      "sub iter: 41\n",
      "sub iter: 42\n",
      "sub iter: 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000032?line=5'>6</a>\u001b[0m A1pert \u001b[39m=\u001b[39m relu(Z1pert)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000032?line=7'>8</a>\u001b[0m Z2pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W2,A1pert) \u001b[39m+\u001b[39m b2 \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000032?line=8'>9</a>\u001b[0m A2pert \u001b[39m=\u001b[39m relu(Z2pert)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000032?line=10'>11</a>\u001b[0m Z3pert \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(W3,A2pert) \u001b[39m+\u001b[39m b3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000032?line=11'>12</a>\u001b[0m A3pert \u001b[39m=\u001b[39m softmax(Z3pert)\n",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 5'\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000009?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000009?line=48'>49</a>\u001b[0m    \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mmaximum(x,\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lossArrayAfterPertZ1 = np.zeros_like(Z1)\n",
    "for i in range(Z1.shape[0]):\n",
    "    Z1pert = Z1.copy()\n",
    "    Z1pert[i] += pert\n",
    "\n",
    "    A1pert = relu(Z1pert)\n",
    "\n",
    "    Z2pert = np.matmul(W2,A1pert) + b2 \n",
    "    A2pert = relu(Z2pert)\n",
    "\n",
    "    Z3pert = np.matmul(W3,A2pert) + b3\n",
    "    A3pert = softmax(Z3pert)\n",
    "    print(f\"sub iter: {i}\")\n",
    "\n",
    "    lossArrayAfterPertZ1[i] = np.sum((A3pert-one_hot_encoding(y_train))**2, axis=0)\n",
    "\n",
    "dZ1 = (lossArrayAfterPertZ1 - lossBeforePert)/pert\n",
    "\n",
    "dW1 = 1/m*np.matmul(dZ1,x_train.T) #shape of dZ1 is 50*m, X is 784*m, dW1 = 50*784\n",
    "\n",
    "db1 = 1/m*np.sum(dZ1, axis = 1) #db1 is 50*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99999689, 1.90408986, 1.99817536, ..., 1.45310182, 0.0673587 ,\n",
       "        1.51451512],\n",
       "       [1.99999689, 1.90410139, 1.99817758, ..., 1.45310182, 0.07905707,\n",
       "        1.52943448],\n",
       "       [1.99999689, 1.90410138, 1.99817758, ..., 1.45310174, 0.07905707,\n",
       "        1.52943404],\n",
       "       ...,\n",
       "       [1.99999689, 1.90410139, 1.99817743, ..., 1.44761844, 0.07906075,\n",
       "        1.56006023],\n",
       "       [1.99999689, 1.90410131, 1.99809366, ..., 1.45310182, 0.07905746,\n",
       "        1.52942324],\n",
       "       [1.99999689, 1.8981823 , 1.99817747, ..., 1.44839261, 0.09191522,\n",
       "        1.52362005]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossArrayAfterPertZ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 63000)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63000,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((A3-Y)**2, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [ 4,  5],\n",
       "       [56, 67]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2], [4,5], [56, 67]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  67, 126, 113,  63, 133,  29, 134, 135, 169],\n",
       "       [126, 163, 171,  33,  81,  89, 178, 175, 114, 159],\n",
       "       [124, 110,  47, 114,  85, 175, 122,  30, 175,   8],\n",
       "       [101,  16,  33,  15,  25, 149,  98,  48,  61, 196],\n",
       "       [ 40, 107, 143,  19, 177,   2,  18,  20, 191, 118],\n",
       "       [ 61,  55, 136, 176,  18,  23,  66,  34, 145, 142]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randint(1, 200, (6, 10))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59,  14,  35, 133,  88, 191, 129,  52, 174, 143])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(a):\n",
    "    c = a.copy()\n",
    "    c[1] += 4\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  67, 126, 113,  63, 133,  29, 134, 135, 169],\n",
       "       [130, 167, 175,  37,  85,  93, 182, 179, 118, 163],\n",
       "       [124, 110,  47, 114,  85, 175, 122,  30, 175,   8],\n",
       "       [101,  16,  33,  15,  25, 149,  98,  48,  61, 196],\n",
       "       [ 40, 107, 143,  19, 177,   2,  18,  20, 191, 118],\n",
       "       [ 61,  55, 136, 176,  18,  23,  66,  34, 145, 142]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = myfunc(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  67, 126, 113,  63, 133,  29, 134, 135, 169],\n",
       "       [126, 163, 171,  33,  81,  89, 178, 175, 114, 159],\n",
       "       [124, 110,  47, 114,  85, 175, 122,  30, 175,   8],\n",
       "       [101,  16,  33,  15,  25, 149,  98,  48,  61, 196],\n",
       "       [ 40, 107, 143,  19, 177,   2,  18,  20, 191, 118],\n",
       "       [ 61,  55, 136, 176,  18,  23,  66,  34, 145, 142]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,    8],\n",
       "       [  20,   29],\n",
       "       [3140, 4493]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = np.vectorize(myfunc)\n",
    "ss(a, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array() missing required argument 'object' (pos 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000019?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray()\n",
      "\u001b[1;31mTypeError\u001b[0m: array() missing required argument 'object' (pos 0)"
     ]
    }
   ],
   "source": [
    "a = np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(a, np.array([1,2]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000024?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49mappend(a, [np\u001b[39m.\u001b[39;49marray([\u001b[39m3\u001b[39;49m,\u001b[39m55\u001b[39;49m])], axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\lib\\function_base.py:5440\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5438\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[0;32m   5439\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 5440\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "np.append(a, [np.array([3,55])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\perturbation_on_chip_learning\\Perturbation-techniques-in-CNNs\\code\\mnistNodePert.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/perturbation_on_chip_learning/Perturbation-techniques-in-CNNs/code/mnistNodePert.ipynb#ch0000025?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49mvstack([a, np\u001b[39m.\u001b[39;49marray([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m])])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 2"
     ]
    }
   ],
   "source": [
    "np.vstack([a, np.array([1,2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factors of 12324049 are [12324049, 1597, 7717]\n",
      "The factors of 880325 are [880325, 5, 176065, 5, 35213, 23, 1531]\n",
      "The factors of 35468037 are [35468037, 11822679, 3940893, 1313631, 437877, 145959, 48653, 11, 4423]\n",
      "The factors of 11931245 are [11931245, 5, 2386249, 179, 13331]\n",
      "The factors of 44357249 are [44357249, 31, 1430879]\n",
      "The factors of 28560023 are [28560023, 67, 426269, 439, 971]\n",
      "The factors of 20507042 are [10253521, 19, 539659, 109, 4951]\n",
      "The factors of 28460977 are [28460977, 29, 981413, 613, 1601]\n",
      "The factors of 34336910 are [17168455, 5, 3433691]\n",
      "The factors of 24294129 are [24294129, 8098043]\n",
      "The factors of 4310270 are [2155135, 5, 431027, 29, 14863, 89, 167]\n",
      "The factors of 30462664 are [3807833]\n",
      "The factors of 3195983 are [3195983, 7, 456569, 17, 26857, 107, 251]\n",
      "The factors of 31280688 are [1955043, 651681, 217227, 72409, 19, 3811, 37, 103]\n",
      "The factors of 47429094 are [23714547, 7904849, 29, 272581]\n",
      "The factors of 37800281 are [37800281]\n",
      "The factors of 27288491 are [27288491, 83, 328777]\n",
      "The factors of 47282187 are [47282187, 15760729, 59, 267131]\n",
      "The factors of 44190730 are [22095365, 5, 4419073]\n",
      "The factors of 14441296 are [902581, 17, 53093]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing  \n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import test\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool()\n",
    "    to_factor = [ random.randint(100000, 50000000) for i in range(20)]\n",
    "    results = pool.map(test.prime_factor, to_factor)\n",
    "    for value, factors in zip(to_factor, results):\n",
    "        print(\"The factors of {} are {}\".format(value, factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
